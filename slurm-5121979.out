/home/ws/fq0795/git/gnn_uncertainty/active_learning.py:173: DeprecationWarning: Please use atoms.calc = calc
  self.atoms.set_calculator(self.calc)
wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241120_171846-df1acpxo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-thunder-44
wandb: ‚≠êÔ∏è View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: üöÄ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/df1acpxo
['H1', 'CH3', 'H2', 'H3', 'C', 'O', 'N', 'H', 'CA', 'HA', 'CB', 'HB1', 'HB2', 'HB3', 'C', 'O', 'N', 'H', 'C', 'H1', 'H2', 'H3']
Uncertainty Slope: 0.6569398045539856, Uncertainty Bias: 0.026700392365455627

Training and Validation Results of Epoch Initital validation:
================================
Training Loss Energy: 0.0, Training Loss Force: 0.0, time: 0
Validation Loss Energy: 12.428229103627228, Validation Loss Force: 10.719659023506846, time: 14.65388298034668
Test Loss Energy: 0.0, Test Loss Force: 0.0, time: 0

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.050 MB of 0.050 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size ‚ñÅ
wandb:  test_error_energy ‚ñÅ
wandb:   test_error_force ‚ñÅ
wandb:   test_error_total ‚ñÅ
wandb: train_error_energy ‚ñÅ
wandb:  train_error_force ‚ñÅ
wandb:  train_error_total ‚ñÅ
wandb: valid_error_energy ‚ñÅ
wandb:  valid_error_force ‚ñÅ
wandb:  valid_error_total ‚ñÅ
wandb: 
wandb: Run summary:
wandb:       dataset_size 800
wandb:  test_error_energy 0.0
wandb:   test_error_force 0.0
wandb:   test_error_total 0.0
wandb: train_error_energy 0.0
wandb:  train_error_force 0.0
wandb:  train_error_total 0.0
wandb: valid_error_energy 12.42823
wandb:  valid_error_force 10.71966
wandb:  valid_error_total 6.0429
wandb: 
wandb: üöÄ View run robust-thunder-44 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/df1acpxo
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241120_171846-df1acpxo/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Found uncertainty sample after 1135 steps.
Found uncertainty sample after 2031 steps.
Found uncertainty sample after 171 steps.
Found uncertainty sample after 263 steps.
Found uncertainty sample after 635 steps.
Found uncertainty sample after 300 steps.
Found uncertainty sample after 263 steps.
Found uncertainty sample after 446 steps.
Found uncertainty sample after 1239 steps.
Found uncertainty sample after 1581 steps.
Found uncertainty sample after 477 steps.
Found uncertainty sample after 618 steps.
Found uncertainty sample after 721 steps.
Found uncertainty sample after 40 steps.
Found uncertainty sample after 1351 steps.
Found uncertainty sample after 541 steps.
Found uncertainty sample after 404 steps.
Found uncertainty sample after 660 steps.
Found uncertainty sample after 383 steps.
Found uncertainty sample after 2893 steps.
Found uncertainty sample after 1788 steps.
Found uncertainty sample after 529 steps.
Found uncertainty sample after 849 steps.
Found uncertainty sample after 562 steps.
Found uncertainty sample after 372 steps.
Found uncertainty sample after 238 steps.
Found uncertainty sample after 376 steps.
Found uncertainty sample after 575 steps.
Found uncertainty sample after 1915 steps.
Found uncertainty sample after 1107 steps.
Found uncertainty sample after 287 steps.
Found uncertainty sample after 388 steps.
Found uncertainty sample after 2221 steps.
Found uncertainty sample after 604 steps.
Found uncertainty sample after 47 steps.
Found uncertainty sample after 991 steps.
Found uncertainty sample after 3122 steps.
Found uncertainty sample after 2359 steps.
Found uncertainty sample after 800 steps.
Found uncertainty sample after 369 steps.
Found uncertainty sample after 288 steps.
Found uncertainty sample after 382 steps.
Found uncertainty sample after 367 steps.
Found uncertainty sample after 1141 steps.
Found uncertainty sample after 302 steps.
Found uncertainty sample after 335 steps.
Found uncertainty sample after 199 steps.
Found uncertainty sample after 36 steps.
Found uncertainty sample after 69 steps.
Found uncertainty sample after 725 steps.
Found uncertainty sample after 387 steps.
Found uncertainty sample after 95 steps.
Found uncertainty sample after 1349 steps.
Found uncertainty sample after 177 steps.
Found uncertainty sample after 3363 steps.
Found uncertainty sample after 480 steps.
Found uncertainty sample after 692 steps.
Found uncertainty sample after 267 steps.
Found uncertainty sample after 845 steps.
Found uncertainty sample after 40 steps.
Found uncertainty sample after 376 steps.
Found uncertainty sample after 390 steps.
Found uncertainty sample after 1477 steps.
Found uncertainty sample after 1279 steps.
Found uncertainty sample after 342 steps.
Found uncertainty sample after 71 steps.
Found uncertainty sample after 114 steps.
Found uncertainty sample after 64 steps.
Found uncertainty sample after 386 steps.
Found uncertainty sample after 338 steps.
Found uncertainty sample after 9 steps.
Found uncertainty sample after 104 steps.
Found uncertainty sample after 170 steps.
Found uncertainty sample after 3262 steps.
Found uncertainty sample after 234 steps.
Found uncertainty sample after 353 steps.
Found uncertainty sample after 695 steps.
Found uncertainty sample after 408 steps.
Found uncertainty sample after 124 steps.
Found uncertainty sample after 2137 steps.
Found uncertainty sample after 553 steps.
Found uncertainty sample after 879 steps.
Found uncertainty sample after 256 steps.
Found uncertainty sample after 1034 steps.
Found uncertainty sample after 1644 steps.
Found uncertainty sample after 103 steps.
Found uncertainty sample after 437 steps.
Found uncertainty sample after 534 steps.
Found uncertainty sample after 1206 steps.
Found uncertainty sample after 649 steps.
Found uncertainty sample after 395 steps.
Found uncertainty sample after 520 steps.
Found uncertainty sample after 2315 steps.
Found uncertainty sample after 389 steps.
Found uncertainty sample after 446 steps.
Found uncertainty sample after 584 steps.
Found uncertainty sample after 2618 steps.
Found uncertainty sample after 1142 steps.
Found uncertainty sample after 374 steps.
Found uncertainty sample after 649 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241120_175233-ps5vuv9x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_38_0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: üöÄ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/ps5vuv9x
Training model 0. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.7260013315579, Training Loss Force: 3.6491176059720893, time: 1.0216529369354248
Validation Loss Energy: 1.761900902168624, Validation Loss Force: 2.9860471328249494, time: 0.06317925453186035
Test Loss Energy: 12.630397243517901, Test Loss Force: 9.922011156106299, time: 14.46855092048645


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.966175575605137, Training Loss Force: 3.1930226209675032, time: 0.9606621265411377
Validation Loss Energy: 1.284236060380117, Validation Loss Force: 2.810107556530713, time: 0.06472015380859375
Test Loss Energy: 11.90133381866689, Test Loss Force: 9.774680747045705, time: 14.611022710800171


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.485595420130514, Training Loss Force: 3.0928850773411605, time: 0.963634729385376
Validation Loss Energy: 1.1831257865897893, Validation Loss Force: 2.801301214478415, time: 0.06352019309997559
Test Loss Energy: 11.744151239539475, Test Loss Force: 9.754582126305621, time: 14.576231718063354


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.699021732836463, Training Loss Force: 3.0486728619270176, time: 0.9559767246246338
Validation Loss Energy: 1.4053374587572685, Validation Loss Force: 2.8226471694836364, time: 0.06132340431213379
Test Loss Energy: 11.388725494401156, Test Loss Force: 9.716859161058856, time: 14.72474718093872


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6450885300432465, Training Loss Force: 3.0426649143661666, time: 0.9520223140716553
Validation Loss Energy: 1.3746651179094347, Validation Loss Force: 2.805309481774261, time: 0.06354355812072754
Test Loss Energy: 11.337617819175863, Test Loss Force: 9.737533783744595, time: 14.684459686279297


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.450103202831101, Training Loss Force: 3.047801859752844, time: 0.9288194179534912
Validation Loss Energy: 1.1898413766963984, Validation Loss Force: 2.7755243661383764, time: 0.06487584114074707
Test Loss Energy: 11.525891329748998, Test Loss Force: 9.648246827741533, time: 14.816775560379028


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5470385156243296, Training Loss Force: 3.007524963631847, time: 0.971771240234375
Validation Loss Energy: 1.271726285092748, Validation Loss Force: 2.793113892651796, time: 0.06553077697753906
Test Loss Energy: 11.514023283442768, Test Loss Force: 9.606295214532228, time: 14.67653512954712


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.445066530132318, Training Loss Force: 3.037759751655463, time: 0.9747283458709717
Validation Loss Energy: 1.208871795253117, Validation Loss Force: 2.772168074101807, time: 0.0640566349029541
Test Loss Energy: 11.102638734775626, Test Loss Force: 9.585476218301817, time: 14.77708625793457


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.2521763486052584, Training Loss Force: 3.0348360739595526, time: 0.9482505321502686
Validation Loss Energy: 1.1977335505700901, Validation Loss Force: 2.7917961366461954, time: 0.0644993782043457
Test Loss Energy: 11.292937452707758, Test Loss Force: 9.555640730475057, time: 14.683463335037231


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.267425678062263, Training Loss Force: 3.0167172946961407, time: 0.9358005523681641
Validation Loss Energy: 1.4824048088468975, Validation Loss Force: 2.794092300879003, time: 0.06248641014099121
Test Loss Energy: 10.958473179410648, Test Loss Force: 9.553416463894436, time: 14.932047605514526


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.517977132368114, Training Loss Force: 3.0058239463112875, time: 0.9640862941741943
Validation Loss Energy: 1.3238786277716519, Validation Loss Force: 2.7629802998025346, time: 0.06279325485229492
Test Loss Energy: 11.377582664016112, Test Loss Force: 9.489092444544307, time: 14.794063329696655

slurmstepd: error: *** JOB 5121979 ON aimat01 CANCELLED AT 2024-11-20T17:55:42 ***
