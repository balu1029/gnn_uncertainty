wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_215238-f9dxm8no
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/f9dxm8no
['H1', 'CH3', 'H2', 'H3', 'C', 'O', 'N', 'H', 'CA', 'HA', 'CB', 'HB1', 'HB2', 'HB3', 'C', 'O', 'N', 'H', 'C', 'H1', 'H2', 'H3']
62
Uncertainty Slope: 0.04418487474322319, Uncertainty Bias: 0.2551637291908264
0.0014572144 0.0016651154
3.8687825 7.084954
(48745, 22, 3)

Training and Validation Results of Epoch Initital validation:
================================
Training Loss Energy: 0.0, Training Loss Force: 0.0, time: 0
Validation Loss Energy: 0.0, Validation Loss Force: 0.0, time: 0
Test Loss Energy: 11.451026227642295, Test Loss Force: 12.623542568550294, time: 7.51609468460083

wandb: - 0.039 MB of 0.050 MB uploadedwandb: \ 0.039 MB of 0.050 MB uploadedwandb: | 0.050 MB of 0.050 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–
wandb:    max_uncertainty â–
wandb:  test_error_energy â–
wandb:   test_error_force â–
wandb:          test_loss â–
wandb: train_error_energy â–
wandb:  train_error_force â–
wandb:         train_loss â–
wandb: valid_error_energy â–
wandb:  valid_error_force â–
wandb:         valid_loss â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 800
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.45103
wandb:   test_error_force 12.62354
wandb:          test_loss 10.22045
wandb: train_error_energy 0.0
wandb:  train_error_force 0.0
wandb:         train_loss 0.0
wandb: valid_error_energy 0.0
wandb:  valid_error_force 0.0
wandb:         valid_loss 0.0
wandb: 
wandb: ğŸš€ View run al_63 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/f9dxm8no
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_215238-f9dxm8no/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Found uncertainty sample 0 after 713 steps.
Found uncertainty sample 1 after 3125 steps.
Found uncertainty sample 2 after 2424 steps.
Found uncertainty sample 3 after 41 steps.
Found uncertainty sample 4 after 464 steps.
Found uncertainty sample 5 after 411 steps.
Found uncertainty sample 6 after 1008 steps.
Found uncertainty sample 7 after 1541 steps.
Found uncertainty sample 8 after 16 steps.
Found uncertainty sample 9 after 673 steps.
Found uncertainty sample 10 after 768 steps.
Found uncertainty sample 11 after 539 steps.
Found uncertainty sample 12 after 577 steps.
Found uncertainty sample 13 after 505 steps.
Found uncertainty sample 14 after 30 steps.
Found uncertainty sample 15 after 245 steps.
Found uncertainty sample 16 after 24 steps.
Found uncertainty sample 17 after 219 steps.
Found uncertainty sample 18 after 821 steps.
Found uncertainty sample 19 after 52 steps.
Found uncertainty sample 20 after 643 steps.
Found uncertainty sample 21 after 879 steps.
Found uncertainty sample 22 after 1185 steps.
Found uncertainty sample 23 after 228 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 1499 steps.
Found uncertainty sample 26 after 716 steps.
Found uncertainty sample 27 after 2042 steps.
Found uncertainty sample 28 after 100 steps.
Found uncertainty sample 29 after 5 steps.
Found uncertainty sample 30 after 112 steps.
Found uncertainty sample 31 after 3887 steps.
Found uncertainty sample 32 after 192 steps.
Found uncertainty sample 33 after 150 steps.
Found uncertainty sample 34 after 645 steps.
Found uncertainty sample 35 after 920 steps.
Found uncertainty sample 36 after 1175 steps.
Found uncertainty sample 37 after 47 steps.
Found uncertainty sample 38 after 284 steps.
Found uncertainty sample 39 after 653 steps.
Found uncertainty sample 40 after 55 steps.
Found uncertainty sample 41 after 2226 steps.
Found uncertainty sample 42 after 247 steps.
Found uncertainty sample 43 after 1097 steps.
Found uncertainty sample 44 after 2058 steps.
Found uncertainty sample 45 after 2083 steps.
Found uncertainty sample 46 after 80 steps.
Found uncertainty sample 47 after 3517 steps.
Found uncertainty sample 48 after 14 steps.
Found uncertainty sample 49 after 830 steps.
Found uncertainty sample 50 after 65 steps.
Found uncertainty sample 51 after 477 steps.
Found uncertainty sample 52 after 230 steps.
Found uncertainty sample 53 after 962 steps.
Found uncertainty sample 54 after 762 steps.
Found uncertainty sample 55 after 363 steps.
Found uncertainty sample 56 after 473 steps.
Found uncertainty sample 57 after 884 steps.
Found uncertainty sample 58 after 85 steps.
Found uncertainty sample 59 after 3007 steps.
Found uncertainty sample 60 after 1475 steps.
Found uncertainty sample 61 after 23 steps.
Found uncertainty sample 62 after 108 steps.
Found uncertainty sample 63 after 6 steps.
Found uncertainty sample 64 after 86 steps.
Found uncertainty sample 65 after 997 steps.
Found uncertainty sample 66 after 398 steps.
Found uncertainty sample 67 after 19 steps.
Found uncertainty sample 68 after 727 steps.
Found uncertainty sample 69 after 69 steps.
Found uncertainty sample 70 after 104 steps.
Found uncertainty sample 71 after 1343 steps.
Found uncertainty sample 72 after 1921 steps.
Found uncertainty sample 73 after 48 steps.
Found uncertainty sample 74 after 1654 steps.
Found uncertainty sample 75 after 124 steps.
Found uncertainty sample 76 after 231 steps.
Found uncertainty sample 77 after 136 steps.
Found uncertainty sample 78 after 3876 steps.
Found uncertainty sample 79 after 308 steps.
Found uncertainty sample 80 after 980 steps.
Found uncertainty sample 81 after 231 steps.
Found uncertainty sample 82 after 695 steps.
Found uncertainty sample 83 after 507 steps.
Found uncertainty sample 84 after 207 steps.
Found uncertainty sample 85 after 12 steps.
Found uncertainty sample 86 after 216 steps.
Found uncertainty sample 87 after 815 steps.
Found uncertainty sample 88 after 586 steps.
Found uncertainty sample 89 after 1596 steps.
Found uncertainty sample 90 after 2094 steps.
Found uncertainty sample 91 after 1106 steps.
Found uncertainty sample 92 after 785 steps.
Found uncertainty sample 93 after 10 steps.
Found uncertainty sample 94 after 1843 steps.
Found uncertainty sample 95 after 118 steps.
Found uncertainty sample 96 after 23 steps.
Found uncertainty sample 97 after 175 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 302 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_220231-sd86t0tm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_0
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sd86t0tm
Training model 0. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.410249352246836, Training Loss Force: 3.8965216676863625, time: 0.5178573131561279
Validation Loss Energy: 2.5853076895455422, Validation Loss Force: 4.211110426358778, time: 0.04874777793884277
Test Loss Energy: 10.239954861682133, Test Loss Force: 11.887851889707122, time: 9.630694389343262


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6684848623065185, Training Loss Force: 3.792231805731698, time: 0.5046663284301758
Validation Loss Energy: 3.290991192158179, Validation Loss Force: 4.047840947136979, time: 0.04151129722595215
Test Loss Energy: 10.060061624494118, Test Loss Force: 12.454536490935515, time: 9.195569515228271


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.676539160196175, Training Loss Force: 3.874377594889911, time: 0.47074055671691895
Validation Loss Energy: 1.769594789167867, Validation Loss Force: 4.78168452595852, time: 0.047829627990722656
Test Loss Energy: 10.093235330850803, Test Loss Force: 12.602646787604556, time: 9.844686269760132


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.1890401256371805, Training Loss Force: 3.9679867654465513, time: 0.5013115406036377
Validation Loss Energy: 11.574379613742927, Validation Loss Force: 4.153836804658108, time: 0.042196035385131836
Test Loss Energy: 12.16663549772062, Test Loss Force: 12.202405223208984, time: 9.594743490219116


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.046826765717299, Training Loss Force: 3.956010545195173, time: 0.4877619743347168
Validation Loss Energy: 1.777670553752376, Validation Loss Force: 5.317942192802746, time: 0.044774532318115234
Test Loss Energy: 9.900024991166516, Test Loss Force: 12.766640771605239, time: 9.800272941589355


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.4217399330740683, Training Loss Force: 4.186197144633935, time: 0.47716665267944336
Validation Loss Energy: 6.609106158320114, Validation Loss Force: 4.370082391064171, time: 0.04792070388793945
Test Loss Energy: 13.673259748027675, Test Loss Force: 12.031524387966208, time: 9.941585063934326


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.9405363705952765, Training Loss Force: 3.71035100080961, time: 0.5045015811920166
Validation Loss Energy: 1.3093769656393859, Validation Loss Force: 4.07630518939922, time: 0.0446619987487793
Test Loss Energy: 10.251177358657523, Test Loss Force: 12.079111960109367, time: 9.634251117706299


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.1470137647548904, Training Loss Force: 3.9062827593691125, time: 0.49947071075439453
Validation Loss Energy: 2.236714688212672, Validation Loss Force: 5.095694782237345, time: 0.04625678062438965
Test Loss Energy: 9.875436106036142, Test Loss Force: 12.746433353088323, time: 9.866692304611206


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.626490137344445, Training Loss Force: 4.187930949986232, time: 0.4716310501098633
Validation Loss Energy: 2.2437361143912127, Validation Loss Force: 4.028135618398953, time: 0.03939533233642578
Test Loss Energy: 10.214503694364478, Test Loss Force: 12.422268220279756, time: 9.85414719581604


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5131654588888215, Training Loss Force: 3.400563680788243, time: 0.48604631423950195
Validation Loss Energy: 3.6978248672924843, Validation Loss Force: 3.9120506218758364, time: 0.04389476776123047
Test Loss Energy: 12.58220813825476, Test Loss Force: 12.439141202144807, time: 9.721047639846802


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.569667146304081, Training Loss Force: 3.3435052211022955, time: 0.48935508728027344
Validation Loss Energy: 1.9343257573289634, Validation Loss Force: 3.897182112325654, time: 0.04256582260131836
Test Loss Energy: 10.225117406900061, Test Loss Force: 12.461376971036527, time: 10.163303136825562


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.542101226924973, Training Loss Force: 3.338674716175233, time: 0.46566343307495117
Validation Loss Energy: 2.4893081179925494, Validation Loss Force: 3.9085502629018514, time: 0.04186892509460449
Test Loss Energy: 10.176313282461024, Test Loss Force: 12.392186178252295, time: 8.610661029815674


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5866485594037374, Training Loss Force: 3.338216311975993, time: 0.5129415988922119
Validation Loss Energy: 3.515522965491653, Validation Loss Force: 3.912856684184095, time: 0.04574394226074219
Test Loss Energy: 12.201639814924063, Test Loss Force: 12.3941592227408, time: 9.961356401443481


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6019099527569702, Training Loss Force: 3.34462004599126, time: 0.5336599349975586
Validation Loss Energy: 1.9788059232503523, Validation Loss Force: 3.8865705915073194, time: 0.042661190032958984
Test Loss Energy: 10.078181413679522, Test Loss Force: 12.310450900607492, time: 8.781083583831787


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.4958813072041024, Training Loss Force: 3.3324379283521246, time: 0.46990132331848145
Validation Loss Energy: 2.4434520794184724, Validation Loss Force: 3.900453567417625, time: 0.035594940185546875
Test Loss Energy: 9.940124122988863, Test Loss Force: 12.332988991245799, time: 8.36567211151123


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5220996647219343, Training Loss Force: 3.3272773402736324, time: 0.4998762607574463
Validation Loss Energy: 3.427710715765279, Validation Loss Force: 3.9163873261619258, time: 0.04042243957519531
Test Loss Energy: 12.204116031060396, Test Loss Force: 12.340427217925496, time: 8.19951844215393


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5549462266353014, Training Loss Force: 3.337847070864759, time: 0.4913301467895508
Validation Loss Energy: 2.2883830595029018, Validation Loss Force: 3.904250076355694, time: 0.040526390075683594
Test Loss Energy: 10.049815192473769, Test Loss Force: 12.318370782800168, time: 8.098424673080444


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.4919257528523207, Training Loss Force: 3.324792042458768, time: 0.4948892593383789
Validation Loss Energy: 2.541626683188888, Validation Loss Force: 3.8845327234837885, time: 0.04089641571044922
Test Loss Energy: 10.125189453813292, Test Loss Force: 12.234508835277465, time: 7.919396162033081


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.511289033948558, Training Loss Force: 3.319852874988493, time: 0.48061227798461914
Validation Loss Energy: 3.0762040886579434, Validation Loss Force: 3.9084162160760028, time: 0.044814348220825195
Test Loss Energy: 12.123897734187413, Test Loss Force: 12.323079691970458, time: 8.23134970664978


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5415293489384747, Training Loss Force: 3.3271267364062913, time: 0.44170212745666504
Validation Loss Energy: 2.1984699263846985, Validation Loss Force: 3.8863051698853734, time: 0.03856945037841797
Test Loss Energy: 10.129203024365395, Test Loss Force: 12.346896686928726, time: 7.954913377761841

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–â–…â–â–ˆâ–‚â–â–‚â–†â–‚â–‚â–…â–â–â–…â–â–â–…â–
wandb:   test_error_force â–â–†â–‡â–„â–ˆâ–‚â–ƒâ–ˆâ–…â–…â–†â–…â–…â–„â–…â–…â–„â–„â–„â–…
wandb:          test_loss â–ƒâ–â–ƒâ–‡â–ƒâ–ˆâ–ƒâ–‚â–ƒâ–„â–â–‚â–ƒâ–‚â–‚â–ƒâ–â–‚â–ƒâ–
wandb: train_error_energy â–ˆâ–ƒâ–†â–„â–„â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–†â–…â–…â–†â–†â–ˆâ–„â–†â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–„â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–‚â–â–ˆâ–â–…â–â–‚â–‚â–ƒâ–â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  valid_error_force â–ƒâ–‚â–…â–‚â–ˆâ–ƒâ–‚â–‡â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:         valid_loss â–â–‚â–â–ˆâ–‚â–…â–â–‚â–â–‚â–â–â–‚â–â–â–‚â–â–â–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 890
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.1292
wandb:   test_error_force 12.3469
wandb:          test_loss 9.80934
wandb: train_error_energy 2.54153
wandb:  train_error_force 3.32713
wandb:         train_loss 0.89572
wandb: valid_error_energy 2.19847
wandb:  valid_error_force 3.88631
wandb:         valid_loss 0.96518
wandb: 
wandb: ğŸš€ View run al_63_0 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sd86t0tm
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_220231-sd86t0tm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.08375129103660583, Uncertainty Bias: 0.2493915557861328
0.0001449585 0.041006565
3.8315806 7.3015895
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 2015 steps.
Found uncertainty sample 2 after 3382 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 539 steps.
Found uncertainty sample 5 after 3021 steps.
Found uncertainty sample 6 after 752 steps.
Found uncertainty sample 7 after 695 steps.
Found uncertainty sample 8 after 3120 steps.
Found uncertainty sample 9 after 358 steps.
Found uncertainty sample 10 after 1269 steps.
Found uncertainty sample 11 after 2090 steps.
Found uncertainty sample 12 after 362 steps.
Found uncertainty sample 13 after 366 steps.
Found uncertainty sample 14 after 1733 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 2348 steps.
Found uncertainty sample 17 after 196 steps.
Found uncertainty sample 18 after 520 steps.
Found uncertainty sample 19 after 1019 steps.
Found uncertainty sample 20 after 371 steps.
Found uncertainty sample 21 after 700 steps.
Found uncertainty sample 22 after 616 steps.
Found uncertainty sample 23 after 3251 steps.
Found uncertainty sample 24 after 602 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 3050 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 452 steps.
Found uncertainty sample 29 after 1677 steps.
Found uncertainty sample 30 after 2686 steps.
Found uncertainty sample 31 after 1067 steps.
Found uncertainty sample 32 after 1312 steps.
Found uncertainty sample 33 after 228 steps.
Found uncertainty sample 34 after 649 steps.
Found uncertainty sample 35 after 2062 steps.
Found uncertainty sample 36 after 2422 steps.
Found uncertainty sample 37 after 112 steps.
Found uncertainty sample 38 after 3362 steps.
Found uncertainty sample 39 after 170 steps.
Found uncertainty sample 40 after 1327 steps.
Found uncertainty sample 41 after 171 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 185 steps.
Found uncertainty sample 45 after 2764 steps.
Found uncertainty sample 46 after 2030 steps.
Found uncertainty sample 47 after 696 steps.
Found uncertainty sample 48 after 2914 steps.
Found uncertainty sample 49 after 1616 steps.
Found uncertainty sample 50 after 1110 steps.
Found uncertainty sample 51 after 900 steps.
Found uncertainty sample 52 after 1543 steps.
Found uncertainty sample 53 after 1726 steps.
Found uncertainty sample 54 after 943 steps.
Found uncertainty sample 55 after 919 steps.
Found uncertainty sample 56 after 1521 steps.
Found uncertainty sample 57 after 49 steps.
Found uncertainty sample 58 after 860 steps.
Found uncertainty sample 59 after 1009 steps.
Found uncertainty sample 60 after 14 steps.
Found uncertainty sample 61 after 792 steps.
Found uncertainty sample 62 after 266 steps.
Found uncertainty sample 63 after 2244 steps.
Found uncertainty sample 64 after 456 steps.
Found uncertainty sample 65 after 145 steps.
Found uncertainty sample 66 after 505 steps.
Found uncertainty sample 67 after 419 steps.
Found uncertainty sample 68 after 100 steps.
Found uncertainty sample 69 after 625 steps.
Found uncertainty sample 70 after 1427 steps.
Found uncertainty sample 71 after 235 steps.
Found uncertainty sample 72 after 21 steps.
Found uncertainty sample 73 after 292 steps.
Found uncertainty sample 74 after 689 steps.
Found uncertainty sample 75 after 315 steps.
Found uncertainty sample 76 after 212 steps.
Found uncertainty sample 77 after 934 steps.
Found uncertainty sample 78 after 61 steps.
Found uncertainty sample 79 after 328 steps.
Found uncertainty sample 80 after 152 steps.
Found uncertainty sample 81 after 3303 steps.
Found uncertainty sample 82 after 1748 steps.
Found uncertainty sample 83 after 420 steps.
Found uncertainty sample 84 after 434 steps.
Found uncertainty sample 85 after 1699 steps.
Found uncertainty sample 86 after 1442 steps.
Found uncertainty sample 87 after 143 steps.
Found uncertainty sample 88 after 1291 steps.
Found uncertainty sample 89 after 106 steps.
Found uncertainty sample 90 after 1848 steps.
Found uncertainty sample 91 after 24 steps.
Found uncertainty sample 92 after 472 steps.
Found uncertainty sample 93 after 2125 steps.
Found uncertainty sample 94 after 272 steps.
Found uncertainty sample 95 after 1371 steps.
Found uncertainty sample 96 after 947 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 1971 steps.
Found uncertainty sample 99 after 384 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_222051-xdvym1qi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_1
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/xdvym1qi
Training model 1. Added 92 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 18.7719946502784, Training Loss Force: 6.09689469775077, time: 0.5199580192565918
Validation Loss Energy: 11.172478728198362, Validation Loss Force: 6.76592847509072, time: 0.04581594467163086
Test Loss Energy: 11.619419804224398, Test Loss Force: 14.251496415220354, time: 9.423891544342041


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.482739525377492, Training Loss Force: 4.605481359675634, time: 0.5287163257598877
Validation Loss Energy: 3.253712146416095, Validation Loss Force: 4.2857540227949835, time: 0.045140981674194336
Test Loss Energy: 10.080618980980642, Test Loss Force: 12.196335346229102, time: 9.373726844787598


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.04248122896574, Training Loss Force: 3.6848943335138196, time: 0.5273687839508057
Validation Loss Energy: 2.330821126686987, Validation Loss Force: 4.064986026035127, time: 0.043935298919677734
Test Loss Energy: 11.087774923427839, Test Loss Force: 12.20472119683513, time: 9.506234169006348


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.923998426927576, Training Loss Force: 3.575466562641013, time: 0.49289894104003906
Validation Loss Energy: 4.926376272997272, Validation Loss Force: 4.121500345948183, time: 0.04483652114868164
Test Loss Energy: 12.860787156458011, Test Loss Force: 12.202721968320361, time: 9.540695190429688


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.004006410404209, Training Loss Force: 3.5444620102017694, time: 0.501079797744751
Validation Loss Energy: 4.949410060128626, Validation Loss Force: 4.041205406588331, time: 0.04674243927001953
Test Loss Energy: 12.776701373982338, Test Loss Force: 12.148444463734975, time: 9.436741590499878


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.063238899270297, Training Loss Force: 3.526461343067582, time: 0.4761040210723877
Validation Loss Energy: 2.859571280607112, Validation Loss Force: 4.015980109722755, time: 0.04844045639038086
Test Loss Energy: 11.340651640822657, Test Loss Force: 12.074351289106206, time: 9.773981809616089


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 3.9505216802019536, Training Loss Force: 3.523419553242447, time: 0.52484130859375
Validation Loss Energy: 2.460622097260964, Validation Loss Force: 4.000968201242596, time: 0.04810667037963867
Test Loss Energy: 9.879236898149301, Test Loss Force: 11.99532585999313, time: 10.033448219299316


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 3.890061081210127, Training Loss Force: 3.5255629448925214, time: 0.5452816486358643
Validation Loss Energy: 4.712574257798621, Validation Loss Force: 4.033970886037137, time: 0.047896623611450195
Test Loss Energy: 10.026267228476064, Test Loss Force: 12.081301765433686, time: 9.439745664596558


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.00627780449025, Training Loss Force: 3.5367999433417663, time: 0.5105998516082764
Validation Loss Energy: 5.777171476657881, Validation Loss Force: 4.051934051377423, time: 0.04803586006164551
Test Loss Energy: 10.187321219644305, Test Loss Force: 12.128105019901627, time: 9.641614198684692


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 3.9859651384738295, Training Loss Force: 3.5085759826366085, time: 0.5386013984680176
Validation Loss Energy: 4.1484553367393255, Validation Loss Force: 3.9970915314119932, time: 0.045165300369262695
Test Loss Energy: 9.989867435574439, Test Loss Force: 11.995038180783048, time: 9.454725742340088


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.105487751551911, Training Loss Force: 3.509936448020974, time: 0.5288326740264893
Validation Loss Energy: 1.9573653338581116, Validation Loss Force: 3.9893626853568396, time: 0.04824686050415039
Test Loss Energy: 11.221787777659028, Test Loss Force: 12.029051739199431, time: 9.536285638809204


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.0341854852222445, Training Loss Force: 3.494587612391522, time: 0.535923957824707
Validation Loss Energy: 5.052311707197942, Validation Loss Force: 3.998852844356282, time: 0.04506087303161621
Test Loss Energy: 13.0455942796745, Test Loss Force: 12.002348271278626, time: 9.713067293167114


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.02650294357513, Training Loss Force: 3.4900391856620336, time: 0.5390183925628662
Validation Loss Energy: 5.538542297026573, Validation Loss Force: 4.009363718720658, time: 0.04652762413024902
Test Loss Energy: 13.56672686848531, Test Loss Force: 11.965433849177517, time: 9.584044694900513


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.062831439873313, Training Loss Force: 3.5031544486373103, time: 0.5058321952819824
Validation Loss Energy: 3.2127924601380915, Validation Loss Force: 4.012805738378409, time: 0.04661107063293457
Test Loss Energy: 11.973239664145511, Test Loss Force: 11.868922074469502, time: 9.48049020767212


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.075297676159777, Training Loss Force: 3.5026140306201436, time: 0.5202186107635498
Validation Loss Energy: 2.6921957823344043, Validation Loss Force: 3.967649326260837, time: 0.040470123291015625
Test Loss Energy: 10.114138666226202, Test Loss Force: 11.924985991824892, time: 9.617531061172485


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.072761180089758, Training Loss Force: 3.4883744895784514, time: 0.5281202793121338
Validation Loss Energy: 5.159848435208892, Validation Loss Force: 4.006798124821278, time: 0.04480886459350586
Test Loss Energy: 10.013620572285916, Test Loss Force: 11.908119552468985, time: 9.511054277420044


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.073250046608205, Training Loss Force: 3.491774805460033, time: 0.5169999599456787
Validation Loss Energy: 5.733465531356883, Validation Loss Force: 4.0234987561041535, time: 0.044461727142333984
Test Loss Energy: 10.142034377034344, Test Loss Force: 11.967692688931914, time: 9.901063919067383


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.0324822709678525, Training Loss Force: 3.502855375242827, time: 0.5064420700073242
Validation Loss Energy: 3.7007908203162425, Validation Loss Force: 4.041105650810955, time: 0.04394674301147461
Test Loss Energy: 9.98230881366941, Test Loss Force: 11.981838950705656, time: 8.820513248443604


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.100634796033862, Training Loss Force: 3.5386802262558623, time: 0.5196871757507324
Validation Loss Energy: 2.0937127773223656, Validation Loss Force: 3.9903036851675306, time: 0.04675745964050293
Test Loss Energy: 11.16600793483833, Test Loss Force: 11.877024778523245, time: 9.982413530349731


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 3.9910408996124436, Training Loss Force: 3.483280437246931, time: 0.5739049911499023
Validation Loss Energy: 5.083581135360582, Validation Loss Force: 4.003172625787281, time: 0.04634809494018555
Test Loss Energy: 13.006090222401093, Test Loss Force: 11.928471680283, time: 8.406086444854736

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–â–ƒâ–‡â–‡â–„â–â–â–‚â–â–„â–‡â–ˆâ–…â–â–â–â–â–ƒâ–‡
wandb:   test_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:          test_loss â–ˆâ–†â–†â–ˆâ–†â–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–†â–…â–…â–‚â–‚â–‚â–â–ƒâ–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‚â–â–ƒâ–ƒâ–‚â–â–ƒâ–„â–ƒâ–â–ƒâ–„â–‚â–‚â–ƒâ–„â–‚â–â–ƒ
wandb:  valid_error_force â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         valid_loss â–ˆâ–â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 972
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 13.00609
wandb:   test_error_force 11.92847
wandb:          test_loss 8.89811
wandb: train_error_energy 3.99104
wandb:  train_error_force 3.48328
wandb:         train_loss 1.41165
wandb: valid_error_energy 5.08358
wandb:  valid_error_force 4.00317
wandb:         valid_loss 1.85511
wandb: 
wandb: ğŸš€ View run al_63_1 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/xdvym1qi
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_222051-xdvym1qi/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.08619660139083862, Uncertainty Bias: 0.2514893412590027
1.5258789e-05 1.0213852
3.9110887 8.783742
(48745, 22, 3)
Found uncertainty sample 0 after 6 steps.
Found uncertainty sample 1 after 10 steps.
Found uncertainty sample 2 after 3802 steps.
Found uncertainty sample 3 after 1767 steps.
Found uncertainty sample 4 after 2474 steps.
Found uncertainty sample 5 after 1064 steps.
Found uncertainty sample 6 after 73 steps.
Found uncertainty sample 7 after 308 steps.
Found uncertainty sample 8 after 17 steps.
Found uncertainty sample 9 after 240 steps.
Found uncertainty sample 10 after 494 steps.
Found uncertainty sample 11 after 2106 steps.
Found uncertainty sample 12 after 2925 steps.
Found uncertainty sample 13 after 27 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 163 steps.
Found uncertainty sample 16 after 47 steps.
Found uncertainty sample 17 after 2000 steps.
Found uncertainty sample 18 after 141 steps.
Found uncertainty sample 19 after 1335 steps.
Found uncertainty sample 20 after 7 steps.
Found uncertainty sample 21 after 329 steps.
Found uncertainty sample 22 after 788 steps.
Found uncertainty sample 23 after 887 steps.
Found uncertainty sample 24 after 357 steps.
Found uncertainty sample 25 after 391 steps.
Found uncertainty sample 26 after 470 steps.
Found uncertainty sample 27 after 134 steps.
Found uncertainty sample 28 after 38 steps.
Found uncertainty sample 29 after 220 steps.
Found uncertainty sample 30 after 229 steps.
Found uncertainty sample 31 after 3421 steps.
Found uncertainty sample 32 after 185 steps.
Found uncertainty sample 33 after 80 steps.
Found uncertainty sample 34 after 1352 steps.
Found uncertainty sample 35 after 2190 steps.
Found uncertainty sample 36 after 228 steps.
Found uncertainty sample 37 after 74 steps.
Found uncertainty sample 38 after 137 steps.
Found uncertainty sample 39 after 133 steps.
Found uncertainty sample 40 after 1542 steps.
Found uncertainty sample 41 after 19 steps.
Found uncertainty sample 42 after 846 steps.
Found uncertainty sample 43 after 676 steps.
Found uncertainty sample 44 after 1168 steps.
Found uncertainty sample 45 after 22 steps.
Found uncertainty sample 46 after 19 steps.
Found uncertainty sample 47 after 1784 steps.
Found uncertainty sample 48 after 21 steps.
Found uncertainty sample 49 after 16 steps.
Found uncertainty sample 50 after 404 steps.
Found uncertainty sample 51 after 326 steps.
Found uncertainty sample 52 after 118 steps.
Found uncertainty sample 53 after 978 steps.
Found uncertainty sample 54 after 2630 steps.
Found uncertainty sample 55 after 11 steps.
Found uncertainty sample 56 after 170 steps.
Found uncertainty sample 57 after 861 steps.
Found uncertainty sample 58 after 505 steps.
Found uncertainty sample 59 after 1381 steps.
Found uncertainty sample 60 after 14 steps.
Found uncertainty sample 61 after 17 steps.
Found uncertainty sample 62 after 64 steps.
Found uncertainty sample 63 after 1465 steps.
Found uncertainty sample 64 after 6 steps.
Found uncertainty sample 65 after 184 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 826 steps.
Found uncertainty sample 68 after 13 steps.
Found uncertainty sample 69 after 1249 steps.
Found uncertainty sample 70 after 14 steps.
Found uncertainty sample 71 after 785 steps.
Found uncertainty sample 72 after 1754 steps.
Found uncertainty sample 73 after 163 steps.
Found uncertainty sample 74 after 178 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 777 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 846 steps.
Found uncertainty sample 79 after 4 steps.
Found uncertainty sample 80 after 1050 steps.
Found uncertainty sample 81 after 1300 steps.
Found uncertainty sample 82 after 802 steps.
Found uncertainty sample 83 after 51 steps.
Found uncertainty sample 84 after 1958 steps.
Found uncertainty sample 85 after 5 steps.
Found uncertainty sample 86 after 257 steps.
Found uncertainty sample 87 after 1448 steps.
Found uncertainty sample 88 after 2812 steps.
Found uncertainty sample 89 after 184 steps.
Found uncertainty sample 90 after 89 steps.
Found uncertainty sample 91 after 153 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 30 steps.
Found uncertainty sample 94 after 1742 steps.
Found uncertainty sample 95 after 239 steps.
Found uncertainty sample 96 after 4 steps.
Found uncertainty sample 97 after 99 steps.
Found uncertainty sample 98 after 508 steps.
Found uncertainty sample 99 after 300 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_223345-8z1rhtve
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_2
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8z1rhtve
Training model 2. Added 98 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 6.072963532835223, Training Loss Force: 3.9418361239081796, time: 0.6229314804077148
Validation Loss Energy: 5.56079451545572, Validation Loss Force: 4.1714917959004305, time: 0.06096768379211426
Test Loss Energy: 10.193375735950543, Test Loss Force: 11.86231851900299, time: 9.585787296295166


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.023498785575069, Training Loss Force: 3.6965395358439856, time: 0.604931116104126
Validation Loss Energy: 3.6145034089011814, Validation Loss Force: 4.0805701942203205, time: 0.048444271087646484
Test Loss Energy: 12.350141319132161, Test Loss Force: 11.995853159183604, time: 9.543736457824707


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.186726078817354, Training Loss Force: 3.6107596594525124, time: 0.5747230052947998
Validation Loss Energy: 5.112569540792337, Validation Loss Force: 4.028972854387642, time: 0.04875755310058594
Test Loss Energy: 13.133751037041256, Test Loss Force: 11.997090341600973, time: 10.303482294082642


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.97079315918673, Training Loss Force: 3.585956412305766, time: 0.5385172367095947
Validation Loss Energy: 3.5887804464944577, Validation Loss Force: 4.058347368118436, time: 0.04924583435058594
Test Loss Energy: 9.915380045624428, Test Loss Force: 11.959413029358158, time: 10.011141777038574


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.055699272025943, Training Loss Force: 3.683854893237046, time: 0.5402786731719971
Validation Loss Energy: 4.8439330217175085, Validation Loss Force: 4.044944466886751, time: 0.04626035690307617
Test Loss Energy: 10.03610543217447, Test Loss Force: 11.542726893871075, time: 9.757616996765137


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.130983837971433, Training Loss Force: 3.599113807705881, time: 0.5467455387115479
Validation Loss Energy: 3.5099597995497933, Validation Loss Force: 4.012982092341984, time: 0.04750800132751465
Test Loss Energy: 11.354332960093242, Test Loss Force: 11.458352555498418, time: 9.872016191482544


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.137261470613463, Training Loss Force: 3.579473917500388, time: 0.5540783405303955
Validation Loss Energy: 5.325626950367006, Validation Loss Force: 4.002540245412846, time: 0.05279350280761719
Test Loss Energy: 12.511963339319568, Test Loss Force: 11.491719207584566, time: 9.798541784286499


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.091952661112783, Training Loss Force: 3.583311115950329, time: 0.6052107810974121
Validation Loss Energy: 3.1446581508447937, Validation Loss Force: 4.026098195713715, time: 0.048249244689941406
Test Loss Energy: 9.680839691721703, Test Loss Force: 11.411673459480957, time: 9.671680450439453


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.06210995603176, Training Loss Force: 3.6104065978743662, time: 0.5869441032409668
Validation Loss Energy: 4.835261839836789, Validation Loss Force: 4.087636376897193, time: 0.04962897300720215
Test Loss Energy: 9.819084677346886, Test Loss Force: 11.455410301000308, time: 9.841516971588135


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 3.9891849626329265, Training Loss Force: 3.583155234449994, time: 0.5871031284332275
Validation Loss Energy: 3.0560814779404493, Validation Loss Force: 3.9939318562312778, time: 0.04973649978637695
Test Loss Energy: 11.201245744540325, Test Loss Force: 11.389326859644921, time: 9.6434907913208


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.163436274560924, Training Loss Force: 3.682735875993278, time: 0.5824813842773438
Validation Loss Energy: 5.243668227148781, Validation Loss Force: 4.2576184401921315, time: 0.04966449737548828
Test Loss Energy: 12.645113906700248, Test Loss Force: 11.3821193182004, time: 9.594505071640015


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 3.993014531390073, Training Loss Force: 3.6484387172170716, time: 0.5573351383209229
Validation Loss Energy: 3.97114434117532, Validation Loss Force: 4.0173272580556185, time: 0.049662113189697266
Test Loss Energy: 9.71530171279283, Test Loss Force: 11.360680233865057, time: 9.881965398788452


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.128163788905544, Training Loss Force: 3.600708708209051, time: 0.5245389938354492
Validation Loss Energy: 5.397036500127756, Validation Loss Force: 4.066243661371327, time: 0.0452427864074707
Test Loss Energy: 9.885543559866836, Test Loss Force: 11.423271896586327, time: 9.777735710144043


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.112026022654471, Training Loss Force: 3.582953411065581, time: 0.5879802703857422
Validation Loss Energy: 2.9502503955873065, Validation Loss Force: 4.064716151030263, time: 0.04870152473449707
Test Loss Energy: 10.8651206252186, Test Loss Force: 11.409767024543875, time: 10.037445306777954


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.064979306666194, Training Loss Force: 3.5835306808263985, time: 0.586284875869751
Validation Loss Energy: 5.247800075820617, Validation Loss Force: 4.005957771464546, time: 0.05416512489318848
Test Loss Energy: 12.224054801001465, Test Loss Force: 11.39736077175645, time: 9.935593843460083


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.015493335322481, Training Loss Force: 3.5807586922760217, time: 0.5944526195526123
Validation Loss Energy: 3.5434732873650927, Validation Loss Force: 3.9964742925394816, time: 0.050302982330322266
Test Loss Energy: 9.697187920518877, Test Loss Force: 11.318369248975523, time: 9.630051612854004


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.1187988911243245, Training Loss Force: 3.5500026671238736, time: 0.5736284255981445
Validation Loss Energy: 4.960761733188025, Validation Loss Force: 4.093194198779551, time: 0.04880499839782715
Test Loss Energy: 9.889259966202722, Test Loss Force: 11.328898361225338, time: 9.692041873931885


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.176549240917333, Training Loss Force: 3.553854286675559, time: 0.5760939121246338
Validation Loss Energy: 3.1868410391876454, Validation Loss Force: 4.023685352848441, time: 0.05493640899658203
Test Loss Energy: 11.07163991607629, Test Loss Force: 11.295462987145397, time: 9.746924877166748


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.144716571150071, Training Loss Force: 3.5971599252449193, time: 0.5469365119934082
Validation Loss Energy: 5.141446611651128, Validation Loss Force: 4.082569763150753, time: 0.04904937744140625
Test Loss Energy: 12.403640269792364, Test Loss Force: 11.36776495520559, time: 9.274287462234497


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.042179787123634, Training Loss Force: 3.5587494517287284, time: 0.5344822406768799
Validation Loss Energy: 3.6698596265862675, Validation Loss Force: 4.026751952326158, time: 0.04338788986206055
Test Loss Energy: 9.48902505157055, Test Loss Force: 11.270303994389801, time: 9.42198371887207

wandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–†â–ˆâ–‚â–‚â–…â–‡â–â–‚â–„â–‡â–â–‚â–„â–†â–â–‚â–„â–‡â–
wandb:   test_error_force â–‡â–ˆâ–ˆâ–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–
wandb:          test_loss â–â–ƒâ–ƒâ–â–ƒâ–…â–‡â–ƒâ–ƒâ–†â–ˆâ–ƒâ–ƒâ–…â–‡â–ƒâ–ƒâ–†â–‡â–ƒ
wandb: train_error_energy â–ˆâ–â–‚â–â–â–‚â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–‚â–‚â–
wandb:  train_error_force â–ˆâ–„â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–
wandb: valid_error_energy â–ˆâ–ƒâ–‡â–ƒâ–†â–ƒâ–‡â–‚â–†â–â–‡â–„â–ˆâ–â–‡â–ƒâ–†â–‚â–‡â–ƒ
wandb:  valid_error_force â–†â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–ƒâ–â–ˆâ–‚â–ƒâ–ƒâ–â–â–„â–‚â–ƒâ–‚
wandb:         valid_loss â–ˆâ–‚â–†â–‚â–…â–‚â–‡â–â–…â–â–ˆâ–ƒâ–‡â–â–‡â–‚â–†â–â–‡â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1060
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.48903
wandb:   test_error_force 11.2703
wandb:          test_loss 8.35676
wandb: train_error_energy 4.04218
wandb:  train_error_force 3.55875
wandb:         train_loss 1.44358
wandb: valid_error_energy 3.66986
wandb:  valid_error_force 4.02675
wandb:         valid_loss 1.46606
wandb: 
wandb: ğŸš€ View run al_63_2 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8z1rhtve
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_223345-8z1rhtve/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.3557069301605225, Uncertainty Bias: -0.2999928295612335
0.00019836426 0.07274437
2.852744 5.067691
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 57 steps.
Found uncertainty sample 3 after 14 steps.
Found uncertainty sample 4 after 7 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 223 steps.
Found uncertainty sample 11 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 120 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 46 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 39 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 72 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 56 steps.
Found uncertainty sample 30 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 73 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 35 steps.
Found uncertainty sample 42 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 391 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 202 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 19 steps.
Found uncertainty sample 54 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 54 steps.
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 78 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 135 steps.
Found uncertainty sample 63 after 15 steps.
Found uncertainty sample 64 after 33 steps.
Found uncertainty sample 65 after 456 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 14 steps.
Found uncertainty sample 70 after 49 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 29 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 14 steps.
Found uncertainty sample 76 after 5 steps.
Found uncertainty sample 77 after 44 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 18 steps.
Found uncertainty sample 86 after 74 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 178 steps.
Found uncertainty sample 96 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 98 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_224019-7j9slw52
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_3
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/7j9slw52
Training model 3. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 6.088059011556335, Training Loss Force: 4.098449635961739, time: 0.5994715690612793
Validation Loss Energy: 3.1962525959785753, Validation Loss Force: 4.099744679714455, time: 0.05377936363220215
Test Loss Energy: 9.685441696385807, Test Loss Force: 11.282221652780706, time: 10.576900720596313


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.052307690204291, Training Loss Force: 3.6686352926578913, time: 0.7150290012359619
Validation Loss Energy: 3.4536563473179305, Validation Loss Force: 4.0183724670266034, time: 0.05675148963928223
Test Loss Energy: 11.3037535400202, Test Loss Force: 11.231690954115761, time: 10.098860740661621


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.108779910185631, Training Loss Force: 3.590739588604395, time: 0.6520178318023682
Validation Loss Energy: 3.551403203659019, Validation Loss Force: 3.978862053773177, time: 0.051635026931762695
Test Loss Energy: 9.582661097393766, Test Loss Force: 11.247832863045028, time: 10.237704277038574


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.144238199599345, Training Loss Force: 3.5499874917849725, time: 0.6454145908355713
Validation Loss Energy: 3.1146065002412335, Validation Loss Force: 3.9299394157213827, time: 0.054234981536865234
Test Loss Energy: 10.75414163273042, Test Loss Force: 11.22644587629499, time: 10.288166522979736


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.039872379190472, Training Loss Force: 3.5241598501386466, time: 0.6047663688659668
Validation Loss Energy: 3.5950893314060295, Validation Loss Force: 3.9437328786350254, time: 0.056189537048339844
Test Loss Energy: 9.650289377646164, Test Loss Force: 11.30254593179599, time: 10.198059797286987


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.06702916698774, Training Loss Force: 3.5454329799626403, time: 0.6814713478088379
Validation Loss Energy: 3.256157498976674, Validation Loss Force: 3.997910063192424, time: 0.05562710762023926
Test Loss Energy: 10.991031073842645, Test Loss Force: 11.251022539104282, time: 10.343230485916138


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.060056851726383, Training Loss Force: 3.546961677872484, time: 0.6169335842132568
Validation Loss Energy: 3.653127153615524, Validation Loss Force: 3.902804062922039, time: 0.053168535232543945
Test Loss Energy: 9.598008317628326, Test Loss Force: 11.217585551547394, time: 10.104076862335205


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.04433485133986, Training Loss Force: 3.541651556859209, time: 0.6514995098114014
Validation Loss Energy: 3.303375604716391, Validation Loss Force: 3.9413436386992204, time: 0.05286693572998047
Test Loss Energy: 11.04618470107657, Test Loss Force: 11.232487475459148, time: 10.230293273925781


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.114579250775645, Training Loss Force: 3.541839161799907, time: 0.6428463459014893
Validation Loss Energy: 3.5249554531823, Validation Loss Force: 3.9517845949517745, time: 0.05447649955749512
Test Loss Energy: 9.526430768077295, Test Loss Force: 11.295430302579035, time: 10.280057668685913


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.075798545961813, Training Loss Force: 3.553889978865995, time: 0.6281185150146484
Validation Loss Energy: 3.257644260864034, Validation Loss Force: 3.958621428408341, time: 0.050394296646118164
Test Loss Energy: 10.828934370862115, Test Loss Force: 11.167126932400532, time: 10.185393571853638


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.108002266027122, Training Loss Force: 3.5339284136082925, time: 0.659543514251709
Validation Loss Energy: 3.9129003273366965, Validation Loss Force: 3.9890447138164546, time: 0.05606436729431152
Test Loss Energy: 9.467740548586375, Test Loss Force: 11.269103800698407, time: 10.740459203720093


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.149700310860477, Training Loss Force: 3.5536197080048706, time: 0.6150996685028076
Validation Loss Energy: 3.3449434808781904, Validation Loss Force: 3.9750731155860697, time: 0.05395317077636719
Test Loss Energy: 11.095590372625269, Test Loss Force: 11.258035665902458, time: 10.141737461090088


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.049015662957338, Training Loss Force: 3.5788307231188363, time: 0.665391206741333
Validation Loss Energy: 3.7067937578213095, Validation Loss Force: 4.099502415683685, time: 0.056885719299316406
Test Loss Energy: 9.551775693067166, Test Loss Force: 11.393187075274865, time: 10.157469987869263


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 3.9907924284797067, Training Loss Force: 3.580005469865465, time: 0.6177978515625
Validation Loss Energy: 3.5811578136219118, Validation Loss Force: 4.024474046963317, time: 0.054981231689453125
Test Loss Energy: 11.259053653278903, Test Loss Force: 11.283324650806998, time: 10.194282054901123


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.0503360167696405, Training Loss Force: 3.5506072789821546, time: 0.6428320407867432
Validation Loss Energy: 3.668490656677103, Validation Loss Force: 3.9532738061997397, time: 0.05490231513977051
Test Loss Energy: 9.464602154047492, Test Loss Force: 11.20611230346593, time: 10.203026294708252


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.190978296535295, Training Loss Force: 3.5565426302161782, time: 0.7117445468902588
Validation Loss Energy: 3.031790278142041, Validation Loss Force: 4.024750156704267, time: 0.05582451820373535
Test Loss Energy: 10.916011497639023, Test Loss Force: 11.218982151083564, time: 10.177329540252686


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 3.9926992866371296, Training Loss Force: 3.605292674938863, time: 0.6371018886566162
Validation Loss Energy: 3.6309978485549284, Validation Loss Force: 3.9688789052846505, time: 0.05936717987060547
Test Loss Energy: 9.613077343658931, Test Loss Force: 11.149777321982487, time: 10.22034764289856


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 3.990689555334289, Training Loss Force: 3.543606629071979, time: 0.6774280071258545
Validation Loss Energy: 3.1909114311752056, Validation Loss Force: 3.9580892491495736, time: 0.05885457992553711
Test Loss Energy: 10.873029586130862, Test Loss Force: 11.164322499611487, time: 10.367553949356079


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.106236592478726, Training Loss Force: 3.5407167670435875, time: 0.6108434200286865
Validation Loss Energy: 3.5584778477634367, Validation Loss Force: 3.953490685947873, time: 0.05470418930053711
Test Loss Energy: 9.619504936873039, Test Loss Force: 11.229321474609396, time: 10.194319725036621


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.069862823612546, Training Loss Force: 3.551291287672174, time: 0.6465115547180176
Validation Loss Energy: 3.3647269664864368, Validation Loss Force: 3.950074050572757, time: 0.05474042892456055
Test Loss Energy: 11.122946715808393, Test Loss Force: 11.270086314898032, time: 10.231670141220093

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ˆâ–â–†â–‚â–‡â–‚â–‡â–â–†â–â–‡â–â–ˆâ–â–‡â–‚â–†â–‚â–‡
wandb:   test_error_force â–…â–ƒâ–„â–ƒâ–…â–„â–ƒâ–ƒâ–…â–â–„â–„â–ˆâ–…â–ƒâ–ƒâ–â–â–ƒâ–„
wandb:          test_loss â–â–‡â–‚â–†â–‚â–‡â–‚â–‡â–‚â–‡â–‚â–‡â–‚â–ˆâ–‚â–†â–‚â–‡â–‚â–‡
wandb: train_error_energy â–ˆâ–â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–
wandb:  train_error_force â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–„â–…â–‚â–…â–ƒâ–†â–ƒâ–…â–ƒâ–ˆâ–ƒâ–†â–…â–†â–â–†â–‚â–…â–„
wandb:  valid_error_force â–ˆâ–…â–„â–‚â–‚â–„â–â–‚â–ƒâ–ƒâ–„â–„â–ˆâ–…â–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒ
wandb:         valid_loss â–…â–…â–…â–â–…â–ƒâ–…â–ƒâ–„â–‚â–ˆâ–„â–‡â–†â–…â–‚â–†â–‚â–„â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1150
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.12295
wandb:   test_error_force 11.27009
wandb:          test_loss 9.97116
wandb: train_error_energy 4.06986
wandb:  train_error_force 3.55129
wandb:         train_loss 1.45425
wandb: valid_error_energy 3.36473
wandb:  valid_error_force 3.95007
wandb:         valid_loss 1.38605
wandb: 
wandb: ğŸš€ View run al_63_3 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/7j9slw52
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_224019-7j9slw52/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.0957653522491455, Uncertainty Bias: -0.24523481726646423
3.2663345e-05 0.09135467
2.72962 4.979906
(48745, 22, 3)
Found uncertainty sample 0 after 9 steps.
Found uncertainty sample 1 after 197 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 39 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 54 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 88 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 78 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 33 steps.
Found uncertainty sample 20 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 83 steps.
Found uncertainty sample 24 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 11 steps.
Found uncertainty sample 27 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 25 steps.
Found uncertainty sample 30 after 239 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 10 steps.
Found uncertainty sample 35 after 86 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 53 steps.
Found uncertainty sample 40 after 88 steps.
Found uncertainty sample 41 after 138 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 119 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 141 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 12 steps.
Found uncertainty sample 52 after 54 steps.
Found uncertainty sample 53 after 838 steps.
Found uncertainty sample 54 after 8 steps.
Found uncertainty sample 55 after 22 steps.
Found uncertainty sample 56 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 43 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 3 steps.
Found uncertainty sample 62 after 46 steps.
Found uncertainty sample 63 after 2 steps.
Found uncertainty sample 64 after 208 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 64 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 736 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 244 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 63 steps.
Found uncertainty sample 78 after 16 steps.
Found uncertainty sample 79 after 221 steps.
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 968 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 50 steps.
Found uncertainty sample 87 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 235 steps.
Found uncertainty sample 98 after 21 steps.
Found uncertainty sample 99 after 170 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_224724-elye7f24
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_4
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/elye7f24
Training model 4. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 6.328584211114703, Training Loss Force: 4.200180060451871, time: 0.6985363960266113
Validation Loss Energy: 3.7595774788634917, Validation Loss Force: 4.280259884288361, time: 0.055124759674072266
Test Loss Energy: 9.511383052999504, Test Loss Force: 11.30903505630602, time: 10.25069808959961


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.8911732790280298, Training Loss Force: 3.7060894838600658, time: 0.6637625694274902
Validation Loss Energy: 1.5927459434162345, Validation Loss Force: 4.016351521858163, time: 0.05607891082763672
Test Loss Energy: 9.624739144708904, Test Loss Force: 11.539830082531646, time: 10.274495124816895


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.360162043069339, Training Loss Force: 3.8267532146119665, time: 0.6560838222503662
Validation Loss Energy: 2.1984403113882904, Validation Loss Force: 4.572351925856931, time: 0.05132603645324707
Test Loss Energy: 10.473978030936326, Test Loss Force: 11.616710501290365, time: 10.476253986358643


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6668232754707764, Training Loss Force: 3.831865064451823, time: 0.6503710746765137
Validation Loss Energy: 2.1871719165894685, Validation Loss Force: 3.9460303788827087, time: 0.05132937431335449
Test Loss Energy: 9.391977262098584, Test Loss Force: 11.111641176065447, time: 10.255772352218628


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.599160673634731, Training Loss Force: 3.5213505653179653, time: 0.6748862266540527
Validation Loss Energy: 1.9786498321790569, Validation Loss Force: 3.9086892084489104, time: 0.05370283126831055
Test Loss Energy: 10.137938873562998, Test Loss Force: 11.076356869315202, time: 10.31672716140747


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.578947779142761, Training Loss Force: 3.4934537753909933, time: 0.6909129619598389
Validation Loss Energy: 2.1636355651957557, Validation Loss Force: 3.8926258562616796, time: 0.06053447723388672
Test Loss Energy: 9.384315037252923, Test Loss Force: 11.088403461302248, time: 10.261041402816772


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.583687668738542, Training Loss Force: 3.4968834340564126, time: 0.6512770652770996
Validation Loss Energy: 1.9238819063775123, Validation Loss Force: 3.8957654912192616, time: 0.05427670478820801
Test Loss Energy: 10.057084558997591, Test Loss Force: 11.077847129789506, time: 10.265458345413208


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6263935012800714, Training Loss Force: 3.4914299183952595, time: 0.707768440246582
Validation Loss Energy: 2.1845615661917575, Validation Loss Force: 3.8819105716213427, time: 0.05442500114440918
Test Loss Energy: 9.303729585771864, Test Loss Force: 11.094550290873675, time: 10.05291748046875


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.598315701320588, Training Loss Force: 3.4954039003520982, time: 0.7339985370635986
Validation Loss Energy: 1.855681241678728, Validation Loss Force: 3.90587155307634, time: 0.07560086250305176
Test Loss Energy: 10.103949022998714, Test Loss Force: 11.135218238607042, time: 10.671229124069214


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6081862306064627, Training Loss Force: 3.4898506346230342, time: 0.6429331302642822
Validation Loss Energy: 2.2730300218911808, Validation Loss Force: 3.8981471604041054, time: 0.054627180099487305
Test Loss Energy: 9.257741706078873, Test Loss Force: 11.10047885338658, time: 10.26241421699524


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6389453581812883, Training Loss Force: 3.4875038274331907, time: 0.6595284938812256
Validation Loss Energy: 1.6878431482553553, Validation Loss Force: 3.8840348352734937, time: 0.055823564529418945
Test Loss Energy: 10.180951515744912, Test Loss Force: 11.136817921730753, time: 10.230794906616211


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.642918215373466, Training Loss Force: 3.497339260635989, time: 0.7053022384643555
Validation Loss Energy: 2.4681659052558516, Validation Loss Force: 3.878606606834066, time: 0.05837869644165039
Test Loss Energy: 9.254507062510209, Test Loss Force: 11.083318378840573, time: 10.063947677612305


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5889236252251946, Training Loss Force: 3.4882876970499046, time: 0.682295560836792
Validation Loss Energy: 1.7996535651840968, Validation Loss Force: 3.8767620156292004, time: 0.056468963623046875
Test Loss Energy: 10.363849522668112, Test Loss Force: 11.087061109099578, time: 10.175947666168213


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6346874673539267, Training Loss Force: 3.4963602472054616, time: 0.6278653144836426
Validation Loss Energy: 2.0991347710257076, Validation Loss Force: 3.902408157180579, time: 0.05704808235168457
Test Loss Energy: 9.425067916361257, Test Loss Force: 11.134970691138735, time: 10.434280633926392


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.566631920134169, Training Loss Force: 3.493968694936218, time: 0.666607141494751
Validation Loss Energy: 1.7584506437524732, Validation Loss Force: 3.931092396558997, time: 0.05594801902770996
Test Loss Energy: 10.067960941850489, Test Loss Force: 11.129828209001305, time: 10.129672527313232


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.66931042417194, Training Loss Force: 3.492381092473079, time: 0.6582839488983154
Validation Loss Energy: 2.1932291853661425, Validation Loss Force: 3.9086642695045666, time: 0.05670785903930664
Test Loss Energy: 9.22961358464112, Test Loss Force: 11.13807420939401, time: 10.269049882888794


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.630265469366016, Training Loss Force: 3.511405847240239, time: 0.7186398506164551
Validation Loss Energy: 2.0127237782346565, Validation Loss Force: 3.911526904044614, time: 0.054268836975097656
Test Loss Energy: 10.299124437714532, Test Loss Force: 11.212144175136094, time: 10.395557165145874


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.615296918565194, Training Loss Force: 3.510535153785009, time: 0.6697001457214355
Validation Loss Energy: 2.341263864292708, Validation Loss Force: 3.886497556027455, time: 0.055068254470825195
Test Loss Energy: 9.277343748975785, Test Loss Force: 11.107536402096466, time: 10.320905208587646


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.643665552976585, Training Loss Force: 3.490009974380073, time: 0.6440281867980957
Validation Loss Energy: 1.8078873135276416, Validation Loss Force: 3.895390961715811, time: 0.05971240997314453
Test Loss Energy: 10.218878436602889, Test Loss Force: 11.139994293599992, time: 10.386803388595581


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.662108187792173, Training Loss Force: 3.4824546218463897, time: 0.653186559677124
Validation Loss Energy: 2.2195711679985455, Validation Loss Force: 3.889952045157379, time: 0.05270862579345703
Test Loss Energy: 9.330251756707218, Test Loss Force: 11.11040588225549, time: 9.546493768692017

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–ƒâ–ˆâ–‚â–†â–‚â–†â–â–†â–â–†â–â–‡â–‚â–†â–â–‡â–â–‡â–‚
wandb:   test_error_force â–„â–‡â–ˆâ–â–â–â–â–â–‚â–â–‚â–â–â–‚â–‚â–‚â–ƒâ–â–‚â–
wandb:          test_loss â–â–„â–ˆâ–†â–‡â–†â–‡â–…â–‡â–…â–ˆâ–…â–ˆâ–†â–‡â–…â–ˆâ–…â–‡â–…
wandb: train_error_energy â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–ƒâ–„â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–„â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒ
wandb:  valid_error_force â–…â–‚â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:         valid_loss â–ˆâ–â–…â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–â–‚â–â–‚â–‚â–‚â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1240
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.33025
wandb:   test_error_force 11.11041
wandb:          test_loss 10.62318
wandb: train_error_energy 2.66211
wandb:  train_error_force 3.48245
wandb:         train_loss 1.01885
wandb: valid_error_energy 2.21957
wandb:  valid_error_force 3.88995
wandb:         valid_loss 0.97797
wandb: 
wandb: ğŸš€ View run al_63_4 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/elye7f24
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_224724-elye7f24/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.3892242908477783, Uncertainty Bias: -0.16414383053779602
0.00063705444 0.039800644
2.7559621 5.6287494
(48745, 22, 3)
Found uncertainty sample 0 after 66 steps.
Found uncertainty sample 1 after 224 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 597 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 16 steps.
Found uncertainty sample 7 after 287 steps.
Found uncertainty sample 8 after 8 steps.
Found uncertainty sample 9 after 37 steps.
Found uncertainty sample 10 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 15 steps.
Found uncertainty sample 22 after 192 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1111 steps.
Found uncertainty sample 25 after 17 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 35 steps.
Found uncertainty sample 28 after 29 steps.
Found uncertainty sample 29 after 8 steps.
Found uncertainty sample 30 after 14 steps.
Found uncertainty sample 31 after 16 steps.
Found uncertainty sample 32 after 34 steps.
Found uncertainty sample 33 after 103 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 107 steps.
Found uncertainty sample 36 after 291 steps.
Found uncertainty sample 37 after 295 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 67 steps.
Found uncertainty sample 40 after 57 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 7 steps.
Found uncertainty sample 43 after 25 steps.
Found uncertainty sample 44 after 59 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 17 steps.
Found uncertainty sample 48 after 13 steps.
Found uncertainty sample 49 after 13 steps.
Found uncertainty sample 50 after 70 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 47 steps.
Found uncertainty sample 56 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 56 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 694 steps.
Found uncertainty sample 61 after 29 steps.
Found uncertainty sample 62 after 258 steps.
Found uncertainty sample 63 after 97 steps.
Found uncertainty sample 64 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 299 steps.
Found uncertainty sample 69 after 31 steps.
Found uncertainty sample 70 after 16 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 15 steps.
Found uncertainty sample 73 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 17 steps.
Found uncertainty sample 76 after 17 steps.
Found uncertainty sample 77 after 21 steps.
Found uncertainty sample 78 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 65 steps.
Found uncertainty sample 81 after 15 steps.
Found uncertainty sample 82 after 93 steps.
Found uncertainty sample 83 after 1210 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 41 steps.
Found uncertainty sample 89 after 31 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 10 steps.
Found uncertainty sample 92 after 154 steps.
Found uncertainty sample 93 after 14 steps.
Found uncertainty sample 94 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 6 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 9 steps.
Found uncertainty sample 99 after 30 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_225438-zwi1w80h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_5
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zwi1w80h
Training model 5. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.915324624408869, Training Loss Force: 4.148418776586223, time: 0.6888642311096191
Validation Loss Energy: 1.170421120132447, Validation Loss Force: 4.187118716561643, time: 0.06383156776428223
Test Loss Energy: 9.586603549052787, Test Loss Force: 11.516289076070393, time: 9.965070009231567


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5847025384266664, Training Loss Force: 3.700807568217004, time: 0.7287459373474121
Validation Loss Energy: 2.77560263434317, Validation Loss Force: 3.578501927383858, time: 0.06809616088867188
Test Loss Energy: 9.245633428909366, Test Loss Force: 11.300656014013144, time: 10.144798040390015


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.560460262220606, Training Loss Force: 3.5270173323620675, time: 0.7355830669403076
Validation Loss Energy: 3.0611644578691157, Validation Loss Force: 3.938480336151735, time: 0.06265568733215332
Test Loss Energy: 9.22304289560687, Test Loss Force: 11.187263940835054, time: 10.322569131851196


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.560792648937367, Training Loss Force: 3.5115596614583455, time: 0.6816649436950684
Validation Loss Energy: 3.0343947278073853, Validation Loss Force: 3.8852056296427824, time: 0.06282258033752441
Test Loss Energy: 9.3389726136262, Test Loss Force: 11.235021524951458, time: 10.094022750854492


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.678375225357184, Training Loss Force: 3.5169156390615663, time: 0.801593542098999
Validation Loss Energy: 2.4011362575955015, Validation Loss Force: 3.783495485173325, time: 0.06384587287902832
Test Loss Energy: 9.296141580866028, Test Loss Force: 11.196857101218692, time: 10.103320121765137


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6144640140215585, Training Loss Force: 3.5145778410001163, time: 0.6924474239349365
Validation Loss Energy: 2.7643080759197023, Validation Loss Force: 4.202149774588353, time: 0.06134486198425293
Test Loss Energy: 9.348853462546376, Test Loss Force: 11.137853811240209, time: 10.213640928268433


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6127059085733, Training Loss Force: 3.501615522483527, time: 0.7135345935821533
Validation Loss Energy: 2.526975944063423, Validation Loss Force: 3.93591229289175, time: 0.06405067443847656
Test Loss Energy: 9.375231507088156, Test Loss Force: 11.17693385516426, time: 10.491755962371826


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6395604252139893, Training Loss Force: 3.5008771675005694, time: 0.7034704685211182
Validation Loss Energy: 2.686066702758169, Validation Loss Force: 3.9313803077646816, time: 0.07527875900268555
Test Loss Energy: 9.255022554406908, Test Loss Force: 11.154326492271235, time: 10.10566759109497


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6284075904002573, Training Loss Force: 3.5166444704549966, time: 0.7431089878082275
Validation Loss Energy: 3.0678507669830353, Validation Loss Force: 3.44683476669786, time: 0.07506465911865234
Test Loss Energy: 9.255410124627984, Test Loss Force: 11.132051580242114, time: 10.281562805175781


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.617003346215835, Training Loss Force: 3.518438722527943, time: 0.7287123203277588
Validation Loss Energy: 2.8518377584998937, Validation Loss Force: 3.568642248936748, time: 0.06107139587402344
Test Loss Energy: 9.38327955344432, Test Loss Force: 11.220740486556656, time: 10.134058237075806


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.662945483526337, Training Loss Force: 3.507341433062829, time: 0.6522183418273926
Validation Loss Energy: 2.9583295002090044, Validation Loss Force: 3.8462098047855946, time: 0.06545305252075195
Test Loss Energy: 9.364253004201192, Test Loss Force: 11.17147771497252, time: 10.201971769332886


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6169607952023775, Training Loss Force: 3.5317623641469718, time: 0.7258999347686768
Validation Loss Energy: 3.0486300776411217, Validation Loss Force: 3.3870717039457174, time: 0.05939912796020508
Test Loss Energy: 9.28573557317712, Test Loss Force: 11.151073512889338, time: 10.047332286834717


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6970356420164188, Training Loss Force: 3.508754931499532, time: 0.7384440898895264
Validation Loss Energy: 3.0402685172078145, Validation Loss Force: 3.7897656256535055, time: 0.06379842758178711
Test Loss Energy: 9.35368025812523, Test Loss Force: 11.151629532902211, time: 9.855965614318848


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6360902962273878, Training Loss Force: 3.5076184493853013, time: 0.7055180072784424
Validation Loss Energy: 2.5298492667224757, Validation Loss Force: 3.9259102175258196, time: 0.06332182884216309
Test Loss Energy: 9.255532730223473, Test Loss Force: 11.172577677237262, time: 10.187846422195435


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.66496154057686, Training Loss Force: 3.517175287190656, time: 0.7108087539672852
Validation Loss Energy: 3.1742110225614755, Validation Loss Force: 3.596805442513695, time: 0.061486244201660156
Test Loss Energy: 9.218305927792578, Test Loss Force: 11.219299092050518, time: 10.032122373580933


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.535399234215793, Training Loss Force: 3.5360648463059903, time: 0.7551214694976807
Validation Loss Energy: 2.8903854617009017, Validation Loss Force: 3.687289165318338, time: 0.06392478942871094
Test Loss Energy: 9.362601885670573, Test Loss Force: 11.228719391228495, time: 10.159490585327148


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.58772779566629, Training Loss Force: 3.5152180837160616, time: 0.7178955078125
Validation Loss Energy: 2.130578437795035, Validation Loss Force: 3.8130770812098316, time: 0.0671837329864502
Test Loss Energy: 9.247594139159315, Test Loss Force: 11.215793734095918, time: 10.308175086975098


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6404546008842793, Training Loss Force: 3.528784243844883, time: 0.6894850730895996
Validation Loss Energy: 3.576047445388296, Validation Loss Force: 3.604356709924971, time: 0.060408592224121094
Test Loss Energy: 9.273020829192216, Test Loss Force: 11.18999959567157, time: 10.167818784713745


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.637483364769951, Training Loss Force: 3.52092921330539, time: 0.7183270454406738
Validation Loss Energy: 2.5564784925531256, Validation Loss Force: 3.763817099401957, time: 0.05887556076049805
Test Loss Energy: 9.400768230653355, Test Loss Force: 11.224484457234357, time: 9.969771146774292


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.701142710793301, Training Loss Force: 3.5299615062226946, time: 0.724402666091919
Validation Loss Energy: 2.753704571190621, Validation Loss Force: 3.844984233806408, time: 0.06482338905334473
Test Loss Energy: 9.386967472119213, Test Loss Force: 11.18711438178064, time: 10.191485404968262

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‚â–â–ƒâ–‚â–ƒâ–„â–‚â–‚â–„â–„â–‚â–„â–‚â–â–„â–‚â–‚â–„â–„
wandb:   test_error_force â–ˆâ–„â–‚â–ƒâ–‚â–â–‚â–â–â–ƒâ–‚â–â–â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚
wandb:          test_loss â–ˆâ–„â–…â–‡â–†â–‡â–‡â–„â–…â–†â–†â–…â–…â–‚â–â–‡â–ƒâ–ƒâ–‡â–…
wandb: train_error_energy â–ˆâ–â–â–â–‚â–â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–†â–‡â–†â–…â–†â–…â–…â–‡â–†â–†â–†â–†â–…â–‡â–†â–„â–ˆâ–…â–†
wandb:  valid_error_force â–ˆâ–ƒâ–†â–…â–„â–ˆâ–†â–†â–‚â–ƒâ–…â–â–„â–†â–ƒâ–„â–…â–ƒâ–„â–…
wandb:         valid_loss â–â–…â–‡â–†â–ƒâ–‡â–…â–…â–…â–„â–†â–…â–…â–„â–†â–…â–ƒâ–ˆâ–…â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 1330
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.38697
wandb:   test_error_force 11.18711
wandb:          test_loss 10.37612
wandb: train_error_energy 2.70114
wandb:  train_error_force 3.52996
wandb:         train_loss 1.0472
wandb: valid_error_energy 2.7537
wandb:  valid_error_force 3.84498
wandb:         valid_loss 1.17565
wandb: 
wandb: ğŸš€ View run al_63_5 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zwi1w80h
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_225438-zwi1w80h/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.701125383377075, Uncertainty Bias: -0.20900514721870422
/home/ws/fq0795/git/gnn_uncertainty/uncertainty/base_uncertainty.py:925: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(10, 8))
6.1035156e-05 0.009973526
2.6272714 5.660155
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 15 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 68 steps.
Found uncertainty sample 6 after 200 steps.
Found uncertainty sample 7 after 71 steps.
Found uncertainty sample 8 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 55 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 23 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 15 steps.
Found uncertainty sample 19 after 198 steps.
Found uncertainty sample 20 after 110 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 258 steps.
Found uncertainty sample 23 after 44 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1284 steps.
Found uncertainty sample 26 after 214 steps.
Found uncertainty sample 27 after 174 steps.
Found uncertainty sample 28 after 126 steps.
Found uncertainty sample 29 after 476 steps.
Found uncertainty sample 30 after 465 steps.
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 102 steps.
Found uncertainty sample 38 after 16 steps.
Found uncertainty sample 39 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 218 steps.
Found uncertainty sample 45 after 21 steps.
Found uncertainty sample 46 after 465 steps.
Found uncertainty sample 47 after 138 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 45 steps.
Found uncertainty sample 50 after 232 steps.
Found uncertainty sample 51 after 145 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 117 steps.
Found uncertainty sample 54 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 66 steps.
Found uncertainty sample 61 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 119 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 29 steps.
Found uncertainty sample 68 after 243 steps.
Found uncertainty sample 69 after 48 steps.
Found uncertainty sample 70 after 384 steps.
Found uncertainty sample 71 after 12 steps.
Found uncertainty sample 72 after 742 steps.
Found uncertainty sample 73 after 1177 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 5 steps.
Found uncertainty sample 77 after 340 steps.
Found uncertainty sample 78 after 14 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 15 steps.
Found uncertainty sample 81 after 53 steps.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 147 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 103 steps.
Found uncertainty sample 89 after 59 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 43 steps.
Found uncertainty sample 92 after 45 steps.
Found uncertainty sample 93 after 119 steps.
Found uncertainty sample 94 after 16 steps.
Found uncertainty sample 95 after 10 steps.
Found uncertainty sample 96 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 14 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_230202-c5bkafkn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_6
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/c5bkafkn
Training model 6. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.008333789801286, Training Loss Force: 3.803534916594307, time: 0.8270087242126465
Validation Loss Energy: 1.426754333885468, Validation Loss Force: 3.796051352973901, time: 0.06719136238098145
Test Loss Energy: 9.804069824038127, Test Loss Force: 11.453047382656004, time: 10.518656015396118


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7504073542258465, Training Loss Force: 3.573892267836385, time: 0.8320531845092773
Validation Loss Energy: 1.763228733953078, Validation Loss Force: 3.8644947270417465, time: 0.06303739547729492
Test Loss Energy: 9.93028226331956, Test Loss Force: 11.350108317843716, time: 10.579697132110596


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7307891076796416, Training Loss Force: 3.5759860950903892, time: 0.7869746685028076
Validation Loss Energy: 1.9878977271477298, Validation Loss Force: 3.8672640632096504, time: 0.06971979141235352
Test Loss Energy: 9.350784743163237, Test Loss Force: 11.264667349961398, time: 10.730481624603271


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7313773859556234, Training Loss Force: 3.560132418067371, time: 0.8217003345489502
Validation Loss Energy: 2.0691842122143984, Validation Loss Force: 3.8689164909415807, time: 0.06700897216796875
Test Loss Energy: 9.368217083912544, Test Loss Force: 11.240175590547704, time: 10.549006462097168


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7688956674280927, Training Loss Force: 3.5456114257791365, time: 0.7917928695678711
Validation Loss Energy: 1.627426829896347, Validation Loss Force: 3.833886626242513, time: 0.06804108619689941
Test Loss Energy: 9.865572734895213, Test Loss Force: 11.153776427200757, time: 10.590397119522095


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7499251592421372, Training Loss Force: 3.5499823369942445, time: 0.9218251705169678
Validation Loss Energy: 1.5153945231636883, Validation Loss Force: 3.6372698817228493, time: 0.09366798400878906
Test Loss Energy: 9.853473826151134, Test Loss Force: 11.24197395050832, time: 10.522974252700806


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.7465157952516865, Training Loss Force: 3.5547132052542527, time: 0.745983362197876
Validation Loss Energy: 2.1927631388437154, Validation Loss Force: 3.924878814041882, time: 0.06527876853942871
Test Loss Energy: 9.20183772812384, Test Loss Force: 11.264441258005979, time: 10.646023750305176


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7348826906587491, Training Loss Force: 3.5459104803385237, time: 0.7598893642425537
Validation Loss Energy: 1.9772109441580716, Validation Loss Force: 4.156350516646487, time: 0.0625
Test Loss Energy: 9.37477969157235, Test Loss Force: 11.271626938236757, time: 11.25926923751831


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6939803057733849, Training Loss Force: 3.549401581224241, time: 0.7536532878875732
Validation Loss Energy: 1.523579172960556, Validation Loss Force: 3.7955689402692485, time: 0.06271553039550781
Test Loss Energy: 9.80024094063837, Test Loss Force: 11.247525789468918, time: 10.651697397232056


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6532128039602159, Training Loss Force: 3.587456440908162, time: 0.8199856281280518
Validation Loss Energy: 1.629087774723109, Validation Loss Force: 3.967180385039514, time: 0.06958913803100586
Test Loss Energy: 9.793460275765316, Test Loss Force: 11.284461802316534, time: 10.587831974029541


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.7043162388862108, Training Loss Force: 3.544767108911608, time: 0.7763416767120361
Validation Loss Energy: 1.851544327283822, Validation Loss Force: 3.6758615620801782, time: 0.06732678413391113
Test Loss Energy: 9.058359798397628, Test Loss Force: 11.263378858044286, time: 10.854743003845215


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.750199529689648, Training Loss Force: 3.5609442648252214, time: 0.7909739017486572
Validation Loss Energy: 1.8928231232269206, Validation Loss Force: 3.732385568101082, time: 0.06587719917297363
Test Loss Energy: 9.240611882931738, Test Loss Force: 11.29059091162085, time: 10.668277978897095


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.7367141146947858, Training Loss Force: 3.544189802039265, time: 0.8078560829162598
Validation Loss Energy: 1.6066888173267948, Validation Loss Force: 3.91304506228155, time: 0.06502342224121094
Test Loss Energy: 9.81240775064808, Test Loss Force: 11.325899760941041, time: 10.755109071731567


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.73146316553294, Training Loss Force: 3.6491838562282566, time: 0.7543430328369141
Validation Loss Energy: 1.6734646778280737, Validation Loss Force: 3.9097288506912475, time: 0.06406044960021973
Test Loss Energy: 9.609469548570747, Test Loss Force: 11.244940360528444, time: 10.597907066345215


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6924501951515156, Training Loss Force: 3.576448004914906, time: 0.8090620040893555
Validation Loss Energy: 1.7987406773530794, Validation Loss Force: 3.826159784209673, time: 0.06616401672363281
Test Loss Energy: 9.086715130965562, Test Loss Force: 11.292664541941345, time: 9.854645252227783


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7093530693973331, Training Loss Force: 3.5421666400757723, time: 0.717355489730835
Validation Loss Energy: 2.235788607234758, Validation Loss Force: 3.844068172940061, time: 0.05940437316894531
Test Loss Energy: 9.152614169625524, Test Loss Force: 11.304987322944221, time: 10.807797193527222


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.733157756904702, Training Loss Force: 3.5859574223213686, time: 0.8464498519897461
Validation Loss Energy: 1.093866243082171, Validation Loss Force: 4.055466697377568, time: 0.06950736045837402
Test Loss Energy: 9.899554947091726, Test Loss Force: 11.570006219177463, time: 9.578486442565918


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.1299439197967995, Training Loss Force: 4.481843979123313, time: 0.7388129234313965
Validation Loss Energy: 1.244478206038278, Validation Loss Force: 4.342701211527723, time: 0.05971479415893555
Test Loss Energy: 9.606907286547907, Test Loss Force: 11.493557927785146, time: 8.645419120788574


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.9597940431370833, Training Loss Force: 4.543717767243534, time: 0.7367031574249268
Validation Loss Energy: 2.2934647034166926, Validation Loss Force: 5.2104709295206995, time: 0.05813932418823242
Test Loss Energy: 9.354171039575768, Test Loss Force: 12.162184036052077, time: 9.307076215744019


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6663169812147505, Training Loss Force: 5.025657161411298, time: 0.7444343566894531
Validation Loss Energy: 1.868618152055273, Validation Loss Force: 3.8233658431011444, time: 0.06279540061950684
Test Loss Energy: 9.219020921917572, Test Loss Force: 11.46993598633548, time: 8.589747428894043

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–ˆâ–ƒâ–ƒâ–‡â–‡â–‚â–„â–‡â–‡â–â–‚â–‡â–…â–â–‚â–ˆâ–…â–ƒâ–‚
wandb:   test_error_force â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–„â–ƒâ–ˆâ–ƒ
wandb:          test_loss â–â–ƒâ–ƒâ–„â–†â–†â–…â–†â–‡â–ˆâ–…â–†â–‡â–‡â–†â–†â–ˆâ–‡â–‡â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–„
wandb:  train_error_force â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–…â–†â–ˆ
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ƒâ–…
wandb: valid_error_energy â–ƒâ–…â–†â–‡â–„â–ƒâ–‡â–†â–„â–„â–…â–†â–„â–„â–…â–ˆâ–â–‚â–ˆâ–†
wandb:  valid_error_force â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–â–‚â–‚â–‚â–‚â–ƒâ–„â–ˆâ–‚
wandb:         valid_loss â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–„â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–„â–â–‚â–ˆâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1420
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.21902
wandb:   test_error_force 11.46994
wandb:          test_loss 13.50396
wandb: train_error_energy 2.66632
wandb:  train_error_force 5.02566
wandb:         train_loss 1.93484
wandb: valid_error_energy 1.86862
wandb:  valid_error_force 3.82337
wandb:         valid_loss 0.79784
wandb: 
wandb: ğŸš€ View run al_63_6 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/c5bkafkn
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_230202-c5bkafkn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 4.218301296234131, Uncertainty Bias: -0.11448469758033752
0.00015258789 0.008034706
2.8270056 6.1140738
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 53 steps.
Found uncertainty sample 2 after 178 steps.
Found uncertainty sample 3 after 251 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 11 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 97 steps.
Found uncertainty sample 9 after 11 steps.
Found uncertainty sample 10 after 319 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 147 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 3 steps.
Found uncertainty sample 20 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 456 steps.
Found uncertainty sample 23 after 271 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 9 steps.
Found uncertainty sample 26 after 650 steps.
Found uncertainty sample 27 after 7 steps.
Found uncertainty sample 28 after 224 steps.
Found uncertainty sample 29 after 39 steps.
Found uncertainty sample 30 after 17 steps.
Found uncertainty sample 31 after 16 steps.
Found uncertainty sample 32 after 59 steps.
Found uncertainty sample 33 after 5 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 204 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 5 steps.
Found uncertainty sample 42 after 79 steps.
Found uncertainty sample 43 after 93 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 592 steps.
Found uncertainty sample 46 after 356 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 96 steps.
Found uncertainty sample 49 after 119 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 690 steps.
Found uncertainty sample 53 after 268 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 42 steps.
Found uncertainty sample 56 after 50 steps.
Found uncertainty sample 57 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 166 steps.
Found uncertainty sample 62 after 61 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 49 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 92 steps.
Found uncertainty sample 69 after 11 steps.
Found uncertainty sample 70 after 150 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 58 steps.
Found uncertainty sample 73 after 20 steps.
Found uncertainty sample 74 after 123 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 14 steps.
Found uncertainty sample 79 after 170 steps.
Found uncertainty sample 80 after 36 steps.
Found uncertainty sample 81 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 48 steps.
Found uncertainty sample 85 after 54 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 19 steps.
Found uncertainty sample 88 after 14 steps.
Found uncertainty sample 89 after 60 steps.
Found uncertainty sample 90 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 13 steps.
Found uncertainty sample 93 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 39 steps.
Found uncertainty sample 96 after 288 steps.
Found uncertainty sample 97 after 591 steps.
Found uncertainty sample 98 after 21 steps.
Found uncertainty sample 99 after 145 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_230924-t5pgi8tj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_7
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t5pgi8tj
Training model 7. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.710502033781449, Training Loss Force: 4.162717750152173, time: 0.8383541107177734
Validation Loss Energy: 3.9008158501180494, Validation Loss Force: 4.756032372340201, time: 0.06856846809387207
Test Loss Energy: 11.038168886953631, Test Loss Force: 11.744331906989121, time: 10.74365758895874


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.17961574516033, Training Loss Force: 4.0274063815849805, time: 0.8788690567016602
Validation Loss Energy: 4.7385056385259325, Validation Loss Force: 4.080999054995377, time: 0.06694436073303223
Test Loss Energy: 11.316825482620999, Test Loss Force: 11.553687583226367, time: 10.727983951568604


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.091608794460545, Training Loss Force: 3.6724881755477177, time: 0.8533647060394287
Validation Loss Energy: 4.364370927471123, Validation Loss Force: 3.8829680273953535, time: 0.06535649299621582
Test Loss Energy: 11.421886101749863, Test Loss Force: 11.435338004183338, time: 11.041147470474243


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.198929129812118, Training Loss Force: 3.626888461633188, time: 0.8295876979827881
Validation Loss Energy: 4.705258908347455, Validation Loss Force: 4.169261096396526, time: 0.0725259780883789
Test Loss Energy: 11.48206084594442, Test Loss Force: 11.371567603283562, time: 10.777190923690796


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.11072214567677, Training Loss Force: 3.604852932755612, time: 0.8028509616851807
Validation Loss Energy: 4.5155846537941, Validation Loss Force: 3.7232168320137866, time: 0.06967782974243164
Test Loss Energy: 11.69670181086812, Test Loss Force: 11.221473947065274, time: 11.042614698410034


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.141916485628356, Training Loss Force: 3.6122267143018214, time: 0.8354105949401855
Validation Loss Energy: 4.78249815901774, Validation Loss Force: 3.9695950092579544, time: 0.06670808792114258
Test Loss Energy: 11.7331418147864, Test Loss Force: 11.21337405493812, time: 10.851684093475342


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.117141588138765, Training Loss Force: 3.606320085841309, time: 0.8241457939147949
Validation Loss Energy: 4.881960013120764, Validation Loss Force: 4.044768112923176, time: 0.0647745132446289
Test Loss Energy: 11.747276782221475, Test Loss Force: 11.239344043697402, time: 10.840122699737549


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.06462570792602, Training Loss Force: 3.6065976846821717, time: 0.8553969860076904
Validation Loss Energy: 4.770853898700592, Validation Loss Force: 3.7981898674513914, time: 0.06721711158752441
Test Loss Energy: 11.615305461311175, Test Loss Force: 11.164862389518062, time: 11.292264699935913


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.100428981430885, Training Loss Force: 3.5988478149484906, time: 0.8741159439086914
Validation Loss Energy: 4.963636391861952, Validation Loss Force: 3.8658465514266744, time: 0.07023143768310547
Test Loss Energy: 12.075796661345983, Test Loss Force: 11.241760491238065, time: 10.86175274848938


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.194641621490457, Training Loss Force: 3.6044718159565945, time: 0.865656852722168
Validation Loss Energy: 4.932521273417441, Validation Loss Force: 3.9769014519432826, time: 0.06878137588500977
Test Loss Energy: 11.589978811512568, Test Loss Force: 11.191628498583102, time: 11.056716918945312


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.089166538392777, Training Loss Force: 3.6250904221966427, time: 0.861438512802124
Validation Loss Energy: 4.905126956401383, Validation Loss Force: 3.818693799081287, time: 0.07489013671875
Test Loss Energy: 12.253077185045402, Test Loss Force: 11.164853879126621, time: 10.818490266799927


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.069629791073451, Training Loss Force: 3.568900981206435, time: 0.8961338996887207
Validation Loss Energy: 5.164028603085395, Validation Loss Force: 3.92751833409771, time: 0.06881260871887207
Test Loss Energy: 11.982915144496332, Test Loss Force: 11.1519057312445, time: 10.14600133895874


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.120367437806286, Training Loss Force: 3.5891593180462533, time: 0.8450760841369629
Validation Loss Energy: 5.103373305065519, Validation Loss Force: 3.8321905553579807, time: 0.058928728103637695
Test Loss Energy: 11.73246593581944, Test Loss Force: 11.161094435857795, time: 10.526426315307617


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.084912349261545, Training Loss Force: 3.5651681973945664, time: 0.867861270904541
Validation Loss Energy: 5.020955883406684, Validation Loss Force: 3.8553867796312895, time: 0.0671999454498291
Test Loss Energy: 12.02753534599437, Test Loss Force: 11.129135276608862, time: 10.1013662815094


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.156006117061845, Training Loss Force: 3.551643783754372, time: 0.749908447265625
Validation Loss Energy: 5.035456098189501, Validation Loss Force: 3.7831765116217833, time: 0.06277704238891602
Test Loss Energy: 11.977366042850685, Test Loss Force: 11.137358562417068, time: 8.754115104675293


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.166667114500511, Training Loss Force: 3.58235875182683, time: 0.778315544128418
Validation Loss Energy: 4.846110503295536, Validation Loss Force: 4.000857089338778, time: 0.058096885681152344
Test Loss Energy: 11.76016130968144, Test Loss Force: 11.170623644730075, time: 8.99465036392212


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 3.9996827856109713, Training Loss Force: 3.5711937358179457, time: 0.7478423118591309
Validation Loss Energy: 5.169845834522758, Validation Loss Force: 3.8682738677864332, time: 0.06769919395446777
Test Loss Energy: 12.225313438931721, Test Loss Force: 11.165180449659783, time: 8.821639060974121


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.129044636392887, Training Loss Force: 3.5419056084378915, time: 0.8342282772064209
Validation Loss Energy: 4.727293801690177, Validation Loss Force: 3.8768049907743647, time: 0.06317424774169922
Test Loss Energy: 11.8761012401705, Test Loss Force: 11.09700188982841, time: 8.84675407409668


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.086776196708593, Training Loss Force: 3.5550311752483235, time: 0.7889573574066162
Validation Loss Energy: 4.868835448321489, Validation Loss Force: 3.88457747998212, time: 0.05943799018859863
Test Loss Energy: 11.760019258424407, Test Loss Force: 11.098174766329086, time: 9.42412281036377


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.071809561968635, Training Loss Force: 3.585661366682409, time: 0.7585728168487549
Validation Loss Energy: 5.188662936037678, Validation Loss Force: 3.879780963909689, time: 0.05878758430480957
Test Loss Energy: 12.13675105998758, Test Loss Force: 11.120714074069948, time: 8.82570767402649

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–ƒâ–ƒâ–„â–…â–…â–…â–„â–‡â–„â–ˆâ–†â–…â–‡â–†â–…â–ˆâ–†â–…â–‡
wandb:   test_error_force â–ˆâ–†â–…â–„â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–
wandb:          test_loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–‚â–â–‚â–‚â–â–ƒâ–‚â–â–‚
wandb: train_error_energy â–ˆâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–†â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–†â–„â–…â–„â–†â–†â–†â–‡â–‡â–†â–ˆâ–ˆâ–‡â–‡â–†â–ˆâ–…â–†â–ˆ
wandb:  valid_error_force â–ˆâ–ƒâ–‚â–„â–â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚
wandb:         valid_loss â–ˆâ–…â–â–ƒâ–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–‚â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1510
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.13675
wandb:   test_error_force 11.12071
wandb:          test_loss 11.16302
wandb: train_error_energy 4.07181
wandb:  train_error_force 3.58566
wandb:         train_loss 1.47892
wandb: valid_error_energy 5.18866
wandb:  valid_error_force 3.87978
wandb:         valid_loss 1.90341
wandb: 
wandb: ğŸš€ View run al_63_7 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t5pgi8tj
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_230924-t5pgi8tj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.9789371490478516, Uncertainty Bias: -0.20156580209732056
7.6293945e-05 0.8642044
2.5402336 5.3072863
(48745, 22, 3)
Found uncertainty sample 0 after 47 steps.
Found uncertainty sample 1 after 10 steps.
Found uncertainty sample 2 after 14 steps.
Found uncertainty sample 3 after 33 steps.
Found uncertainty sample 4 after 21 steps.
Found uncertainty sample 5 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 34 steps.
Found uncertainty sample 8 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 103 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 96 steps.
Found uncertainty sample 16 after 41 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 2 steps.
Found uncertainty sample 19 after 70 steps.
Found uncertainty sample 20 after 14 steps.
Found uncertainty sample 21 after 14 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 42 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 8 steps.
Found uncertainty sample 27 after 376 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 21 steps.
Found uncertainty sample 36 after 66 steps.
Found uncertainty sample 37 after 162 steps.
Found uncertainty sample 38 after 67 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 12 steps.
Found uncertainty sample 44 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 39 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 11 steps.
Found uncertainty sample 49 after 371 steps.
Found uncertainty sample 50 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 14 steps.
Found uncertainty sample 53 after 157 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 140 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 204 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 607 steps.
Found uncertainty sample 61 after 140 steps.
Found uncertainty sample 62 after 10 steps.
Found uncertainty sample 63 after 45 steps.
Found uncertainty sample 64 after 17 steps.
Found uncertainty sample 65 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 73 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 11 steps.
Found uncertainty sample 73 after 53 steps.
Found uncertainty sample 74 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 7 steps.
Found uncertainty sample 80 after 149 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 35 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 17 steps.
Found uncertainty sample 87 after 130 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 52 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 31 steps.
Found uncertainty sample 96 after 32 steps.
Found uncertainty sample 97 after 68 steps.
Found uncertainty sample 98 after 468 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_231627-vss7tmx7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_8
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/vss7tmx7
Training model 8. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.361595585916355, Training Loss Force: 3.7715612918146473, time: 0.9089107513427734
Validation Loss Energy: 1.7415818972644217, Validation Loss Force: 3.901517035258576, time: 0.06947755813598633
Test Loss Energy: 10.128696844935384, Test Loss Force: 11.014718729503919, time: 10.771057605743408


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.500839545173381, Training Loss Force: 3.554577753138682, time: 0.9058127403259277
Validation Loss Energy: 3.548675172617685, Validation Loss Force: 3.8482995547201266, time: 0.06683683395385742
Test Loss Energy: 9.534845512045173, Test Loss Force: 11.065566779499902, time: 10.760810375213623


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6649950716873474, Training Loss Force: 3.5275718379138663, time: 0.8471035957336426
Validation Loss Energy: 2.569438393518988, Validation Loss Force: 3.7142834572137367, time: 0.06815743446350098
Test Loss Energy: 10.639991691707378, Test Loss Force: 10.984140487354768, time: 10.86255931854248


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6065431984276923, Training Loss Force: 3.5508870164983124, time: 0.8775720596313477
Validation Loss Energy: 1.6675245330221582, Validation Loss Force: 4.075390687216895, time: 0.06905508041381836
Test Loss Energy: 10.048688057578383, Test Loss Force: 11.018875381886483, time: 10.778139114379883


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5571698936113805, Training Loss Force: 3.5213244052331873, time: 0.8346073627471924
Validation Loss Energy: 3.570410687293707, Validation Loss Force: 3.7376975632463356, time: 0.0699775218963623
Test Loss Energy: 9.328131069751088, Test Loss Force: 10.985030315230947, time: 10.937663316726685


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6552759420051477, Training Loss Force: 3.5316216196452785, time: 0.8392314910888672
Validation Loss Energy: 2.7161478570235644, Validation Loss Force: 3.9675471211228563, time: 0.06870198249816895
Test Loss Energy: 10.380218861899285, Test Loss Force: 11.054895368841963, time: 10.787829875946045


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.709650271144276, Training Loss Force: 3.520830329118671, time: 0.8471696376800537
Validation Loss Energy: 1.682496750727056, Validation Loss Force: 3.7385052955731886, time: 0.0637660026550293
Test Loss Energy: 10.156970445102884, Test Loss Force: 11.068871991700718, time: 10.853411674499512


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6500723399734407, Training Loss Force: 3.513595323148739, time: 0.8607728481292725
Validation Loss Energy: 3.7952840350970343, Validation Loss Force: 3.913420482461584, time: 0.07298946380615234
Test Loss Energy: 9.350690074967813, Test Loss Force: 11.0962718381396, time: 10.870553970336914


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6580192147495207, Training Loss Force: 3.514754458561547, time: 0.8829183578491211
Validation Loss Energy: 2.103289888623293, Validation Loss Force: 3.991221972725337, time: 0.0727071762084961
Test Loss Energy: 10.253217082645872, Test Loss Force: 11.11325004574869, time: 11.04341745376587


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5248355814320216, Training Loss Force: 3.5229840875258924, time: 0.9510843753814697
Validation Loss Energy: 1.870958070775255, Validation Loss Force: 3.8334299318242153, time: 0.07114839553833008
Test Loss Energy: 10.113572531671391, Test Loss Force: 11.137375541230895, time: 10.706226587295532


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.610340883058851, Training Loss Force: 3.5211613401642414, time: 1.0043272972106934
Validation Loss Energy: 3.758241018649633, Validation Loss Force: 3.9117123874431723, time: 0.09860658645629883
Test Loss Energy: 9.339335081689946, Test Loss Force: 11.1651115970956, time: 10.82498025894165


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6564075287912208, Training Loss Force: 3.5102650789488603, time: 0.9339609146118164
Validation Loss Energy: 2.293673678410146, Validation Loss Force: 3.918610009576577, time: 0.07154107093811035
Test Loss Energy: 10.28878225562982, Test Loss Force: 11.191896951303372, time: 9.968268394470215


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6164779030168304, Training Loss Force: 3.5233071225626547, time: 0.8686635494232178
Validation Loss Energy: 1.712335585294396, Validation Loss Force: 3.8222047348397035, time: 0.06570935249328613
Test Loss Energy: 9.76665113909379, Test Loss Force: 11.112657787789578, time: 11.117769479751587


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.582288142285051, Training Loss Force: 3.5187574931324637, time: 0.9267277717590332
Validation Loss Energy: 3.7655650521974895, Validation Loss Force: 3.856537867781163, time: 0.07418966293334961
Test Loss Energy: 9.30131149677488, Test Loss Force: 11.176063477612848, time: 9.787382125854492


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.707025988338104, Training Loss Force: 3.5136123128045735, time: 0.7825572490692139
Validation Loss Energy: 2.449011905323143, Validation Loss Force: 3.9167306819849466, time: 0.06017708778381348
Test Loss Energy: 10.173947850636727, Test Loss Force: 11.226184278520158, time: 9.38306212425232


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6794768316513506, Training Loss Force: 3.5081187354241994, time: 0.8363192081451416
Validation Loss Energy: 1.8376921910502273, Validation Loss Force: 3.919227471118235, time: 0.0654449462890625
Test Loss Energy: 9.908417169773195, Test Loss Force: 11.199311763914345, time: 9.586484670639038


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.664862035779422, Training Loss Force: 3.528547413789537, time: 0.8410789966583252
Validation Loss Energy: 3.7829850161562857, Validation Loss Force: 3.8458285952077658, time: 0.06646418571472168
Test Loss Energy: 9.38038266749324, Test Loss Force: 11.243652721982087, time: 9.422717094421387


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.7158476544810255, Training Loss Force: 3.519267503451062, time: 0.8162717819213867
Validation Loss Energy: 2.462874173002878, Validation Loss Force: 3.987924910681584, time: 0.06392168998718262
Test Loss Energy: 10.314486440382442, Test Loss Force: 11.272362068181064, time: 9.514687299728394


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6756496188647496, Training Loss Force: 3.5139155614610296, time: 0.837054967880249
Validation Loss Energy: 1.6820052085874204, Validation Loss Force: 3.92170310668266, time: 0.060492515563964844
Test Loss Energy: 9.87100321334224, Test Loss Force: 11.27003960800451, time: 9.663535356521606


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5665592725553754, Training Loss Force: 3.5268220129251033, time: 0.8257627487182617
Validation Loss Energy: 3.9450007567519387, Validation Loss Force: 3.919002129942556, time: 0.06426072120666504
Test Loss Energy: 9.36815518422159, Test Loss Force: 11.165608408100184, time: 9.753394365310669

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–‚â–ˆâ–…â–â–‡â–…â–â–†â–…â–â–†â–ƒâ–â–†â–„â–â–†â–„â–
wandb:   test_error_force â–‚â–ƒâ–â–‚â–â–ƒâ–ƒâ–„â–„â–…â–…â–†â–„â–†â–‡â–†â–‡â–ˆâ–ˆâ–…
wandb:          test_loss â–‚â–â–ˆâ–†â–‚â–ˆâ–‡â–‚â–‡â–ˆâ–â–ˆâ–†â–â–‡â–†â–‚â–‡â–…â–‚
wandb: train_error_energy â–ˆâ–â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–‡â–„â–â–‡â–„â–â–ˆâ–‚â–‚â–‡â–ƒâ–â–‡â–ƒâ–‚â–ˆâ–ƒâ–â–ˆ
wandb:  valid_error_force â–…â–„â–â–ˆâ–â–†â–â–…â–†â–ƒâ–…â–…â–ƒâ–„â–…â–…â–„â–†â–…â–…
wandb:         valid_loss â–‚â–†â–ƒâ–‚â–†â–„â–â–‡â–‚â–â–‡â–ƒâ–â–‡â–ƒâ–‚â–‡â–ƒâ–â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1600
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.36816
wandb:   test_error_force 11.16561
wandb:          test_loss 10.41826
wandb: train_error_energy 2.56656
wandb:  train_error_force 3.52682
wandb:         train_loss 0.99737
wandb: valid_error_energy 3.945
wandb:  valid_error_force 3.919
wandb:         valid_loss 1.70798
wandb: 
wandb: ğŸš€ View run al_63_8 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/vss7tmx7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_231627-vss7tmx7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.952937602996826, Uncertainty Bias: -0.11291864514350891
0.00032806396 0.00606966
2.6831 5.46799
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 45 steps.
Found uncertainty sample 2 after 3 steps.
Found uncertainty sample 3 after 8 steps.
Found uncertainty sample 4 after 7 steps.
Found uncertainty sample 5 after 66 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 77 steps.
Found uncertainty sample 10 after 106 steps.
Found uncertainty sample 11 after 10 steps.
Found uncertainty sample 12 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 11 steps.
Found uncertainty sample 15 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 7 steps.
Found uncertainty sample 20 after 51 steps.
Found uncertainty sample 21 after 81 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 95 steps.
Found uncertainty sample 24 after 47 steps.
Found uncertainty sample 25 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 19 steps.
Found uncertainty sample 30 after 170 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 89 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 9 steps.
Found uncertainty sample 41 after 7 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 23 steps.
Found uncertainty sample 44 after 23 steps.
Found uncertainty sample 45 after 43 steps.
Found uncertainty sample 46 after 442 steps.
Found uncertainty sample 47 after 33 steps.
Found uncertainty sample 48 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 19 steps.
Found uncertainty sample 51 after 70 steps.
Found uncertainty sample 52 after 282 steps.
Found uncertainty sample 53 after 36 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 73 steps.
Found uncertainty sample 56 after 702 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 96 steps.
Found uncertainty sample 59 after 51 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 542 steps.
Found uncertainty sample 64 after 178 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 100 steps.
Found uncertainty sample 67 after 10 steps.
Found uncertainty sample 68 after 51 steps.
Found uncertainty sample 69 after 15 steps.
Found uncertainty sample 70 after 3 steps.
Found uncertainty sample 71 after 8 steps.
Found uncertainty sample 72 after 165 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 87 steps.
Found uncertainty sample 77 after 1811 steps.
Found uncertainty sample 78 after 74 steps.
Found uncertainty sample 79 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 12 steps.
Found uncertainty sample 84 after 4 steps.
Found uncertainty sample 85 after 13 steps.
Found uncertainty sample 86 after 15 steps.
Found uncertainty sample 87 after 42 steps.
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 163 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 4 steps.
Found uncertainty sample 94 after 70 steps.
Found uncertainty sample 95 after 135 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 36 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_232346-hh5ubfnb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_9
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hh5ubfnb
Training model 9. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.600597643560271, Training Loss Force: 4.098910208184219, time: 0.9033374786376953
Validation Loss Energy: 1.2605221303425265, Validation Loss Force: 4.45064275204597, time: 0.07525086402893066
Test Loss Energy: 9.530425107476356, Test Loss Force: 11.3971583316944, time: 10.019624471664429


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.8618416929431274, Training Loss Force: 3.987887733336823, time: 0.9257421493530273
Validation Loss Energy: 1.7593849634887861, Validation Loss Force: 3.9111661800282107, time: 0.07629752159118652
Test Loss Energy: 9.89542819066507, Test Loss Force: 11.169949374576838, time: 10.101422786712646


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.525309322223767, Training Loss Force: 3.530879078458371, time: 0.9379961490631104
Validation Loss Energy: 3.507339874378807, Validation Loss Force: 3.884655414183195, time: 0.07838988304138184
Test Loss Energy: 11.090315291594345, Test Loss Force: 11.179238072792145, time: 10.340365409851074


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6446830348989696, Training Loss Force: 3.5059839495205503, time: 0.9584276676177979
Validation Loss Energy: 2.2662903835168113, Validation Loss Force: 3.8494247019145, time: 0.06662845611572266
Test Loss Energy: 10.387206564777385, Test Loss Force: 11.20630783262381, time: 9.995173931121826


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6684492081597595, Training Loss Force: 3.4927244514201474, time: 0.9271667003631592
Validation Loss Energy: 2.2728670280763614, Validation Loss Force: 3.8307494406455365, time: 0.0685422420501709
Test Loss Energy: 9.239565487169179, Test Loss Force: 11.178239213977724, time: 10.004186391830444


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5891372165367783, Training Loss Force: 3.5004675104888987, time: 0.88942551612854
Validation Loss Energy: 3.660467776014752, Validation Loss Force: 3.7965474598310283, time: 0.07003355026245117
Test Loss Energy: 9.334538377922582, Test Loss Force: 11.154868154204518, time: 10.583298683166504


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6049459276497693, Training Loss Force: 3.498081186258514, time: 0.9726018905639648
Validation Loss Energy: 2.8931585832442863, Validation Loss Force: 3.942091807029538, time: 0.07201051712036133
Test Loss Energy: 9.25940686558956, Test Loss Force: 11.184226327983747, time: 11.206894397735596


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.669147563721611, Training Loss Force: 3.4999440321320407, time: 0.975090742111206
Validation Loss Energy: 1.7877969898363069, Validation Loss Force: 3.829223488988493, time: 0.07314562797546387
Test Loss Energy: 10.01363074744982, Test Loss Force: 11.277921806650209, time: 11.257570266723633


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6745332539515574, Training Loss Force: 3.4932641762220484, time: 0.9801559448242188
Validation Loss Energy: 2.974411111266201, Validation Loss Force: 3.7335684979388235, time: 0.07032346725463867
Test Loss Energy: 10.786946930070647, Test Loss Force: 11.221129502378473, time: 10.847243070602417


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.574531158286161, Training Loss Force: 3.490277624201639, time: 0.944221019744873
Validation Loss Energy: 2.1379746161461726, Validation Loss Force: 3.966417075207273, time: 0.06790924072265625
Test Loss Energy: 10.247619446121254, Test Loss Force: 11.173951944573647, time: 10.376258373260498


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5927347390831748, Training Loss Force: 3.5064834384279124, time: 0.9613437652587891
Validation Loss Energy: 2.240260360357853, Validation Loss Force: 3.7624917726453946, time: 0.06583213806152344
Test Loss Energy: 9.221018341099809, Test Loss Force: 11.207474903405393, time: 10.708524703979492


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6358238236290794, Training Loss Force: 3.498603038710475, time: 0.9719440937042236
Validation Loss Energy: 3.896010649948928, Validation Loss Force: 3.74876511047553, time: 0.07024145126342773
Test Loss Energy: 9.267808953556136, Test Loss Force: 11.172510121050518, time: 10.111854314804077


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6162593499310716, Training Loss Force: 3.5027745981137697, time: 0.8455765247344971
Validation Loss Energy: 2.9725063993919347, Validation Loss Force: 3.7902728659679292, time: 0.06118917465209961
Test Loss Energy: 9.179895915147343, Test Loss Force: 11.290782798797514, time: 8.725837469100952


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.595236681116605, Training Loss Force: 3.5061486742728816, time: 0.871577262878418
Validation Loss Energy: 1.9004280539023655, Validation Loss Force: 3.82378791246725, time: 0.06073880195617676
Test Loss Energy: 10.010760101483747, Test Loss Force: 11.14654802247333, time: 8.886108160018921


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6641339305273357, Training Loss Force: 3.5112036041911177, time: 0.8356940746307373
Validation Loss Energy: 3.3013408540609026, Validation Loss Force: 3.856962052504156, time: 0.06204795837402344
Test Loss Energy: 10.765166963754838, Test Loss Force: 11.265418488768068, time: 8.662034034729004


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6439895717563715, Training Loss Force: 3.5147258057479274, time: 0.8723301887512207
Validation Loss Energy: 2.0364423843243116, Validation Loss Force: 3.8732596526329024, time: 0.06273746490478516
Test Loss Energy: 10.216072586192475, Test Loss Force: 11.205123629141871, time: 8.699406862258911


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.62395154800585, Training Loss Force: 3.492204960101393, time: 0.8499867916107178
Validation Loss Energy: 2.420882000783513, Validation Loss Force: 3.812983560168542, time: 0.06085848808288574
Test Loss Energy: 9.221061601867703, Test Loss Force: 11.208127918564555, time: 8.835174083709717


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6652479652709355, Training Loss Force: 3.5108294528050847, time: 0.8706057071685791
Validation Loss Energy: 3.705282502327898, Validation Loss Force: 3.7344530511210214, time: 0.06229567527770996
Test Loss Energy: 9.320626858392544, Test Loss Force: 11.228266694362558, time: 8.672702312469482


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6320317092556644, Training Loss Force: 3.4905020789020393, time: 0.8804402351379395
Validation Loss Energy: 2.6537951521042817, Validation Loss Force: 3.846443607388819, time: 0.06155896186828613
Test Loss Energy: 9.23903075012975, Test Loss Force: 11.282029122078551, time: 8.643550634384155


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.670543272834157, Training Loss Force: 3.5002423037549675, time: 0.8687455654144287
Validation Loss Energy: 1.8287389430289034, Validation Loss Force: 3.833257585905267, time: 0.06392240524291992
Test Loss Energy: 9.85409663758133, Test Loss Force: 11.272606668489157, time: 8.851168394088745

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–ˆâ–…â–â–‚â–â–„â–‡â–…â–â–â–â–„â–‡â–…â–â–‚â–â–ƒ
wandb:   test_error_force â–ˆâ–‚â–‚â–ƒâ–‚â–â–‚â–…â–ƒâ–‚â–ƒâ–‚â–…â–â–„â–ƒâ–ƒâ–ƒâ–…â–…
wandb:          test_loss â–„â–…â–ˆâ–†â–‚â–â–‚â–„â–†â–…â–â–â–â–…â–†â–…â–‚â–â–â–ƒ
wandb: train_error_energy â–ˆâ–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–‚â–‡â–„â–„â–‡â–…â–‚â–†â–ƒâ–„â–ˆâ–†â–ƒâ–†â–ƒâ–„â–‡â–…â–ƒ
wandb:  valid_error_force â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–â–ƒâ–â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:         valid_loss â–â–â–ˆâ–ƒâ–‚â–‡â–…â–â–„â–‚â–‚â–ˆâ–„â–â–†â–‚â–ƒâ–‡â–ƒâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 1690
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.8541
wandb:   test_error_force 11.27261
wandb:          test_loss 11.35794
wandb: train_error_energy 2.67054
wandb:  train_error_force 3.50024
wandb:         train_loss 1.02049
wandb: valid_error_energy 1.82874
wandb:  valid_error_force 3.83326
wandb:         valid_loss 0.90388
wandb: 
wandb: ğŸš€ View run al_63_9 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hh5ubfnb
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_232346-hh5ubfnb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.174394130706787, Uncertainty Bias: -0.13861048221588135
0.000102996826 0.0029144287
2.6158164 5.5258536
(48745, 22, 3)
Found uncertainty sample 0 after 122 steps.
Found uncertainty sample 1 after 47 steps.
Found uncertainty sample 2 after 112 steps.
Found uncertainty sample 3 after 215 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 119 steps.
Found uncertainty sample 7 after 4 steps.
Found uncertainty sample 8 after 54 steps.
Found uncertainty sample 9 after 57 steps.
Found uncertainty sample 10 after 58 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 39 steps.
Found uncertainty sample 13 after 48 steps.
Found uncertainty sample 14 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 19 steps.
Found uncertainty sample 17 after 44 steps.
Found uncertainty sample 18 after 7 steps.
Found uncertainty sample 19 after 475 steps.
Found uncertainty sample 20 after 20 steps.
Found uncertainty sample 21 after 2 steps.
Found uncertainty sample 22 after 56 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 160 steps.
Found uncertainty sample 25 after 5 steps.
Found uncertainty sample 26 after 196 steps.
Found uncertainty sample 27 after 93 steps.
Found uncertainty sample 28 after 16 steps.
Found uncertainty sample 29 after 69 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 23 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 481 steps.
Found uncertainty sample 36 after 56 steps.
Found uncertainty sample 37 after 75 steps.
Found uncertainty sample 38 after 21 steps.
Found uncertainty sample 39 after 72 steps.
Found uncertainty sample 40 after 422 steps.
Found uncertainty sample 41 after 18 steps.
Found uncertainty sample 42 after 36 steps.
Found uncertainty sample 43 after 5 steps.
Found uncertainty sample 44 after 25 steps.
Found uncertainty sample 45 after 98 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 29 steps.
Found uncertainty sample 48 after 15 steps.
Found uncertainty sample 49 after 8 steps.
Found uncertainty sample 50 after 20 steps.
Found uncertainty sample 51 after 94 steps.
Found uncertainty sample 52 after 26 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 93 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 42 steps.
Found uncertainty sample 57 after 249 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 17 steps.
Found uncertainty sample 60 after 10 steps.
Found uncertainty sample 61 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 13 steps.
Found uncertainty sample 64 after 10 steps.
Found uncertainty sample 65 after 13 steps.
Found uncertainty sample 66 after 5 steps.
Found uncertainty sample 67 after 4 steps.
Found uncertainty sample 68 after 25 steps.
Found uncertainty sample 69 after 39 steps.
Found uncertainty sample 70 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 376 steps.
Found uncertainty sample 73 after 46 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1348 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 3 steps.
Found uncertainty sample 80 after 28 steps.
Found uncertainty sample 81 after 72 steps.
Found uncertainty sample 82 after 40 steps.
Found uncertainty sample 83 after 231 steps.
Found uncertainty sample 84 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 43 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 3 steps.
Found uncertainty sample 90 after 5 steps.
Found uncertainty sample 91 after 194 steps.
Found uncertainty sample 92 after 4 steps.
Found uncertainty sample 93 after 138 steps.
Found uncertainty sample 94 after 27 steps.
Found uncertainty sample 95 after 138 steps.
Found uncertainty sample 96 after 6 steps.
Found uncertainty sample 97 after 2 steps.
Found uncertainty sample 98 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_233056-8y24rjbw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_10
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8y24rjbw
Training model 10. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.110997286873206, Training Loss Force: 3.8125701430892494, time: 0.9842195510864258
Validation Loss Energy: 1.7998310324934015, Validation Loss Force: 3.9402465478172344, time: 0.07974720001220703
Test Loss Energy: 10.203414976079316, Test Loss Force: 11.188631684315204, time: 10.933241128921509


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6149760271430926, Training Loss Force: 3.5311052887039236, time: 1.0269434452056885
Validation Loss Energy: 2.109801179788513, Validation Loss Force: 3.857213445969908, time: 0.08178281784057617
Test Loss Energy: 10.430904085956664, Test Loss Force: 11.261163353669426, time: 11.26958417892456


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.622188493508252, Training Loss Force: 3.507658637189907, time: 0.9582960605621338
Validation Loss Energy: 3.821288005965919, Validation Loss Force: 3.7514867954462057, time: 0.07513213157653809
Test Loss Energy: 9.298868967798969, Test Loss Force: 11.257137173748308, time: 11.152631521224976


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6097924166654054, Training Loss Force: 3.506967090442411, time: 0.9692482948303223
Validation Loss Energy: 2.246427182354323, Validation Loss Force: 3.842459165691587, time: 0.07837271690368652
Test Loss Energy: 10.409873710255768, Test Loss Force: 11.206462521229229, time: 10.925724506378174


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.681049772064913, Training Loss Force: 3.5086745452870285, time: 0.9545958042144775
Validation Loss Energy: 1.8005511445113318, Validation Loss Force: 3.8802703906924387, time: 0.07373499870300293
Test Loss Energy: 10.170578136663629, Test Loss Force: 11.31140454513884, time: 11.09901475906372


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5868087400324713, Training Loss Force: 3.531808438104416, time: 0.95432448387146
Validation Loss Energy: 3.6301135180418758, Validation Loss Force: 3.844259000398121, time: 0.07938194274902344
Test Loss Energy: 9.424939719555168, Test Loss Force: 11.287362600716062, time: 10.85173225402832


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.615098819236484, Training Loss Force: 3.515556954346285, time: 1.0062248706817627
Validation Loss Energy: 2.419532069086326, Validation Loss Force: 3.852538507251154, time: 0.07370805740356445
Test Loss Energy: 10.441158352200366, Test Loss Force: 11.314287208243954, time: 10.907545328140259


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6106969921776804, Training Loss Force: 3.5242518663959856, time: 1.033968210220337
Validation Loss Energy: 1.4682658190860614, Validation Loss Force: 3.82048194365766, time: 0.07865476608276367
Test Loss Energy: 9.924666775957443, Test Loss Force: 11.26174032556654, time: 10.557533025741577


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.624500819147195, Training Loss Force: 3.532671114043027, time: 0.8977127075195312
Validation Loss Energy: 3.827886138098051, Validation Loss Force: 3.8538415670931236, time: 0.06341934204101562
Test Loss Energy: 9.406570844036162, Test Loss Force: 11.303791391443394, time: 10.38907527923584


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6099592415824047, Training Loss Force: 3.527785415001418, time: 1.007314682006836
Validation Loss Energy: 2.0841329939916875, Validation Loss Force: 3.747310301610611, time: 0.07143092155456543
Test Loss Energy: 10.095055561351622, Test Loss Force: 11.326562307738621, time: 10.42263913154602


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6159020604061887, Training Loss Force: 3.5191305674930455, time: 0.8979673385620117
Validation Loss Energy: 1.7576646773346345, Validation Loss Force: 3.8312206085583194, time: 0.0650794506072998
Test Loss Energy: 10.125383537370434, Test Loss Force: 11.291355354617538, time: 8.837235927581787


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.617918494536307, Training Loss Force: 3.5251178465856725, time: 0.9509623050689697
Validation Loss Energy: 3.9023825502287974, Validation Loss Force: 3.785641236398058, time: 0.06319618225097656
Test Loss Energy: 9.497217107739583, Test Loss Force: 11.395868067750571, time: 8.866475343704224


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.674427897835096, Training Loss Force: 3.535921236635571, time: 0.9524295330047607
Validation Loss Energy: 2.2083414062965425, Validation Loss Force: 3.805928733618112, time: 0.0628819465637207
Test Loss Energy: 10.189911754819642, Test Loss Force: 11.39191559015095, time: 9.280102014541626


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.675977631352172, Training Loss Force: 3.5242100761687767, time: 1.1043133735656738
Validation Loss Energy: 1.7166260857327056, Validation Loss Force: 3.9230117332532775, time: 0.0660855770111084
Test Loss Energy: 9.985629097423724, Test Loss Force: 11.508362525461411, time: 9.056637287139893


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6363617941987347, Training Loss Force: 3.520437217874993, time: 0.9386024475097656
Validation Loss Energy: 4.106201639608068, Validation Loss Force: 3.797654570824581, time: 0.062442779541015625
Test Loss Energy: 9.46346234916112, Test Loss Force: 11.28692296088905, time: 8.898364305496216


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6674503712762987, Training Loss Force: 3.523006883438756, time: 0.8901810646057129
Validation Loss Energy: 2.2455108988188304, Validation Loss Force: 3.8646347859345354, time: 0.06197953224182129
Test Loss Energy: 10.293671920851093, Test Loss Force: 11.393144276166565, time: 8.910888433456421


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.614962995033703, Training Loss Force: 3.5548397838276347, time: 1.055044412612915
Validation Loss Energy: 1.7511771561007419, Validation Loss Force: 3.8869865374719126, time: 0.09396171569824219
Test Loss Energy: 10.116706813243013, Test Loss Force: 11.438219370911014, time: 8.896848440170288


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6277341262429874, Training Loss Force: 3.5148292000051677, time: 0.9211411476135254
Validation Loss Energy: 3.8602548838341355, Validation Loss Force: 3.7204142068085666, time: 0.06352448463439941
Test Loss Energy: 9.377916841029307, Test Loss Force: 11.305966213815546, time: 8.966950416564941


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6762007180027476, Training Loss Force: 3.5162414949681535, time: 0.9251320362091064
Validation Loss Energy: 2.045745501241175, Validation Loss Force: 3.8058002535209114, time: 0.06543564796447754
Test Loss Energy: 10.276127200965707, Test Loss Force: 11.409352309671522, time: 8.886881113052368


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.58399815042905, Training Loss Force: 3.510562549022881, time: 0.882225513458252
Validation Loss Energy: 1.6641547138349735, Validation Loss Force: 3.8719516940697223, time: 0.062029123306274414
Test Loss Energy: 9.872338733915619, Test Loss Force: 11.393388797199776, time: 9.114405155181885

wandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–ˆâ–â–ˆâ–†â–‚â–ˆâ–…â–‚â–†â–†â–‚â–†â–…â–‚â–‡â–†â–â–‡â–…
wandb:   test_error_force â–â–ƒâ–‚â–â–„â–ƒâ–„â–ƒâ–„â–„â–ƒâ–†â–…â–ˆâ–ƒâ–…â–†â–„â–†â–…
wandb:          test_loss â–‡â–‡â–â–‡â–†â–‚â–ˆâ–†â–‚â–†â–†â–‚â–†â–…â–‚â–†â–†â–‚â–†â–…
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–ƒâ–‡â–ƒâ–‚â–‡â–„â–â–‡â–ƒâ–‚â–‡â–ƒâ–‚â–ˆâ–ƒâ–‚â–‡â–ƒâ–‚
wandb:  valid_error_force â–ˆâ–…â–‚â–…â–†â–…â–…â–„â–…â–‚â–…â–ƒâ–„â–‡â–ƒâ–†â–†â–â–„â–†
wandb:         valid_loss â–‚â–‚â–†â–ƒâ–‚â–†â–ƒâ–â–‡â–‚â–‚â–‡â–ƒâ–‚â–ˆâ–ƒâ–‚â–‡â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1780
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.87234
wandb:   test_error_force 11.39339
wandb:          test_loss 11.41096
wandb: train_error_energy 2.584
wandb:  train_error_force 3.51056
wandb:         train_loss 0.9935
wandb: valid_error_energy 1.66415
wandb:  valid_error_force 3.87195
wandb:         valid_loss 0.867
wandb: 
wandb: ğŸš€ View run al_63_10 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8y24rjbw
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_233056-8y24rjbw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.3853256702423096, Uncertainty Bias: -0.15986302495002747
0.00033569336 0.011378288
2.5804825 5.6768637
(48745, 22, 3)
Found uncertainty sample 0 after 4 steps.
Found uncertainty sample 1 after 34 steps.
Found uncertainty sample 2 after 11 steps.
Found uncertainty sample 3 after 3 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 10 steps.
Found uncertainty sample 6 after 31 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 34 steps.
Found uncertainty sample 9 after 343 steps.
Found uncertainty sample 10 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 52 steps.
Found uncertainty sample 13 after 23 steps.
Found uncertainty sample 14 after 33 steps.
Found uncertainty sample 15 after 60 steps.
Found uncertainty sample 16 after 16 steps.
Found uncertainty sample 17 after 19 steps.
Found uncertainty sample 18 after 47 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 36 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 126 steps.
Found uncertainty sample 25 after 225 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 2 steps.
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 91 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 428 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 145 steps.
Found uncertainty sample 39 after 189 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 30 steps.
Found uncertainty sample 42 after 10 steps.
Found uncertainty sample 43 after 162 steps.
Found uncertainty sample 44 after 36 steps.
Found uncertainty sample 45 after 13 steps.
Found uncertainty sample 46 after 640 steps.
Found uncertainty sample 47 after 103 steps.
Found uncertainty sample 48 after 402 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 5 steps.
Found uncertainty sample 52 after 10 steps.
Found uncertainty sample 53 after 120 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 18 steps.
Found uncertainty sample 59 after 45 steps.
Found uncertainty sample 60 after 35 steps.
Found uncertainty sample 61 after 10 steps.
Found uncertainty sample 62 after 900 steps.
Found uncertainty sample 63 after 207 steps.
Found uncertainty sample 64 after 9 steps.
Found uncertainty sample 65 after 187 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 54 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 53 steps.
Found uncertainty sample 71 after 3 steps.
Found uncertainty sample 72 after 19 steps.
Found uncertainty sample 73 after 19 steps.
Found uncertainty sample 74 after 54 steps.
Found uncertainty sample 75 after 184 steps.
Found uncertainty sample 76 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 23 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 66 steps.
Found uncertainty sample 81 after 834 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 410 steps.
Found uncertainty sample 84 after 16 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 568 steps.
Found uncertainty sample 87 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 3 steps.
Found uncertainty sample 94 after 21 steps.
Found uncertainty sample 95 after 16 steps.
Found uncertainty sample 96 after 8 steps.
Found uncertainty sample 97 after 9 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 115 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_233812-v20aegf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_11
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/v20aegf1
Training model 11. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.77211461415041, Training Loss Force: 3.877263821306842, time: 1.0388755798339844
Validation Loss Energy: 3.151669000238903, Validation Loss Force: 3.8290932275381255, time: 0.07858085632324219
Test Loss Energy: 10.99646665852454, Test Loss Force: 11.474831331368687, time: 10.852254390716553


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.676297005290334, Training Loss Force: 3.508818137098088, time: 1.0558645725250244
Validation Loss Energy: 2.281812983683156, Validation Loss Force: 3.810152989985508, time: 0.0805051326751709
Test Loss Energy: 10.310431427721445, Test Loss Force: 11.4788111541384, time: 10.934221982955933


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6436986197318384, Training Loss Force: 3.5366775042864957, time: 1.0486712455749512
Validation Loss Energy: 2.4381053478864545, Validation Loss Force: 3.8727642137832126, time: 0.0887000560760498
Test Loss Energy: 9.435111922309238, Test Loss Force: 11.3627166143188, time: 11.51386284828186


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.609434935032561, Training Loss Force: 3.526402919626484, time: 1.033630609512329
Validation Loss Energy: 3.6224259771132923, Validation Loss Force: 3.857620039830372, time: 0.08672666549682617
Test Loss Energy: 9.424959141900569, Test Loss Force: 11.338313204426331, time: 10.970179319381714


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.602002121945839, Training Loss Force: 3.527902049086576, time: 0.9915778636932373
Validation Loss Energy: 2.6622621463781857, Validation Loss Force: 3.8330743291981686, time: 0.07317590713500977
Test Loss Energy: 9.385063975015889, Test Loss Force: 11.445908306727729, time: 11.128535032272339


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.640524940926276, Training Loss Force: 3.521180863945919, time: 0.9566717147827148
Validation Loss Energy: 1.8616604092524673, Validation Loss Force: 3.832295209856567, time: 0.07988357543945312
Test Loss Energy: 10.293884051101319, Test Loss Force: 11.431807781896042, time: 10.172305583953857


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.649439230338682, Training Loss Force: 3.5169413453068934, time: 0.9592781066894531
Validation Loss Energy: 3.326718122618574, Validation Loss Force: 3.8175551804035854, time: 0.06273412704467773
Test Loss Energy: 11.085424078181312, Test Loss Force: 11.534940258760612, time: 10.922611951828003


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.655894399443996, Training Loss Force: 3.5334166807027305, time: 1.0693836212158203
Validation Loss Energy: 1.9603603452054132, Validation Loss Force: 3.724833298678649, time: 0.07712531089782715
Test Loss Energy: 10.219690043489463, Test Loss Force: 11.479162215945998, time: 9.929787158966064


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.685641683969392, Training Loss Force: 3.532332055518862, time: 0.9453096389770508
Validation Loss Energy: 2.4486006354106147, Validation Loss Force: 3.815460531747843, time: 0.06433558464050293
Test Loss Energy: 9.379331748399341, Test Loss Force: 11.406154892653479, time: 8.9181649684906


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6094982391159376, Training Loss Force: 3.522899605568749, time: 0.9813551902770996
Validation Loss Energy: 3.6871464343957983, Validation Loss Force: 3.789224762298047, time: 0.06498408317565918
Test Loss Energy: 9.444845464514215, Test Loss Force: 11.415571186118441, time: 8.93854308128357


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.646073869251983, Training Loss Force: 3.535553764814012, time: 0.9365150928497314
Validation Loss Energy: 2.8639485106777602, Validation Loss Force: 3.9689033988907028, time: 0.06592297554016113
Test Loss Energy: 9.391135362881709, Test Loss Force: 11.476776601670108, time: 9.073165893554688


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.696135395826292, Training Loss Force: 3.5181349991171453, time: 0.9442946910858154
Validation Loss Energy: 1.7130466235446296, Validation Loss Force: 3.78908514874355, time: 0.06471538543701172
Test Loss Energy: 10.08583458719927, Test Loss Force: 11.601191688248212, time: 8.931228637695312


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6843745002782904, Training Loss Force: 3.5336963078613755, time: 1.0025532245635986
Validation Loss Energy: 3.0775490102993475, Validation Loss Force: 3.82410944671397, time: 0.06372952461242676
Test Loss Energy: 10.925633821851738, Test Loss Force: 11.595159825904503, time: 8.872584342956543


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6561582208673333, Training Loss Force: 3.515136377887843, time: 0.9281749725341797
Validation Loss Energy: 2.1578360359554933, Validation Loss Force: 3.837379859803828, time: 0.06447148323059082
Test Loss Energy: 10.46423594103408, Test Loss Force: 11.634953581565803, time: 9.089162588119507


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.644247661460843, Training Loss Force: 3.518965026663991, time: 1.007805347442627
Validation Loss Energy: 2.310407264726619, Validation Loss Force: 3.7826816294449586, time: 0.06363105773925781
Test Loss Energy: 9.474877125024346, Test Loss Force: 11.542090465814573, time: 8.90665888786316


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6077029181771754, Training Loss Force: 3.5288763322735024, time: 0.9860105514526367
Validation Loss Energy: 3.7472649884394147, Validation Loss Force: 3.8287371795737872, time: 0.06434345245361328
Test Loss Energy: 9.459231294972833, Test Loss Force: 11.421205247118424, time: 9.351067543029785


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.663051551902113, Training Loss Force: 3.5336172489924116, time: 0.9110028743743896
Validation Loss Energy: 2.7646247114452116, Validation Loss Force: 3.75723644679778, time: 0.06796908378601074
Test Loss Energy: 9.50353474669627, Test Loss Force: 11.437955390226573, time: 9.09410834312439


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6057664080166543, Training Loss Force: 3.5373066502972352, time: 0.9673283100128174
Validation Loss Energy: 1.7450675600747751, Validation Loss Force: 3.8767709223950373, time: 0.06877589225769043
Test Loss Energy: 10.116494794910265, Test Loss Force: 11.663038013218669, time: 8.918351411819458


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6682377465280234, Training Loss Force: 3.535307636152642, time: 0.9641408920288086
Validation Loss Energy: 3.151297922107214, Validation Loss Force: 3.797913535041295, time: 0.06413865089416504
Test Loss Energy: 11.148053112013272, Test Loss Force: 11.689276375423338, time: 8.985856294631958


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.667329620608784, Training Loss Force: 3.5368377203980716, time: 0.9697294235229492
Validation Loss Energy: 2.2741817780484923, Validation Loss Force: 3.8418107531386454, time: 0.06858563423156738
Test Loss Energy: 10.453697245850343, Test Loss Force: 11.62817338767879, time: 10.034425497055054

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–…â–â–â–â–…â–ˆâ–„â–â–â–â–„â–‡â–…â–â–â–â–„â–ˆâ–…
wandb:   test_error_force â–„â–„â–â–â–ƒâ–ƒâ–…â–„â–‚â–ƒâ–„â–†â–†â–‡â–…â–ƒâ–ƒâ–‡â–ˆâ–‡
wandb:          test_loss â–‡â–†â–ƒâ–‚â–‚â–†â–ˆâ–…â–‚â–‚â–â–…â–‡â–†â–‚â–â–â–…â–ˆâ–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–ƒâ–ƒâ–ˆâ–„â–‚â–‡â–‚â–„â–ˆâ–…â–â–†â–ƒâ–ƒâ–ˆâ–…â–â–†â–ƒ
wandb:  valid_error_force â–„â–ƒâ–…â–…â–„â–„â–„â–â–„â–ƒâ–ˆâ–ƒâ–„â–„â–ƒâ–„â–‚â–…â–ƒâ–„
wandb:         valid_loss â–†â–ƒâ–ƒâ–ˆâ–„â–‚â–‡â–â–ƒâ–ˆâ–…â–â–…â–ƒâ–‚â–ˆâ–„â–â–†â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1870
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.4537
wandb:   test_error_force 11.62817
wandb:          test_loss 11.8311
wandb: train_error_energy 2.66733
wandb:  train_error_force 3.53684
wandb:         train_loss 1.03668
wandb: valid_error_energy 2.27418
wandb:  valid_error_force 3.84181
wandb:         valid_loss 1.00428
wandb: 
wandb: ğŸš€ View run al_63_11 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/v20aegf1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_233812-v20aegf1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.4033639430999756, Uncertainty Bias: -0.1713903844356537
0.00015258789 0.03963089
2.5696425 5.6294203
(48745, 22, 3)
Found uncertainty sample 0 after 18 steps.
Found uncertainty sample 1 after 1443 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 485 steps.
Found uncertainty sample 5 after 53 steps.
Found uncertainty sample 6 after 202 steps.
Found uncertainty sample 7 after 9 steps.
Found uncertainty sample 8 after 223 steps.
Found uncertainty sample 9 after 59 steps.
Found uncertainty sample 10 after 148 steps.
Found uncertainty sample 11 after 21 steps.
Found uncertainty sample 12 after 581 steps.
Found uncertainty sample 13 after 241 steps.
Found uncertainty sample 14 after 939 steps.
Found uncertainty sample 15 after 88 steps.
Found uncertainty sample 16 after 1083 steps.
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 15 steps.
Found uncertainty sample 20 after 191 steps.
Found uncertainty sample 21 after 2 steps.
Found uncertainty sample 22 after 738 steps.
Found uncertainty sample 23 after 418 steps.
Found uncertainty sample 24 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 96 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 50 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 388 steps.
Found uncertainty sample 31 after 17 steps.
Found uncertainty sample 32 after 509 steps.
Found uncertainty sample 33 after 55 steps.
Found uncertainty sample 34 after 1000 steps.
Found uncertainty sample 35 after 271 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 59 steps.
Found uncertainty sample 38 after 561 steps.
Found uncertainty sample 39 after 155 steps.
Found uncertainty sample 40 after 19 steps.
Found uncertainty sample 41 after 10 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 138 steps.
Found uncertainty sample 44 after 27 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 61 steps.
Found uncertainty sample 47 after 92 steps.
Found uncertainty sample 48 after 21 steps.
Found uncertainty sample 49 after 12 steps.
Found uncertainty sample 50 after 17 steps.
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 238 steps.
Found uncertainty sample 54 after 21 steps.
Found uncertainty sample 55 after 267 steps.
Found uncertainty sample 56 after 8 steps.
Found uncertainty sample 57 after 6 steps.
Found uncertainty sample 58 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 178 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 21 steps.
Found uncertainty sample 64 after 119 steps.
Found uncertainty sample 65 after 27 steps.
Found uncertainty sample 66 after 94 steps.
Found uncertainty sample 67 after 20 steps.
Found uncertainty sample 68 after 17 steps.
Found uncertainty sample 69 after 72 steps.
Found uncertainty sample 70 after 18 steps.
Found uncertainty sample 71 after 325 steps.
Found uncertainty sample 72 after 78 steps.
Found uncertainty sample 73 after 32 steps.
Found uncertainty sample 74 after 40 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 315 steps.
Found uncertainty sample 77 after 9 steps.
Found uncertainty sample 78 after 9 steps.
Found uncertainty sample 79 after 383 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 270 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 192 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 476 steps.
Found uncertainty sample 86 after 812 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1455 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 125 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 419 steps.
Found uncertainty sample 93 after 977 steps.
Found uncertainty sample 94 after 534 steps.
Found uncertainty sample 95 after 18 steps.
Found uncertainty sample 96 after 222 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 16 steps.
Found uncertainty sample 99 after 17 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_234630-4ku13f0b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_12
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/4ku13f0b
Training model 12. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1086741129410473, Training Loss Force: 3.8428852538766334, time: 1.03193998336792
Validation Loss Energy: 2.425683860915446, Validation Loss Force: 4.050929368190998, time: 0.06836581230163574
Test Loss Energy: 9.55561891123037, Test Loss Force: 11.661951597631809, time: 9.141447067260742


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6160136624130055, Training Loss Force: 3.559841549033293, time: 1.0241436958312988
Validation Loss Energy: 2.0384755762957782, Validation Loss Force: 3.818327174332517, time: 0.06843328475952148
Test Loss Energy: 9.588399479471523, Test Loss Force: 11.556168980209215, time: 9.153843641281128


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6359142630134156, Training Loss Force: 3.518488250507974, time: 0.9806838035583496
Validation Loss Energy: 3.082399857643384, Validation Loss Force: 3.774481949480741, time: 0.07192802429199219
Test Loss Energy: 10.866551844560831, Test Loss Force: 11.709104827375183, time: 9.333894968032837


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6445057670499894, Training Loss Force: 3.5379662232736937, time: 0.985130786895752
Validation Loss Energy: 2.739984032422269, Validation Loss Force: 3.8275917694871344, time: 0.0672597885131836
Test Loss Energy: 9.49935555693188, Test Loss Force: 11.571554751351195, time: 9.257275104522705


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.611677193366095, Training Loss Force: 3.518411214166359, time: 0.9958407878875732
Validation Loss Energy: 2.302874478854455, Validation Loss Force: 3.863233417664639, time: 0.06757283210754395
Test Loss Energy: 9.55584906019431, Test Loss Force: 11.540025191998987, time: 9.229981184005737


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6307600253666146, Training Loss Force: 3.5062724379990455, time: 1.000971794128418
Validation Loss Energy: 3.2672444184215643, Validation Loss Force: 3.769456974864589, time: 0.06555700302124023
Test Loss Energy: 11.076243445616008, Test Loss Force: 11.826334299007703, time: 9.359547138214111


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6250837919830334, Training Loss Force: 3.5128323392753966, time: 1.0556719303131104
Validation Loss Energy: 2.7541924392893486, Validation Loss Force: 3.768136101490945, time: 0.06859850883483887
Test Loss Energy: 9.55443810724178, Test Loss Force: 11.569099804198622, time: 9.549250364303589


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.655252045798866, Training Loss Force: 3.532657327539294, time: 0.9991226196289062
Validation Loss Energy: 2.337325960446975, Validation Loss Force: 3.8912257126720857, time: 0.06715703010559082
Test Loss Energy: 9.49088565163778, Test Loss Force: 11.4096464567791, time: 9.212705612182617


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.619039919269494, Training Loss Force: 3.5345486146189233, time: 1.0092227458953857
Validation Loss Energy: 3.0782282625289996, Validation Loss Force: 3.8220787041031086, time: 0.06991457939147949
Test Loss Energy: 10.983892748694103, Test Loss Force: 11.707683470760568, time: 9.39633059501648


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.570325020756537, Training Loss Force: 3.521985463382413, time: 0.9821634292602539
Validation Loss Energy: 2.6100372135577095, Validation Loss Force: 3.833275622106087, time: 0.06781744956970215
Test Loss Energy: 9.571474791732335, Test Loss Force: 11.581146540745745, time: 9.190337419509888


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6425723236730243, Training Loss Force: 3.5384559625115344, time: 1.0292468070983887
Validation Loss Energy: 2.2036581174848164, Validation Loss Force: 3.7900277072382624, time: 0.06789779663085938
Test Loss Energy: 9.599001062950592, Test Loss Force: 11.64295242153814, time: 9.164750099182129


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.632136910804772, Training Loss Force: 3.520659199260575, time: 1.0599725246429443
Validation Loss Energy: 3.4606350210204537, Validation Loss Force: 3.9172833468051396, time: 0.06715083122253418
Test Loss Energy: 11.058602015041254, Test Loss Force: 11.867332683090396, time: 9.731411695480347


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.640311272927954, Training Loss Force: 3.5127865664001576, time: 1.0751569271087646
Validation Loss Energy: 2.8595416656107036, Validation Loss Force: 3.7893686065663164, time: 0.07505083084106445
Test Loss Energy: 9.61788507596096, Test Loss Force: 11.671318726682, time: 11.281941413879395


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.7289489158767473, Training Loss Force: 3.51440610374437, time: 1.0549359321594238
Validation Loss Energy: 2.399684677433198, Validation Loss Force: 3.79012078294126, time: 0.07645750045776367
Test Loss Energy: 9.600082791993122, Test Loss Force: 11.672171281375276, time: 11.02845311164856


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.638638072319571, Training Loss Force: 3.519267003522915, time: 1.171739101409912
Validation Loss Energy: 3.06646598756451, Validation Loss Force: 3.889037988313345, time: 0.07148075103759766
Test Loss Energy: 10.854607666391225, Test Loss Force: 12.100729753338523, time: 10.21764063835144


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6063883360591102, Training Loss Force: 3.521338814225448, time: 1.0039341449737549
Validation Loss Energy: 2.801942167614968, Validation Loss Force: 3.7154934413527085, time: 0.07442402839660645
Test Loss Energy: 9.546841489353481, Test Loss Force: 11.595042004781133, time: 10.963205337524414


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6140465600806255, Training Loss Force: 3.5305828017097545, time: 1.0534381866455078
Validation Loss Energy: 2.0379917162416774, Validation Loss Force: 3.860328809990584, time: 0.07006406784057617
Test Loss Energy: 9.543237205676123, Test Loss Force: 11.595841435978912, time: 10.973305702209473


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6603973730883914, Training Loss Force: 3.5088718165541626, time: 1.1098999977111816
Validation Loss Energy: 3.2104110136223936, Validation Loss Force: 3.7926759113907815, time: 0.07639694213867188
Test Loss Energy: 10.775554071379903, Test Loss Force: 11.878090545335693, time: 10.970233917236328


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6752748468674166, Training Loss Force: 3.5155940220892687, time: 1.0383024215698242
Validation Loss Energy: 2.7356413160692394, Validation Loss Force: 3.764359410106864, time: 0.07473325729370117
Test Loss Energy: 9.492505423948526, Test Loss Force: 11.714913027305355, time: 11.212451219558716


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.657166463533888, Training Loss Force: 3.5275672509294598, time: 1.3455431461334229
Validation Loss Energy: 2.4897755005298516, Validation Loss Force: 3.8627414301867113, time: 0.08128833770751953
Test Loss Energy: 9.641362923659646, Test Loss Force: 11.694547711882025, time: 10.749034881591797

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‡â–â–â–ˆâ–â–â–ˆâ–â–â–ˆâ–‚â–â–‡â–â–â–‡â–â–‚
wandb:   test_error_force â–„â–‚â–„â–ƒâ–‚â–…â–ƒâ–â–„â–ƒâ–ƒâ–†â–„â–„â–ˆâ–ƒâ–ƒâ–†â–„â–„
wandb:          test_loss â–…â–ƒâ–ˆâ–â–‚â–ˆâ–‚â–‚â–ˆâ–‚â–‚â–ˆâ–‚â–â–‡â–â–‚â–†â–â–‚
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–â–‚â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–â–†â–„â–‚â–‡â–…â–‚â–†â–„â–‚â–ˆâ–…â–ƒâ–†â–…â–â–‡â–„â–ƒ
wandb:  valid_error_force â–ˆâ–ƒâ–‚â–ƒâ–„â–‚â–‚â–…â–ƒâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–…â–â–„â–ƒâ–‚â–„
wandb:         valid_loss â–„â–â–…â–„â–‚â–†â–„â–‚â–†â–ƒâ–‚â–ˆâ–„â–‚â–†â–„â–â–†â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1960
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.64136
wandb:   test_error_force 11.69455
wandb:          test_loss 10.4947
wandb: train_error_energy 2.65717
wandb:  train_error_force 3.52757
wandb:         train_loss 1.0279
wandb: valid_error_energy 2.48978
wandb:  valid_error_force 3.86274
wandb:         valid_loss 1.05547
wandb: 
wandb: ğŸš€ View run al_63_12 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/4ku13f0b
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_234630-4ku13f0b/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.207716226577759, Uncertainty Bias: -0.1544625163078308
0.00018119812 0.06522083
2.6166594 5.5169888
(48745, 22, 3)
Found uncertainty sample 0 after 1237 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 265 steps.
Found uncertainty sample 3 after 9 steps.
Found uncertainty sample 4 after 59 steps.
Found uncertainty sample 5 after 11 steps.
Found uncertainty sample 6 after 38 steps.
Found uncertainty sample 7 after 41 steps.
Found uncertainty sample 8 after 33 steps.
Found uncertainty sample 9 after 263 steps.
Found uncertainty sample 10 after 64 steps.
Found uncertainty sample 11 after 87 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 10 steps.
Found uncertainty sample 14 after 285 steps.
Found uncertainty sample 15 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 225 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 281 steps.
Found uncertainty sample 20 after 352 steps.
Found uncertainty sample 21 after 2 steps.
Found uncertainty sample 22 after 30 steps.
Found uncertainty sample 23 after 14 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 100 steps.
Found uncertainty sample 26 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 53 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 61 steps.
Found uncertainty sample 32 after 32 steps.
Found uncertainty sample 33 after 80 steps.
Found uncertainty sample 34 after 490 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 127 steps.
Found uncertainty sample 37 after 19 steps.
Found uncertainty sample 38 after 93 steps.
Found uncertainty sample 39 after 2371 steps.
Found uncertainty sample 40 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 86 steps.
Found uncertainty sample 45 after 6 steps.
Found uncertainty sample 46 after 25 steps.
Found uncertainty sample 47 after 154 steps.
Found uncertainty sample 48 after 50 steps.
Found uncertainty sample 49 after 3535 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1250 steps.
Found uncertainty sample 53 after 63 steps.
Found uncertainty sample 54 after 6 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 58 steps.
Found uncertainty sample 57 after 92 steps.
Found uncertainty sample 58 after 54 steps.
Found uncertainty sample 59 after 17 steps.
Found uncertainty sample 60 after 9 steps.
Found uncertainty sample 61 after 9 steps.
Found uncertainty sample 62 after 7 steps.
Found uncertainty sample 63 after 46 steps.
Found uncertainty sample 64 after 118 steps.
Found uncertainty sample 65 after 66 steps.
Found uncertainty sample 66 after 28 steps.
Found uncertainty sample 67 after 176 steps.
Found uncertainty sample 68 after 376 steps.
Found uncertainty sample 69 after 28 steps.
Found uncertainty sample 70 after 8 steps.
Found uncertainty sample 71 after 73 steps.
Found uncertainty sample 72 after 9 steps.
Found uncertainty sample 73 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 26 steps.
Found uncertainty sample 77 after 45 steps.
Found uncertainty sample 78 after 53 steps.
Found uncertainty sample 79 after 123 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 61 steps.
Found uncertainty sample 82 after 1081 steps.
Found uncertainty sample 83 after 27 steps.
Found uncertainty sample 84 after 40 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 261 steps.
Found uncertainty sample 87 after 190 steps.
Found uncertainty sample 88 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 763 steps.
Found uncertainty sample 94 after 17 steps.
Found uncertainty sample 95 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 187 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_235441-zh7wuc1b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_13
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zh7wuc1b
Training model 13. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.2480985099430515, Training Loss Force: 4.110317164918669, time: 1.1206259727478027
Validation Loss Energy: 5.835058551855582, Validation Loss Force: 4.856657450061553, time: 0.07279229164123535
Test Loss Energy: 9.873640478727241, Test Loss Force: 11.730840054679769, time: 8.978854417800903


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.2399291656958793, Training Loss Force: 4.484246897034773, time: 1.086867094039917
Validation Loss Energy: 1.300715024528222, Validation Loss Force: 4.262785032385095, time: 0.06746459007263184
Test Loss Energy: 10.020944605833773, Test Loss Force: 11.69496304696373, time: 8.99111533164978


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5656104145792407, Training Loss Force: 3.616949035516094, time: 1.0438892841339111
Validation Loss Energy: 3.383618884834388, Validation Loss Force: 3.851759165240407, time: 0.06779193878173828
Test Loss Energy: 10.87975660522685, Test Loss Force: 11.560490333148113, time: 9.228577613830566


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5506109224732256, Training Loss Force: 3.5288317907220237, time: 1.0899882316589355
Validation Loss Energy: 1.4301016078216275, Validation Loss Force: 3.9947975938706453, time: 0.06857061386108398
Test Loss Energy: 9.493353012303787, Test Loss Force: 11.61226493860548, time: 9.121111631393433


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.2054702559029096, Training Loss Force: 4.327551669535626, time: 1.0053184032440186
Validation Loss Energy: 3.5014395873500126, Validation Loss Force: 5.415420064815554, time: 0.07610154151916504
Test Loss Energy: 9.36713912275839, Test Loss Force: 12.44080871658573, time: 10.431172609329224


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.577495954044397, Training Loss Force: 4.1875269581737875, time: 1.1580204963684082
Validation Loss Energy: 1.6966253863726066, Validation Loss Force: 3.8951553777594192, time: 0.07768940925598145
Test Loss Energy: 10.101117058510452, Test Loss Force: 11.752528882863896, time: 11.377955436706543


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.599611505003133, Training Loss Force: 3.542329994348166, time: 1.1538424491882324
Validation Loss Energy: 3.287618979277676, Validation Loss Force: 3.7857286340378344, time: 0.0809016227722168
Test Loss Energy: 10.854541928863476, Test Loss Force: 11.844954480556934, time: 10.395221471786499


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.635188954240187, Training Loss Force: 3.512714583432591, time: 1.0933716297149658
Validation Loss Energy: 2.2591111962483805, Validation Loss Force: 3.8309801258807177, time: 0.07286667823791504
Test Loss Energy: 10.404579258174952, Test Loss Force: 11.807874373958343, time: 10.398612976074219


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5306063726497428, Training Loss Force: 3.5089282693157267, time: 1.270378589630127
Validation Loss Energy: 2.3200323612284937, Validation Loss Force: 3.85536651673901, time: 0.07626700401306152
Test Loss Energy: 9.449255936406994, Test Loss Force: 11.583908771319695, time: 10.827764987945557


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.60500333388856, Training Loss Force: 3.5111364655331756, time: 1.175611972808838
Validation Loss Energy: 3.8723658586811482, Validation Loss Force: 3.794350160699013, time: 0.07866668701171875
Test Loss Energy: 9.60684388063425, Test Loss Force: 11.552028998867423, time: 10.858649253845215


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5590335302221487, Training Loss Force: 3.5115326967141565, time: 1.1254425048828125
Validation Loss Energy: 2.730476059891153, Validation Loss Force: 3.914456784666732, time: 0.0787045955657959
Test Loss Energy: 9.46684918650812, Test Loss Force: 11.631315867205567, time: 11.355938911437988


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.591895099301879, Training Loss Force: 3.5890563438432994, time: 1.137655258178711
Validation Loss Energy: 1.7702032882764012, Validation Loss Force: 3.7637302584350447, time: 0.07890582084655762
Test Loss Energy: 10.061981349631534, Test Loss Force: 11.622321325424295, time: 10.935234546661377


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5979474762087, Training Loss Force: 3.499240839155709, time: 1.1941375732421875
Validation Loss Energy: 3.2919773938054933, Validation Loss Force: 3.740161620015172, time: 0.08277225494384766
Test Loss Energy: 10.861613122179033, Test Loss Force: 11.803613302570215, time: 11.406275987625122


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6151088938836398, Training Loss Force: 3.5029926121495216, time: 1.16546630859375
Validation Loss Energy: 2.213331310522211, Validation Loss Force: 3.8188393133681506, time: 0.08213257789611816
Test Loss Energy: 10.408914964530211, Test Loss Force: 11.877786549490189, time: 11.227231979370117


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6002736785092933, Training Loss Force: 3.5195999005264245, time: 1.2345507144927979
Validation Loss Energy: 2.3442008703274957, Validation Loss Force: 3.7479145143380856, time: 0.07900738716125488
Test Loss Energy: 9.557106252523832, Test Loss Force: 11.63951711459736, time: 11.206421136856079


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5932702234436675, Training Loss Force: 3.51091580383524, time: 1.2034330368041992
Validation Loss Energy: 4.04614420824501, Validation Loss Force: 3.786831068979323, time: 0.08582496643066406
Test Loss Energy: 9.707332106597999, Test Loss Force: 11.570883876028079, time: 11.477123498916626


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5961820370338526, Training Loss Force: 3.5013855643895315, time: 1.2212133407592773
Validation Loss Energy: 2.5945945516147524, Validation Loss Force: 3.87364308890219, time: 0.07677698135375977
Test Loss Energy: 9.584711714683161, Test Loss Force: 11.709405596266787, time: 11.281354904174805


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.7272254038102512, Training Loss Force: 3.906698046135158, time: 1.1874175071716309
Validation Loss Energy: 2.049530208714509, Validation Loss Force: 3.8889711875695667, time: 0.0819404125213623
Test Loss Energy: 10.5807220288084, Test Loss Force: 11.925992097295916, time: 11.481644630432129


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6234994972249344, Training Loss Force: 3.5062226062465203, time: 1.1169171333312988
Validation Loss Energy: 3.2397109331893015, Validation Loss Force: 3.8181203146959497, time: 0.08038616180419922
Test Loss Energy: 11.097327704918607, Test Loss Force: 11.860791302445298, time: 11.376474142074585


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6496022595425095, Training Loss Force: 3.515430328275589, time: 1.1328275203704834
Validation Loss Energy: 2.0081937971299015, Validation Loss Force: 3.8303887166291335, time: 0.07789778709411621
Test Loss Energy: 10.326755300477794, Test Loss Force: 11.833547748762337, time: 11.373635053634644

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.039 MB of 0.048 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–„â–‡â–‚â–â–„â–‡â–…â–â–‚â–â–„â–‡â–…â–‚â–‚â–‚â–†â–ˆâ–…
wandb:   test_error_force â–‚â–‚â–â–â–ˆâ–ƒâ–ƒâ–ƒâ–â–â–‚â–‚â–ƒâ–„â–‚â–â–‚â–„â–ƒâ–ƒ
wandb:          test_loss â–„â–ˆâ–ˆâ–ƒâ–„â–†â–‡â–…â–‚â–â–‚â–„â–†â–„â–‚â–â–‚â–†â–†â–„
wandb: train_error_energy â–ˆâ–â–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–„â–„â–„
wandb:  train_error_force â–…â–ˆâ–‚â–â–‡â–†â–â–â–â–â–â–‚â–â–â–â–â–â–„â–â–
wandb:         train_loss â–ˆâ–ƒâ–‚â–â–†â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–‚â–â–
wandb: valid_error_energy â–ˆâ–â–„â–â–„â–‚â–„â–‚â–ƒâ–…â–ƒâ–‚â–„â–‚â–ƒâ–…â–ƒâ–‚â–„â–‚
wandb:  valid_error_force â–†â–ƒâ–â–‚â–ˆâ–‚â–â–â–â–â–‚â–â–â–â–â–â–‚â–‚â–â–
wandb:         valid_loss â–ˆâ–â–ƒâ–â–„â–â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–ƒâ–‚â–â–ƒâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 2050
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.32676
wandb:   test_error_force 11.83355
wandb:          test_loss 11.83262
wandb: train_error_energy 2.6496
wandb:  train_error_force 3.51543
wandb:         train_loss 1.01712
wandb: valid_error_energy 2.00819
wandb:  valid_error_force 3.83039
wandb:         valid_loss 0.91557
wandb: 
wandb: ğŸš€ View run al_63_13 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zh7wuc1b
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_235441-zh7wuc1b/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.369013547897339, Uncertainty Bias: -0.1641881763935089
0.0 0.010859489
2.6394644 5.5084662
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 50 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 5 steps.
Found uncertainty sample 6 after 118 steps.
Found uncertainty sample 7 after 103 steps.
Found uncertainty sample 8 after 43 steps.
Found uncertainty sample 9 after 59 steps.
Found uncertainty sample 10 after 5 steps.
Found uncertainty sample 11 after 52 steps.
Found uncertainty sample 12 after 88 steps.
Found uncertainty sample 13 after 614 steps.
Found uncertainty sample 14 after 493 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 279 steps.
Found uncertainty sample 18 after 574 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 587 steps.
Found uncertainty sample 21 after 111 steps.
Found uncertainty sample 22 after 606 steps.
Found uncertainty sample 23 after 17 steps.
Found uncertainty sample 24 after 17 steps.
Found uncertainty sample 25 after 111 steps.
Found uncertainty sample 26 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 116 steps.
Found uncertainty sample 31 after 216 steps.
Found uncertainty sample 32 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 182 steps.
Found uncertainty sample 36 after 561 steps.
Found uncertainty sample 37 after 38 steps.
Found uncertainty sample 38 after 18 steps.
Found uncertainty sample 39 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 43 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 120 steps.
Found uncertainty sample 44 after 54 steps.
Found uncertainty sample 45 after 30 steps.
Found uncertainty sample 46 after 691 steps.
Found uncertainty sample 47 after 648 steps.
Found uncertainty sample 48 after 124 steps.
Found uncertainty sample 49 after 17 steps.
Found uncertainty sample 50 after 21 steps.
Found uncertainty sample 51 after 238 steps.
Found uncertainty sample 52 after 294 steps.
Found uncertainty sample 53 after 30 steps.
Found uncertainty sample 54 after 90 steps.
Found uncertainty sample 55 after 59 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 5 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 165 steps.
Found uncertainty sample 60 after 38 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 51 steps.
Found uncertainty sample 65 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 14 steps.
Found uncertainty sample 68 after 58 steps.
Found uncertainty sample 69 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 17 steps.
Found uncertainty sample 72 after 249 steps.
Found uncertainty sample 73 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 4 steps.
Found uncertainty sample 77 after 78 steps.
Found uncertainty sample 78 after 179 steps.
Found uncertainty sample 79 after 324 steps.
Found uncertainty sample 80 after 52 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1269 steps.
Found uncertainty sample 83 after 826 steps.
Found uncertainty sample 84 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 312 steps.
Found uncertainty sample 88 after 212 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 494 steps.
Found uncertainty sample 92 after 13 steps.
Found uncertainty sample 93 after 98 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 21 steps.
Found uncertainty sample 98 after 39 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_000245-g59q49h5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_14
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/g59q49h5
Training model 14. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.841945962140291, Training Loss Force: 3.858017530843213, time: 1.1566438674926758
Validation Loss Energy: 5.441074890463624, Validation Loss Force: 3.8997198725817928, time: 0.07666993141174316
Test Loss Energy: 9.842935471383052, Test Loss Force: 11.443181586636419, time: 10.663825750350952


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.155550792100337, Training Loss Force: 3.6140590410479922, time: 1.2811555862426758
Validation Loss Energy: 3.2000761592178746, Validation Loss Force: 3.8011524804379606, time: 0.0836334228515625
Test Loss Energy: 10.953382435789953, Test Loss Force: 11.507256160584369, time: 9.804078102111816


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.171624142796903, Training Loss Force: 3.5704463608849366, time: 1.1617693901062012
Validation Loss Energy: 1.7248838823440105, Validation Loss Force: 3.8012237345646573, time: 0.06885695457458496
Test Loss Energy: 10.189536197074379, Test Loss Force: 11.542555576193942, time: 9.436188697814941


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.112395269972682, Training Loss Force: 3.5601505550047463, time: 1.1688194274902344
Validation Loss Energy: 5.921005947484811, Validation Loss Force: 3.7815724030945193, time: 0.07693886756896973
Test Loss Energy: 9.92701304191389, Test Loss Force: 11.221527867972798, time: 9.787608861923218


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.996925860582293, Training Loss Force: 3.575897766856061, time: 1.0789713859558105
Validation Loss Energy: 5.372199315583533, Validation Loss Force: 3.836201049345286, time: 0.07192206382751465
Test Loss Energy: 12.431676287810197, Test Loss Force: 11.647886480691747, time: 10.303050756454468


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.143892558463427, Training Loss Force: 3.5518836206499085, time: 1.110501766204834
Validation Loss Energy: 3.877150350619698, Validation Loss Force: 3.789792791289309, time: 0.06901001930236816
Test Loss Energy: 9.567969191289867, Test Loss Force: 11.275167256413074, time: 10.034687757492065


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.172783333444953, Training Loss Force: 3.5745943381144616, time: 1.1478593349456787
Validation Loss Energy: 2.6939588766318603, Validation Loss Force: 3.8958523321861733, time: 0.08315443992614746
Test Loss Energy: 9.555548906848827, Test Loss Force: 11.370234204479011, time: 11.146191358566284


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.095006110794686, Training Loss Force: 3.560027585138053, time: 1.2134993076324463
Validation Loss Energy: 4.954738532790678, Validation Loss Force: 3.7829243388140203, time: 0.09046745300292969
Test Loss Energy: 11.918003451140395, Test Loss Force: 11.649264124205342, time: 11.158476829528809


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.144191518379196, Training Loss Force: 3.560096140647638, time: 1.1887547969818115
Validation Loss Energy: 5.757403800558995, Validation Loss Force: 3.8363157239554395, time: 0.07617402076721191
Test Loss Energy: 10.112678045230252, Test Loss Force: 11.285810053502862, time: 10.364060640335083


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.205536813245871, Training Loss Force: 3.5695355909033704, time: 1.1531670093536377
Validation Loss Energy: 3.083658717659887, Validation Loss Force: 3.8110178369482917, time: 0.08209371566772461
Test Loss Energy: 10.797519531586534, Test Loss Force: 11.556252274791088, time: 10.230290174484253


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.158088858111125, Training Loss Force: 3.5622236379875356, time: 1.151576280593872
Validation Loss Energy: 1.8935024867911463, Validation Loss Force: 3.8147771601389904, time: 0.0791473388671875
Test Loss Energy: 10.458602038583855, Test Loss Force: 11.654397494394892, time: 10.414389848709106


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.158324405509895, Training Loss Force: 3.548323280590168, time: 1.1493628025054932
Validation Loss Energy: 5.503255249463958, Validation Loss Force: 3.8462619093657415, time: 0.07430791854858398
Test Loss Energy: 9.978790314737777, Test Loss Force: 11.465559929066725, time: 10.264904022216797


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.091152384194664, Training Loss Force: 3.600820107393212, time: 1.0905563831329346
Validation Loss Energy: 5.2210281190674275, Validation Loss Force: 3.7975076091882687, time: 0.07173943519592285
Test Loss Energy: 12.42610878972935, Test Loss Force: 11.946086567762913, time: 10.395647048950195


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.109070128352602, Training Loss Force: 3.582436286802066, time: 1.1549949645996094
Validation Loss Energy: 3.355945897382201, Validation Loss Force: 3.837901573612737, time: 0.08056473731994629
Test Loss Energy: 9.731561349580906, Test Loss Force: 11.594536646079282, time: 10.64374041557312


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.057811489893109, Training Loss Force: 3.5589642834681063, time: 1.100022792816162
Validation Loss Energy: 2.6360931776721683, Validation Loss Force: 3.714769432624724, time: 0.07818603515625
Test Loss Energy: 9.778886997523266, Test Loss Force: 11.593718809119855, time: 10.185770988464355


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.104692271817267, Training Loss Force: 3.549649464789112, time: 1.1304490566253662
Validation Loss Energy: 4.612170513208321, Validation Loss Force: 3.835449763646926, time: 0.07301807403564453
Test Loss Energy: 12.09658542586806, Test Loss Force: 12.09313388156673, time: 10.361384153366089


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.070592547653751, Training Loss Force: 3.5491253813816797, time: 1.1286492347717285
Validation Loss Energy: 5.819607762488799, Validation Loss Force: 3.7955505700647096, time: 0.07516193389892578
Test Loss Energy: 10.267061358425455, Test Loss Force: 11.558380096486347, time: 10.420419216156006


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.132643216831972, Training Loss Force: 3.5514539556071902, time: 1.1988930702209473
Validation Loss Energy: 3.2870628630857213, Validation Loss Force: 3.7869746905784467, time: 0.07805609703063965
Test Loss Energy: 11.344495152769232, Test Loss Force: 11.96984599050599, time: 10.48065185546875


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.133427582192184, Training Loss Force: 3.532343100314071, time: 1.2013678550720215
Validation Loss Energy: 2.0485402216917143, Validation Loss Force: 3.796362867109054, time: 0.08401155471801758
Test Loss Energy: 10.579598885636344, Test Loss Force: 11.944028689827237, time: 10.643612623214722


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.099258692671497, Training Loss Force: 3.5493927837619985, time: 1.1403937339782715
Validation Loss Energy: 5.6435372567367565, Validation Loss Force: 3.85615198215127, time: 0.07628273963928223
Test Loss Energy: 10.151661420433987, Test Loss Force: 11.656248516191427, time: 10.38089895248413

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–ƒâ–‚â–ˆâ–â–â–‡â–‚â–„â–ƒâ–‚â–ˆâ–â–‚â–‡â–ƒâ–…â–ƒâ–‚
wandb:   test_error_force â–ƒâ–ƒâ–„â–â–„â–â–‚â–„â–‚â–„â–„â–ƒâ–‡â–„â–„â–ˆâ–„â–‡â–‡â–„
wandb:          test_loss â–ƒâ–†â–„â–‚â–ˆâ–â–‚â–†â–â–„â–„â–â–‡â–â–‚â–†â–â–…â–„â–
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–ƒâ–â–ˆâ–‡â–…â–ƒâ–†â–ˆâ–ƒâ–â–‡â–‡â–„â–ƒâ–†â–ˆâ–„â–‚â–ˆ
wandb:  valid_error_force â–ˆâ–„â–„â–„â–†â–„â–ˆâ–„â–†â–…â–…â–†â–„â–†â–â–†â–„â–„â–„â–†
wandb:         valid_loss â–ˆâ–ƒâ–â–ˆâ–‡â–„â–‚â–†â–‡â–ƒâ–â–‡â–†â–ƒâ–‚â–…â–‡â–ƒâ–‚â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 2140
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.15166
wandb:   test_error_force 11.65625
wandb:          test_loss 8.16926
wandb: train_error_energy 4.09926
wandb:  train_error_force 3.54939
wandb:         train_loss 1.45728
wandb: valid_error_energy 5.64354
wandb:  valid_error_force 3.85615
wandb:         valid_loss 1.98431
wandb: 
wandb: ğŸš€ View run al_63_14 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/g59q49h5
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_000245-g59q49h5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.6431593894958496, Uncertainty Bias: -0.22183680534362793
0.000120162964 0.29038745
2.4886804 4.981978
(48745, 22, 3)
Found uncertainty sample 0 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 621 steps.
Found uncertainty sample 3 after 1117 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 9 steps.
Found uncertainty sample 6 after 4 steps.
Found uncertainty sample 7 after 146 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 66 steps.
Found uncertainty sample 10 after 167 steps.
Found uncertainty sample 11 after 137 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 241 steps.
Found uncertainty sample 14 after 85 steps.
Found uncertainty sample 15 after 771 steps.
Found uncertainty sample 16 after 34 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 859 steps.
Found uncertainty sample 21 after 6 steps.
Found uncertainty sample 22 after 56 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 9 steps.
Found uncertainty sample 25 after 380 steps.
Found uncertainty sample 26 after 33 steps.
Found uncertainty sample 27 after 80 steps.
Found uncertainty sample 28 after 19 steps.
Found uncertainty sample 29 after 4 steps.
Found uncertainty sample 30 after 296 steps.
Found uncertainty sample 31 after 10 steps.
Found uncertainty sample 32 after 2 steps.
Found uncertainty sample 33 after 97 steps.
Found uncertainty sample 34 after 5 steps.
Found uncertainty sample 35 after 7 steps.
Found uncertainty sample 36 after 174 steps.
Found uncertainty sample 37 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 30 steps.
Found uncertainty sample 42 after 336 steps.
Found uncertainty sample 43 after 55 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 405 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 127 steps.
Found uncertainty sample 50 after 8 steps.
Found uncertainty sample 51 after 45 steps.
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 473 steps.
Found uncertainty sample 55 after 33 steps.
Found uncertainty sample 56 after 50 steps.
Found uncertainty sample 57 after 8 steps.
Found uncertainty sample 58 after 61 steps.
Found uncertainty sample 59 after 208 steps.
Found uncertainty sample 60 after 19 steps.
Found uncertainty sample 61 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 253 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 679 steps.
Found uncertainty sample 67 after 115 steps.
Found uncertainty sample 68 after 606 steps.
Found uncertainty sample 69 after 161 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 206 steps.
Found uncertainty sample 74 after 7 steps.
Found uncertainty sample 75 after 18 steps.
Found uncertainty sample 76 after 11 steps.
Found uncertainty sample 77 after 21 steps.
Found uncertainty sample 78 after 250 steps.
Found uncertainty sample 79 after 168 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 360 steps.
Found uncertainty sample 82 after 42 steps.
Found uncertainty sample 83 after 33 steps.
Found uncertainty sample 84 after 126 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 24 steps.
Found uncertainty sample 87 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 583 steps.
Found uncertainty sample 91 after 20 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 17 steps.
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 50 steps.
Found uncertainty sample 99 after 323 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_001035-5qnwfzyp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_15
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5qnwfzyp
Training model 15. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.171071361562054, Training Loss Force: 3.867665557946673, time: 1.1348273754119873
Validation Loss Energy: 3.33984046072639, Validation Loss Force: 3.774332204480105, time: 0.08015155792236328
Test Loss Energy: 9.81007617386042, Test Loss Force: 11.53177303148185, time: 10.762303113937378


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6449885221725875, Training Loss Force: 3.5313281041752904, time: 1.1809475421905518
Validation Loss Energy: 1.8930720116647815, Validation Loss Force: 3.7614630411912082, time: 0.0774834156036377
Test Loss Energy: 10.669149547193378, Test Loss Force: 11.872053026496198, time: 10.604394912719727


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5795806295693167, Training Loss Force: 3.518347377282308, time: 1.1489598751068115
Validation Loss Energy: 2.287843309493173, Validation Loss Force: 3.785629323598751, time: 0.08667778968811035
Test Loss Energy: 10.829791472202878, Test Loss Force: 11.908235191201568, time: 10.861122369766235


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.605071421219557, Training Loss Force: 3.5128563147908882, time: 1.1517009735107422
Validation Loss Energy: 3.638374432021224, Validation Loss Force: 3.7774388844040896, time: 0.0885767936706543
Test Loss Energy: 9.754734856355201, Test Loss Force: 11.697576674445049, time: 10.60362458229065


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6520383354862465, Training Loss Force: 3.5174932502484073, time: 1.148787498474121
Validation Loss Energy: 1.6885979409927638, Validation Loss Force: 3.736841845718535, time: 0.08212089538574219
Test Loss Energy: 10.476140090052931, Test Loss Force: 12.01892047492562, time: 10.765554666519165


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6505872356531013, Training Loss Force: 3.5000845056335788, time: 1.1843509674072266
Validation Loss Energy: 2.2337125514527005, Validation Loss Force: 3.7442585096310976, time: 0.07680416107177734
Test Loss Energy: 10.760175197152414, Test Loss Force: 12.018570210530038, time: 10.566478967666626


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.63910118812611, Training Loss Force: 3.5111009821036614, time: 1.1959586143493652
Validation Loss Energy: 3.84165577541308, Validation Loss Force: 3.8112238059082744, time: 0.08248019218444824
Test Loss Energy: 9.758711111755348, Test Loss Force: 11.685658843514698, time: 10.643479585647583


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.684273839186396, Training Loss Force: 3.4999005204802747, time: 1.1831254959106445
Validation Loss Energy: 1.9758662451858138, Validation Loss Force: 3.7792500752370657, time: 0.07801032066345215
Test Loss Energy: 10.535762670724328, Test Loss Force: 12.014648015934261, time: 10.800689458847046


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.634890958324414, Training Loss Force: 3.5129458914073064, time: 1.1995875835418701
Validation Loss Energy: 2.1809967445000265, Validation Loss Force: 3.7206316432295647, time: 0.0843355655670166
Test Loss Energy: 10.684682135411597, Test Loss Force: 11.978397409028792, time: 10.621041774749756


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6547853586152357, Training Loss Force: 3.5069232572257887, time: 1.1476097106933594
Validation Loss Energy: 3.7335035898827855, Validation Loss Force: 3.7724536562304847, time: 0.08303546905517578
Test Loss Energy: 9.869034169903523, Test Loss Force: 11.646563508865423, time: 10.734429597854614


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.576828093788473, Training Loss Force: 3.507494915724129, time: 1.1577112674713135
Validation Loss Energy: 1.7291186598285515, Validation Loss Force: 3.8173142523876917, time: 0.08595633506774902
Test Loss Energy: 10.572579216329022, Test Loss Force: 12.127021686708604, time: 11.109277248382568


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6614254813768077, Training Loss Force: 3.50292448835951, time: 1.1168878078460693
Validation Loss Energy: 1.9050749363087323, Validation Loss Force: 3.8062678587273595, time: 0.07915949821472168
Test Loss Energy: 10.546497272787427, Test Loss Force: 12.042054997112267, time: 10.709519863128662


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.675086977830621, Training Loss Force: 3.5235542439617937, time: 1.111260175704956
Validation Loss Energy: 3.76944049734779, Validation Loss Force: 3.811429774868258, time: 0.07862687110900879
Test Loss Energy: 9.845907873651395, Test Loss Force: 11.793767917500405, time: 10.959653615951538


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.683391864194808, Training Loss Force: 3.5060618711777294, time: 1.185903787612915
Validation Loss Energy: 1.891723304647897, Validation Loss Force: 3.780690967280364, time: 0.07810091972351074
Test Loss Energy: 10.451895854791402, Test Loss Force: 12.14411870404544, time: 10.67296051979065


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6873393682617186, Training Loss Force: 3.506761551711028, time: 1.2348108291625977
Validation Loss Energy: 2.2347523050296103, Validation Loss Force: 3.7294512340960457, time: 0.07441997528076172
Test Loss Energy: 10.701160057911107, Test Loss Force: 12.237015343744105, time: 10.552445411682129


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.617796649582031, Training Loss Force: 3.5014373224867, time: 1.4431555271148682
Validation Loss Energy: 3.812714687174834, Validation Loss Force: 3.7307771175254727, time: 0.08253288269042969
Test Loss Energy: 9.797466851475138, Test Loss Force: 11.919695783281728, time: 10.592408180236816


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.662503742401518, Training Loss Force: 3.5129964309414565, time: 1.23136305809021
Validation Loss Energy: 1.7377566083392417, Validation Loss Force: 3.789886980338036, time: 0.07696080207824707
Test Loss Energy: 10.27353381189472, Test Loss Force: 12.14671489105334, time: 10.62514877319336


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6215349957486906, Training Loss Force: 3.5137849596449677, time: 1.2227756977081299
Validation Loss Energy: 2.030517158351173, Validation Loss Force: 3.7377904162801876, time: 0.08367133140563965
Test Loss Energy: 10.354128813667106, Test Loss Force: 12.167349428629587, time: 10.869535446166992


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6270776936245577, Training Loss Force: 3.520535133450927, time: 1.2152338027954102
Validation Loss Energy: 3.960329300757607, Validation Loss Force: 3.825990555658768, time: 0.07911491394042969
Test Loss Energy: 9.883997322001374, Test Loss Force: 11.95004603087836, time: 10.619193077087402


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6445571929509994, Training Loss Force: 3.5235399835934906, time: 1.219733476638794
Validation Loss Energy: 1.6663626454187068, Validation Loss Force: 3.765453717624526, time: 0.08402466773986816
Test Loss Energy: 10.43244427561517, Test Loss Force: 12.372737732765403, time: 11.110198736190796

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.039 MB of 0.048 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‡â–ˆâ–â–†â–ˆâ–â–†â–‡â–‚â–†â–†â–‚â–†â–‡â–â–„â–…â–‚â–…
wandb:   test_error_force â–â–„â–„â–‚â–…â–…â–‚â–…â–…â–‚â–†â–…â–ƒâ–†â–‡â–„â–†â–†â–„â–ˆ
wandb:          test_loss â–â–†â–‡â–ƒâ–‡â–ˆâ–ƒâ–‡â–ˆâ–„â–ˆâ–ˆâ–„â–‡â–ˆâ–„â–‡â–‡â–„â–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–‚â–ƒâ–‡â–â–ƒâ–ˆâ–‚â–ƒâ–‡â–â–‚â–‡â–‚â–ƒâ–ˆâ–â–‚â–ˆâ–
wandb:  valid_error_force â–…â–„â–…â–…â–‚â–ƒâ–‡â–…â–â–„â–‡â–‡â–‡â–…â–‚â–‚â–†â–‚â–ˆâ–„
wandb:         valid_loss â–…â–‚â–ƒâ–†â–â–‚â–‡â–‚â–‚â–‡â–â–‚â–‡â–â–‚â–‡â–â–‚â–ˆâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 2230
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.43244
wandb:   test_error_force 12.37274
wandb:          test_loss 11.67162
wandb: train_error_energy 2.64456
wandb:  train_error_force 3.52354
wandb:         train_loss 1.03534
wandb: valid_error_energy 1.66636
wandb:  valid_error_force 3.76545
wandb:         valid_loss 0.82662
wandb: 
wandb: ğŸš€ View run al_63_15 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5qnwfzyp
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_001035-5qnwfzyp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.3998706340789795, Uncertainty Bias: -0.17975205183029175
0.00012207031 0.0042419434
2.5319245 5.5143747
(48745, 22, 3)
Found uncertainty sample 0 after 24 steps.
Found uncertainty sample 1 after 84 steps.
Found uncertainty sample 2 after 251 steps.
Found uncertainty sample 3 after 16 steps.
Found uncertainty sample 4 after 35 steps.
Found uncertainty sample 5 after 436 steps.
Found uncertainty sample 6 after 10 steps.
Found uncertainty sample 7 after 58 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 10 steps.
Found uncertainty sample 11 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 61 steps.
Found uncertainty sample 14 after 130 steps.
Found uncertainty sample 15 after 652 steps.
Found uncertainty sample 16 after 432 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 16 steps.
Found uncertainty sample 19 after 6 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 33 steps.
Found uncertainty sample 22 after 9 steps.
Found uncertainty sample 23 after 19 steps.
Found uncertainty sample 24 after 55 steps.
Found uncertainty sample 25 after 2604 steps.
Found uncertainty sample 26 after 131 steps.
Found uncertainty sample 27 after 218 steps.
Found uncertainty sample 28 after 16 steps.
Found uncertainty sample 29 after 42 steps.
Found uncertainty sample 30 after 66 steps.
Found uncertainty sample 31 after 131 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 103 steps.
Found uncertainty sample 37 after 29 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 49 steps.
Found uncertainty sample 40 after 274 steps.
Found uncertainty sample 41 after 54 steps.
Found uncertainty sample 42 after 250 steps.
Found uncertainty sample 43 after 189 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 561 steps.
Found uncertainty sample 46 after 450 steps.
Found uncertainty sample 47 after 868 steps.
Found uncertainty sample 48 after 210 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 944 steps.
Found uncertainty sample 51 after 24 steps.
Found uncertainty sample 52 after 8 steps.
Found uncertainty sample 53 after 17 steps.
Found uncertainty sample 54 after 925 steps.
Found uncertainty sample 55 after 70 steps.
Found uncertainty sample 56 after 58 steps.
Found uncertainty sample 57 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 401 steps.
Found uncertainty sample 60 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 16 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 161 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1973 steps.
Found uncertainty sample 67 after 316 steps.
Found uncertainty sample 68 after 51 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 18 steps.
Found uncertainty sample 71 after 69 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 106 steps.
Found uncertainty sample 74 after 435 steps.
Found uncertainty sample 75 after 85 steps.
Found uncertainty sample 76 after 217 steps.
Found uncertainty sample 77 after 187 steps.
Found uncertainty sample 78 after 504 steps.
Found uncertainty sample 79 after 38 steps.
Found uncertainty sample 80 after 49 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 232 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 40 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1296 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 356 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 169 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 11 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 91 steps.
Found uncertainty sample 99 after 63 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_001907-42jku0jc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_16
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/42jku0jc
Training model 16. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.199044790811953, Training Loss Force: 4.047837268520678, time: 1.222003698348999
Validation Loss Energy: 2.3575612417523093, Validation Loss Force: 3.793094752054272, time: 0.08102250099182129
Test Loss Energy: 9.937108660471443, Test Loss Force: 12.02703946763542, time: 10.834718227386475


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.56856514003338, Training Loss Force: 3.5050842847347545, time: 1.278134822845459
Validation Loss Energy: 2.333070196395431, Validation Loss Force: 3.739634784815907, time: 0.07783150672912598
Test Loss Energy: 10.767638591274634, Test Loss Force: 12.253361488078278, time: 10.758360624313354


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.593193807123491, Training Loss Force: 3.5031093649737515, time: 1.2719557285308838
Validation Loss Energy: 3.375517401963524, Validation Loss Force: 3.7554332720540566, time: 0.09264445304870605
Test Loss Energy: 11.413405102635906, Test Loss Force: 12.458521401287182, time: 10.981367349624634


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6124962053660425, Training Loss Force: 3.5077594318539314, time: 1.2677383422851562
Validation Loss Energy: 1.7746039542746586, Validation Loss Force: 3.797103910026702, time: 0.08870768547058105
Test Loss Energy: 10.42755138343999, Test Loss Force: 12.346011526943956, time: 10.818971872329712


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.637404953638583, Training Loss Force: 3.5076455747140676, time: 1.2621593475341797
Validation Loss Energy: 2.6272307229950966, Validation Loss Force: 3.720226830722268, time: 0.08083438873291016
Test Loss Energy: 9.917498233090253, Test Loss Force: 12.120362974579091, time: 10.718195915222168


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.601853291248394, Training Loss Force: 3.5030358628037592, time: 1.2838661670684814
Validation Loss Energy: 3.62166222194276, Validation Loss Force: 3.810078395821622, time: 0.08053398132324219
Test Loss Energy: 9.840151263014656, Test Loss Force: 11.951989142105134, time: 10.73329496383667


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6210449512442553, Training Loss Force: 3.504415481178518, time: 1.2613556385040283
Validation Loss Energy: 2.417848022335674, Validation Loss Force: 3.7366952180859414, time: 0.08940696716308594
Test Loss Energy: 9.813221996921806, Test Loss Force: 12.054041797187505, time: 10.647571802139282


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.604605849448665, Training Loss Force: 3.5048284561876577, time: 1.2662434577941895
Validation Loss Energy: 2.327080396369973, Validation Loss Force: 3.741655395980629, time: 0.08987712860107422
Test Loss Energy: 10.8617880025625, Test Loss Force: 12.545724831599538, time: 10.670047521591187


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.616989334727146, Training Loss Force: 3.505686299748262, time: 1.3170678615570068
Validation Loss Energy: 3.2019962352632096, Validation Loss Force: 3.783199557878387, time: 0.08805584907531738
Test Loss Energy: 11.576262965700481, Test Loss Force: 12.875763878549884, time: 10.530771255493164


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.645318201774318, Training Loss Force: 3.5118940318623375, time: 1.2214100360870361
Validation Loss Energy: 1.7413812723639408, Validation Loss Force: 3.770343977407392, time: 0.08205962181091309
Test Loss Energy: 10.652075132523562, Test Loss Force: 12.554749334621915, time: 10.684783458709717


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6295374030070904, Training Loss Force: 3.5068728487221907, time: 1.295729160308838
Validation Loss Energy: 2.8080407415182114, Validation Loss Force: 3.7524302446175017, time: 0.07911562919616699
Test Loss Energy: 9.8895087167839, Test Loss Force: 12.251354998917215, time: 10.804094791412354


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6505571156857735, Training Loss Force: 3.5082001947523778, time: 1.224052906036377
Validation Loss Energy: 3.714289023942385, Validation Loss Force: 3.7459276375489723, time: 0.08295154571533203
Test Loss Energy: 9.973324619591482, Test Loss Force: 12.217697226158586, time: 10.900394916534424


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6630476651700357, Training Loss Force: 3.5099232086035665, time: 1.2387299537658691
Validation Loss Energy: 2.613384042155577, Validation Loss Force: 3.7389862609283924, time: 0.07594609260559082
Test Loss Energy: 9.888746241669505, Test Loss Force: 12.274099894399159, time: 11.143907308578491


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6626803754640904, Training Loss Force: 3.511070497651779, time: 1.2906379699707031
Validation Loss Energy: 2.080094109688276, Validation Loss Force: 3.748593098560301, time: 0.07482290267944336
Test Loss Energy: 10.85417621519291, Test Loss Force: 12.642154398257839, time: 10.702691078186035


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6685458132464155, Training Loss Force: 3.514445478645295, time: 1.263685703277588
Validation Loss Energy: 3.4487728789440153, Validation Loss Force: 3.7855224424087055, time: 0.08552336692810059
Test Loss Energy: 11.401831156804976, Test Loss Force: 12.773083408393475, time: 10.87356162071228


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6484053996731656, Training Loss Force: 3.503710525913822, time: 1.2864329814910889
Validation Loss Energy: 1.5403661442047656, Validation Loss Force: 3.7755025535111013, time: 0.08326363563537598
Test Loss Energy: 10.421849388531777, Test Loss Force: 12.66211088846602, time: 9.49258804321289


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6115996585525867, Training Loss Force: 3.5000069005122283, time: 1.1722025871276855
Validation Loss Energy: 2.890150434417375, Validation Loss Force: 3.729335668809309, time: 0.09000778198242188
Test Loss Energy: 9.945242412298457, Test Loss Force: 12.44730131986399, time: 11.62935185432434


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.665754612798539, Training Loss Force: 3.5165856677167744, time: 1.302032470703125
Validation Loss Energy: 3.8471686181279594, Validation Loss Force: 3.7765900696198127, time: 0.08398175239562988
Test Loss Energy: 9.977392437477082, Test Loss Force: 12.292623239351864, time: 9.523646116256714


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6752102870578542, Training Loss Force: 3.515079826438897, time: 1.2045750617980957
Validation Loss Energy: 2.181095498266245, Validation Loss Force: 3.8024057737258152, time: 0.07516312599182129
Test Loss Energy: 9.868782164389069, Test Loss Force: 12.426908253569723, time: 9.05870270729065


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6380165037719556, Training Loss Force: 3.509305240718371, time: 1.197256326675415
Validation Loss Energy: 2.2072184857936286, Validation Loss Force: 3.788370380785122, time: 0.07666969299316406
Test Loss Energy: 10.825720912339104, Test Loss Force: 12.891969822928617, time: 8.978026628494263

wandb: - 0.039 MB of 0.040 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–…â–‡â–ƒâ–â–â–â–…â–ˆâ–„â–â–‚â–â–…â–‡â–ƒâ–‚â–‚â–â–…
wandb:   test_error_force â–‚â–ƒâ–…â–„â–‚â–â–‚â–…â–ˆâ–…â–ƒâ–ƒâ–ƒâ–†â–‡â–†â–…â–„â–…â–ˆ
wandb:          test_loss â–„â–†â–ˆâ–…â–‚â–â–‚â–†â–ˆâ–…â–‚â–â–‚â–†â–‡â–„â–‚â–â–‚â–…
wandb: train_error_energy â–ˆâ–â–â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–ƒâ–‡â–‚â–„â–‡â–„â–ƒâ–†â–‚â–…â–ˆâ–„â–ƒâ–‡â–â–…â–ˆâ–ƒâ–ƒ
wandb:  valid_error_force â–‡â–ƒâ–„â–‡â–â–ˆâ–‚â–ƒâ–†â–…â–„â–ƒâ–‚â–ƒâ–†â–…â–‚â–…â–‡â–†
wandb:         valid_loss â–ƒâ–ƒâ–‡â–‚â–ƒâ–‡â–ƒâ–ƒâ–†â–‚â–„â–‡â–ƒâ–‚â–‡â–â–„â–ˆâ–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2320
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.82572
wandb:   test_error_force 12.89197
wandb:          test_loss 12.04499
wandb: train_error_energy 2.63802
wandb:  train_error_force 3.50931
wandb:         train_loss 1.01566
wandb: valid_error_energy 2.20722
wandb:  valid_error_force 3.78837
wandb:         valid_loss 0.98233
wandb: 
wandb: ğŸš€ View run al_63_16 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/42jku0jc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_001907-42jku0jc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.4077162742614746, Uncertainty Bias: -0.1828421652317047
3.2424927e-05 0.011372089
2.522294 5.5191703
(48745, 22, 3)
Found uncertainty sample 0 after 9 steps.
Found uncertainty sample 1 after 1224 steps.
Found uncertainty sample 2 after 1697 steps.
Found uncertainty sample 3 after 7 steps.
Found uncertainty sample 4 after 25 steps.
Found uncertainty sample 5 after 65 steps.
Found uncertainty sample 6 after 33 steps.
Found uncertainty sample 7 after 65 steps.
Found uncertainty sample 8 after 737 steps.
Found uncertainty sample 9 after 35 steps.
Found uncertainty sample 10 after 44 steps.
Found uncertainty sample 11 after 61 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 25 steps.
Found uncertainty sample 14 after 15 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 12 steps.
Found uncertainty sample 17 after 12 steps.
Found uncertainty sample 18 after 13 steps.
Found uncertainty sample 19 after 66 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 70 steps.
Found uncertainty sample 22 after 1094 steps.
Found uncertainty sample 23 after 39 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 245 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 95 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 463 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 197 steps.
Found uncertainty sample 35 after 76 steps.
Found uncertainty sample 36 after 462 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 401 steps.
Found uncertainty sample 39 after 7 steps.
Found uncertainty sample 40 after 577 steps.
Found uncertainty sample 41 after 1185 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 116 steps.
Found uncertainty sample 44 after 939 steps.
Found uncertainty sample 45 after 697 steps.
Found uncertainty sample 46 after 67 steps.
Found uncertainty sample 47 after 530 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 98 steps.
Found uncertainty sample 50 after 10 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 55 steps.
Found uncertainty sample 53 after 31 steps.
Found uncertainty sample 54 after 348 steps.
Found uncertainty sample 55 after 24 steps.
Found uncertainty sample 56 after 134 steps.
Found uncertainty sample 57 after 170 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 487 steps.
Found uncertainty sample 60 after 181 steps.
Found uncertainty sample 61 after 237 steps.
Found uncertainty sample 62 after 232 steps.
Found uncertainty sample 63 after 140 steps.
Found uncertainty sample 64 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 18 steps.
Found uncertainty sample 67 after 682 steps.
Found uncertainty sample 68 after 12 steps.
Found uncertainty sample 69 after 12 steps.
Found uncertainty sample 70 after 5 steps.
Found uncertainty sample 71 after 24 steps.
Found uncertainty sample 72 after 34 steps.
Found uncertainty sample 73 after 16 steps.
Found uncertainty sample 74 after 20 steps.
Found uncertainty sample 75 after 312 steps.
Found uncertainty sample 76 after 315 steps.
Found uncertainty sample 77 after 494 steps.
Found uncertainty sample 78 after 129 steps.
Found uncertainty sample 79 after 19 steps.
Found uncertainty sample 80 after 36 steps.
Found uncertainty sample 81 after 29 steps.
Found uncertainty sample 82 after 439 steps.
Found uncertainty sample 83 after 160 steps.
Found uncertainty sample 84 after 250 steps.
Found uncertainty sample 85 after 329 steps.
Found uncertainty sample 86 after 21 steps.
Found uncertainty sample 87 after 172 steps.
Found uncertainty sample 88 after 71 steps.
Found uncertainty sample 89 after 86 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 101 steps.
Found uncertainty sample 92 after 65 steps.
Found uncertainty sample 93 after 568 steps.
Found uncertainty sample 94 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 232 steps.
Found uncertainty sample 97 after 612 steps.
Found uncertainty sample 98 after 101 steps.
Found uncertainty sample 99 after 290 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_002748-5vu0fq16
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_17
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5vu0fq16
Training model 17. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.7453177570127765, Training Loss Force: 3.7914075029697187, time: 1.2672638893127441
Validation Loss Energy: 2.4815331794241886, Validation Loss Force: 3.8073475814159634, time: 0.09045171737670898
Test Loss Energy: 11.14120106710636, Test Loss Force: 13.018200542064543, time: 10.637712717056274


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.633116223034687, Training Loss Force: 3.5558296113824888, time: 1.3050379753112793
Validation Loss Energy: 2.369760504915682, Validation Loss Force: 3.7360915620313313, time: 0.08559203147888184
Test Loss Energy: 11.102973484611024, Test Loss Force: 13.007086796233699, time: 10.67685866355896


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6565644120958987, Training Loss Force: 3.5156949100201675, time: 1.3766307830810547
Validation Loss Energy: 3.8808466584421, Validation Loss Force: 3.7374269088994603, time: 0.10006332397460938
Test Loss Energy: 9.988393515051701, Test Loss Force: 12.310143393351156, time: 10.882409811019897


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6380465705884633, Training Loss Force: 3.521207664839331, time: 1.3163142204284668
Validation Loss Energy: 1.718411057273749, Validation Loss Force: 3.723735762458371, time: 0.0816192626953125
Test Loss Energy: 10.464770971856883, Test Loss Force: 12.791093583737117, time: 10.673184871673584


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6020143255707153, Training Loss Force: 3.5202126326024676, time: 1.3107993602752686
Validation Loss Energy: 2.4258382819681463, Validation Loss Force: 3.7483442657897266, time: 0.09731936454772949
Test Loss Energy: 11.056950896510637, Test Loss Force: 12.979488874514349, time: 10.869534969329834


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.684911827767886, Training Loss Force: 3.526931814713983, time: 1.314734935760498
Validation Loss Energy: 3.8690385136335492, Validation Loss Force: 3.778428314754019, time: 0.08746504783630371
Test Loss Energy: 9.89746431464126, Test Loss Force: 12.335273669947581, time: 10.707342386245728


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6591170252630283, Training Loss Force: 3.538530571419219, time: 1.2624130249023438
Validation Loss Energy: 1.633515216686164, Validation Loss Force: 3.7917458223682416, time: 0.08650445938110352
Test Loss Energy: 10.532361773744341, Test Loss Force: 12.793826990897644, time: 10.718775749206543


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6212681120442998, Training Loss Force: 3.5249834684766865, time: 1.2724716663360596
Validation Loss Energy: 2.339420943106437, Validation Loss Force: 3.7715893659405664, time: 0.08269524574279785
Test Loss Energy: 10.787276685588662, Test Loss Force: 12.94000866498989, time: 10.870978116989136


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.663137895339548, Training Loss Force: 3.5367166851176326, time: 1.2952685356140137
Validation Loss Energy: 3.432354926141895, Validation Loss Force: 3.7814006138484366, time: 0.07279777526855469
Test Loss Energy: 9.85004955240825, Test Loss Force: 12.4542544309154, time: 10.71677827835083


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.722500481773025, Training Loss Force: 3.538295002112112, time: 1.346449613571167
Validation Loss Energy: 1.7310070611878787, Validation Loss Force: 3.7060468141712946, time: 0.09698271751403809
Test Loss Energy: 10.678850698615369, Test Loss Force: 12.847984366061706, time: 10.853775978088379


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6298000407429565, Training Loss Force: 3.533767997742255, time: 1.233612060546875
Validation Loss Energy: 1.9159522684200185, Validation Loss Force: 3.7595324996960144, time: 0.07820391654968262
Test Loss Energy: 10.715603530533347, Test Loss Force: 13.023957502443395, time: 8.983705759048462


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.621655535849214, Training Loss Force: 3.528007368215815, time: 1.256598949432373
Validation Loss Energy: 3.7848321680563295, Validation Loss Force: 3.757088260481165, time: 0.07483506202697754
Test Loss Energy: 10.137663486622417, Test Loss Force: 12.452299372498015, time: 9.037346124649048


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6711441519922414, Training Loss Force: 3.5194948586105688, time: 1.2176575660705566
Validation Loss Energy: 1.6411981372299698, Validation Loss Force: 3.765143094165957, time: 0.07291030883789062
Test Loss Energy: 10.590698758363331, Test Loss Force: 13.087606622104483, time: 9.257271528244019


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.655703397386311, Training Loss Force: 3.5325900545214397, time: 1.2143285274505615
Validation Loss Energy: 2.259436515870581, Validation Loss Force: 3.743964141020181, time: 0.07239794731140137
Test Loss Energy: 10.982179362077511, Test Loss Force: 13.196754504780488, time: 8.977492094039917


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.710932185327997, Training Loss Force: 3.5162887217546417, time: 1.191220998764038
Validation Loss Energy: 3.8534331918794917, Validation Loss Force: 3.707016649636383, time: 0.07566595077514648
Test Loss Energy: 10.080357150044401, Test Loss Force: 12.520286278314128, time: 9.021133661270142


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6387913549337343, Training Loss Force: 3.5250759054708185, time: 1.2374377250671387
Validation Loss Energy: 1.6100917028821498, Validation Loss Force: 3.8320228854910976, time: 0.07356786727905273
Test Loss Energy: 10.555206411013977, Test Loss Force: 13.20561473626352, time: 9.314455509185791


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6479345382716053, Training Loss Force: 3.5307195106416445, time: 1.2481772899627686
Validation Loss Energy: 2.2957812418763526, Validation Loss Force: 3.7572750798945984, time: 0.07354521751403809
Test Loss Energy: 10.901523313984928, Test Loss Force: 13.466485322629424, time: 8.98307466506958


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6641182665710677, Training Loss Force: 3.5170948368003296, time: 1.2155344486236572
Validation Loss Energy: 3.8719292044859834, Validation Loss Force: 3.7601848089590098, time: 0.07343578338623047
Test Loss Energy: 10.047765419228677, Test Loss Force: 12.510226151586307, time: 9.002140283584595


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5955126487318423, Training Loss Force: 3.519594186628796, time: 1.2462139129638672
Validation Loss Energy: 1.7722079229299026, Validation Loss Force: 3.785839300603361, time: 0.0777442455291748
Test Loss Energy: 10.739736734331741, Test Loss Force: 13.284727582252188, time: 9.689009666442871


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6817714193280073, Training Loss Force: 3.5280712185934098, time: 1.2235636711120605
Validation Loss Energy: 2.4414886942242537, Validation Loss Force: 3.74643276250651, time: 0.07548713684082031
Test Loss Energy: 11.270789912093338, Test Loss Force: 13.359106542820394, time: 9.013941764831543

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.039 MB of 0.048 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–‡â–‚â–„â–‡â–â–„â–†â–â–…â–…â–‚â–…â–‡â–‚â–„â–†â–‚â–…â–ˆ
wandb:   test_error_force â–…â–…â–â–„â–…â–â–„â–…â–‚â–„â–…â–‚â–†â–†â–‚â–†â–ˆâ–‚â–‡â–‡
wandb:          test_loss â–ˆâ–ˆâ–‚â–…â–ˆâ–â–†â–‡â–‚â–†â–†â–‚â–…â–‡â–â–…â–†â–â–†â–ˆ
wandb: train_error_energy â–ˆâ–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–‚
wandb:  train_error_force â–ˆâ–‚â–â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–ƒâ–ˆâ–â–„â–ˆâ–â–ƒâ–‡â–â–‚â–ˆâ–â–ƒâ–ˆâ–â–ƒâ–ˆâ–â–„
wandb:  valid_error_force â–‡â–ƒâ–ƒâ–‚â–ƒâ–…â–†â–…â–…â–â–„â–„â–„â–ƒâ–â–ˆâ–„â–„â–…â–ƒ
wandb:         valid_loss â–ƒâ–ƒâ–ˆâ–â–ƒâ–ˆâ–â–ƒâ–†â–â–‚â–ˆâ–â–‚â–‡â–â–ƒâ–ˆâ–â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2410
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.27079
wandb:   test_error_force 13.35911
wandb:          test_loss 12.57421
wandb: train_error_energy 2.68177
wandb:  train_error_force 3.52807
wandb:         train_loss 1.0414
wandb: valid_error_energy 2.44149
wandb:  valid_error_force 3.74643
wandb:         valid_loss 1.04667
wandb: 
wandb: ğŸš€ View run al_63_17 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5vu0fq16
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_002748-5vu0fq16/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1649410724639893, Uncertainty Bias: -0.15302354097366333
8.046627e-05 0.027734756
2.5711365 5.4141912
(48745, 22, 3)
Found uncertainty sample 0 after 1869 steps.
Found uncertainty sample 1 after 20 steps.
Found uncertainty sample 2 after 336 steps.
Found uncertainty sample 3 after 767 steps.
Found uncertainty sample 4 after 309 steps.
Found uncertainty sample 5 after 43 steps.
Found uncertainty sample 6 after 35 steps.
Found uncertainty sample 7 after 67 steps.
Found uncertainty sample 8 after 43 steps.
Found uncertainty sample 9 after 5 steps.
Found uncertainty sample 10 after 51 steps.
Found uncertainty sample 11 after 15 steps.
Found uncertainty sample 12 after 15 steps.
Found uncertainty sample 13 after 208 steps.
Found uncertainty sample 14 after 31 steps.
Found uncertainty sample 15 after 1299 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 23 steps.
Found uncertainty sample 18 after 175 steps.
Found uncertainty sample 19 after 6 steps.
Found uncertainty sample 20 after 78 steps.
Found uncertainty sample 21 after 580 steps.
Found uncertainty sample 22 after 147 steps.
Found uncertainty sample 23 after 226 steps.
Found uncertainty sample 24 after 16 steps.
Found uncertainty sample 25 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 96 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 266 steps.
Found uncertainty sample 34 after 1151 steps.
Found uncertainty sample 35 after 141 steps.
Found uncertainty sample 36 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 139 steps.
Found uncertainty sample 39 after 26 steps.
Found uncertainty sample 40 after 38 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 63 steps.
Found uncertainty sample 45 after 35 steps.
Found uncertainty sample 46 after 374 steps.
Found uncertainty sample 47 after 42 steps.
Found uncertainty sample 48 after 34 steps.
Found uncertainty sample 49 after 13 steps.
Found uncertainty sample 50 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 812 steps.
Found uncertainty sample 53 after 54 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 176 steps.
Found uncertainty sample 56 after 2262 steps.
Found uncertainty sample 57 after 435 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 17 steps.
Found uncertainty sample 60 after 73 steps.
Found uncertainty sample 61 after 18 steps.
Found uncertainty sample 62 after 13 steps.
Found uncertainty sample 63 after 11 steps.
Found uncertainty sample 64 after 171 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 9 steps.
Found uncertainty sample 67 after 1944 steps.
Found uncertainty sample 68 after 365 steps.
Found uncertainty sample 69 after 485 steps.
Found uncertainty sample 70 after 229 steps.
Found uncertainty sample 71 after 360 steps.
Found uncertainty sample 72 after 933 steps.
Found uncertainty sample 73 after 32 steps.
Found uncertainty sample 74 after 16 steps.
Found uncertainty sample 75 after 665 steps.
Found uncertainty sample 76 after 1084 steps.
Found uncertainty sample 77 after 623 steps.
Found uncertainty sample 78 after 1035 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 678 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1539 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 72 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 18 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 33 steps.
Found uncertainty sample 89 after 218 steps.
Found uncertainty sample 90 after 79 steps.
Found uncertainty sample 91 after 15 steps.
Found uncertainty sample 92 after 10 steps.
Found uncertainty sample 93 after 216 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 291 steps.
Found uncertainty sample 96 after 18 steps.
Found uncertainty sample 97 after 173 steps.
Found uncertainty sample 98 after 216 steps.
Found uncertainty sample 99 after 92 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_003647-jph77rne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_18
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jph77rne
Training model 18. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.749931785465618, Training Loss Force: 4.017245687864693, time: 1.5020966529846191
Validation Loss Energy: 2.8375309318703144, Validation Loss Force: 4.679890212551902, time: 0.0942220687866211
Test Loss Energy: 11.249636254628863, Test Loss Force: 13.289645557931783, time: 10.293549537658691


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6410003511496853, Training Loss Force: 3.7567911831987386, time: 1.3202941417694092
Validation Loss Energy: 2.7734551017652693, Validation Loss Force: 3.850789886448183, time: 0.0877828598022461
Test Loss Energy: 10.99651653175, Test Loss Force: 12.92390675256878, time: 9.881709814071655


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.656249642555634, Training Loss Force: 3.5290764784424855, time: 1.3457465171813965
Validation Loss Energy: 1.8133664218669137, Validation Loss Force: 3.744288124627506, time: 0.08690476417541504
Test Loss Energy: 10.66582731420968, Test Loss Force: 12.886762414665142, time: 10.042478799819946


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6118006376309895, Training Loss Force: 3.5125273495553415, time: 1.3543384075164795
Validation Loss Energy: 2.538956768794641, Validation Loss Force: 3.750075741068461, time: 0.08183526992797852
Test Loss Energy: 9.913659547584846, Test Loss Force: 12.399220718594306, time: 9.973686695098877


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6286573670316526, Training Loss Force: 3.5069353972845274, time: 1.3323616981506348
Validation Loss Energy: 3.833651264955267, Validation Loss Force: 3.7225048474196827, time: 0.07676219940185547
Test Loss Energy: 10.135045568971165, Test Loss Force: 12.47591974243978, time: 9.851791620254517


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.613629410870704, Training Loss Force: 3.5372231391919002, time: 1.5364539623260498
Validation Loss Energy: 2.529394130988199, Validation Loss Force: 3.770486930999078, time: 0.08161377906799316
Test Loss Energy: 10.035309288552565, Test Loss Force: 12.575776408757253, time: 9.938693761825562


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6320536479100936, Training Loss Force: 3.5250495802218977, time: 1.3593273162841797
Validation Loss Energy: 2.0659808938810813, Validation Loss Force: 3.718899165939674, time: 0.08263230323791504
Test Loss Energy: 10.844955171092137, Test Loss Force: 13.012460154798665, time: 9.846588134765625


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6930562618814715, Training Loss Force: 3.5380712324154, time: 1.3578190803527832
Validation Loss Energy: 2.8174567516920597, Validation Loss Force: 3.852929736940549, time: 0.08383798599243164
Test Loss Energy: 11.305596028109154, Test Loss Force: 13.391060441947934, time: 10.166452646255493


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.668574179662366, Training Loss Force: 3.5115209470248776, time: 1.331439733505249
Validation Loss Energy: 1.8992027612596112, Validation Loss Force: 3.7766228019842636, time: 0.0899965763092041
Test Loss Energy: 10.588034777344047, Test Loss Force: 12.960846066981956, time: 10.134432315826416


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6622389648289366, Training Loss Force: 3.5194019409344253, time: 1.3341779708862305
Validation Loss Energy: 2.9170954050969504, Validation Loss Force: 3.7883160495135155, time: 0.07788991928100586
Test Loss Energy: 9.914700692552467, Test Loss Force: 12.642561908442648, time: 10.13092589378357


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.557185071078598, Training Loss Force: 3.54508477216014, time: 1.376964807510376
Validation Loss Energy: 3.9216407593218, Validation Loss Force: 3.814392387854827, time: 0.08129215240478516
Test Loss Energy: 10.00382571329351, Test Loss Force: 12.372753210019757, time: 11.134284496307373


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6369550651083284, Training Loss Force: 3.536156517341148, time: 1.4983768463134766
Validation Loss Energy: 2.5026031360230325, Validation Loss Force: 3.7384929374355895, time: 0.09731769561767578
Test Loss Energy: 10.036592837447799, Test Loss Force: 12.664522448943542, time: 11.42540717124939


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.657284019659179, Training Loss Force: 3.538204808535586, time: 1.502410650253296
Validation Loss Energy: 2.156005361572248, Validation Loss Force: 3.7809893439359077, time: 0.11048698425292969
Test Loss Energy: 10.626649804153294, Test Loss Force: 12.914264595646587, time: 11.066686868667603


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6442919556854156, Training Loss Force: 3.533806151492186, time: 1.3988375663757324
Validation Loss Energy: 3.04885764550826, Validation Loss Force: 3.804971701628914, time: 0.0896146297454834
Test Loss Energy: 11.252987436933145, Test Loss Force: 13.084391568757434, time: 10.468016147613525


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6333006768665963, Training Loss Force: 3.526671006207802, time: 1.296743392944336
Validation Loss Energy: 1.7670128290862612, Validation Loss Force: 3.760185922304739, time: 0.08434605598449707
Test Loss Energy: 10.50196115862006, Test Loss Force: 13.085145076484025, time: 11.352486610412598


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6708849273149324, Training Loss Force: 3.5270639214180743, time: 1.3314709663391113
Validation Loss Energy: 2.5726606387297086, Validation Loss Force: 3.8016986878529218, time: 0.08310317993164062
Test Loss Energy: 10.109375822158702, Test Loss Force: 12.88799986754975, time: 11.45128345489502


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.652660430136634, Training Loss Force: 3.5349813964257955, time: 1.4441454410552979
Validation Loss Energy: 3.908064621494583, Validation Loss Force: 3.7910430785436935, time: 0.08766865730285645
Test Loss Energy: 10.019763508061807, Test Loss Force: 12.608260661489721, time: 11.241499662399292


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.666918745896965, Training Loss Force: 3.5191678000995874, time: 1.496145248413086
Validation Loss Energy: 2.390053346195238, Validation Loss Force: 3.769045370948341, time: 0.09215760231018066
Test Loss Energy: 9.990475716385605, Test Loss Force: 12.894267493384127, time: 11.356017351150513


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.61268520490617, Training Loss Force: 3.5389331424003356, time: 1.4298806190490723
Validation Loss Energy: 2.0801183806251826, Validation Loss Force: 3.6987270113372106, time: 0.08975934982299805
Test Loss Energy: 10.72046656872167, Test Loss Force: 13.248928804852213, time: 11.163700103759766


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6303526092487304, Training Loss Force: 3.5548460717144033, time: 1.504077434539795
Validation Loss Energy: 3.335142141747314, Validation Loss Force: 3.7614277481315783, time: 0.09759879112243652
Test Loss Energy: 11.265050951775287, Test Loss Force: 13.219007602228148, time: 11.370231866836548

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–†â–…â–â–‚â–‚â–†â–ˆâ–„â–â–â–‚â–…â–ˆâ–„â–‚â–‚â–â–…â–ˆ
wandb:   test_error_force â–‡â–…â–…â–â–‚â–‚â–…â–ˆâ–…â–ƒâ–â–ƒâ–…â–†â–†â–…â–ƒâ–…â–‡â–‡
wandb:          test_loss â–ˆâ–†â–…â–â–â–‚â–…â–†â–„â–â–â–‚â–„â–†â–„â–â–â–â–„â–†
wandb: train_error_energy â–ˆâ–â–‚â–â–â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–â–
wandb:  train_error_force â–ˆâ–„â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–„â–â–„â–ˆâ–ƒâ–‚â–„â–â–…â–ˆâ–ƒâ–‚â–…â–â–„â–ˆâ–ƒâ–‚â–†
wandb:  valid_error_force â–ˆâ–‚â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–
wandb:         valid_loss â–‡â–„â–â–ƒâ–‡â–ƒâ–â–„â–â–„â–ˆâ–‚â–‚â–…â–â–ƒâ–ˆâ–‚â–â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 2500
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.26505
wandb:   test_error_force 13.21901
wandb:          test_loss 12.61627
wandb: train_error_energy 2.63035
wandb:  train_error_force 3.55485
wandb:         train_loss 1.03167
wandb: valid_error_energy 3.33514
wandb:  valid_error_force 3.76143
wandb:         valid_loss 1.411
wandb: 
wandb: ğŸš€ View run al_63_18 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jph77rne
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_003647-jph77rne/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1418046951293945, Uncertainty Bias: -0.141834557056427
0.0002632141 0.055588722
2.5327957 5.4134326
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 55 steps.
Found uncertainty sample 2 after 145 steps.
Found uncertainty sample 3 after 697 steps.
Found uncertainty sample 4 after 1610 steps.
Found uncertainty sample 5 after 20 steps.
Found uncertainty sample 6 after 2608 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 19 steps.
Found uncertainty sample 9 after 35 steps.
Found uncertainty sample 10 after 10 steps.
Found uncertainty sample 11 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 697 steps.
Found uncertainty sample 14 after 64 steps.
Found uncertainty sample 15 after 8 steps.
Found uncertainty sample 16 after 6 steps.
Found uncertainty sample 17 after 429 steps.
Found uncertainty sample 18 after 633 steps.
Found uncertainty sample 19 after 1452 steps.
Found uncertainty sample 20 after 124 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 363 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 51 steps.
Found uncertainty sample 25 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 46 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 63 steps.
Found uncertainty sample 30 after 91 steps.
Found uncertainty sample 31 after 177 steps.
Found uncertainty sample 32 after 363 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 105 steps.
Found uncertainty sample 37 after 62 steps.
Found uncertainty sample 38 after 89 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 98 steps.
Found uncertainty sample 41 after 237 steps.
Found uncertainty sample 42 after 47 steps.
Found uncertainty sample 43 after 737 steps.
Found uncertainty sample 44 after 90 steps.
Found uncertainty sample 45 after 113 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 706 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 133 steps.
Found uncertainty sample 50 after 2055 steps.
Found uncertainty sample 51 after 1047 steps.
Found uncertainty sample 52 after 1105 steps.
Found uncertainty sample 53 after 24 steps.
Found uncertainty sample 54 after 57 steps.
Found uncertainty sample 55 after 1278 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1141 steps.
Found uncertainty sample 58 after 52 steps.
Found uncertainty sample 59 after 1477 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 18 steps.
Found uncertainty sample 62 after 41 steps.
Found uncertainty sample 63 after 2332 steps.
Found uncertainty sample 64 after 85 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 21 steps.
Found uncertainty sample 67 after 140 steps.
Found uncertainty sample 68 after 286 steps.
Found uncertainty sample 69 after 192 steps.
Found uncertainty sample 70 after 168 steps.
Found uncertainty sample 71 after 131 steps.
Found uncertainty sample 72 after 37 steps.
Found uncertainty sample 73 after 630 steps.
Found uncertainty sample 74 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 21 steps.
Found uncertainty sample 77 after 288 steps.
Found uncertainty sample 78 after 393 steps.
Found uncertainty sample 79 after 686 steps.
Found uncertainty sample 80 after 592 steps.
Found uncertainty sample 81 after 107 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 504 steps.
Found uncertainty sample 84 after 1110 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 63 steps.
Found uncertainty sample 87 after 719 steps.
Found uncertainty sample 88 after 3102 steps.
Found uncertainty sample 89 after 7 steps.
Found uncertainty sample 90 after 703 steps.
Found uncertainty sample 91 after 1189 steps.
Found uncertainty sample 92 after 31 steps.
Found uncertainty sample 93 after 37 steps.
Found uncertainty sample 94 after 139 steps.
Found uncertainty sample 95 after 131 steps.
Found uncertainty sample 96 after 105 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 195 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_004701-m6jablhc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_19
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/m6jablhc
Training model 19. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.7718471439630026, Training Loss Force: 3.9548486296108107, time: 1.3360934257507324
Validation Loss Energy: 2.9902922087574897, Validation Loss Force: 3.7023823480367613, time: 0.08986330032348633
Test Loss Energy: 10.032786500756327, Test Loss Force: 12.646059590723652, time: 10.81610918045044


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6236918628763797, Training Loss Force: 3.5248335102113844, time: 1.3790643215179443
Validation Loss Energy: 2.06693870521229, Validation Loss Force: 3.718931230296687, time: 0.09000062942504883
Test Loss Energy: 10.669289956994088, Test Loss Force: 13.202308824578479, time: 10.845571041107178


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5826938932740724, Training Loss Force: 3.518757748899591, time: 1.365220069885254
Validation Loss Energy: 3.0406715483619435, Validation Loss Force: 3.7275179092365276, time: 0.09221291542053223
Test Loss Energy: 9.965918182138335, Test Loss Force: 12.655346442300598, time: 10.898653030395508


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.655943965717022, Training Loss Force: 3.5143327466756347, time: 1.3941574096679688
Validation Loss Energy: 2.4203336780116658, Validation Loss Force: 3.701497015512552, time: 0.09512615203857422
Test Loss Energy: 10.957125180038641, Test Loss Force: 13.498264439301286, time: 11.341054916381836


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.7006562587250365, Training Loss Force: 3.529673787643494, time: 1.3797345161437988
Validation Loss Energy: 2.781448924104077, Validation Loss Force: 3.7530848919065294, time: 0.09340476989746094
Test Loss Energy: 10.134185543786325, Test Loss Force: 12.895562835583448, time: 10.891093492507935


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6372314046171135, Training Loss Force: 3.5270923539957337, time: 1.3664977550506592
Validation Loss Energy: 2.23542053513654, Validation Loss Force: 3.705925125483045, time: 0.09662175178527832
Test Loss Energy: 10.89967717925153, Test Loss Force: 13.349499089642364, time: 10.803297281265259


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.629821387790907, Training Loss Force: 3.5334089498573586, time: 1.3968620300292969
Validation Loss Energy: 3.0162170207449974, Validation Loss Force: 3.776710978966051, time: 0.10213017463684082
Test Loss Energy: 10.04135946339336, Test Loss Force: 12.791148087548166, time: 10.764029502868652


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6639517296184656, Training Loss Force: 3.537253109992656, time: 1.3198328018188477
Validation Loss Energy: 2.257912345566705, Validation Loss Force: 3.7370608408235553, time: 0.09095120429992676
Test Loss Energy: 11.042731953662372, Test Loss Force: 13.531149757637202, time: 10.96010446548462


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6425373615799095, Training Loss Force: 3.5261583861316503, time: 1.421363115310669
Validation Loss Energy: 2.771566533404083, Validation Loss Force: 3.751170605258988, time: 0.09439730644226074
Test Loss Energy: 10.043514285179818, Test Loss Force: 12.875358070443369, time: 10.829981327056885


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.671613291105524, Training Loss Force: 3.5227540497905956, time: 1.37261962890625
Validation Loss Energy: 2.1978137204116495, Validation Loss Force: 3.7284597997238023, time: 0.09309792518615723
Test Loss Energy: 11.027699608584681, Test Loss Force: 13.5434272447047, time: 10.95460319519043


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6099902028779005, Training Loss Force: 3.520006296035835, time: 1.3168139457702637
Validation Loss Energy: 2.9780036552666003, Validation Loss Force: 3.7251793265314213, time: 0.09066152572631836
Test Loss Energy: 10.193165531613555, Test Loss Force: 12.937765605599676, time: 10.941420793533325


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6451277507882893, Training Loss Force: 3.562601476919886, time: 1.4199700355529785
Validation Loss Energy: 2.497005345028981, Validation Loss Force: 3.716246174400517, time: 0.09554719924926758
Test Loss Energy: 11.217743625656475, Test Loss Force: 13.733973090414606, time: 10.9589102268219


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6562724549382994, Training Loss Force: 3.520890594935367, time: 1.632737159729004
Validation Loss Energy: 2.9581628323532776, Validation Loss Force: 3.677672418909436, time: 0.08319091796875
Test Loss Energy: 10.116110279103653, Test Loss Force: 12.856417832890754, time: 11.07603645324707


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6639199264166233, Training Loss Force: 3.5335757620356802, time: 1.4162306785583496
Validation Loss Energy: 1.9793223486670453, Validation Loss Force: 3.721795646189902, time: 0.08428359031677246
Test Loss Energy: 10.78821520689357, Test Loss Force: 13.520482651138881, time: 11.33125114440918


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6688638906986855, Training Loss Force: 3.5430188168892527, time: 1.3490240573883057
Validation Loss Energy: 2.895655149708429, Validation Loss Force: 3.7554580996638274, time: 0.08612203598022461
Test Loss Energy: 10.147238171295974, Test Loss Force: 12.906529577808923, time: 10.987922668457031


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.661722933798309, Training Loss Force: 3.5163213443398758, time: 1.38002347946167
Validation Loss Energy: 2.2291806776602052, Validation Loss Force: 3.7072218392543554, time: 0.09821438789367676
Test Loss Energy: 10.899996120743381, Test Loss Force: 13.582658097428318, time: 10.788326025009155


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.666059785155995, Training Loss Force: 3.529014730955577, time: 1.346984624862671
Validation Loss Energy: 2.854665990656896, Validation Loss Force: 3.728879085725584, time: 0.09137415885925293
Test Loss Energy: 9.95229919157569, Test Loss Force: 12.855900466376319, time: 10.934052228927612


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.648290265000615, Training Loss Force: 3.530102291048317, time: 1.3978734016418457
Validation Loss Energy: 2.267994247821451, Validation Loss Force: 3.755574555627148, time: 0.09736227989196777
Test Loss Energy: 10.90554706436416, Test Loss Force: 13.826309671744786, time: 10.868807792663574


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6798406617648896, Training Loss Force: 3.52460137075425, time: 1.396512746810913
Validation Loss Energy: 2.840749391705555, Validation Loss Force: 3.724130554854101, time: 0.08503532409667969
Test Loss Energy: 9.975941315476948, Test Loss Force: 12.933577103368597, time: 10.950262546539307


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.63085038864727, Training Loss Force: 3.541537118663211, time: 1.4493465423583984
Validation Loss Energy: 2.2230746443405733, Validation Loss Force: 3.769736535977301, time: 0.09075164794921875
Test Loss Energy: 10.758132353464402, Test Loss Force: 13.481766259738533, time: 10.14224886894226

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–…â–â–‡â–‚â–†â–â–‡â–‚â–‡â–‚â–ˆâ–‚â–†â–‚â–†â–â–†â–â–…
wandb:   test_error_force â–â–„â–â–†â–‚â–…â–‚â–†â–‚â–†â–ƒâ–‡â–‚â–†â–ƒâ–‡â–‚â–ˆâ–ƒâ–†
wandb:          test_loss â–â–†â–‚â–‡â–‚â–†â–‚â–‡â–‚â–‡â–‚â–ˆâ–‚â–†â–‚â–†â–â–†â–â–†
wandb: train_error_energy â–ˆâ–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–‚â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‚â–ˆâ–„â–†â–ƒâ–ˆâ–ƒâ–†â–‚â–ˆâ–„â–‡â–â–‡â–ƒâ–‡â–ƒâ–‡â–ƒ
wandb:  valid_error_force â–ƒâ–„â–…â–ƒâ–†â–ƒâ–ˆâ–…â–†â–…â–„â–„â–â–„â–†â–ƒâ–…â–‡â–„â–ˆ
wandb:         valid_loss â–‡â–â–ˆâ–„â–†â–‚â–ˆâ–ƒâ–†â–ƒâ–‡â–…â–‡â–â–‡â–‚â–†â–ƒâ–†â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2590
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.75813
wandb:   test_error_force 13.48177
wandb:          test_loss 11.80675
wandb: train_error_energy 2.63085
wandb:  train_error_force 3.54154
wandb:         train_loss 1.02316
wandb: valid_error_energy 2.22307
wandb:  valid_error_force 3.76974
wandb:         valid_loss 0.96277
wandb: 
wandb: ğŸš€ View run al_63_19 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/m6jablhc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_004701-m6jablhc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.342017889022827, Uncertainty Bias: -0.17063996195793152
6.9111586e-05 0.047647953
2.5093904 5.508037
(48745, 22, 3)
Found uncertainty sample 0 after 14 steps.
Found uncertainty sample 1 after 128 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 85 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 23 steps.
Found uncertainty sample 6 after 396 steps.
Found uncertainty sample 7 after 431 steps.
Found uncertainty sample 8 after 169 steps.
Found uncertainty sample 9 after 115 steps.
Found uncertainty sample 10 after 121 steps.
Found uncertainty sample 11 after 217 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 544 steps.
Found uncertainty sample 14 after 732 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 40 steps.
Found uncertainty sample 17 after 162 steps.
Found uncertainty sample 18 after 23 steps.
Found uncertainty sample 19 after 134 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 201 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 383 steps.
Found uncertainty sample 24 after 66 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 60 steps.
Found uncertainty sample 27 after 10 steps.
Found uncertainty sample 28 after 673 steps.
Found uncertainty sample 29 after 148 steps.
Found uncertainty sample 30 after 552 steps.
Found uncertainty sample 31 after 60 steps.
Found uncertainty sample 32 after 24 steps.
Found uncertainty sample 33 after 252 steps.
Found uncertainty sample 34 after 58 steps.
Found uncertainty sample 35 after 445 steps.
Found uncertainty sample 36 after 121 steps.
Found uncertainty sample 37 after 957 steps.
Found uncertainty sample 38 after 16 steps.
Found uncertainty sample 39 after 31 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 9 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1754 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 22 steps.
Found uncertainty sample 47 after 343 steps.
Found uncertainty sample 48 after 500 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 22 steps.
Found uncertainty sample 51 after 17 steps.
Found uncertainty sample 52 after 78 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 924 steps.
Found uncertainty sample 55 after 67 steps.
Found uncertainty sample 56 after 141 steps.
Found uncertainty sample 57 after 56 steps.
Found uncertainty sample 58 after 736 steps.
Found uncertainty sample 59 after 787 steps.
Found uncertainty sample 60 after 532 steps.
Found uncertainty sample 61 after 130 steps.
Found uncertainty sample 62 after 105 steps.
Found uncertainty sample 63 after 155 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 577 steps.
Found uncertainty sample 66 after 5 steps.
Found uncertainty sample 67 after 43 steps.
Found uncertainty sample 68 after 120 steps.
Found uncertainty sample 69 after 495 steps.
Found uncertainty sample 70 after 270 steps.
Found uncertainty sample 71 after 139 steps.
Found uncertainty sample 72 after 81 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 332 steps.
Found uncertainty sample 75 after 1324 steps.
Found uncertainty sample 76 after 223 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 112 steps.
Found uncertainty sample 79 after 227 steps.
Found uncertainty sample 80 after 25 steps.
Found uncertainty sample 81 after 25 steps.
Found uncertainty sample 82 after 204 steps.
Found uncertainty sample 83 after 38 steps.
Found uncertainty sample 84 after 960 steps.
Found uncertainty sample 85 after 719 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 647 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 29 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 113 steps.
Found uncertainty sample 94 after 56 steps.
Found uncertainty sample 95 after 8 steps.
Found uncertainty sample 96 after 1258 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 106 steps.
Found uncertainty sample 99 after 75 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_005611-bh04htlz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_20
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/bh04htlz
Training model 20. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.457885087223856, Training Loss Force: 3.94309598390874, time: 1.477520227432251
Validation Loss Energy: 1.5830053929621966, Validation Loss Force: 3.7719362844699216, time: 0.10255551338195801
Test Loss Energy: 10.422728057302907, Test Loss Force: 13.017064351180014, time: 11.162553787231445


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.096200204415932, Training Loss Force: 3.5810571830728914, time: 1.4215047359466553
Validation Loss Energy: 2.416743917375591, Validation Loss Force: 3.763828232859253, time: 0.09007406234741211
Test Loss Energy: 9.755225040912528, Test Loss Force: 12.336818368947744, time: 11.228650331497192


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.132932198455878, Training Loss Force: 3.5774208845853175, time: 1.443613052368164
Validation Loss Energy: 1.841845694296505, Validation Loss Force: 3.8136967694429487, time: 0.09275555610656738
Test Loss Energy: 10.417984183247672, Test Loss Force: 12.670285518151319, time: 11.306832313537598


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.096457445597589, Training Loss Force: 3.5765776762920365, time: 1.4381330013275146
Validation Loss Energy: 2.5236357955399336, Validation Loss Force: 3.754325381718493, time: 0.08567070960998535
Test Loss Energy: 9.857871730643952, Test Loss Force: 12.190811892338484, time: 10.950344324111938


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.115608788080074, Training Loss Force: 3.5637904552488417, time: 1.4309844970703125
Validation Loss Energy: 2.0796272838238385, Validation Loss Force: 3.770295324199007, time: 0.09131813049316406
Test Loss Energy: 10.488335466998512, Test Loss Force: 12.799330010522347, time: 11.073235750198364


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.141536827444786, Training Loss Force: 3.562248126806279, time: 1.4313838481903076
Validation Loss Energy: 2.4707073395528987, Validation Loss Force: 3.7566398048212664, time: 0.09084534645080566
Test Loss Energy: 9.865228537359503, Test Loss Force: 12.352426674002032, time: 11.045578002929688


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.125393324968313, Training Loss Force: 3.5610522901105437, time: 1.3974909782409668
Validation Loss Energy: 1.8025497670978932, Validation Loss Force: 3.764862753711234, time: 0.09372353553771973
Test Loss Energy: 10.678576239657197, Test Loss Force: 12.997647102689697, time: 11.201292514801025


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.151594820599986, Training Loss Force: 3.5780862942163973, time: 1.506763219833374
Validation Loss Energy: 2.562147537673874, Validation Loss Force: 3.733741623199782, time: 0.09580230712890625
Test Loss Energy: 10.108367436369097, Test Loss Force: 12.786681904068237, time: 11.015175580978394


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.129454388709153, Training Loss Force: 3.560167734277522, time: 1.5019423961639404
Validation Loss Energy: 1.8047648797615827, Validation Loss Force: 3.767498265722435, time: 0.10386419296264648
Test Loss Energy: 10.931459051891252, Test Loss Force: 13.083217778264034, time: 11.22152590751648


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.174963048836055, Training Loss Force: 3.575140537814341, time: 1.424931287765503
Validation Loss Energy: 2.459317812738692, Validation Loss Force: 3.7144208440767743, time: 0.08726119995117188
Test Loss Energy: 10.139528278931786, Test Loss Force: 12.85736212260893, time: 11.22470211982727


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.084788059453064, Training Loss Force: 3.547299997131489, time: 1.5510845184326172
Validation Loss Energy: 1.8606403062276857, Validation Loss Force: 3.7640373191872794, time: 0.09569001197814941
Test Loss Energy: 10.913063176281039, Test Loss Force: 13.55262262706076, time: 11.018808841705322


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.044032659266173, Training Loss Force: 3.5745805593730333, time: 1.398386001586914
Validation Loss Energy: 2.2131459384582266, Validation Loss Force: 3.7255735622542865, time: 0.07911181449890137
Test Loss Energy: 10.404450114733544, Test Loss Force: 13.268762125214284, time: 11.454023122787476


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.116911064603797, Training Loss Force: 3.5558993549585316, time: 1.492757797241211
Validation Loss Energy: 1.9627423483939759, Validation Loss Force: 3.8717657653328725, time: 0.09465980529785156
Test Loss Energy: 11.131131613922163, Test Loss Force: 13.783565906636902, time: 9.817058324813843


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.148372635032787, Training Loss Force: 3.563032128964152, time: 1.358020544052124
Validation Loss Energy: 2.2361763855523913, Validation Loss Force: 3.7144348722329674, time: 0.08419275283813477
Test Loss Energy: 10.225096972634145, Test Loss Force: 13.115575335749892, time: 9.255308628082275


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.098631050346737, Training Loss Force: 3.5687524197983422, time: 1.365065336227417
Validation Loss Energy: 1.6295175818420358, Validation Loss Force: 3.8063825333375125, time: 0.08028841018676758
Test Loss Energy: 11.011620821112153, Test Loss Force: 14.040762162974854, time: 9.433109283447266


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.136944334628136, Training Loss Force: 3.578316767385727, time: 1.3431434631347656
Validation Loss Energy: 2.343942796787365, Validation Loss Force: 3.817235650179179, time: 0.08395600318908691
Test Loss Energy: 10.289707136992286, Test Loss Force: 13.240080343357125, time: 9.299378395080566


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.085054813113651, Training Loss Force: 3.585125022317743, time: 1.3396856784820557
Validation Loss Energy: 2.047138853421818, Validation Loss Force: 3.8096624498570293, time: 0.08070707321166992
Test Loss Energy: 11.165474937183017, Test Loss Force: 13.94836056225406, time: 9.452014207839966


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.097177116253462, Training Loss Force: 3.5737075240174905, time: 1.35322904586792
Validation Loss Energy: 2.824544421941514, Validation Loss Force: 3.8924486116215213, time: 0.07999753952026367
Test Loss Energy: 10.138942950529758, Test Loss Force: 13.204100035810285, time: 9.43408489227295


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.1117412900976245, Training Loss Force: 3.5641647912422005, time: 1.339374303817749
Validation Loss Energy: 1.664719959461911, Validation Loss Force: 3.7846967852156057, time: 0.0811002254486084
Test Loss Energy: 10.76747754202597, Test Loss Force: 13.781665314724739, time: 9.211565971374512


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.092223076339816, Training Loss Force: 3.5404159922766874, time: 1.376516342163086
Validation Loss Energy: 2.4665132548547755, Validation Loss Force: 3.727605084207158, time: 0.08236384391784668
Test Loss Energy: 10.239829612036258, Test Loss Force: 13.105754745064349, time: 9.280462503433228

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–â–„â–‚â–…â–‚â–†â–ƒâ–‡â–ƒâ–‡â–„â–ˆâ–ƒâ–‡â–„â–ˆâ–ƒâ–†â–ƒ
wandb:   test_error_force â–„â–‚â–ƒâ–â–ƒâ–‚â–„â–ƒâ–„â–„â–†â–…â–‡â–„â–ˆâ–…â–ˆâ–…â–‡â–„
wandb:          test_loss â–ˆâ–‚â–…â–‚â–„â–â–…â–‚â–…â–‚â–…â–‚â–…â–â–…â–‚â–†â–â–„â–
wandb: train_error_energy â–ˆâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–â–â–â–‚â–â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–†â–‚â–†â–„â–†â–‚â–‡â–‚â–†â–ƒâ–…â–ƒâ–…â–â–…â–„â–ˆâ–â–†
wandb:  valid_error_force â–ƒâ–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–â–‡â–â–…â–…â–…â–ˆâ–„â–‚
wandb:         valid_loss â–â–…â–„â–†â–„â–†â–„â–†â–„â–†â–„â–…â–…â–…â–„â–†â–…â–ˆâ–„â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 2680
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.23983
wandb:   test_error_force 13.10575
wandb:          test_loss 8.50718
wandb: train_error_energy 4.09222
wandb:  train_error_force 3.54042
wandb:         train_loss 1.45421
wandb: valid_error_energy 2.46651
wandb:  valid_error_force 3.72761
wandb:         valid_loss 1.10499
wandb: 
wandb: ğŸš€ View run al_63_20 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/bh04htlz
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_005611-bh04htlz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.469090461730957, Uncertainty Bias: -0.206773579120636
8.749962e-05 0.013161659
2.4450026 4.864822
(48745, 22, 3)
Found uncertainty sample 0 after 42 steps.
Found uncertainty sample 1 after 506 steps.
Found uncertainty sample 2 after 79 steps.
Found uncertainty sample 3 after 637 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1313 steps.
Found uncertainty sample 6 after 1132 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 813 steps.
Found uncertainty sample 11 after 1679 steps.
Found uncertainty sample 12 after 14 steps.
Found uncertainty sample 13 after 97 steps.
Found uncertainty sample 14 after 252 steps.
Found uncertainty sample 15 after 298 steps.
Found uncertainty sample 16 after 392 steps.
Found uncertainty sample 17 after 187 steps.
Found uncertainty sample 18 after 211 steps.
Found uncertainty sample 19 after 62 steps.
Found uncertainty sample 20 after 242 steps.
Found uncertainty sample 21 after 17 steps.
Found uncertainty sample 22 after 1145 steps.
Found uncertainty sample 23 after 1819 steps.
Found uncertainty sample 24 after 103 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 481 steps.
Found uncertainty sample 27 after 56 steps.
Found uncertainty sample 28 after 168 steps.
Found uncertainty sample 29 after 747 steps.
Found uncertainty sample 30 after 182 steps.
Found uncertainty sample 31 after 361 steps.
Found uncertainty sample 32 after 442 steps.
Found uncertainty sample 33 after 54 steps.
Found uncertainty sample 34 after 7 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 207 steps.
Found uncertainty sample 37 after 307 steps.
Found uncertainty sample 38 after 14 steps.
Found uncertainty sample 39 after 250 steps.
Found uncertainty sample 40 after 444 steps.
Found uncertainty sample 41 after 307 steps.
Found uncertainty sample 42 after 30 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 149 steps.
Found uncertainty sample 49 after 110 steps.
Found uncertainty sample 50 after 19 steps.
Found uncertainty sample 51 after 101 steps.
Found uncertainty sample 52 after 114 steps.
Found uncertainty sample 53 after 17 steps.
Found uncertainty sample 54 after 99 steps.
Found uncertainty sample 55 after 116 steps.
Found uncertainty sample 56 after 174 steps.
Found uncertainty sample 57 after 328 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 53 steps.
Found uncertainty sample 60 after 18 steps.
Found uncertainty sample 61 after 1095 steps.
Found uncertainty sample 62 after 326 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 271 steps.
Found uncertainty sample 65 after 555 steps.
Found uncertainty sample 66 after 11 steps.
Found uncertainty sample 67 after 396 steps.
Found uncertainty sample 68 after 207 steps.
Found uncertainty sample 69 after 328 steps.
Found uncertainty sample 70 after 442 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 232 steps.
Found uncertainty sample 73 after 1001 steps.
Found uncertainty sample 74 after 28 steps.
Found uncertainty sample 75 after 19 steps.
Found uncertainty sample 76 after 29 steps.
Found uncertainty sample 77 after 172 steps.
Found uncertainty sample 78 after 14 steps.
Found uncertainty sample 79 after 290 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 275 steps.
Found uncertainty sample 82 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1776 steps.
Found uncertainty sample 85 after 18 steps.
Found uncertainty sample 86 after 576 steps.
Found uncertainty sample 87 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1379 steps.
Found uncertainty sample 90 after 190 steps.
Found uncertainty sample 91 after 288 steps.
Found uncertainty sample 92 after 1368 steps.
Found uncertainty sample 93 after 1022 steps.
Found uncertainty sample 94 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 58 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 611 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_010551-najogohr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_21
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/najogohr
Training model 21. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.900536978758878, Training Loss Force: 3.813836641498407, time: 1.573714256286621
Validation Loss Energy: 2.683136488132332, Validation Loss Force: 3.746010915809549, time: 0.11155080795288086
Test Loss Energy: 10.05866442806474, Test Loss Force: 12.524529596792545, time: 9.375303030014038


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.042098521015277, Training Loss Force: 3.544944377951527, time: 1.4291551113128662
Validation Loss Energy: 5.576924000379291, Validation Loss Force: 3.70388146806172, time: 0.08420157432556152
Test Loss Energy: 10.529967103479313, Test Loss Force: 12.408781217727721, time: 9.264737129211426


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.109307641286954, Training Loss Force: 3.5604074141213364, time: 1.431652307510376
Validation Loss Energy: 6.001200463059849, Validation Loss Force: 3.7290338407820043, time: 0.08330178260803223
Test Loss Energy: 10.75902474611609, Test Loss Force: 12.48500704515615, time: 9.451666831970215


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.083300665664306, Training Loss Force: 3.550706755996527, time: 1.4354584217071533
Validation Loss Energy: 3.5879223909406246, Validation Loss Force: 3.696150395315107, time: 0.09052252769470215
Test Loss Energy: 10.166876041775048, Test Loss Force: 12.894497250636899, time: 9.821083068847656


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.087080098182356, Training Loss Force: 3.5587731172885992, time: 1.4243433475494385
Validation Loss Energy: 1.7435193415031391, Validation Loss Force: 3.7236640629933824, time: 0.08046531677246094
Test Loss Energy: 10.76817124851429, Test Loss Force: 13.57570205786961, time: 9.267662525177002


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.102615048808055, Training Loss Force: 3.554391659063593, time: 1.4299900531768799
Validation Loss Energy: 5.003551840291822, Validation Loss Force: 3.736727171108382, time: 0.08243513107299805
Test Loss Energy: 12.957866180232374, Test Loss Force: 14.522205736892674, time: 9.429885149002075


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.12260274761727, Training Loss Force: 3.54903388365382, time: 1.5197124481201172
Validation Loss Energy: 5.315392408412727, Validation Loss Force: 3.701004805365479, time: 0.08914613723754883
Test Loss Energy: 12.829168035145246, Test Loss Force: 14.402567092355465, time: 9.219502687454224


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.149936952921882, Training Loss Force: 3.53684506674469, time: 1.4226946830749512
Validation Loss Energy: 3.15728315741518, Validation Loss Force: 3.7265194607861876, time: 0.08060002326965332
Test Loss Energy: 11.589042612495296, Test Loss Force: 14.071482125291835, time: 9.284162044525146


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.112116031575521, Training Loss Force: 3.5480870637323507, time: 1.609095811843872
Validation Loss Energy: 2.696609307475839, Validation Loss Force: 3.744160312537743, time: 0.08368945121765137
Test Loss Energy: 10.212517369002098, Test Loss Force: 13.165461221874056, time: 9.217619895935059


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.128066793796499, Training Loss Force: 3.5568809747037085, time: 1.4521727561950684
Validation Loss Energy: 5.405513737182372, Validation Loss Force: 3.716551899137876, time: 0.08785080909729004
Test Loss Energy: 10.604018565531952, Test Loss Force: 12.890870359629286, time: 9.19640326499939


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.143943752712973, Training Loss Force: 3.569300394946724, time: 1.4412992000579834
Validation Loss Energy: 6.118536414844789, Validation Loss Force: 3.8014515251009415, time: 0.08268976211547852
Test Loss Energy: 10.960023422989124, Test Loss Force: 13.039872563974283, time: 9.422824382781982


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.031466490321072, Training Loss Force: 3.588842253818653, time: 1.4101898670196533
Validation Loss Energy: 3.838568022366499, Validation Loss Force: 3.6943940924266014, time: 0.0826265811920166
Test Loss Energy: 10.437430940645958, Test Loss Force: 13.06175128347341, time: 10.737653732299805


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.092958810618443, Training Loss Force: 3.558865458439356, time: 1.5754809379577637
Validation Loss Energy: 2.1644359494407928, Validation Loss Force: 3.7215470360884737, time: 0.10172867774963379
Test Loss Energy: 11.310335425465327, Test Loss Force: 14.164063726620643, time: 11.120169639587402


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.132897347535271, Training Loss Force: 3.5819564177934895, time: 1.571650505065918
Validation Loss Energy: 5.204073422289048, Validation Loss Force: 3.829875241578624, time: 0.09226775169372559
Test Loss Energy: 13.066162179719953, Test Loss Force: 14.615689337123232, time: 10.615541934967041


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.05646631420107, Training Loss Force: 3.5813260951890924, time: 1.468325138092041
Validation Loss Energy: 5.647396781043123, Validation Loss Force: 3.7712383280320108, time: 0.08873558044433594
Test Loss Energy: 13.425480004326417, Test Loss Force: 14.769069366100466, time: 10.3507239818573


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.090606532481154, Training Loss Force: 3.5542223767649634, time: 1.46075439453125
Validation Loss Energy: 3.318189226322102, Validation Loss Force: 3.7138710739554783, time: 0.09510254859924316
Test Loss Energy: 11.74738052505895, Test Loss Force: 14.09438601820204, time: 11.276052474975586


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.128404346144942, Training Loss Force: 3.569271665508069, time: 1.484834909439087
Validation Loss Energy: 2.45039223135875, Validation Loss Force: 3.8045288126976637, time: 0.10033345222473145
Test Loss Energy: 10.42035951044218, Test Loss Force: 13.54272862317387, time: 11.038235664367676


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.093573437607467, Training Loss Force: 3.5563672846622993, time: 1.420954942703247
Validation Loss Energy: 5.653491458236313, Validation Loss Force: 3.7658469513362345, time: 0.10164451599121094
Test Loss Energy: 10.813553137041161, Test Loss Force: 12.758157991138113, time: 11.224483489990234


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.089819601803538, Training Loss Force: 3.5441093968078685, time: 1.4767203330993652
Validation Loss Energy: 5.7786154860692225, Validation Loss Force: 3.7491184864101177, time: 0.09449291229248047
Test Loss Energy: 10.927156375181243, Test Loss Force: 12.838509591052052, time: 11.46767807006836


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.094585974360266, Training Loss Force: 3.59958949276114, time: 1.4339423179626465
Validation Loss Energy: 3.648018900738794, Validation Loss Force: 3.747657999481977, time: 0.08809590339660645
Test Loss Energy: 10.474267780619723, Test Loss Force: 13.269825938134124, time: 11.210198640823364

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–‚â–â–‚â–‡â–‡â–„â–â–‚â–ƒâ–‚â–„â–‡â–ˆâ–…â–‚â–ƒâ–ƒâ–‚
wandb:   test_error_force â–â–â–â–‚â–„â–‡â–‡â–†â–ƒâ–‚â–ƒâ–ƒâ–†â–ˆâ–ˆâ–†â–„â–‚â–‚â–„
wandb:          test_loss â–â–â–â–â–ƒâ–‡â–†â–„â–â–â–â–â–„â–‡â–ˆâ–„â–‚â–â–â–
wandb: train_error_energy â–ˆâ–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–‚â–â–‚
wandb:  train_error_force â–ˆâ–â–‚â–â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–ƒ
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–‡â–ˆâ–„â–â–†â–‡â–ƒâ–ƒâ–‡â–ˆâ–„â–‚â–‡â–‡â–„â–‚â–‡â–‡â–„
wandb:  valid_error_force â–„â–â–ƒâ–â–ƒâ–ƒâ–â–ƒâ–„â–‚â–‡â–â–‚â–ˆâ–…â–‚â–‡â–…â–„â–„
wandb:         valid_loss â–‚â–‡â–ˆâ–ƒâ–â–†â–†â–ƒâ–‚â–†â–ˆâ–„â–â–‡â–ˆâ–ƒâ–‚â–‡â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2770
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.47427
wandb:   test_error_force 13.26983
wandb:          test_loss 8.49743
wandb: train_error_energy 4.09459
wandb:  train_error_force 3.59959
wandb:         train_loss 1.47526
wandb: valid_error_energy 3.64802
wandb:  valid_error_force 3.74766
wandb:         valid_loss 1.36102
wandb: 
wandb: ğŸš€ View run al_63_21 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/najogohr
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_010551-najogohr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.582522392272949, Uncertainty Bias: -0.23849815130233765
0.00010585785 0.00057792664
2.4536228 5.031882
(48745, 22, 3)
Found uncertainty sample 0 after 2927 steps.
Found uncertainty sample 1 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 19 steps.
Found uncertainty sample 4 after 1006 steps.
Found uncertainty sample 5 after 998 steps.
Found uncertainty sample 6 after 204 steps.
Found uncertainty sample 7 after 59 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1379 steps.
Found uncertainty sample 10 after 26 steps.
Found uncertainty sample 11 after 501 steps.
Found uncertainty sample 12 after 375 steps.
Found uncertainty sample 13 after 741 steps.
Found uncertainty sample 14 after 867 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 813 steps.
Found uncertainty sample 17 after 768 steps.
Found uncertainty sample 18 after 576 steps.
Found uncertainty sample 19 after 45 steps.
Found uncertainty sample 20 after 858 steps.
Found uncertainty sample 21 after 52 steps.
Found uncertainty sample 22 after 80 steps.
Found uncertainty sample 23 after 939 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 526 steps.
Found uncertainty sample 26 after 405 steps.
Found uncertainty sample 27 after 187 steps.
Found uncertainty sample 28 after 32 steps.
Found uncertainty sample 29 after 652 steps.
Found uncertainty sample 30 after 1816 steps.
Found uncertainty sample 31 after 8 steps.
Found uncertainty sample 32 after 14 steps.
Found uncertainty sample 33 after 217 steps.
Found uncertainty sample 34 after 440 steps.
Found uncertainty sample 35 after 620 steps.
Found uncertainty sample 36 after 290 steps.
Found uncertainty sample 37 after 226 steps.
Found uncertainty sample 38 after 191 steps.
Found uncertainty sample 39 after 1203 steps.
Found uncertainty sample 40 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 853 steps.
Found uncertainty sample 43 after 11 steps.
Found uncertainty sample 44 after 406 steps.
Found uncertainty sample 45 after 152 steps.
Found uncertainty sample 46 after 38 steps.
Found uncertainty sample 47 after 53 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 340 steps.
Found uncertainty sample 50 after 740 steps.
Found uncertainty sample 51 after 2464 steps.
Found uncertainty sample 52 after 238 steps.
Found uncertainty sample 53 after 98 steps.
Found uncertainty sample 54 after 389 steps.
Found uncertainty sample 55 after 64 steps.
Found uncertainty sample 56 after 77 steps.
Found uncertainty sample 57 after 118 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 202 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 456 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 233 steps.
Found uncertainty sample 64 after 653 steps.
Found uncertainty sample 65 after 1255 steps.
Found uncertainty sample 66 after 97 steps.
Found uncertainty sample 67 after 12 steps.
Found uncertainty sample 68 after 1284 steps.
Found uncertainty sample 69 after 464 steps.
Found uncertainty sample 70 after 6 steps.
Found uncertainty sample 71 after 1324 steps.
Found uncertainty sample 72 after 70 steps.
Found uncertainty sample 73 after 403 steps.
Found uncertainty sample 74 after 1072 steps.
Found uncertainty sample 75 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 359 steps.
Found uncertainty sample 78 after 466 steps.
Found uncertainty sample 79 after 48 steps.
Found uncertainty sample 80 after 1151 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 429 steps.
Found uncertainty sample 83 after 60 steps.
Found uncertainty sample 84 after 29 steps.
Found uncertainty sample 85 after 21 steps.
Found uncertainty sample 86 after 65 steps.
Found uncertainty sample 87 after 906 steps.
Found uncertainty sample 88 after 9 steps.
Found uncertainty sample 89 after 5 steps.
Found uncertainty sample 90 after 359 steps.
Found uncertainty sample 91 after 49 steps.
Found uncertainty sample 92 after 89 steps.
Found uncertainty sample 93 after 211 steps.
Found uncertainty sample 94 after 499 steps.
Found uncertainty sample 95 after 1180 steps.
Found uncertainty sample 96 after 62 steps.
Found uncertainty sample 97 after 10 steps.
Found uncertainty sample 98 after 5 steps.
Found uncertainty sample 99 after 14 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_011624-v77beonz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_22
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/v77beonz
Training model 22. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.3480703274941614, Training Loss Force: 3.974621126909056, time: 1.5731806755065918
Validation Loss Energy: 4.270370924848562, Validation Loss Force: 3.9379702011384143, time: 0.10367894172668457
Test Loss Energy: 13.070387875859053, Test Loss Force: 15.178563763823359, time: 11.189852476119995


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.023271371320535, Training Loss Force: 3.557959340390229, time: 1.5650217533111572
Validation Loss Energy: 3.5611724784328818, Validation Loss Force: 3.707538363445292, time: 0.09317708015441895
Test Loss Energy: 10.295897162046407, Test Loss Force: 12.968085323041368, time: 11.358841180801392


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.079813569410661, Training Loss Force: 3.539935294831345, time: 1.5531468391418457
Validation Loss Energy: 5.449545447443863, Validation Loss Force: 3.733475533570399, time: 0.09254670143127441
Test Loss Energy: 10.808118638891436, Test Loss Force: 12.793702846436853, time: 11.431260824203491


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.01348956645901, Training Loss Force: 3.5328082591509915, time: 1.5597772598266602
Validation Loss Energy: 3.4308373245778236, Validation Loss Force: 3.7528525366527536, time: 0.10532712936401367
Test Loss Energy: 11.766781307690541, Test Loss Force: 13.98461610285495, time: 11.777873516082764


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.080967212070459, Training Loss Force: 3.527035405674411, time: 1.617605209350586
Validation Loss Energy: 4.761957820982485, Validation Loss Force: 3.7385331292164294, time: 0.1057889461517334
Test Loss Energy: 12.550258645877859, Test Loss Force: 14.295549470243065, time: 11.399510860443115


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.037972302545594, Training Loss Force: 3.5338485174446324, time: 1.5480382442474365
Validation Loss Energy: 3.8512857706361645, Validation Loss Force: 3.7505155126316687, time: 0.09136414527893066
Test Loss Energy: 10.316920118551257, Test Loss Force: 12.939200853971062, time: 11.224883556365967


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.101184537583276, Training Loss Force: 3.5446671576802045, time: 1.556157112121582
Validation Loss Energy: 5.418192518351501, Validation Loss Force: 3.714776224033675, time: 0.10195279121398926
Test Loss Energy: 10.750048698697803, Test Loss Force: 13.014488351286937, time: 11.453337907791138


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.024745369317629, Training Loss Force: 3.547518201054155, time: 1.5505189895629883
Validation Loss Energy: 2.979383981302206, Validation Loss Force: 3.731457260431709, time: 0.09868717193603516
Test Loss Energy: 11.327838861791575, Test Loss Force: 13.928882855010297, time: 11.234285354614258


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.0779728971827645, Training Loss Force: 3.5657047060219087, time: 1.5877115726470947
Validation Loss Energy: 4.914569464203597, Validation Loss Force: 3.7483866842620257, time: 0.11008167266845703
Test Loss Energy: 12.798900699745776, Test Loss Force: 14.52859806677523, time: 11.302305936813354


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.12566628402921, Training Loss Force: 3.569150152310406, time: 1.581132173538208
Validation Loss Energy: 3.5903756482558844, Validation Loss Force: 3.7700836771758026, time: 0.09224820137023926
Test Loss Energy: 10.34207868695309, Test Loss Force: 12.974206985170271, time: 11.346911191940308


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.088444339315498, Training Loss Force: 3.559901755099105, time: 1.6646263599395752
Validation Loss Energy: 5.018719171835839, Validation Loss Force: 3.787939515987752, time: 0.09973454475402832
Test Loss Energy: 10.612813222700218, Test Loss Force: 12.892911748390103, time: 11.083353519439697


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.0992572680874755, Training Loss Force: 3.54279586597064, time: 1.560978889465332
Validation Loss Energy: 3.1670474221332596, Validation Loss Force: 3.7337025447646717, time: 0.09505462646484375
Test Loss Energy: 11.722138377618322, Test Loss Force: 14.584691287856547, time: 11.37132716178894


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.12916102060726, Training Loss Force: 3.560019752427735, time: 1.6200275421142578
Validation Loss Energy: 4.85099875905609, Validation Loss Force: 3.7756269142291017, time: 0.10989737510681152
Test Loss Energy: 12.779547696550186, Test Loss Force: 14.900035775326158, time: 11.102007150650024


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.107701957098212, Training Loss Force: 3.5631181515335926, time: 1.6159658432006836
Validation Loss Energy: 4.025352922082294, Validation Loss Force: 3.7176881797895454, time: 0.10602784156799316
Test Loss Energy: 10.383484926299472, Test Loss Force: 13.233265408420124, time: 11.318631410598755


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.080048860186011, Training Loss Force: 3.5363131259381313, time: 1.6481835842132568
Validation Loss Energy: 5.553320180234362, Validation Loss Force: 3.7275772505639173, time: 0.10772824287414551
Test Loss Energy: 10.717764507781013, Test Loss Force: 13.192148139998027, time: 10.917085409164429


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.095052355483607, Training Loss Force: 3.5619920617065013, time: 1.5865983963012695
Validation Loss Energy: 3.1941821069251657, Validation Loss Force: 3.753752231336875, time: 0.0913231372833252
Test Loss Energy: 11.817717847104598, Test Loss Force: 14.53914427441958, time: 11.363359928131104


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.087878893286328, Training Loss Force: 3.5645162341965357, time: 1.565053939819336
Validation Loss Energy: 4.8606764054764025, Validation Loss Force: 3.786984933359159, time: 0.10957169532775879
Test Loss Energy: 12.680170051021763, Test Loss Force: 15.069498644121216, time: 11.653608083724976


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.060971046501582, Training Loss Force: 3.542760934129847, time: 1.454129695892334
Validation Loss Energy: 4.1058411382608115, Validation Loss Force: 3.7301003146564256, time: 0.0856025218963623
Test Loss Energy: 10.577593035337543, Test Loss Force: 13.13061547308188, time: 10.381774663925171


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.132682038329784, Training Loss Force: 3.6387266203870334, time: 1.5744667053222656
Validation Loss Energy: 5.450761888988066, Validation Loss Force: 3.7457961514183022, time: 0.08829450607299805
Test Loss Energy: 10.647187775600242, Test Loss Force: 12.848921311195488, time: 10.2329580783844


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.1198550362066175, Training Loss Force: 3.5453742113897766, time: 1.56732177734375
Validation Loss Energy: 3.444247239887606, Validation Loss Force: 3.7068362876281817, time: 0.09226417541503906
Test Loss Energy: 11.857981768084617, Test Loss Force: 14.4084818283968, time: 10.050129652023315

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–â–‚â–…â–‡â–â–‚â–„â–‡â–â–‚â–…â–‡â–â–‚â–…â–‡â–‚â–‚â–…
wandb:   test_error_force â–ˆâ–‚â–â–„â–…â–â–‚â–„â–†â–‚â–â–†â–‡â–‚â–‚â–†â–ˆâ–‚â–â–†
wandb:          test_loss â–ˆâ–‚â–‚â–„â–…â–â–â–ƒâ–…â–â–â–„â–…â–â–â–„â–…â–â–â–„
wandb: train_error_energy â–ˆâ–â–‚â–â–‚â–‚â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–ƒâ–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–
wandb: valid_error_energy â–…â–ƒâ–ˆâ–‚â–†â–ƒâ–ˆâ–â–†â–ƒâ–‡â–‚â–†â–„â–ˆâ–‚â–†â–„â–ˆâ–‚
wandb:  valid_error_force â–ˆâ–â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–ƒâ–‚â–‚â–
wandb:         valid_loss â–†â–‚â–ˆâ–‚â–†â–ƒâ–‡â–â–†â–‚â–†â–â–†â–ƒâ–ˆâ–‚â–†â–„â–ˆâ–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 2860
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.85798
wandb:   test_error_force 14.40848
wandb:          test_loss 9.912
wandb: train_error_energy 4.11986
wandb:  train_error_force 3.54537
wandb:         train_loss 1.46244
wandb: valid_error_energy 3.44425
wandb:  valid_error_force 3.70684
wandb:         valid_loss 1.30673
wandb: 
wandb: ğŸš€ View run al_63_22 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/v77beonz
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_011624-v77beonz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.7789173126220703, Uncertainty Bias: -0.27381205558776855
0.0002670288 0.33630198
2.398714 5.1523094
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 20 steps.
Found uncertainty sample 3 after 65 steps.
Found uncertainty sample 4 after 776 steps.
Found uncertainty sample 5 after 789 steps.
Found uncertainty sample 6 after 205 steps.
Found uncertainty sample 7 after 222 steps.
Found uncertainty sample 8 after 642 steps.
Found uncertainty sample 9 after 1278 steps.
Found uncertainty sample 10 after 1422 steps.
Found uncertainty sample 11 after 203 steps.
Found uncertainty sample 12 after 1423 steps.
Found uncertainty sample 13 after 461 steps.
Found uncertainty sample 14 after 1119 steps.
Found uncertainty sample 15 after 367 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 34 steps.
Found uncertainty sample 18 after 21 steps.
Found uncertainty sample 19 after 24 steps.
Found uncertainty sample 20 after 124 steps.
Found uncertainty sample 21 after 538 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 994 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 19 steps.
Found uncertainty sample 27 after 56 steps.
Found uncertainty sample 28 after 6 steps.
Found uncertainty sample 29 after 5 steps.
Found uncertainty sample 30 after 1034 steps.
Found uncertainty sample 31 after 410 steps.
Found uncertainty sample 32 after 737 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 126 steps.
Found uncertainty sample 35 after 228 steps.
Found uncertainty sample 36 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 41 steps.
Found uncertainty sample 39 after 106 steps.
Found uncertainty sample 40 after 485 steps.
Found uncertainty sample 41 after 1013 steps.
Found uncertainty sample 42 after 153 steps.
Found uncertainty sample 43 after 251 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 112 steps.
Found uncertainty sample 46 after 508 steps.
Found uncertainty sample 47 after 12 steps.
Found uncertainty sample 48 after 1177 steps.
Found uncertainty sample 49 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 66 steps.
Found uncertainty sample 54 after 151 steps.
Found uncertainty sample 55 after 13 steps.
Found uncertainty sample 56 after 37 steps.
Found uncertainty sample 57 after 351 steps.
Found uncertainty sample 58 after 809 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 939 steps.
Found uncertainty sample 61 after 103 steps.
Found uncertainty sample 62 after 33 steps.
Found uncertainty sample 63 after 413 steps.
Found uncertainty sample 64 after 353 steps.
Found uncertainty sample 65 after 608 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1790 steps.
Found uncertainty sample 68 after 158 steps.
Found uncertainty sample 69 after 90 steps.
Found uncertainty sample 70 after 416 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 117 steps.
Found uncertainty sample 73 after 3843 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 63 steps.
Found uncertainty sample 76 after 14 steps.
Found uncertainty sample 77 after 38 steps.
Found uncertainty sample 78 after 719 steps.
Found uncertainty sample 79 after 464 steps.
Found uncertainty sample 80 after 903 steps.
Found uncertainty sample 81 after 3255 steps.
Found uncertainty sample 82 after 166 steps.
Found uncertainty sample 83 after 2896 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 991 steps.
Found uncertainty sample 86 after 1289 steps.
Found uncertainty sample 87 after 116 steps.
Found uncertainty sample 88 after 89 steps.
Found uncertainty sample 89 after 247 steps.
Found uncertainty sample 90 after 70 steps.
Found uncertainty sample 91 after 464 steps.
Found uncertainty sample 92 after 47 steps.
Found uncertainty sample 93 after 332 steps.
Found uncertainty sample 94 after 551 steps.
Found uncertainty sample 95 after 28 steps.
Found uncertainty sample 96 after 1996 steps.
Found uncertainty sample 97 after 571 steps.
Found uncertainty sample 98 after 16 steps.
Found uncertainty sample 99 after 162 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_012736-zs3oisw1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_23
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zs3oisw1
Training model 23. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.825146896353937, Training Loss Force: 3.7163940470648944, time: 1.6872437000274658
Validation Loss Energy: 5.131979610242862, Validation Loss Force: 3.748242060651746, time: 0.10714912414550781
Test Loss Energy: 13.135083689729298, Test Loss Force: 15.043631516849285, time: 11.450058221817017


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.118937224155181, Training Loss Force: 3.557576880513176, time: 1.610563039779663
Validation Loss Energy: 5.2944183108812055, Validation Loss Force: 3.7066708444527574, time: 0.09984135627746582
Test Loss Energy: 10.763827199643808, Test Loss Force: 13.096341921041159, time: 10.075452089309692


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.155805351945644, Training Loss Force: 3.5453899778816926, time: 1.6012811660766602
Validation Loss Energy: 1.988187531041155, Validation Loss Force: 3.7232779546943435, time: 0.09885144233703613
Test Loss Energy: 11.122942985225844, Test Loss Force: 14.566785121154389, time: 11.616816282272339


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.056012036861719, Training Loss Force: 3.5689358034884213, time: 1.646036148071289
Validation Loss Energy: 3.2580290331481967, Validation Loss Force: 3.7203346025888973, time: 0.11311674118041992
Test Loss Energy: 12.115860403929624, Test Loss Force: 14.98021758710181, time: 9.609079837799072


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.061972159639178, Training Loss Force: 3.565120595130679, time: 1.5266056060791016
Validation Loss Energy: 5.946793038605562, Validation Loss Force: 3.718763115091512, time: 0.0860295295715332
Test Loss Energy: 10.899338377070839, Test Loss Force: 12.961797873823967, time: 9.34960389137268


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.152935666215976, Training Loss Force: 3.5616143102842623, time: 1.5353679656982422
Validation Loss Energy: 4.81832896463471, Validation Loss Force: 3.786020998626438, time: 0.08727884292602539
Test Loss Energy: 12.620930022937912, Test Loss Force: 14.707880234541626, time: 9.139742612838745


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.092125143858471, Training Loss Force: 3.5809949275502273, time: 1.4752299785614014
Validation Loss Energy: 2.491518888607893, Validation Loss Force: 3.714513808445199, time: 0.08723926544189453
Test Loss Energy: 10.347802962765583, Test Loss Force: 13.308588015181016, time: 9.243116617202759


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.065998241663844, Training Loss Force: 3.564885401443865, time: 1.4529392719268799
Validation Loss Energy: 3.80549742348245, Validation Loss Force: 3.7025841976175444, time: 0.0854332447052002
Test Loss Energy: 10.40796901003957, Test Loss Force: 13.19140123540255, time: 9.374451398849487


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.147280026735529, Training Loss Force: 3.5498412466405087, time: 1.4756624698638916
Validation Loss Energy: 5.105724913915663, Validation Loss Force: 3.80188906997269, time: 0.08815646171569824
Test Loss Energy: 13.194747148258983, Test Loss Force: 15.040686935400307, time: 9.156708717346191


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.11492533549285, Training Loss Force: 3.5739341056928997, time: 1.6202654838562012
Validation Loss Energy: 5.656386825140812, Validation Loss Force: 3.6972216565761657, time: 0.08881926536560059
Test Loss Energy: 10.884082733311416, Test Loss Force: 13.106711280419747, time: 9.287460088729858


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.137942376946713, Training Loss Force: 3.5796440824306837, time: 1.4581336975097656
Validation Loss Energy: 1.9510198198720343, Validation Loss Force: 3.7755596681470314, time: 0.0922856330871582
Test Loss Energy: 10.932730038049748, Test Loss Force: 13.720216124256366, time: 9.408292770385742


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.098074173881596, Training Loss Force: 3.5806960265447594, time: 1.5031609535217285
Validation Loss Energy: 3.1138141320854493, Validation Loss Force: 3.7531842023456132, time: 0.08658599853515625
Test Loss Energy: 12.010823212859167, Test Loss Force: 14.86585269457549, time: 9.25113296508789


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.163054665629291, Training Loss Force: 3.580737476525989, time: 1.4821276664733887
Validation Loss Energy: 5.866858377923821, Validation Loss Force: 3.7101368010436966, time: 0.08478403091430664
Test Loss Energy: 11.125772939862754, Test Loss Force: 12.958827043063526, time: 9.276071548461914


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.138692666619646, Training Loss Force: 3.5524431523707176, time: 1.5547664165496826
Validation Loss Energy: 4.700616479324006, Validation Loss Force: 3.7647943942834337, time: 0.10534787178039551
Test Loss Energy: 12.733045532736593, Test Loss Force: 14.843156226718271, time: 9.985085487365723


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.1446115704650435, Training Loss Force: 3.592452287048018, time: 1.6192805767059326
Validation Loss Energy: 2.4199068212589223, Validation Loss Force: 3.7132530557409558, time: 0.08874034881591797
Test Loss Energy: 10.364848787183384, Test Loss Force: 13.406932667064693, time: 11.016117572784424


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.054254062757473, Training Loss Force: 3.560633091158374, time: 1.7114238739013672
Validation Loss Energy: 3.8672092865997527, Validation Loss Force: 3.799009735246698, time: 0.10834932327270508
Test Loss Energy: 10.294565712471234, Test Loss Force: 13.104671197362823, time: 11.403187990188599


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.09056103437155, Training Loss Force: 3.565011424997584, time: 1.6514437198638916
Validation Loss Energy: 5.393521667659286, Validation Loss Force: 3.69364837345689, time: 0.10903453826904297
Test Loss Energy: 12.871499449637465, Test Loss Force: 14.40182013180125, time: 10.51634430885315


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.078014986759168, Training Loss Force: 3.5597503372865336, time: 1.5992183685302734
Validation Loss Energy: 5.430845469899701, Validation Loss Force: 3.828466191223193, time: 0.10531091690063477
Test Loss Energy: 10.850769997469564, Test Loss Force: 12.868108779116897, time: 10.51719045639038


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.10304283954104, Training Loss Force: 3.5408618906618243, time: 1.8134233951568604
Validation Loss Energy: 1.971301193690575, Validation Loss Force: 3.757937075265442, time: 0.10478091239929199
Test Loss Energy: 10.94343542498826, Test Loss Force: 13.931126035644445, time: 10.899168968200684


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.052169784977059, Training Loss Force: 3.551916233371237, time: 1.6305437088012695
Validation Loss Energy: 3.1967024989879222, Validation Loss Force: 3.775721437281548, time: 0.0938117504119873
Test Loss Energy: 11.834368649462684, Test Loss Force: 14.490206455228424, time: 10.86886191368103

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‚â–ƒâ–…â–‚â–‡â–â–â–ˆâ–‚â–ƒâ–…â–ƒâ–‡â–â–â–‡â–‚â–ƒâ–…
wandb:   test_error_force â–ˆâ–‚â–†â–ˆâ–â–‡â–‚â–‚â–ˆâ–‚â–„â–‡â–â–‡â–ƒâ–‚â–†â–â–„â–†
wandb:          test_loss â–‡â–â–„â–†â–‚â–†â–‚â–â–ˆâ–â–ƒâ–†â–‚â–‡â–â–â–‡â–â–ƒâ–…
wandb: train_error_energy â–ˆâ–‚â–‚â–â–â–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–‚â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–‚â–â–‚â–â–‚â–â–â–â–â–
wandb: valid_error_energy â–‡â–‡â–â–ƒâ–ˆâ–†â–‚â–„â–‡â–‡â–â–ƒâ–ˆâ–†â–‚â–„â–‡â–‡â–â–ƒ
wandb:  valid_error_force â–„â–‚â–ƒâ–‚â–‚â–†â–‚â–â–‡â–â–…â–„â–‚â–…â–‚â–†â–â–ˆâ–„â–…
wandb:         valid_loss â–†â–†â–â–ƒâ–ˆâ–†â–â–ƒâ–†â–‡â–â–‚â–‡â–…â–â–„â–‡â–‡â–â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2950
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.83437
wandb:   test_error_force 14.49021
wandb:          test_loss 9.82414
wandb: train_error_energy 4.05217
wandb:  train_error_force 3.55192
wandb:         train_loss 1.45006
wandb: valid_error_energy 3.1967
wandb:  valid_error_force 3.77572
wandb:         valid_loss 1.275
wandb: 
wandb: ğŸš€ View run al_63_23 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zs3oisw1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_012736-zs3oisw1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.7696661949157715, Uncertainty Bias: -0.2727915346622467
0.00022888184 0.05792308
2.5117087 5.229575
(48745, 22, 3)
Found uncertainty sample 0 after 15 steps.
Found uncertainty sample 1 after 3774 steps.
Found uncertainty sample 2 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 318 steps.
Found uncertainty sample 6 after 1275 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 844 steps.
Found uncertainty sample 9 after 66 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 38 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 78 steps.
Found uncertainty sample 14 after 113 steps.
Found uncertainty sample 15 after 341 steps.
Found uncertainty sample 16 after 2057 steps.
Found uncertainty sample 17 after 75 steps.
Found uncertainty sample 18 after 321 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 47 steps.
Found uncertainty sample 21 after 1333 steps.
Found uncertainty sample 22 after 583 steps.
Found uncertainty sample 23 after 804 steps.
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 212 steps.
Found uncertainty sample 27 after 651 steps.
Found uncertainty sample 28 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 8 steps.
Found uncertainty sample 31 after 82 steps.
Found uncertainty sample 32 after 501 steps.
Found uncertainty sample 33 after 404 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 308 steps.
Found uncertainty sample 36 after 189 steps.
Found uncertainty sample 37 after 10 steps.
Found uncertainty sample 38 after 479 steps.
Found uncertainty sample 39 after 283 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 155 steps.
Found uncertainty sample 42 after 206 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 29 steps.
Found uncertainty sample 45 after 147 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 204 steps.
Found uncertainty sample 48 after 294 steps.
Found uncertainty sample 49 after 2091 steps.
Found uncertainty sample 50 after 44 steps.
Found uncertainty sample 51 after 103 steps.
Found uncertainty sample 52 after 431 steps.
Found uncertainty sample 53 after 53 steps.
Found uncertainty sample 54 after 34 steps.
Found uncertainty sample 55 after 500 steps.
Found uncertainty sample 56 after 241 steps.
Found uncertainty sample 57 after 403 steps.
Found uncertainty sample 58 after 717 steps.
Found uncertainty sample 59 after 3 steps.
Found uncertainty sample 60 after 170 steps.
Found uncertainty sample 61 after 4 steps.
Found uncertainty sample 62 after 29 steps.
Found uncertainty sample 63 after 206 steps.
Found uncertainty sample 64 after 202 steps.
Found uncertainty sample 65 after 85 steps.
Found uncertainty sample 66 after 304 steps.
Found uncertainty sample 67 after 858 steps.
Found uncertainty sample 68 after 1196 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1061 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 56 steps.
Found uncertainty sample 73 after 43 steps.
Found uncertainty sample 74 after 582 steps.
Found uncertainty sample 75 after 1210 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 197 steps.
Found uncertainty sample 78 after 1632 steps.
Found uncertainty sample 79 after 629 steps.
Found uncertainty sample 80 after 129 steps.
Found uncertainty sample 81 after 69 steps.
Found uncertainty sample 82 after 170 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 505 steps.
Found uncertainty sample 85 after 23 steps.
Found uncertainty sample 86 after 67 steps.
Found uncertainty sample 87 after 123 steps.
Found uncertainty sample 88 after 1472 steps.
Found uncertainty sample 89 after 98 steps.
Found uncertainty sample 90 after 219 steps.
Found uncertainty sample 91 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 130 steps.
Found uncertainty sample 94 after 3027 steps.
Found uncertainty sample 95 after 444 steps.
Found uncertainty sample 96 after 69 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 178 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_013748-yn086icf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_24
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/yn086icf
Training model 24. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.619683486214269, Training Loss Force: 3.7776228114625106, time: 1.6935670375823975
Validation Loss Energy: 3.4011578654516823, Validation Loss Force: 3.6759137779948983, time: 0.10898756980895996
Test Loss Energy: 10.496425370490888, Test Loss Force: 13.40684778698523, time: 11.305490016937256


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.124065451147385, Training Loss Force: 3.544363975668232, time: 1.7219669818878174
Validation Loss Energy: 2.1144441654848265, Validation Loss Force: 3.7052838383427735, time: 0.10052752494812012
Test Loss Energy: 11.176323060873102, Test Loss Force: 13.929537063955873, time: 10.701366186141968


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.030442055882989, Training Loss Force: 3.53007832018666, time: 1.7335901260375977
Validation Loss Energy: 4.44920141466438, Validation Loss Force: 3.7577054993536767, time: 0.13596439361572266
Test Loss Energy: 12.314504001344936, Test Loss Force: 14.466747241789571, time: 10.73160696029663


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.086831006035231, Training Loss Force: 3.54150419832491, time: 1.6117973327636719
Validation Loss Energy: 5.322692616361969, Validation Loss Force: 3.765137861441027, time: 0.10035872459411621
Test Loss Energy: 13.040087813924163, Test Loss Force: 14.711783445135453, time: 10.892162561416626


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.062604539673372, Training Loss Force: 3.54620595344049, time: 1.7161755561828613
Validation Loss Energy: 3.2247990031556637, Validation Loss Force: 3.70411360064635, time: 0.10921716690063477
Test Loss Energy: 11.781215006186496, Test Loss Force: 14.511959580002788, time: 10.99475884437561


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.15753114244454, Training Loss Force: 3.542185434653847, time: 1.5707910060882568
Validation Loss Energy: 2.586121879277627, Validation Loss Force: 3.672217915510789, time: 0.10202741622924805
Test Loss Energy: 10.283491454856678, Test Loss Force: 13.290403728886561, time: 10.842499017715454


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.130610773189761, Training Loss Force: 3.5627300385611607, time: 1.5980238914489746
Validation Loss Energy: 5.046744978549748, Validation Loss Force: 3.7721519395377525, time: 0.10229158401489258
Test Loss Energy: 10.611601156429863, Test Loss Force: 12.908083211554295, time: 11.029002904891968


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.066109996598855, Training Loss Force: 3.576802236367791, time: 1.5618350505828857
Validation Loss Energy: 6.1889190145045205, Validation Loss Force: 3.7063180251910346, time: 0.10897302627563477
Test Loss Energy: 10.926023751666278, Test Loss Force: 12.897114165694093, time: 10.85214114189148


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.059105047937114, Training Loss Force: 3.5681823199561205, time: 1.5478508472442627
Validation Loss Energy: 3.7316163575364745, Validation Loss Force: 3.726468024213478, time: 0.10462117195129395
Test Loss Energy: 10.592769366911963, Test Loss Force: 13.254185349551097, time: 10.995514631271362


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.125287450044693, Training Loss Force: 3.5652510236270114, time: 1.6140034198760986
Validation Loss Energy: 2.1785978184563732, Validation Loss Force: 3.7384000844017375, time: 0.10399508476257324
Test Loss Energy: 11.414429170861904, Test Loss Force: 14.347401984253743, time: 11.153169870376587


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.054753350985845, Training Loss Force: 3.5480032965964776, time: 1.5595118999481201
Validation Loss Energy: 4.752619745009712, Validation Loss Force: 3.720391383221109, time: 0.10809564590454102
Test Loss Energy: 12.786802117427667, Test Loss Force: 14.936922460546011, time: 11.572062015533447


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.071544654316886, Training Loss Force: 3.5588516764450486, time: 1.5241212844848633
Validation Loss Energy: 5.368103539313337, Validation Loss Force: 3.812560488791279, time: 0.09766173362731934
Test Loss Energy: 13.032486136433285, Test Loss Force: 15.28667806995109, time: 11.176037073135376


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.0263259171053685, Training Loss Force: 3.5618037513213014, time: 1.6002459526062012
Validation Loss Energy: 3.1983335504818435, Validation Loss Force: 3.793418735661597, time: 0.09569597244262695
Test Loss Energy: 11.706678311924867, Test Loss Force: 14.430172885603996, time: 10.974166870117188


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.066961503335912, Training Loss Force: 3.5667348697205026, time: 1.6198441982269287
Validation Loss Energy: 2.660707247732172, Validation Loss Force: 3.701898599317233, time: 0.10354995727539062
Test Loss Energy: 10.302156524640438, Test Loss Force: 13.295553121928164, time: 10.916668176651001


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.081365179324542, Training Loss Force: 3.5450278859937305, time: 1.7795004844665527
Validation Loss Energy: 5.350212518776331, Validation Loss Force: 3.728645060453214, time: 0.12273263931274414
Test Loss Energy: 10.663429566214047, Test Loss Force: 12.859889074667938, time: 10.848133087158203


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.1252791374538935, Training Loss Force: 3.5762414710784576, time: 1.694037675857544
Validation Loss Energy: 6.041518497305557, Validation Loss Force: 3.784854991643787, time: 0.10326623916625977
Test Loss Energy: 10.904702758921541, Test Loss Force: 12.875280455673424, time: 10.878193378448486


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.07654524666156, Training Loss Force: 3.5943357580751814, time: 1.6055538654327393
Validation Loss Energy: 3.8825841457877743, Validation Loss Force: 3.769463988942685, time: 0.10754108428955078
Test Loss Energy: 10.526131798814344, Test Loss Force: 13.248754190272749, time: 11.086887836456299


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.114461825810892, Training Loss Force: 3.549016703725651, time: 1.555506706237793
Validation Loss Energy: 1.9550425606623658, Validation Loss Force: 3.692329838109278, time: 0.10388469696044922
Test Loss Energy: 11.123228356137973, Test Loss Force: 14.069666525376606, time: 10.863911628723145


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.098132849255598, Training Loss Force: 3.5395811572641063, time: 1.625878095626831
Validation Loss Energy: 4.992439981902587, Validation Loss Force: 3.7330157217840574, time: 0.10915040969848633
Test Loss Energy: 12.875502061603326, Test Loss Force: 14.854281798495798, time: 11.12362265586853


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.14099874942084, Training Loss Force: 3.5484829306245764, time: 1.612678050994873
Validation Loss Energy: 5.352840905374863, Validation Loss Force: 3.759628692767055, time: 0.10609793663024902
Test Loss Energy: 12.832687877086595, Test Loss Force: 14.591987537890867, time: 10.891165971755981

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ƒâ–†â–ˆâ–…â–â–‚â–ƒâ–‚â–„â–‡â–ˆâ–…â–â–‚â–ƒâ–‚â–ƒâ–ˆâ–‡
wandb:   test_error_force â–ƒâ–„â–†â–†â–†â–‚â–â–â–‚â–…â–‡â–ˆâ–†â–‚â–â–â–‚â–„â–‡â–†
wandb:          test_loss â–‚â–„â–†â–ˆâ–…â–â–â–â–‚â–…â–ˆâ–ˆâ–…â–â–â–â–‚â–„â–‡â–‡
wandb: train_error_energy â–ˆâ–‚â–â–‚â–â–ƒâ–‚â–â–â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–‚
wandb:         train_loss â–ˆâ–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–â–‚
wandb: valid_error_energy â–ƒâ–â–…â–‡â–ƒâ–‚â–†â–ˆâ–„â–â–†â–‡â–ƒâ–‚â–‡â–ˆâ–„â–â–†â–‡
wandb:  valid_error_force â–â–ƒâ–…â–†â–ƒâ–â–†â–ƒâ–„â–„â–ƒâ–ˆâ–‡â–‚â–„â–‡â–†â–‚â–„â–…
wandb:         valid_loss â–ƒâ–â–…â–†â–‚â–‚â–†â–ˆâ–ƒâ–â–…â–‡â–ƒâ–‚â–†â–ˆâ–„â–â–†â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 3040
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.83269
wandb:   test_error_force 14.59199
wandb:          test_loss 10.31154
wandb: train_error_energy 4.141
wandb:  train_error_force 3.54848
wandb:         train_loss 1.46458
wandb: valid_error_energy 5.35284
wandb:  valid_error_force 3.75963
wandb:         valid_loss 1.86354
wandb: 
wandb: ğŸš€ View run al_63_24 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/yn086icf
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_013748-yn086icf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.9117088317871094, Uncertainty Bias: -0.30939823389053345
0.0001296997 0.8409691
2.403432 5.312649
(48745, 22, 3)
Found uncertainty sample 0 after 251 steps.
Found uncertainty sample 1 after 51 steps.
Found uncertainty sample 2 after 129 steps.
Found uncertainty sample 3 after 3 steps.
Found uncertainty sample 4 after 368 steps.
Found uncertainty sample 5 after 3123 steps.
Found uncertainty sample 6 after 279 steps.
Found uncertainty sample 7 after 231 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 517 steps.
Found uncertainty sample 10 after 889 steps.
Found uncertainty sample 11 after 36 steps.
Found uncertainty sample 12 after 138 steps.
Found uncertainty sample 13 after 1653 steps.
Found uncertainty sample 14 after 462 steps.
Found uncertainty sample 15 after 1124 steps.
Found uncertainty sample 16 after 477 steps.
Found uncertainty sample 17 after 1341 steps.
Found uncertainty sample 18 after 318 steps.
Found uncertainty sample 19 after 233 steps.
Found uncertainty sample 20 after 35 steps.
Found uncertainty sample 21 after 23 steps.
Found uncertainty sample 22 after 307 steps.
Found uncertainty sample 23 after 167 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2852 steps.
Found uncertainty sample 26 after 3653 steps.
Found uncertainty sample 27 after 1617 steps.
Found uncertainty sample 28 after 328 steps.
Found uncertainty sample 29 after 390 steps.
Found uncertainty sample 30 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 82 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 94 steps.
Found uncertainty sample 35 after 2744 steps.
Found uncertainty sample 36 after 19 steps.
Found uncertainty sample 37 after 681 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 17 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 91 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 25 steps.
Found uncertainty sample 47 after 725 steps.
Found uncertainty sample 48 after 2385 steps.
Found uncertainty sample 49 after 38 steps.
Found uncertainty sample 50 after 509 steps.
Found uncertainty sample 51 after 697 steps.
Found uncertainty sample 52 after 64 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 393 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1714 steps.
Found uncertainty sample 57 after 1093 steps.
Found uncertainty sample 58 after 1093 steps.
Found uncertainty sample 59 after 578 steps.
Found uncertainty sample 60 after 18 steps.
Found uncertainty sample 61 after 2816 steps.
Found uncertainty sample 62 after 77 steps.
Found uncertainty sample 63 after 533 steps.
Found uncertainty sample 64 after 241 steps.
Found uncertainty sample 65 after 121 steps.
Found uncertainty sample 66 after 111 steps.
Found uncertainty sample 67 after 625 steps.
Found uncertainty sample 68 after 253 steps.
Found uncertainty sample 69 after 152 steps.
Found uncertainty sample 70 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 959 steps.
Found uncertainty sample 73 after 186 steps.
Found uncertainty sample 74 after 459 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Did not find any uncertainty samples for sample 76.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 136 steps.
Found uncertainty sample 79 after 1472 steps.
Found uncertainty sample 80 after 333 steps.
Found uncertainty sample 81 after 115 steps.
Found uncertainty sample 82 after 1872 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 467 steps.
Found uncertainty sample 88 after 112 steps.
Found uncertainty sample 89 after 1915 steps.
Found uncertainty sample 90 after 58 steps.
Found uncertainty sample 91 after 39 steps.
Found uncertainty sample 92 after 536 steps.
Found uncertainty sample 93 after 652 steps.
Found uncertainty sample 94 after 842 steps.
Found uncertainty sample 95 after 2006 steps.
Found uncertainty sample 96 after 656 steps.
Found uncertainty sample 97 after 295 steps.
Found uncertainty sample 98 after 6 steps.
Found uncertainty sample 99 after 235 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_015032-5h9tb3zj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_25
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5h9tb3zj
Training model 25. Added 98 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9034407775318605, Training Loss Force: 3.8386090343877637, time: 1.5965206623077393
Validation Loss Energy: 1.506082610815561, Validation Loss Force: 4.083590478515685, time: 0.0922245979309082
Test Loss Energy: 10.167796819496704, Test Loss Force: 13.19618151528024, time: 9.464135646820068


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6003944421243435, Training Loss Force: 3.7332058880510814, time: 1.6226451396942139
Validation Loss Energy: 1.983570820304061, Validation Loss Force: 3.687610922234204, time: 0.09193825721740723
Test Loss Energy: 10.985123090155728, Test Loss Force: 13.94077912317351, time: 9.390295028686523


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.630587252469684, Training Loss Force: 3.537858841627779, time: 1.5909183025360107
Validation Loss Energy: 1.6961022252142492, Validation Loss Force: 3.6792057186482943, time: 0.09389400482177734
Test Loss Energy: 11.079548098229441, Test Loss Force: 14.583615788886876, time: 9.58332085609436


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6525621330693774, Training Loss Force: 3.525668037619818, time: 1.6371393203735352
Validation Loss Energy: 3.625503821382879, Validation Loss Force: 3.6641186593313844, time: 0.09208917617797852
Test Loss Energy: 10.598216012771113, Test Loss Force: 13.52568901950247, time: 9.408466100692749


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.606264248473621, Training Loss Force: 3.528752964131525, time: 1.5872886180877686
Validation Loss Energy: 2.2380944575754134, Validation Loss Force: 3.6751189604785086, time: 0.09782552719116211
Test Loss Energy: 10.898974528691548, Test Loss Force: 14.160257231379413, time: 9.395167827606201


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6532147956492316, Training Loss Force: 3.527255459593942, time: 1.5769617557525635
Validation Loss Energy: 1.7932742611551251, Validation Loss Force: 3.685495120009598, time: 0.09150457382202148
Test Loss Energy: 10.913804550287889, Test Loss Force: 14.324116676583671, time: 9.548468589782715


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6009378400454373, Training Loss Force: 3.528563209120781, time: 1.5522449016571045
Validation Loss Energy: 3.713810396613213, Validation Loss Force: 3.6952087274969783, time: 0.09511685371398926
Test Loss Energy: 10.566675676698566, Test Loss Force: 13.708975226182549, time: 9.427112817764282


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.655489502983268, Training Loss Force: 3.532543570065093, time: 1.5999259948730469
Validation Loss Energy: 2.0446909401660607, Validation Loss Force: 3.683580165354619, time: 0.09311199188232422
Test Loss Energy: 11.108326199064807, Test Loss Force: 14.433348837782699, time: 10.186718940734863


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.640243607857019, Training Loss Force: 3.5398580083904676, time: 1.614203691482544
Validation Loss Energy: 1.6493012344521416, Validation Loss Force: 3.659849201127365, time: 0.09691548347473145
Test Loss Energy: 11.114139258078747, Test Loss Force: 14.870382008838614, time: 9.41146469116211


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6962691655865627, Training Loss Force: 3.5342340242340256, time: 1.615875005722046
Validation Loss Energy: 3.6484725891236223, Validation Loss Force: 3.7232193927089643, time: 0.09342741966247559
Test Loss Energy: 10.611973347324376, Test Loss Force: 14.15726443215098, time: 9.708485841751099


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6544608839997217, Training Loss Force: 3.536155858911186, time: 1.665571689605713
Validation Loss Energy: 2.2860468148238273, Validation Loss Force: 3.6600821130540053, time: 0.11036181449890137
Test Loss Energy: 11.429688899609218, Test Loss Force: 15.093196064582314, time: 11.900274991989136


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.654279817629936, Training Loss Force: 3.5477827873890986, time: 1.733752965927124
Validation Loss Energy: 1.690826024134204, Validation Loss Force: 3.7203495214216744, time: 0.10755419731140137
Test Loss Energy: 10.95239533763702, Test Loss Force: 14.680713335003594, time: 11.026546955108643


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.670116572151308, Training Loss Force: 3.5400844265577307, time: 1.67213773727417
Validation Loss Energy: 4.1394439164036285, Validation Loss Force: 3.69621997942321, time: 0.10066723823547363
Test Loss Energy: 10.666361794771557, Test Loss Force: 13.689269892781297, time: 10.842320203781128


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6429308245527583, Training Loss Force: 3.535170718350517, time: 1.614098310470581
Validation Loss Energy: 1.9034831859190677, Validation Loss Force: 3.7179892284748397, time: 0.10049676895141602
Test Loss Energy: 11.343794101696517, Test Loss Force: 15.088335465262332, time: 11.142670631408691


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6893084769618407, Training Loss Force: 3.526606299508867, time: 1.6915404796600342
Validation Loss Energy: 1.7990167314267433, Validation Loss Force: 3.6891629261813206, time: 0.10359668731689453
Test Loss Energy: 11.079189575256745, Test Loss Force: 14.494995226844642, time: 11.258000373840332


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.615502465286916, Training Loss Force: 3.5319856111723484, time: 1.7444572448730469
Validation Loss Energy: 4.053507654231695, Validation Loss Force: 3.70793794322766, time: 0.11055183410644531
Test Loss Energy: 10.683248094816312, Test Loss Force: 13.90027938274408, time: 11.22914457321167


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.653241639778054, Training Loss Force: 3.537223750597313, time: 1.6429758071899414
Validation Loss Energy: 2.335196798073614, Validation Loss Force: 3.6921028269150047, time: 0.10909652709960938
Test Loss Energy: 11.453073039737488, Test Loss Force: 15.119640504167021, time: 11.115653991699219


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.664139490396711, Training Loss Force: 3.530273558096262, time: 1.715383768081665
Validation Loss Energy: 1.7671418658563265, Validation Loss Force: 3.7020886474332824, time: 0.10726666450500488
Test Loss Energy: 11.047660496217043, Test Loss Force: 14.656374445536326, time: 11.28458833694458


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.655822623978045, Training Loss Force: 3.5367916792967136, time: 1.6669204235076904
Validation Loss Energy: 3.8852294552413964, Validation Loss Force: 3.6957770904919593, time: 0.10673189163208008
Test Loss Energy: 10.660184043334024, Test Loss Force: 13.998829724959851, time: 11.162105560302734


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.690210980003762, Training Loss Force: 3.5390379770015032, time: 1.669877052307129
Validation Loss Energy: 1.8761381901188328, Validation Loss Force: 3.7439052450310832, time: 0.10885262489318848
Test Loss Energy: 11.41626205736402, Test Loss Force: 15.3964619516666, time: 11.430569171905518

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–…â–†â–ƒâ–…â–…â–ƒâ–†â–†â–ƒâ–ˆâ–…â–„â–‡â–†â–„â–ˆâ–†â–„â–ˆ
wandb:   test_error_force â–â–ƒâ–…â–‚â–„â–…â–ƒâ–…â–†â–„â–‡â–†â–ƒâ–‡â–…â–ƒâ–‡â–†â–„â–ˆ
wandb:          test_loss â–â–…â–…â–â–„â–…â–â–†â–†â–‚â–‡â–†â–‚â–‡â–…â–ƒâ–ˆâ–†â–‚â–‡
wandb: train_error_energy â–ˆâ–â–‚â–‚â–â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–ƒ
wandb:  train_error_force â–ˆâ–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–â–â–‚
wandb: valid_error_energy â–â–‚â–‚â–‡â–ƒâ–‚â–‡â–‚â–â–‡â–ƒâ–â–ˆâ–‚â–‚â–ˆâ–ƒâ–‚â–‡â–‚
wandb:  valid_error_force â–ˆâ–â–â–â–â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:         valid_loss â–‚â–‚â–â–†â–‚â–â–†â–‚â–â–†â–‚â–â–ˆâ–‚â–â–ˆâ–ƒâ–â–‡â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 3128
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.41626
wandb:   test_error_force 15.39646
wandb:          test_loss 11.70706
wandb: train_error_energy 2.69021
wandb:  train_error_force 3.53904
wandb:         train_loss 1.04959
wandb: valid_error_energy 1.87614
wandb:  valid_error_force 3.74391
wandb:         valid_loss 0.86809
wandb: 
wandb: ğŸš€ View run al_63_25 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5h9tb3zj
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_015032-5h9tb3zj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2600998878479004, Uncertainty Bias: -0.18185457587242126
0.00022125244 0.0011091232
2.5454633 5.2951813
(48745, 22, 3)
Found uncertainty sample 0 after 312 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 523 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 932 steps.
Found uncertainty sample 5 after 405 steps.
Found uncertainty sample 6 after 274 steps.
Found uncertainty sample 7 after 241 steps.
Found uncertainty sample 8 after 38 steps.
Found uncertainty sample 9 after 709 steps.
Found uncertainty sample 10 after 414 steps.
Found uncertainty sample 11 after 245 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 171 steps.
Found uncertainty sample 14 after 295 steps.
Found uncertainty sample 15 after 31 steps.
Found uncertainty sample 16 after 315 steps.
Found uncertainty sample 17 after 243 steps.
Found uncertainty sample 18 after 9 steps.
Found uncertainty sample 19 after 129 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 455 steps.
Found uncertainty sample 22 after 559 steps.
Found uncertainty sample 23 after 148 steps.
Found uncertainty sample 24 after 15 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 896 steps.
Found uncertainty sample 27 after 54 steps.
Found uncertainty sample 28 after 3428 steps.
Found uncertainty sample 29 after 1338 steps.
Found uncertainty sample 30 after 124 steps.
Found uncertainty sample 31 after 380 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 158 steps.
Found uncertainty sample 34 after 423 steps.
Found uncertainty sample 35 after 309 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 260 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 59 steps.
Found uncertainty sample 40 after 1399 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 26 steps.
Found uncertainty sample 43 after 14 steps.
Found uncertainty sample 44 after 698 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 14 steps.
Found uncertainty sample 47 after 618 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 2711 steps.
Found uncertainty sample 50 after 377 steps.
Found uncertainty sample 51 after 599 steps.
Found uncertainty sample 52 after 1465 steps.
Found uncertainty sample 53 after 49 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 134 steps.
Found uncertainty sample 56 after 899 steps.
Found uncertainty sample 57 after 856 steps.
Found uncertainty sample 58 after 2132 steps.
Found uncertainty sample 59 after 45 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 54 steps.
Found uncertainty sample 62 after 156 steps.
Found uncertainty sample 63 after 76 steps.
Found uncertainty sample 64 after 44 steps.
Found uncertainty sample 65 after 89 steps.
Found uncertainty sample 66 after 440 steps.
Found uncertainty sample 67 after 38 steps.
Found uncertainty sample 68 after 333 steps.
Found uncertainty sample 69 after 190 steps.
Found uncertainty sample 70 after 2051 steps.
Found uncertainty sample 71 after 172 steps.
Found uncertainty sample 72 after 16 steps.
Found uncertainty sample 73 after 138 steps.
Found uncertainty sample 74 after 341 steps.
Found uncertainty sample 75 after 22 steps.
Found uncertainty sample 76 after 19 steps.
Found uncertainty sample 77 after 58 steps.
Found uncertainty sample 78 after 125 steps.
Found uncertainty sample 79 after 302 steps.
Found uncertainty sample 80 after 29 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 3 steps.
Found uncertainty sample 83 after 1420 steps.
Found uncertainty sample 84 after 54 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 537 steps.
Found uncertainty sample 87 after 609 steps.
Found uncertainty sample 88 after 1510 steps.
Found uncertainty sample 89 after 19 steps.
Found uncertainty sample 90 after 91 steps.
Found uncertainty sample 91 after 46 steps.
Found uncertainty sample 92 after 8 steps.
Found uncertainty sample 93 after 29 steps.
Found uncertainty sample 94 after 795 steps.
Found uncertainty sample 95 after 15 steps.
Found uncertainty sample 96 after 173 steps.
Found uncertainty sample 97 after 417 steps.
Found uncertainty sample 98 after 59 steps.
Found uncertainty sample 99 after 53 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_020104-ng6ae6sh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_26
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ng6ae6sh
Training model 26. Added 100 samples to the dataset.
Epoch 0, Batch 100/101, Loss: 0.8047260046005249, Variance: 0.1259780377149582

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.7422322226077998, Training Loss Force: 4.078713845643621, time: 1.7026398181915283
Validation Loss Energy: 7.290886796124841, Validation Loss Force: 3.836331756133946, time: 0.10769915580749512
Test Loss Energy: 14.551141571421466, Test Loss Force: 16.341406214541347, time: 10.501131534576416

Epoch 1, Batch 100/101, Loss: 2.301118850708008, Variance: 0.14840242266654968

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.823818064294718, Training Loss Force: 3.7534991954105905, time: 1.6889448165893555
Validation Loss Energy: 2.477989177297602, Validation Loss Force: 3.6964157056024805, time: 0.10688281059265137
Test Loss Energy: 10.469363015865582, Test Loss Force: 14.150028892684427, time: 10.647857189178467

Epoch 2, Batch 100/101, Loss: 1.8634562492370605, Variance: 0.1631220579147339

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.068045395379538, Training Loss Force: 3.559763184464177, time: 1.8493602275848389
Validation Loss Energy: 3.873069047843988, Validation Loss Force: 3.74518904399193, time: 0.11227750778198242
Test Loss Energy: 10.500844847077293, Test Loss Force: 13.278863390941176, time: 12.007405757904053

Epoch 3, Batch 100/101, Loss: 0.9204038381576538, Variance: 0.1732262670993805

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.095561343763074, Training Loss Force: 3.574157031739236, time: 1.8858952522277832
Validation Loss Energy: 5.662774089591751, Validation Loss Force: 3.7476959645713577, time: 0.11222720146179199
Test Loss Energy: 12.933663429048796, Test Loss Force: 15.017952513528872, time: 11.351129293441772

Epoch 4, Batch 100/101, Loss: 1.2044718265533447, Variance: 0.1764397919178009

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.080574426036415, Training Loss Force: 3.58089580479044, time: 1.7900736331939697
Validation Loss Energy: 5.37609869766702, Validation Loss Force: 3.773129457088376, time: 0.10437822341918945
Test Loss Energy: 11.01750833322733, Test Loss Force: 13.334892030975327, time: 11.097795009613037

Epoch 5, Batch 100/101, Loss: 2.0771846771240234, Variance: 0.17012639343738556

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.048779529554418, Training Loss Force: 3.5536076083618995, time: 1.7153630256652832
Validation Loss Energy: 1.8804717770368793, Validation Loss Force: 3.715965945280367, time: 0.10897445678710938
Test Loss Energy: 11.289805358190154, Test Loss Force: 14.687235405220768, time: 10.845722675323486

Epoch 6, Batch 100/101, Loss: 1.7699848413467407, Variance: 0.18498635292053223

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.090080987594726, Training Loss Force: 3.558270009264486, time: 1.7408344745635986
Validation Loss Energy: 2.8378923238941556, Validation Loss Force: 3.74948555649718, time: 0.11019563674926758
Test Loss Energy: 11.670634798983182, Test Loss Force: 14.631803854250132, time: 11.700695753097534

Epoch 7, Batch 100/101, Loss: 0.9278959035873413, Variance: 0.1813608705997467

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.057201417893756, Training Loss Force: 3.559503144380699, time: 1.6655387878417969
Validation Loss Energy: 5.9521259646505325, Validation Loss Force: 3.7138091719329105, time: 0.10822439193725586
Test Loss Energy: 11.203290635385507, Test Loss Force: 13.254189336611194, time: 11.00388216972351

Epoch 8, Batch 100/101, Loss: 1.3273513317108154, Variance: 0.17707392573356628

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.1357714452030665, Training Loss Force: 3.564709880954634, time: 1.681861400604248
Validation Loss Energy: 4.8197878928788285, Validation Loss Force: 3.820578359397847, time: 0.1159963607788086
Test Loss Energy: 12.753881139385806, Test Loss Force: 15.682424809319205, time: 11.09363603591919

Epoch 9, Batch 100/101, Loss: 1.8454267978668213, Variance: 0.18829038739204407

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.150001185317577, Training Loss Force: 3.562443126686249, time: 1.7075252532958984
Validation Loss Energy: 2.2575151038103702, Validation Loss Force: 3.7273886497973168, time: 0.1116948127746582
Test Loss Energy: 10.530902733582746, Test Loss Force: 13.758027140577758, time: 11.393040895462036

Epoch 10, Batch 100/101, Loss: 1.7463407516479492, Variance: 0.18069660663604736

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.1411482084093985, Training Loss Force: 3.557684704674932, time: 1.656564712524414
Validation Loss Energy: 3.8887104419996854, Validation Loss Force: 3.7363064377571513, time: 0.10322761535644531
Test Loss Energy: 10.745035504561857, Test Loss Force: 13.659609052176592, time: 11.20700740814209

Epoch 11, Batch 100/101, Loss: 0.9778727889060974, Variance: 0.18955671787261963

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.074178393712276, Training Loss Force: 3.558340577746584, time: 1.6829476356506348
Validation Loss Energy: 5.40953781398758, Validation Loss Force: 3.7339907899740754, time: 0.10990285873413086
Test Loss Energy: 13.500562986443697, Test Loss Force: 15.930127059494458, time: 11.16731595993042

Epoch 12, Batch 100/101, Loss: 1.3486003875732422, Variance: 0.18865054845809937

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.141666905161612, Training Loss Force: 3.5832994785371213, time: 1.7681760787963867
Validation Loss Energy: 5.506102074494645, Validation Loss Force: 3.753576656715311, time: 0.1048898696899414
Test Loss Energy: 11.148184195357608, Test Loss Force: 13.738689353495744, time: 10.940362930297852

Epoch 13, Batch 100/101, Loss: 2.2265896797180176, Variance: 0.17980806529521942

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.094183340320985, Training Loss Force: 3.5841826725556705, time: 1.7243762016296387
Validation Loss Energy: 2.0864365063063106, Validation Loss Force: 3.7615821691842797, time: 0.10198211669921875
Test Loss Energy: 11.38228503534048, Test Loss Force: 15.047114398044023, time: 11.086267471313477

Epoch 14, Batch 100/101, Loss: 1.5750333070755005, Variance: 0.19227561354637146

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.09851103752989, Training Loss Force: 3.611586094755616, time: 1.7527520656585693
Validation Loss Energy: 3.567354887269567, Validation Loss Force: 3.727560216374254, time: 0.11025643348693848
Test Loss Energy: 12.117769147164655, Test Loss Force: 14.950020507898204, time: 10.94709300994873

Epoch 15, Batch 100/101, Loss: 0.7776706218719482, Variance: 0.18378393352031708

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.09860823371441, Training Loss Force: 3.5861687799236757, time: 1.647625207901001
Validation Loss Energy: 5.7712747523354215, Validation Loss Force: 3.7209821244652552, time: 0.10969662666320801
Test Loss Energy: 11.284731131723698, Test Loss Force: 13.37935522624354, time: 10.840513229370117

Epoch 16, Batch 100/101, Loss: 1.321211338043213, Variance: 0.18612486124038696

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.0855357470395415, Training Loss Force: 3.5702776986974625, time: 1.666074514389038
Validation Loss Energy: 4.896999755244165, Validation Loss Force: 3.7453256515129567, time: 0.10863661766052246
Test Loss Energy: 12.924765019716592, Test Loss Force: 15.194983022912275, time: 11.272704601287842

Epoch 17, Batch 100/101, Loss: 1.8420202732086182, Variance: 0.19063980877399445

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.124393028697384, Training Loss Force: 3.548931218986729, time: 1.7470343112945557
Validation Loss Energy: 2.489129537337515, Validation Loss Force: 3.681127910050516, time: 0.10972166061401367
Test Loss Energy: 10.660781486802883, Test Loss Force: 14.03748316277967, time: 10.97046422958374

Epoch 18, Batch 100/101, Loss: 1.72726571559906, Variance: 0.1839786022901535

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.044080524930743, Training Loss Force: 3.573926732316932, time: 1.6961684226989746
Validation Loss Energy: 3.956281398353786, Validation Loss Force: 3.747328337811431, time: 0.10965824127197266
Test Loss Energy: 10.894321419709746, Test Loss Force: 13.75569681819848, time: 11.167519569396973

Epoch 19, Batch 100/101, Loss: 0.9621902704238892, Variance: 0.1905515491962433

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.098769505701476, Training Loss Force: 3.553448792349372, time: 1.75014066696167
Validation Loss Energy: 5.613697141821827, Validation Loss Force: 3.742782881201034, time: 0.11012434959411621
Test Loss Energy: 13.13633921262313, Test Loss Force: 15.261972081834484, time: 10.74706220626831

wandb: - 0.039 MB of 0.056 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–â–â–…â–‚â–‚â–ƒâ–‚â–…â–â–â–†â–‚â–ƒâ–„â–‚â–…â–â–‚â–†
wandb:   test_error_force â–ˆâ–ƒâ–â–…â–â–„â–„â–â–‡â–‚â–‚â–‡â–‚â–…â–…â–â–…â–ƒâ–‚â–†
wandb:          test_loss â–ˆâ–‚â–â–„â–â–‚â–‚â–â–ƒâ–â–â–„â–â–‚â–‚â–â–ƒâ–â–â–ƒ
wandb: train_error_energy â–â–‚â–‡â–‡â–‡â–†â–‡â–†â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–†â–‡
wandb:  train_error_force â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb:         train_loss â–ˆâ–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‚â–„â–†â–†â–â–‚â–†â–…â–â–„â–†â–†â–â–ƒâ–†â–…â–‚â–„â–†
wandb:  valid_error_force â–ˆâ–‚â–„â–„â–…â–ƒâ–„â–‚â–‡â–ƒâ–ƒâ–ƒâ–„â–…â–ƒâ–ƒâ–„â–â–„â–„
wandb:         valid_loss â–ˆâ–â–‚â–ƒâ–ƒâ–â–â–ƒâ–‚â–â–‚â–ƒâ–ƒâ–â–‚â–ƒâ–‚â–â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3218
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 13.13634
wandb:   test_error_force 15.26197
wandb:          test_loss 10.88356
wandb: train_error_energy 4.09877
wandb:  train_error_force 3.55345
wandb:         train_loss 1.45705
wandb: valid_error_energy 5.6137
wandb:  valid_error_force 3.74278
wandb:         valid_loss 1.98618
wandb: 
wandb: ğŸš€ View run al_63_26 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ng6ae6sh
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_020104-ng6ae6sh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.4217987060546875, Uncertainty Bias: -0.19058647751808167
0.00033187866 1.1795511
2.540504 4.9053445
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 145 steps.
Found uncertainty sample 2 after 530 steps.
Found uncertainty sample 3 after 5 steps.
Found uncertainty sample 4 after 2 steps.
Found uncertainty sample 5 after 332 steps.
Found uncertainty sample 6 after 37 steps.
Found uncertainty sample 7 after 11 steps.
Found uncertainty sample 8 after 148 steps.
Found uncertainty sample 9 after 136 steps.
Found uncertainty sample 10 after 688 steps.
Found uncertainty sample 11 after 88 steps.
Found uncertainty sample 12 after 261 steps.
Found uncertainty sample 13 after 241 steps.
Found uncertainty sample 14 after 296 steps.
Found uncertainty sample 15 after 126 steps.
Found uncertainty sample 16 after 221 steps.
Found uncertainty sample 17 after 748 steps.
Found uncertainty sample 18 after 900 steps.
Found uncertainty sample 19 after 206 steps.
Found uncertainty sample 20 after 211 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 588 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 538 steps.
Found uncertainty sample 25 after 152 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 32 steps.
Found uncertainty sample 28 after 405 steps.
Found uncertainty sample 29 after 72 steps.
Found uncertainty sample 30 after 25 steps.
Found uncertainty sample 31 after 371 steps.
Found uncertainty sample 32 after 21 steps.
Found uncertainty sample 33 after 363 steps.
Found uncertainty sample 34 after 145 steps.
Found uncertainty sample 35 after 48 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 93 steps.
Found uncertainty sample 38 after 915 steps.
Found uncertainty sample 39 after 191 steps.
Found uncertainty sample 40 after 57 steps.
Found uncertainty sample 41 after 98 steps.
Found uncertainty sample 42 after 48 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 48 steps.
Found uncertainty sample 45 after 32 steps.
Found uncertainty sample 46 after 177 steps.
Found uncertainty sample 47 after 546 steps.
Found uncertainty sample 48 after 311 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 534 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 15 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 29 steps.
Found uncertainty sample 55 after 289 steps.
Found uncertainty sample 56 after 296 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 75 steps.
Found uncertainty sample 59 after 1058 steps.
Found uncertainty sample 60 after 71 steps.
Found uncertainty sample 61 after 190 steps.
Found uncertainty sample 62 after 900 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 446 steps.
Found uncertainty sample 65 after 2091 steps.
Found uncertainty sample 66 after 11 steps.
Found uncertainty sample 67 after 25 steps.
Found uncertainty sample 68 after 17 steps.
Found uncertainty sample 69 after 15 steps.
Found uncertainty sample 70 after 202 steps.
Found uncertainty sample 71 after 17 steps.
Found uncertainty sample 72 after 413 steps.
Found uncertainty sample 73 after 965 steps.
Found uncertainty sample 74 after 151 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 14 steps.
Found uncertainty sample 77 after 624 steps.
Found uncertainty sample 78 after 654 steps.
Found uncertainty sample 79 after 122 steps.
Found uncertainty sample 80 after 965 steps.
Found uncertainty sample 81 after 445 steps.
Found uncertainty sample 82 after 92 steps.
Found uncertainty sample 83 after 375 steps.
Did not find any uncertainty samples for sample 84.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 20 steps.
Found uncertainty sample 87 after 332 steps.
Found uncertainty sample 88 after 344 steps.
Found uncertainty sample 89 after 50 steps.
Found uncertainty sample 90 after 96 steps.
Found uncertainty sample 91 after 705 steps.
Found uncertainty sample 92 after 9 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 410 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 29 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 37 steps.
Found uncertainty sample 99 after 86 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_021053-kdbswya4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_27
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/kdbswya4
Training model 27. Added 99 samples to the dataset.
Epoch 0, Batch 100/104, Loss: 0.8737689852714539, Variance: 0.16352367401123047

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.715459838651643, Training Loss Force: 3.7725930492568853, time: 1.764488697052002
Validation Loss Energy: 3.6727872805169257, Validation Loss Force: 3.680259500381398, time: 0.10727214813232422
Test Loss Energy: 11.125584848553835, Test Loss Force: 14.553045091840929, time: 10.917734146118164

Epoch 1, Batch 100/104, Loss: 1.2316054105758667, Variance: 0.14348678290843964

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.643201372021939, Training Loss Force: 3.5468207197389137, time: 1.7260127067565918
Validation Loss Energy: 2.384097057876242, Validation Loss Force: 3.733452264644649, time: 0.10755777359008789
Test Loss Energy: 10.89185608288477, Test Loss Force: 13.30090253481639, time: 10.971962928771973

Epoch 2, Batch 100/104, Loss: 1.0155028104782104, Variance: 0.14586816728115082

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.660490995311536, Training Loss Force: 3.548699923149674, time: 1.9845643043518066
Validation Loss Energy: 1.6875041901479668, Validation Loss Force: 3.701673814814419, time: 0.11484241485595703
Test Loss Energy: 10.423910013108861, Test Loss Force: 13.22596057804594, time: 11.083071231842041

Epoch 3, Batch 100/104, Loss: 0.8071260452270508, Variance: 0.13835325837135315

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.628384750805907, Training Loss Force: 3.5557629627659106, time: 1.751368522644043
Validation Loss Energy: 3.559205307863183, Validation Loss Force: 3.6964521120078393, time: 0.11369204521179199
Test Loss Energy: 10.475665438708692, Test Loss Force: 12.850736304891052, time: 10.921457529067993

Epoch 4, Batch 100/104, Loss: 1.4693477153778076, Variance: 0.13544601202011108

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6713355040637676, Training Loss Force: 3.548354182197533, time: 1.7779967784881592
Validation Loss Energy: 2.358766549839216, Validation Loss Force: 3.7016551106061613, time: 0.11702919006347656
Test Loss Energy: 10.647496187522846, Test Loss Force: 13.212754157270684, time: 11.13060712814331

Epoch 5, Batch 100/104, Loss: 1.1288080215454102, Variance: 0.13939312100410461

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6537791209962993, Training Loss Force: 3.542844212771055, time: 1.7987897396087646
Validation Loss Energy: 1.6891829485064027, Validation Loss Force: 3.6660143531052407, time: 0.10542488098144531
Test Loss Energy: 10.670845521315927, Test Loss Force: 13.86316896049904, time: 10.893515586853027

Epoch 6, Batch 100/104, Loss: 0.8337948322296143, Variance: 0.13636119663715363

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.676040442097312, Training Loss Force: 3.548673609651564, time: 1.6717145442962646
Validation Loss Energy: 3.7783986997576107, Validation Loss Force: 3.673576308635521, time: 0.10920858383178711
Test Loss Energy: 10.243554943790896, Test Loss Force: 12.57047210327528, time: 11.690955638885498

Epoch 7, Batch 100/104, Loss: 1.1551897525787354, Variance: 0.13540181517601013

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6799495905780026, Training Loss Force: 3.540157581062467, time: 1.6884350776672363
Validation Loss Energy: 1.849057502594814, Validation Loss Force: 3.738942729110364, time: 0.10747838020324707
Test Loss Energy: 10.69208815317421, Test Loss Force: 13.747776558284322, time: 11.15487265586853

Epoch 8, Batch 100/104, Loss: 1.0371015071868896, Variance: 0.14030134677886963

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.701292261213322, Training Loss Force: 3.5420709577639795, time: 1.7477891445159912
Validation Loss Energy: 1.8325007712474939, Validation Loss Force: 3.6918724756835424, time: 0.10855317115783691
Test Loss Energy: 10.433242596964638, Test Loss Force: 13.390107819040987, time: 10.890807867050171

Epoch 9, Batch 100/104, Loss: 0.8830744028091431, Variance: 0.1360360085964203

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6402545985194235, Training Loss Force: 3.5441578536795557, time: 1.8049871921539307
Validation Loss Energy: 3.8341313396338874, Validation Loss Force: 3.6856150273446797, time: 0.11198234558105469
Test Loss Energy: 10.333084089213079, Test Loss Force: 12.802095612610284, time: 10.293191194534302

Epoch 10, Batch 100/104, Loss: 1.4618492126464844, Variance: 0.13625378906726837

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.738272464621601, Training Loss Force: 3.5329084264380772, time: 1.8150651454925537
Validation Loss Energy: 1.9615504004558244, Validation Loss Force: 3.681748154956498, time: 0.11281585693359375
Test Loss Energy: 10.68176358876559, Test Loss Force: 13.501282377992872, time: 11.62804627418518

Epoch 11, Batch 100/104, Loss: 0.9671843647956848, Variance: 0.13295027613639832

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.671259629552293, Training Loss Force: 3.547286318781999, time: 1.7594125270843506
Validation Loss Energy: 1.678891013579761, Validation Loss Force: 3.72973914530173, time: 0.10294604301452637
Test Loss Energy: 10.565802934890463, Test Loss Force: 13.256911687257693, time: 9.84387493133545

Epoch 12, Batch 100/104, Loss: 0.8135890960693359, Variance: 0.13772650063037872

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.675782688644079, Training Loss Force: 3.5435980848571873, time: 1.6256802082061768
Validation Loss Energy: 3.7144458943556913, Validation Loss Force: 3.693366251648999, time: 0.09900069236755371
Test Loss Energy: 10.452906623490613, Test Loss Force: 12.988815195654674, time: 9.889636754989624

Epoch 13, Batch 100/104, Loss: 1.5134849548339844, Variance: 0.1331087350845337

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.7318242016709746, Training Loss Force: 3.5661459436806426, time: 1.6269636154174805
Validation Loss Energy: 2.414584137994664, Validation Loss Force: 3.705100470301102, time: 0.09899449348449707
Test Loss Energy: 10.96063928154609, Test Loss Force: 13.687148105303313, time: 9.715994119644165

Epoch 14, Batch 100/104, Loss: 1.030856728553772, Variance: 0.13486923277378082

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.692114398902617, Training Loss Force: 3.5546586308545822, time: 1.6579134464263916
Validation Loss Energy: 1.8410916252338203, Validation Loss Force: 3.6853006185106296, time: 0.1064755916595459
Test Loss Energy: 10.338471166995369, Test Loss Force: 13.193740898227887, time: 9.948558807373047

Epoch 15, Batch 100/104, Loss: 0.7324398756027222, Variance: 0.1313888132572174

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.579081093729687, Training Loss Force: 3.5521649098451986, time: 1.6435184478759766
Validation Loss Energy: 3.7592332323638873, Validation Loss Force: 3.6990182625800845, time: 0.10015869140625
Test Loss Energy: 10.347421035050614, Test Loss Force: 13.141563240631864, time: 9.739353656768799

Epoch 16, Batch 100/104, Loss: 1.373178243637085, Variance: 0.13286027312278748

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6925937766696317, Training Loss Force: 3.560130314034257, time: 1.6717121601104736
Validation Loss Energy: 2.126838709489178, Validation Loss Force: 3.690072418307862, time: 0.10096526145935059
Test Loss Energy: 10.59179118485132, Test Loss Force: 13.322459277062118, time: 9.921562910079956

Epoch 17, Batch 100/104, Loss: 1.140673279762268, Variance: 0.13632240891456604

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6457522376419456, Training Loss Force: 3.5430014000596843, time: 1.6885457038879395
Validation Loss Energy: 1.6741910802493765, Validation Loss Force: 3.68527133751794, time: 0.10043668746948242
Test Loss Energy: 10.519409755382826, Test Loss Force: 13.396058284736682, time: 9.757248401641846

Epoch 18, Batch 100/104, Loss: 0.9004833102226257, Variance: 0.13525736331939697

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6556016187115805, Training Loss Force: 3.5521217184540372, time: 1.691054344177246
Validation Loss Energy: 4.076343711161465, Validation Loss Force: 3.6727958532790446, time: 0.10014867782592773
Test Loss Energy: 10.516057850278173, Test Loss Force: 12.611853818704626, time: 9.70975637435913

Epoch 19, Batch 100/104, Loss: 1.5397028923034668, Variance: 0.1318061500787735

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6751647143210704, Training Loss Force: 3.5246542225078006, time: 1.6452910900115967
Validation Loss Energy: 2.2937550639829825, Validation Loss Force: 3.651985306235206, time: 0.10021185874938965
Test Loss Energy: 10.840529585676805, Test Loss Force: 13.493446606447005, time: 9.871463537216187

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–†â–‚â–ƒâ–„â–„â–â–…â–ƒâ–‚â–„â–„â–ƒâ–‡â–‚â–‚â–„â–ƒâ–ƒâ–†
wandb:   test_error_force â–ˆâ–„â–ƒâ–‚â–ƒâ–†â–â–…â–„â–‚â–„â–ƒâ–‚â–…â–ƒâ–ƒâ–„â–„â–â–„
wandb:          test_loss â–â–†â–…â–„â–†â–†â–ƒâ–‡â–…â–ƒâ–†â–‡â–‚â–ˆâ–…â–„â–†â–†â–„â–ˆ
wandb: train_error_energy â–ˆâ–â–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–‚â–â–â–‚â–‚â–â–‚â–â–â–
wandb: valid_error_energy â–‡â–ƒâ–â–†â–ƒâ–â–‡â–‚â–â–‡â–‚â–â–‡â–ƒâ–â–‡â–‚â–â–ˆâ–ƒ
wandb:  valid_error_force â–ƒâ–ˆâ–…â–…â–…â–‚â–ƒâ–ˆâ–„â–„â–ƒâ–‡â–„â–…â–„â–…â–„â–„â–ƒâ–
wandb:         valid_loss â–†â–ƒâ–â–†â–ƒâ–â–‡â–‚â–â–‡â–‚â–â–†â–ƒâ–â–‡â–‚â–â–ˆâ–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 3307
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.84053
wandb:   test_error_force 13.49345
wandb:          test_loss 11.20991
wandb: train_error_energy 2.67516
wandb:  train_error_force 3.52465
wandb:         train_loss 1.03841
wandb: valid_error_energy 2.29376
wandb:  valid_error_force 3.65199
wandb:         valid_loss 0.94978
wandb: 
wandb: ğŸš€ View run al_63_27 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/kdbswya4
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_021053-kdbswya4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.391131639480591, Uncertainty Bias: -0.20220254361629486
7.6293945e-06 0.0023612976
2.3488176 5.2938056
(48745, 22, 3)
Found uncertainty sample 0 after 1170 steps.
Found uncertainty sample 1 after 144 steps.
Found uncertainty sample 2 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 67 steps.
Found uncertainty sample 5 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 604 steps.
Found uncertainty sample 9 after 492 steps.
Found uncertainty sample 10 after 255 steps.
Found uncertainty sample 11 after 376 steps.
Found uncertainty sample 12 after 23 steps.
Found uncertainty sample 13 after 128 steps.
Found uncertainty sample 14 after 732 steps.
Found uncertainty sample 15 after 1334 steps.
Found uncertainty sample 16 after 2937 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 2093 steps.
Found uncertainty sample 20 after 59 steps.
Found uncertainty sample 21 after 773 steps.
Found uncertainty sample 22 after 501 steps.
Found uncertainty sample 23 after 392 steps.
Found uncertainty sample 24 after 794 steps.
Found uncertainty sample 25 after 64 steps.
Found uncertainty sample 26 after 986 steps.
Found uncertainty sample 27 after 89 steps.
Found uncertainty sample 28 after 71 steps.
Found uncertainty sample 29 after 8 steps.
Found uncertainty sample 30 after 20 steps.
Found uncertainty sample 31 after 122 steps.
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 449 steps.
Found uncertainty sample 35 after 1840 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 12 steps.
Found uncertainty sample 38 after 4 steps.
Found uncertainty sample 39 after 136 steps.
Found uncertainty sample 40 after 172 steps.
Found uncertainty sample 41 after 1031 steps.
Found uncertainty sample 42 after 142 steps.
Found uncertainty sample 43 after 158 steps.
Found uncertainty sample 44 after 20 steps.
Found uncertainty sample 45 after 856 steps.
Found uncertainty sample 46 after 300 steps.
Found uncertainty sample 47 after 679 steps.
Found uncertainty sample 48 after 1019 steps.
Found uncertainty sample 49 after 23 steps.
Found uncertainty sample 50 after 10 steps.
Found uncertainty sample 51 after 1165 steps.
Found uncertainty sample 52 after 275 steps.
Found uncertainty sample 53 after 55 steps.
Found uncertainty sample 54 after 104 steps.
Found uncertainty sample 55 after 1025 steps.
Found uncertainty sample 56 after 47 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 217 steps.
Found uncertainty sample 59 after 703 steps.
Found uncertainty sample 60 after 619 steps.
Found uncertainty sample 61 after 458 steps.
Found uncertainty sample 62 after 827 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 564 steps.
Found uncertainty sample 65 after 715 steps.
Found uncertainty sample 66 after 1673 steps.
Found uncertainty sample 67 after 160 steps.
Found uncertainty sample 68 after 23 steps.
Found uncertainty sample 69 after 3661 steps.
Found uncertainty sample 70 after 372 steps.
Found uncertainty sample 71 after 773 steps.
Found uncertainty sample 72 after 129 steps.
Found uncertainty sample 73 after 1192 steps.
Found uncertainty sample 74 after 580 steps.
Found uncertainty sample 75 after 113 steps.
Found uncertainty sample 76 after 225 steps.
Found uncertainty sample 77 after 61 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 167 steps.
Found uncertainty sample 80 after 346 steps.
Found uncertainty sample 81 after 453 steps.
Found uncertainty sample 82 after 226 steps.
Found uncertainty sample 83 after 1332 steps.
Found uncertainty sample 84 after 2181 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 402 steps.
Found uncertainty sample 87 after 999 steps.
Found uncertainty sample 88 after 1093 steps.
Found uncertainty sample 89 after 11 steps.
Found uncertainty sample 90 after 29 steps.
Found uncertainty sample 91 after 35 steps.
Found uncertainty sample 92 after 54 steps.
Found uncertainty sample 93 after 460 steps.
Found uncertainty sample 94 after 558 steps.
Found uncertainty sample 95 after 38 steps.
Found uncertainty sample 96 after 571 steps.
Found uncertainty sample 97 after 21 steps.
Found uncertainty sample 98 after 74 steps.
Found uncertainty sample 99 after 615 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_022215-f6phxg5a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_28
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/f6phxg5a
Training model 28. Added 100 samples to the dataset.
Epoch 0, Batch 100/107, Loss: 0.8088977932929993, Variance: 0.1354948878288269

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.3985710212160067, Training Loss Force: 3.712484954360039, time: 1.7442986965179443
Validation Loss Energy: 3.773331974676597, Validation Loss Force: 3.6979190563412114, time: 0.09994697570800781
Test Loss Energy: 10.416379258655477, Test Loss Force: 12.740380949224456, time: 9.431028842926025

Epoch 1, Batch 100/107, Loss: 1.277538776397705, Variance: 0.13202762603759766

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.649652460277447, Training Loss Force: 3.5431837425586648, time: 1.7985248565673828
Validation Loss Energy: 2.551607938989674, Validation Loss Force: 3.70888194907182, time: 0.10054731369018555
Test Loss Energy: 10.187464963653168, Test Loss Force: 12.661783739692561, time: 11.164226531982422

Epoch 2, Batch 100/107, Loss: 0.8770772218704224, Variance: 0.13413524627685547

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.649821529630318, Training Loss Force: 3.534122288257774, time: 1.8767454624176025
Validation Loss Energy: 1.5943632450903946, Validation Loss Force: 3.7128017053821605, time: 0.12136960029602051
Test Loss Energy: 10.392423043019686, Test Loss Force: 13.315875288501076, time: 11.688293695449829

Epoch 3, Batch 100/107, Loss: 0.6931825876235962, Variance: 0.13293972611427307

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.637331784160982, Training Loss Force: 3.5389363598153936, time: 1.8736867904663086
Validation Loss Energy: 3.122665119301506, Validation Loss Force: 3.6937636047399076, time: 0.11511898040771484
Test Loss Energy: 11.044105041202027, Test Loss Force: 13.576500024813734, time: 10.759400606155396

Epoch 4, Batch 100/107, Loss: 1.471545934677124, Variance: 0.13703760504722595

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.660112698221325, Training Loss Force: 3.550347872606484, time: 1.7929856777191162
Validation Loss Energy: 2.0491340803039035, Validation Loss Force: 3.6765351362466094, time: 0.11139464378356934
Test Loss Energy: 10.763343198527052, Test Loss Force: 13.542710254718042, time: 10.924515724182129

Epoch 5, Batch 100/107, Loss: 0.9656301140785217, Variance: 0.13662956655025482

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.663697754711564, Training Loss Force: 3.559576946829777, time: 1.874962329864502
Validation Loss Energy: 2.317082997056107, Validation Loss Force: 3.6774644459271393, time: 0.11758828163146973
Test Loss Energy: 10.238306293937333, Test Loss Force: 13.072174064794105, time: 11.789692640304565

Epoch 6, Batch 100/107, Loss: 0.6615124940872192, Variance: 0.13267311453819275

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6459464059976163, Training Loss Force: 3.545892133973227, time: 1.7905070781707764
Validation Loss Energy: 3.747310412945184, Validation Loss Force: 3.7160215012322757, time: 0.11587643623352051
Test Loss Energy: 10.406861754485943, Test Loss Force: 13.097739443896513, time: 11.370080471038818

Epoch 7, Batch 100/107, Loss: 1.4132717847824097, Variance: 0.1370270848274231

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.692563400770251, Training Loss Force: 3.536903001362322, time: 1.7737386226654053
Validation Loss Energy: 2.6622788465641305, Validation Loss Force: 3.6735402362338805, time: 0.11537981033325195
Test Loss Energy: 10.412495260561776, Test Loss Force: 13.156003700808693, time: 11.157753944396973

Epoch 8, Batch 100/107, Loss: 0.8102190494537354, Variance: 0.13127507269382477

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.717952692421292, Training Loss Force: 3.551294532791726, time: 1.8274903297424316
Validation Loss Energy: 1.7630326181028022, Validation Loss Force: 3.708960105942041, time: 0.12223386764526367
Test Loss Energy: 10.798755902602203, Test Loss Force: 13.839416402027858, time: 11.129557847976685

Epoch 9, Batch 100/107, Loss: 0.7676821947097778, Variance: 0.137838214635849

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6755974789696917, Training Loss Force: 3.549179837669789, time: 1.7546672821044922
Validation Loss Energy: 3.306012564077041, Validation Loss Force: 3.729128586503596, time: 0.15576577186584473
Test Loss Energy: 11.461205760991072, Test Loss Force: 13.860221846917158, time: 11.331358909606934

Epoch 10, Batch 100/107, Loss: 1.438714623451233, Variance: 0.13564738631248474

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6874643928251176, Training Loss Force: 3.5450465926334958, time: 1.7340574264526367
Validation Loss Energy: 1.846252929034567, Validation Loss Force: 3.7525010534059065, time: 0.12024879455566406
Test Loss Energy: 10.557665200666577, Test Loss Force: 13.520638883272827, time: 11.2336745262146

Epoch 11, Batch 100/107, Loss: 1.0358978509902954, Variance: 0.13803356885910034

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6802327929855716, Training Loss Force: 3.547865315569355, time: 1.719209909439087
Validation Loss Energy: 2.60411120824256, Validation Loss Force: 3.6970068921849184, time: 0.11758804321289062
Test Loss Energy: 10.345778268361677, Test Loss Force: 13.241978221270479, time: 11.489900827407837

Epoch 12, Batch 100/107, Loss: 0.8122292757034302, Variance: 0.13171103596687317

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6787801546245698, Training Loss Force: 3.567916491111408, time: 1.8289835453033447
Validation Loss Energy: 3.788059089319115, Validation Loss Force: 3.74750146307239, time: 0.10806131362915039
Test Loss Energy: 10.589460960399837, Test Loss Force: 12.767675966293693, time: 11.139012575149536

Epoch 13, Batch 100/107, Loss: 1.3500635623931885, Variance: 0.13516724109649658

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6698384057998177, Training Loss Force: 3.5506999187692565, time: 1.813643455505371
Validation Loss Energy: 2.8709776189418355, Validation Loss Force: 3.676846093708898, time: 0.1166536808013916
Test Loss Energy: 10.348231975095553, Test Loss Force: 12.820463336311931, time: 11.27960729598999

Epoch 14, Batch 100/107, Loss: 0.7528505325317383, Variance: 0.12988212704658508

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.680066766084021, Training Loss Force: 3.550141275178445, time: 1.8074257373809814
Validation Loss Energy: 1.6751562953296872, Validation Loss Force: 3.6929120065913064, time: 0.11372184753417969
Test Loss Energy: 10.521470310011487, Test Loss Force: 13.587175077452889, time: 11.125715255737305

Epoch 15, Batch 100/107, Loss: 0.8396018743515015, Variance: 0.13406577706336975

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6763486616575602, Training Loss Force: 3.555471574976106, time: 1.820955753326416
Validation Loss Energy: 3.090457586027073, Validation Loss Force: 3.7518179044662, time: 0.11819338798522949
Test Loss Energy: 11.357198532467047, Test Loss Force: 13.95402534460134, time: 11.185139656066895

Epoch 16, Batch 100/107, Loss: 1.539692759513855, Variance: 0.14076581597328186

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.71658232510716, Training Loss Force: 3.544199827653975, time: 2.0453901290893555
Validation Loss Energy: 2.0623382493228486, Validation Loss Force: 3.7421527275180586, time: 0.12360978126525879
Test Loss Energy: 10.551852794504578, Test Loss Force: 13.384436111799843, time: 11.270715713500977

Epoch 17, Batch 100/107, Loss: 0.9444806575775146, Variance: 0.13702034950256348

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6549145046983105, Training Loss Force: 3.55214320606664, time: 1.7462210655212402
Validation Loss Energy: 2.3918242339128013, Validation Loss Force: 3.6787435688359214, time: 0.11527872085571289
Test Loss Energy: 10.253967116267464, Test Loss Force: 13.020862415304686, time: 11.106778860092163

Epoch 18, Batch 100/107, Loss: 0.549164354801178, Variance: 0.13251231610774994

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.639020968950151, Training Loss Force: 3.532302590214812, time: 1.8061316013336182
Validation Loss Energy: 3.880796557884266, Validation Loss Force: 3.7396885594146485, time: 0.1200249195098877
Test Loss Energy: 10.793855810402702, Test Loss Force: 12.930920606830554, time: 11.281134366989136

Epoch 19, Batch 100/107, Loss: 1.3160440921783447, Variance: 0.13107264041900635

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6377208006855484, Training Loss Force: 3.5507855485824953, time: 1.7421579360961914
Validation Loss Energy: 3.2301370505909905, Validation Loss Force: 3.7438443450196717, time: 0.11851048469543457
Test Loss Energy: 10.286596502650797, Test Loss Force: 12.863802269233314, time: 11.114452600479126

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.039 MB of 0.056 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‚â–†â–„â–â–‚â–‚â–„â–ˆâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‡â–ƒâ–â–„â–‚
wandb:   test_error_force â–â–â–…â–†â–†â–ƒâ–ƒâ–„â–‡â–‡â–†â–„â–‚â–‚â–†â–ˆâ–…â–ƒâ–‚â–‚
wandb:          test_loss â–‚â–‚â–ƒâ–†â–…â–â–â–â–…â–ˆâ–„â–‚â–ƒâ–‚â–„â–‡â–ƒâ–â–‚â–‚
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–„â–â–†â–‚â–ƒâ–ˆâ–„â–‚â–†â–‚â–„â–ˆâ–…â–â–†â–‚â–ƒâ–ˆâ–†
wandb:  valid_error_force â–ƒâ–„â–„â–ƒâ–â–â–…â–â–„â–†â–ˆâ–ƒâ–ˆâ–â–ƒâ–ˆâ–‡â–â–‡â–‡
wandb:         valid_loss â–ˆâ–ƒâ–â–…â–‚â–‚â–ˆâ–ƒâ–â–†â–‚â–ƒâ–ˆâ–„â–â–…â–‚â–ƒâ–ˆâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 3397
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.2866
wandb:   test_error_force 12.8638
wandb:          test_loss 10.30662
wandb: train_error_energy 2.63772
wandb:  train_error_force 3.55079
wandb:         train_loss 1.03688
wandb: valid_error_energy 3.23014
wandb:  valid_error_force 3.74384
wandb:         valid_loss 1.31364
wandb: 
wandb: ğŸš€ View run al_63_28 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/f6phxg5a
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_022215-f6phxg5a/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.399977207183838, Uncertainty Bias: -0.1959361732006073
0.0004119873 0.09135437
2.424614 5.330306
(48745, 22, 3)
Found uncertainty sample 0 after 219 steps.
Found uncertainty sample 1 after 13 steps.
Found uncertainty sample 2 after 334 steps.
Found uncertainty sample 3 after 92 steps.
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 34 steps.
Found uncertainty sample 8 after 94 steps.
Found uncertainty sample 9 after 382 steps.
Found uncertainty sample 10 after 424 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 257 steps.
Found uncertainty sample 13 after 1253 steps.
Found uncertainty sample 14 after 528 steps.
Found uncertainty sample 15 after 1880 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1348 steps.
Found uncertainty sample 18 after 921 steps.
Found uncertainty sample 19 after 1219 steps.
Found uncertainty sample 20 after 242 steps.
Found uncertainty sample 21 after 3 steps.
Found uncertainty sample 22 after 124 steps.
Found uncertainty sample 23 after 388 steps.
Found uncertainty sample 24 after 58 steps.
Found uncertainty sample 25 after 198 steps.
Found uncertainty sample 26 after 48 steps.
Found uncertainty sample 27 after 1168 steps.
Found uncertainty sample 28 after 757 steps.
Found uncertainty sample 29 after 39 steps.
Found uncertainty sample 30 after 1636 steps.
Found uncertainty sample 31 after 1135 steps.
Found uncertainty sample 32 after 945 steps.
Found uncertainty sample 33 after 459 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 958 steps.
Found uncertainty sample 36 after 99 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 4 steps.
Found uncertainty sample 39 after 265 steps.
Found uncertainty sample 40 after 51 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 225 steps.
Found uncertainty sample 43 after 553 steps.
Found uncertainty sample 44 after 1214 steps.
Found uncertainty sample 45 after 179 steps.
Found uncertainty sample 46 after 69 steps.
Found uncertainty sample 47 after 2435 steps.
Found uncertainty sample 48 after 11 steps.
Found uncertainty sample 49 after 536 steps.
Found uncertainty sample 50 after 1061 steps.
Found uncertainty sample 51 after 210 steps.
Found uncertainty sample 52 after 20 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1882 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 14 steps.
Found uncertainty sample 57 after 138 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 92 steps.
Found uncertainty sample 60 after 839 steps.
Found uncertainty sample 61 after 491 steps.
Found uncertainty sample 62 after 331 steps.
Found uncertainty sample 63 after 137 steps.
Found uncertainty sample 64 after 86 steps.
Found uncertainty sample 65 after 141 steps.
Found uncertainty sample 66 after 42 steps.
Found uncertainty sample 67 after 53 steps.
Found uncertainty sample 68 after 15 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1801 steps.
Found uncertainty sample 71 after 631 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 54 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 187 steps.
Found uncertainty sample 76 after 17 steps.
Found uncertainty sample 77 after 102 steps.
Found uncertainty sample 78 after 96 steps.
Found uncertainty sample 79 after 947 steps.
Found uncertainty sample 80 after 211 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 92 steps.
Found uncertainty sample 83 after 15 steps.
Found uncertainty sample 84 after 19 steps.
Found uncertainty sample 85 after 1729 steps.
Found uncertainty sample 86 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 463 steps.
Found uncertainty sample 90 after 22 steps.
Found uncertainty sample 91 after 29 steps.
Found uncertainty sample 92 after 385 steps.
Found uncertainty sample 93 after 40 steps.
Found uncertainty sample 94 after 1522 steps.
Found uncertainty sample 95 after 175 steps.
Found uncertainty sample 96 after 110 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 20 steps.
Found uncertainty sample 99 after 7 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_023300-80503dmp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_29
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/80503dmp
Training model 29. Added 100 samples to the dataset.
Epoch 0, Batch 100/109, Loss: 0.386181116104126, Variance: 0.10394419729709625

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.5689269439405717, Training Loss Force: 4.225433378115912, time: 2.006514072418213
Validation Loss Energy: 1.7261487544274532, Validation Loss Force: 3.920796843258734, time: 0.11408090591430664
Test Loss Energy: 9.774439079908143, Test Loss Force: 12.102293580900707, time: 11.628936529159546

Epoch 1, Batch 100/109, Loss: 3.089597225189209, Variance: 0.09813745319843292

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.074673280157832, Training Loss Force: 3.761683021964695, time: 1.888141393661499
Validation Loss Energy: 2.233894917483215, Validation Loss Force: 3.8747575479775582, time: 0.11383700370788574
Test Loss Energy: 9.626126725561763, Test Loss Force: 11.930756862613425, time: 11.63814091682434

Epoch 2, Batch 100/109, Loss: 1.8392319679260254, Variance: 0.11327014863491058

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.735714658995952, Training Loss Force: 4.86803175161755, time: 2.134080648422241
Validation Loss Energy: 1.350182923651107, Validation Loss Force: 3.7026437616140804, time: 0.12447071075439453
Test Loss Energy: 9.844823831647682, Test Loss Force: 12.495908045190644, time: 11.700810432434082

Epoch 3, Batch 100/109, Loss: 0.6126348376274109, Variance: 0.11902742087841034

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.584201916609701, Training Loss Force: 3.547264958867037, time: 1.9171738624572754
Validation Loss Energy: 2.4615005270416495, Validation Loss Force: 3.654627832324504, time: 0.11809849739074707
Test Loss Energy: 9.984526971421104, Test Loss Force: 12.596457499340488, time: 11.706454753875732

Epoch 4, Batch 100/109, Loss: 0.8305380940437317, Variance: 0.126249760389328

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6133120975502444, Training Loss Force: 3.517652448590298, time: 1.8980138301849365
Validation Loss Energy: 3.813121503704444, Validation Loss Force: 3.6656893674867588, time: 0.11864471435546875
Test Loss Energy: 10.179929545971596, Test Loss Force: 12.33536937804289, time: 11.91951608657837

Epoch 5, Batch 100/109, Loss: 1.4204975366592407, Variance: 0.1264045387506485

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.664518323113774, Training Loss Force: 3.518922588126886, time: 2.0956544876098633
Validation Loss Energy: 2.3305308774552698, Validation Loss Force: 3.670982435754608, time: 0.11827373504638672
Test Loss Energy: 10.037227740628683, Test Loss Force: 12.677612069842397, time: 11.697429180145264

Epoch 6, Batch 100/109, Loss: 0.6353417634963989, Variance: 0.1270325779914856

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6456932729231952, Training Loss Force: 3.517054159066388, time: 1.9893951416015625
Validation Loss Energy: 1.917890714669893, Validation Loss Force: 3.6694292071271892, time: 0.12180614471435547
Test Loss Energy: 10.302741400709333, Test Loss Force: 12.98261184775762, time: 11.788445711135864

Epoch 7, Batch 100/109, Loss: 1.191424012184143, Variance: 0.13020336627960205

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6097636481692486, Training Loss Force: 3.5207511573674255, time: 1.9910542964935303
Validation Loss Energy: 3.023007427684638, Validation Loss Force: 3.675763253652251, time: 0.1250612735748291
Test Loss Energy: 10.832590467667858, Test Loss Force: 13.22050613526926, time: 11.674830675125122

Epoch 8, Batch 100/109, Loss: 1.3364019393920898, Variance: 0.1310538649559021

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.631200143717741, Training Loss Force: 3.519455417007565, time: 1.9308280944824219
Validation Loss Energy: 1.7961060560184618, Validation Loss Force: 3.6813552552485085, time: 0.12529993057250977
Test Loss Energy: 10.182491444262638, Test Loss Force: 13.049636449805075, time: 12.319142818450928

Epoch 9, Batch 100/109, Loss: 0.9528288841247559, Variance: 0.13264405727386475

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6137361025036503, Training Loss Force: 3.513121495095511, time: 1.9641637802124023
Validation Loss Energy: 2.9362623198401225, Validation Loss Force: 3.664974265524612, time: 0.12564921379089355
Test Loss Energy: 10.069458588878245, Test Loss Force: 12.81087551532457, time: 11.546116590499878

Epoch 10, Batch 100/109, Loss: 0.947165310382843, Variance: 0.13002648949623108

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6504408487687527, Training Loss Force: 3.5114955261780585, time: 2.016474485397339
Validation Loss Energy: 3.938403737965536, Validation Loss Force: 3.6714252133512857, time: 0.12028789520263672
Test Loss Energy: 10.477902431239036, Test Loss Force: 12.756126497747516, time: 11.66105031967163

Epoch 11, Batch 100/109, Loss: 1.560307264328003, Variance: 0.1313735842704773

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6947203857039885, Training Loss Force: 3.5256894156296736, time: 1.8630638122558594
Validation Loss Energy: 2.2565359162411527, Validation Loss Force: 3.6555266363320413, time: 0.11868548393249512
Test Loss Energy: 9.94812818111354, Test Loss Force: 12.7437885565898, time: 11.799875020980835

Epoch 12, Batch 100/109, Loss: 0.7607036232948303, Variance: 0.13159960508346558

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6372904114489906, Training Loss Force: 3.5318341402669557, time: 2.042166233062744
Validation Loss Energy: 2.19637316237207, Validation Loss Force: 3.6926665138579207, time: 0.13550543785095215
Test Loss Energy: 10.554451177892958, Test Loss Force: 13.186135872539166, time: 11.113975286483765

Epoch 13, Batch 100/109, Loss: 0.9050887227058411, Variance: 0.1336732804775238

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.633257606626114, Training Loss Force: 3.527185717562094, time: 2.060959577560425
Validation Loss Energy: 3.1816422713030965, Validation Loss Force: 3.6878566376367345, time: 0.11514878273010254
Test Loss Energy: 10.92278624021228, Test Loss Force: 13.399401455614944, time: 10.602323770523071

Epoch 14, Batch 100/109, Loss: 1.4337029457092285, Variance: 0.13509748876094818

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.681626554353943, Training Loss Force: 3.5241417098099257, time: 1.9024004936218262
Validation Loss Energy: 1.7237613515121017, Validation Loss Force: 3.689820690838391, time: 0.11241817474365234
Test Loss Energy: 10.452096858579173, Test Loss Force: 13.096813578592903, time: 10.657931089401245

Epoch 15, Batch 100/109, Loss: 0.887222409248352, Variance: 0.1314212530851364

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6485854223606773, Training Loss Force: 3.540503436609862, time: 1.9245917797088623
Validation Loss Energy: 2.9077260441091743, Validation Loss Force: 3.7152548513628467, time: 0.12054181098937988
Test Loss Energy: 10.27495703846384, Test Loss Force: 12.751570388575225, time: 10.866770267486572

Epoch 16, Batch 100/109, Loss: 0.8169152736663818, Variance: 0.13137736916542053

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6455371593807016, Training Loss Force: 3.5345680066698995, time: 1.8458669185638428
Validation Loss Energy: 3.6551754870889135, Validation Loss Force: 3.6924206871208165, time: 0.1184380054473877
Test Loss Energy: 10.480226094528462, Test Loss Force: 12.488469426799778, time: 10.967885971069336

Epoch 17, Batch 100/109, Loss: 0.8057081699371338, Variance: 0.12762974202632904

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.630201766766668, Training Loss Force: 3.5240590321431915, time: 1.788619041442871
Validation Loss Energy: 2.8572593068649437, Validation Loss Force: 3.6778797238842946, time: 0.10557389259338379
Test Loss Energy: 10.34007109260695, Test Loss Force: 13.042267537349144, time: 11.074723958969116

Epoch 18, Batch 100/109, Loss: 0.8089500665664673, Variance: 0.13463661074638367

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6735943450800788, Training Loss Force: 3.519523488395461, time: 1.9035327434539795
Validation Loss Energy: 2.246729010381628, Validation Loss Force: 3.663520236001703, time: 0.1125633716583252
Test Loss Energy: 10.593190116001587, Test Loss Force: 13.093308495768762, time: 10.857569217681885

Epoch 19, Batch 100/109, Loss: 1.1335701942443848, Variance: 0.13469162583351135

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6543838545978997, Training Loss Force: 3.5383969129472828, time: 1.9536561965942383
Validation Loss Energy: 3.000975985713678, Validation Loss Force: 3.692802676040656, time: 0.10824799537658691
Test Loss Energy: 10.913232171670536, Test Loss Force: 13.373920877317113, time: 10.64340591430664

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‚â–ƒâ–„â–ƒâ–…â–ˆâ–„â–ƒâ–†â–ƒâ–†â–ˆâ–…â–…â–†â–…â–†â–ˆ
wandb:   test_error_force â–‚â–â–„â–„â–ƒâ–…â–†â–‡â–†â–…â–…â–…â–‡â–ˆâ–‡â–…â–„â–†â–‡â–ˆ
wandb:          test_loss â–ˆâ–†â–…â–ƒâ–‚â–‚â–„â–…â–ƒâ–‚â–‚â–â–ƒâ–…â–ƒâ–‚â–‚â–‚â–ƒâ–…
wandb: train_error_energy â–â–ˆâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–…â–‚â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–‡â–ˆâ–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–ƒâ–â–„â–ˆâ–„â–ƒâ–†â–‚â–…â–ˆâ–ƒâ–ƒâ–†â–‚â–…â–‡â–…â–ƒâ–…
wandb:  valid_error_force â–ˆâ–‡â–‚â–â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚
wandb:         valid_loss â–ƒâ–„â–â–„â–ˆâ–ƒâ–ƒâ–…â–‚â–…â–ˆâ–ƒâ–ƒâ–†â–‚â–…â–‡â–…â–ƒâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 3487
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.91323
wandb:   test_error_force 13.37392
wandb:          test_loss 11.42585
wandb: train_error_energy 2.65438
wandb:  train_error_force 3.5384
wandb:         train_loss 1.03284
wandb: valid_error_energy 3.00098
wandb:  valid_error_force 3.6928
wandb:         valid_loss 1.21851
wandb: 
wandb: ğŸš€ View run al_63_29 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/80503dmp
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_023300-80503dmp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.32857608795166, Uncertainty Bias: -0.17636622488498688
0.00017166138 0.039710283
2.3751385 5.3879156
(48745, 22, 3)
Found uncertainty sample 0 after 898 steps.
Found uncertainty sample 1 after 253 steps.
Found uncertainty sample 2 after 783 steps.
Found uncertainty sample 3 after 866 steps.
Found uncertainty sample 4 after 13 steps.
Found uncertainty sample 5 after 276 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 104 steps.
Found uncertainty sample 8 after 910 steps.
Found uncertainty sample 9 after 183 steps.
Found uncertainty sample 10 after 3348 steps.
Found uncertainty sample 11 after 452 steps.
Found uncertainty sample 12 after 1169 steps.
Found uncertainty sample 13 after 318 steps.
Found uncertainty sample 14 after 37 steps.
Found uncertainty sample 15 after 633 steps.
Found uncertainty sample 16 after 1243 steps.
Found uncertainty sample 17 after 218 steps.
Found uncertainty sample 18 after 571 steps.
Found uncertainty sample 19 after 11 steps.
Found uncertainty sample 20 after 69 steps.
Found uncertainty sample 21 after 42 steps.
Found uncertainty sample 22 after 352 steps.
Found uncertainty sample 23 after 37 steps.
Found uncertainty sample 24 after 262 steps.
Found uncertainty sample 25 after 1306 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 41 steps.
Found uncertainty sample 28 after 156 steps.
Found uncertainty sample 29 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 212 steps.
Found uncertainty sample 32 after 2837 steps.
Found uncertainty sample 33 after 1577 steps.
Found uncertainty sample 34 after 1148 steps.
Found uncertainty sample 35 after 1171 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 281 steps.
Found uncertainty sample 38 after 10 steps.
Found uncertainty sample 39 after 219 steps.
Found uncertainty sample 40 after 256 steps.
Found uncertainty sample 41 after 55 steps.
Found uncertainty sample 42 after 93 steps.
Found uncertainty sample 43 after 325 steps.
Found uncertainty sample 44 after 247 steps.
Found uncertainty sample 45 after 1700 steps.
Found uncertainty sample 46 after 125 steps.
Found uncertainty sample 47 after 226 steps.
Found uncertainty sample 48 after 1378 steps.
Found uncertainty sample 49 after 49 steps.
Found uncertainty sample 50 after 866 steps.
Found uncertainty sample 51 after 671 steps.
Found uncertainty sample 52 after 2323 steps.
Found uncertainty sample 53 after 2547 steps.
Found uncertainty sample 54 after 1371 steps.
Found uncertainty sample 55 after 144 steps.
Found uncertainty sample 56 after 29 steps.
Found uncertainty sample 57 after 307 steps.
Found uncertainty sample 58 after 21 steps.
Found uncertainty sample 59 after 774 steps.
Found uncertainty sample 60 after 21 steps.
Found uncertainty sample 61 after 79 steps.
Found uncertainty sample 62 after 52 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 2446 steps.
Found uncertainty sample 65 after 501 steps.
Found uncertainty sample 66 after 1569 steps.
Found uncertainty sample 67 after 2348 steps.
Found uncertainty sample 68 after 169 steps.
Found uncertainty sample 69 after 37 steps.
Found uncertainty sample 70 after 115 steps.
Found uncertainty sample 71 after 325 steps.
Found uncertainty sample 72 after 269 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 40 steps.
Found uncertainty sample 75 after 191 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 429 steps.
Found uncertainty sample 78 after 560 steps.
Found uncertainty sample 79 after 1410 steps.
Found uncertainty sample 80 after 32 steps.
Found uncertainty sample 81 after 45 steps.
Found uncertainty sample 82 after 1349 steps.
Found uncertainty sample 83 after 1080 steps.
Found uncertainty sample 84 after 953 steps.
Found uncertainty sample 85 after 608 steps.
Found uncertainty sample 86 after 759 steps.
Found uncertainty sample 87 after 148 steps.
Found uncertainty sample 88 after 13 steps.
Found uncertainty sample 89 after 236 steps.
Found uncertainty sample 90 after 542 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 358 steps.
Found uncertainty sample 93 after 533 steps.
Found uncertainty sample 94 after 649 steps.
Found uncertainty sample 95 after 824 steps.
Found uncertainty sample 96 after 557 steps.
Found uncertainty sample 97 after 152 steps.
Found uncertainty sample 98 after 981 steps.
Found uncertainty sample 99 after 10 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_024602-w111hiyj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_30
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/w111hiyj
Training model 30. Added 99 samples to the dataset.
Epoch 0, Batch 100/112, Loss: 1.2664294242858887, Variance: 0.137356698513031

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.2260953163213233, Training Loss Force: 3.7025667459232325, time: 1.9186620712280273
Validation Loss Energy: 2.4231740456371207, Validation Loss Force: 3.682059223753359, time: 0.12355828285217285
Test Loss Energy: 9.969741618887568, Test Loss Force: 12.486743957758947, time: 11.258284330368042

Epoch 1, Batch 100/112, Loss: 0.6156167984008789, Variance: 0.1289193332195282

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6092694354316706, Training Loss Force: 3.542855082267537, time: 1.8770570755004883
Validation Loss Energy: 2.896334513272654, Validation Loss Force: 3.6464238103119406, time: 0.1266477108001709
Test Loss Energy: 10.511528228241989, Test Loss Force: 13.291823700522405, time: 11.343102216720581

Epoch 2, Batch 100/112, Loss: 0.7281889319419861, Variance: 0.13114504516124725

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6395048010033832, Training Loss Force: 3.5337363155223285, time: 2.1638619899749756
Validation Loss Energy: 3.1633019044296082, Validation Loss Force: 3.7087182872495634, time: 0.12159967422485352
Test Loss Energy: 11.224871537816474, Test Loss Force: 14.296983949181795, time: 11.259809732437134

Epoch 3, Batch 100/112, Loss: 1.3038301467895508, Variance: 0.1358688771724701

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.679205452238069, Training Loss Force: 3.5459633228636602, time: 1.8779573440551758
Validation Loss Energy: 2.3609455901012657, Validation Loss Force: 3.6615182177106664, time: 0.1237490177154541
Test Loss Energy: 10.47991577534553, Test Loss Force: 13.329565542516868, time: 11.226210832595825

Epoch 4, Batch 100/112, Loss: 0.8975620269775391, Variance: 0.13304783403873444

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.666072962802939, Training Loss Force: 3.549718302017896, time: 1.9880239963531494
Validation Loss Energy: 2.7419117905531394, Validation Loss Force: 3.6900258804563633, time: 0.12375640869140625
Test Loss Energy: 10.380978697626528, Test Loss Force: 13.055297156490917, time: 11.393054008483887

Epoch 5, Batch 100/112, Loss: 0.9160618185997009, Variance: 0.13399836421012878

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.7065450711748653, Training Loss Force: 3.5516175406050317, time: 1.9213285446166992
Validation Loss Energy: 3.3566982964262913, Validation Loss Force: 3.688541122591317, time: 0.11502289772033691
Test Loss Energy: 11.24716404420554, Test Loss Force: 13.885797471131895, time: 11.381377935409546

Epoch 6, Batch 100/112, Loss: 1.392437219619751, Variance: 0.13309156894683838

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6551763737867655, Training Loss Force: 3.5560077848920115, time: 1.8792779445648193
Validation Loss Energy: 2.457930472624989, Validation Loss Force: 3.656996141360592, time: 0.12114906311035156
Test Loss Energy: 10.529477329286063, Test Loss Force: 13.197694210545357, time: 11.53670883178711

Epoch 7, Batch 100/112, Loss: 0.5461764335632324, Variance: 0.13521763682365417

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6368552121271134, Training Loss Force: 3.5418130781417596, time: 1.8235530853271484
Validation Loss Energy: 3.1745560484030904, Validation Loss Force: 3.66635203086504, time: 0.11854863166809082
Test Loss Energy: 10.428334847744457, Test Loss Force: 12.770029432274457, time: 11.389457941055298

Epoch 8, Batch 100/112, Loss: 0.8901666402816772, Variance: 0.13604173064231873

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.7376617363746933, Training Loss Force: 3.5404134872487956, time: 1.8360180854797363
Validation Loss Energy: 3.201655217466221, Validation Loss Force: 3.683854382407829, time: 0.11574578285217285
Test Loss Energy: 11.049549126948904, Test Loss Force: 13.483516173421519, time: 11.508908033370972

Epoch 9, Batch 100/112, Loss: 1.3498481512069702, Variance: 0.13542310893535614

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.694436145082807, Training Loss Force: 3.53630323350788, time: 1.885322093963623
Validation Loss Energy: 2.307718980127833, Validation Loss Force: 3.6606205270488585, time: 0.12087082862854004
Test Loss Energy: 10.374949367415729, Test Loss Force: 13.253827008444295, time: 11.291054964065552

Epoch 10, Batch 100/112, Loss: 1.18831205368042, Variance: 0.141370952129364

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6819790737929736, Training Loss Force: 3.5318686343222767, time: 1.9410693645477295
Validation Loss Energy: 2.878309334575227, Validation Loss Force: 3.6717142379027, time: 0.12016844749450684
Test Loss Energy: 10.636648427564557, Test Loss Force: 13.512377046557202, time: 11.97449278831482

Epoch 11, Batch 100/112, Loss: 0.9632413387298584, Variance: 0.13193675875663757

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6633079706714975, Training Loss Force: 3.533735758849464, time: 2.1485512256622314
Validation Loss Energy: 3.387408268359787, Validation Loss Force: 3.654695523744866, time: 0.15718507766723633
Test Loss Energy: 11.316892166784074, Test Loss Force: 13.751898362362645, time: 11.261629819869995

Epoch 12, Batch 100/112, Loss: 1.4044630527496338, Variance: 0.13264644145965576

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6845986063107783, Training Loss Force: 3.5464520895914413, time: 1.9103243350982666
Validation Loss Energy: 2.4175183606651274, Validation Loss Force: 3.6864020514409614, time: 0.1272566318511963
Test Loss Energy: 10.509183501150842, Test Loss Force: 13.511980597549659, time: 11.37233829498291

Epoch 13, Batch 100/112, Loss: 0.7745561599731445, Variance: 0.13119404017925262

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.694560687713547, Training Loss Force: 3.567919588152344, time: 1.8927972316741943
Validation Loss Energy: 2.767564946182615, Validation Loss Force: 3.7379566387976224, time: 0.12394595146179199
Test Loss Energy: 10.424232181554615, Test Loss Force: 12.899845203934701, time: 11.46717643737793

Epoch 14, Batch 100/112, Loss: 1.0382471084594727, Variance: 0.13009552657604218

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.633328546384031, Training Loss Force: 3.548804861490249, time: 1.9619529247283936
Validation Loss Energy: 3.2798515001257043, Validation Loss Force: 3.6983520364954683, time: 0.12616491317749023
Test Loss Energy: 11.011526418720738, Test Loss Force: 13.575229514304509, time: 11.288784265518188

Epoch 15, Batch 100/112, Loss: 1.1985456943511963, Variance: 0.13488534092903137

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.702531898199397, Training Loss Force: 3.557345056257759, time: 1.8630518913269043
Validation Loss Energy: 2.0300964249999427, Validation Loss Force: 3.6646253429729425, time: 0.1264019012451172
Test Loss Energy: 10.330845054188787, Test Loss Force: 13.018710881094094, time: 11.4019935131073

Epoch 16, Batch 100/112, Loss: 0.8557565808296204, Variance: 0.13067902624607086

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6501362964369783, Training Loss Force: 3.5373856938341084, time: 1.9107372760772705
Validation Loss Energy: 2.7777289020176346, Validation Loss Force: 3.652016479915636, time: 0.12015819549560547
Test Loss Energy: 10.348770722511224, Test Loss Force: 12.919092137341949, time: 11.32138204574585

Epoch 17, Batch 100/112, Loss: 0.8491725921630859, Variance: 0.13326919078826904

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.7063609297555353, Training Loss Force: 3.5497388333060207, time: 2.056091070175171
Validation Loss Energy: 2.87318427017798, Validation Loss Force: 3.664752598389841, time: 0.12778472900390625
Test Loss Energy: 10.832739378533553, Test Loss Force: 13.065902493868677, time: 11.47533392906189

Epoch 18, Batch 100/112, Loss: 1.4164657592773438, Variance: 0.13745911419391632

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.7323300926654293, Training Loss Force: 3.5341370484127728, time: 1.9110898971557617
Validation Loss Energy: 2.2353291294521362, Validation Loss Force: 3.7067097002187217, time: 0.128021240234375
Test Loss Energy: 10.369552030531425, Test Loss Force: 13.031282389362012, time: 11.306665420532227

Epoch 19, Batch 100/112, Loss: 0.7302126884460449, Variance: 0.1323680579662323

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.665053669935987, Training Loss Force: 3.545864503489568, time: 1.9359207153320312
Validation Loss Energy: 2.89474164953726, Validation Loss Force: 3.6723072058383055, time: 0.12291955947875977
Test Loss Energy: 10.42260914498414, Test Loss Force: 12.936205266132175, time: 10.970777988433838

wandb: - 0.039 MB of 0.059 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–„â–ˆâ–„â–ƒâ–ˆâ–„â–ƒâ–‡â–ƒâ–„â–ˆâ–„â–ƒâ–†â–ƒâ–ƒâ–…â–ƒâ–ƒ
wandb:   test_error_force â–â–„â–ˆâ–„â–ƒâ–†â–„â–‚â–…â–„â–…â–†â–…â–ƒâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:          test_loss â–‚â–„â–ˆâ–„â–ƒâ–ˆâ–ƒâ–ƒâ–‡â–‚â–‚â–ˆâ–‚â–‚â–‡â–‚â–‚â–†â–â–
wandb: train_error_energy â–ˆâ–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–…â–‡â–ƒâ–…â–ˆâ–ƒâ–‡â–‡â–‚â–…â–ˆâ–ƒâ–…â–‡â–â–…â–…â–‚â–…
wandb:  valid_error_force â–„â–â–†â–‚â–„â–„â–‚â–ƒâ–„â–‚â–ƒâ–‚â–„â–ˆâ–…â–‚â–â–‚â–†â–ƒ
wandb:         valid_loss â–ƒâ–…â–‡â–‚â–„â–ˆâ–ƒâ–‡â–‡â–‚â–…â–ˆâ–ƒâ–…â–ˆâ–â–„â–…â–‚â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 3576
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.42261
wandb:   test_error_force 12.93621
wandb:          test_loss 9.72552
wandb: train_error_energy 2.66505
wandb:  train_error_force 3.54586
wandb:         train_loss 1.03646
wandb: valid_error_energy 2.89474
wandb:  valid_error_force 3.67231
wandb:         valid_loss 1.13351
wandb: 
wandb: ğŸš€ View run al_63_30 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/w111hiyj
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_024602-w111hiyj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.002026319503784, Uncertainty Bias: -0.16350482404232025
0.00017547607 0.025568008
2.3651001 5.083776
(48745, 22, 3)
Found uncertainty sample 0 after 455 steps.
Found uncertainty sample 1 after 420 steps.
Found uncertainty sample 2 after 42 steps.
Found uncertainty sample 3 after 43 steps.
Found uncertainty sample 4 after 37 steps.
Found uncertainty sample 5 after 29 steps.
Found uncertainty sample 6 after 521 steps.
Found uncertainty sample 7 after 640 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 30 steps.
Found uncertainty sample 11 after 8 steps.
Found uncertainty sample 12 after 649 steps.
Found uncertainty sample 13 after 338 steps.
Found uncertainty sample 14 after 12 steps.
Found uncertainty sample 15 after 444 steps.
Found uncertainty sample 16 after 2 steps.
Found uncertainty sample 17 after 536 steps.
Found uncertainty sample 18 after 148 steps.
Found uncertainty sample 19 after 144 steps.
Found uncertainty sample 20 after 3 steps.
Found uncertainty sample 21 after 603 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 968 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 92 steps.
Found uncertainty sample 29 after 746 steps.
Found uncertainty sample 30 after 3853 steps.
Found uncertainty sample 31 after 705 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 209 steps.
Found uncertainty sample 34 after 54 steps.
Found uncertainty sample 35 after 2009 steps.
Found uncertainty sample 36 after 331 steps.
Found uncertainty sample 37 after 453 steps.
Found uncertainty sample 38 after 644 steps.
Found uncertainty sample 39 after 166 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 17 steps.
Found uncertainty sample 42 after 153 steps.
Found uncertainty sample 43 after 656 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 466 steps.
Found uncertainty sample 46 after 238 steps.
Found uncertainty sample 47 after 2977 steps.
Found uncertainty sample 48 after 77 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 190 steps.
Found uncertainty sample 51 after 3312 steps.
Found uncertainty sample 52 after 920 steps.
Found uncertainty sample 53 after 32 steps.
Found uncertainty sample 54 after 328 steps.
Found uncertainty sample 55 after 674 steps.
Found uncertainty sample 56 after 17 steps.
Found uncertainty sample 57 after 302 steps.
Found uncertainty sample 58 after 72 steps.
Found uncertainty sample 59 after 608 steps.
Found uncertainty sample 60 after 674 steps.
Found uncertainty sample 61 after 50 steps.
Found uncertainty sample 62 after 444 steps.
Found uncertainty sample 63 after 975 steps.
Found uncertainty sample 64 after 1013 steps.
Found uncertainty sample 65 after 964 steps.
Found uncertainty sample 66 after 48 steps.
Found uncertainty sample 67 after 15 steps.
Found uncertainty sample 68 after 12 steps.
Found uncertainty sample 69 after 126 steps.
Found uncertainty sample 70 after 531 steps.
Found uncertainty sample 71 after 103 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 12 steps.
Found uncertainty sample 74 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 67 steps.
Found uncertainty sample 77 after 884 steps.
Found uncertainty sample 78 after 455 steps.
Found uncertainty sample 79 after 255 steps.
Found uncertainty sample 80 after 26 steps.
Found uncertainty sample 81 after 8 steps.
Found uncertainty sample 82 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1005 steps.
Found uncertainty sample 85 after 28 steps.
Found uncertainty sample 86 after 4 steps.
Found uncertainty sample 87 after 153 steps.
Found uncertainty sample 88 after 207 steps.
Found uncertainty sample 89 after 734 steps.
Found uncertainty sample 90 after 767 steps.
Did not find any uncertainty samples for sample 91.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 322 steps.
Found uncertainty sample 94 after 123 steps.
Found uncertainty sample 95 after 625 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 670 steps.
Found uncertainty sample 98 after 1703 steps.
Found uncertainty sample 99 after 17 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_025733-u32y3nug
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_31
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/u32y3nug
Training model 31. Added 99 samples to the dataset.
Epoch 0, Batch 100/115, Loss: 0.640153169631958, Variance: 0.12038987874984741

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.6287469674813893, Training Loss Force: 4.047883289127267, time: 2.0134220123291016
Validation Loss Energy: 1.4228607411997747, Validation Loss Force: 3.978392518767464, time: 0.13305878639221191
Test Loss Energy: 9.87394771085311, Test Loss Force: 12.43238219330345, time: 11.303894758224487

Epoch 1, Batch 100/115, Loss: 0.6576606035232544, Variance: 0.1015939712524414

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6840816022457041, Training Loss Force: 3.5527879520974888, time: 2.0257034301757812
Validation Loss Energy: 1.7986503664586433, Validation Loss Force: 3.64031254426738, time: 0.1273815631866455
Test Loss Energy: 9.901628395546323, Test Loss Force: 12.361536579385428, time: 10.950152397155762

Epoch 2, Batch 100/115, Loss: 0.8780088424682617, Variance: 0.09873684495687485

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6855854593147113, Training Loss Force: 3.535477124095873, time: 2.0884132385253906
Validation Loss Energy: 1.8959626468499298, Validation Loss Force: 3.570214515776145, time: 0.1094059944152832
Test Loss Energy: 9.781648687162747, Test Loss Force: 12.1660157593578, time: 11.693342685699463

Epoch 3, Batch 100/115, Loss: 0.4147276282310486, Variance: 0.0969097837805748

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7185127473683142, Training Loss Force: 3.531652276152415, time: 2.0292956829071045
Validation Loss Energy: 1.6843377421140628, Validation Loss Force: 3.688995293425961, time: 0.13242745399475098
Test Loss Energy: 10.089369794477497, Test Loss Force: 12.664011032708276, time: 10.059372663497925

Epoch 4, Batch 100/115, Loss: 0.9016834497451782, Variance: 0.09404369443655014

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7051012385219486, Training Loss Force: 3.537281191452433, time: 1.8866801261901855
Validation Loss Energy: 1.6529022027685614, Validation Loss Force: 3.884434971728726, time: 0.11153888702392578
Test Loss Energy: 10.028016493970332, Test Loss Force: 12.677033433172817, time: 9.720381736755371

Epoch 5, Batch 100/115, Loss: 0.442385733127594, Variance: 0.09406980872154236

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.710840234670827, Training Loss Force: 3.554465243259879, time: 1.8383922576904297
Validation Loss Energy: 2.0917642738494004, Validation Loss Force: 3.7022886414378497, time: 0.11155247688293457
Test Loss Energy: 9.71953355040345, Test Loss Force: 12.366340403902232, time: 9.527597427368164

Epoch 6, Batch 100/115, Loss: 0.7636129856109619, Variance: 0.09336129575967789

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.7118575993491771, Training Loss Force: 3.549436748534967, time: 1.8877551555633545
Validation Loss Energy: 2.0984845028965498, Validation Loss Force: 3.829806659481678, time: 0.11126542091369629
Test Loss Energy: 9.933583605040981, Test Loss Force: 12.507583383592046, time: 9.53929591178894

Epoch 7, Batch 100/115, Loss: 0.9269514083862305, Variance: 0.09116924554109573

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6976975639656073, Training Loss Force: 3.5538074224514586, time: 2.0893635749816895
Validation Loss Energy: 1.3103421250524099, Validation Loss Force: 3.6622897663013063, time: 0.10962200164794922
Test Loss Energy: 9.83672315143497, Test Loss Force: 12.413535266946859, time: 9.486124753952026

Epoch 8, Batch 100/115, Loss: 0.5986394286155701, Variance: 0.08994544297456741

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.702374378913947, Training Loss Force: 3.567516941033339, time: 1.8174991607666016
Validation Loss Energy: 1.4408654714473013, Validation Loss Force: 3.7126184857865856, time: 0.11302709579467773
Test Loss Energy: 10.064917826389921, Test Loss Force: 12.677560070184883, time: 9.55150055885315

Epoch 9, Batch 100/115, Loss: 0.7686530947685242, Variance: 0.0965527817606926

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.7263917721448752, Training Loss Force: 3.5747168510542426, time: 1.8592839241027832
Validation Loss Energy: 1.7524707896155338, Validation Loss Force: 3.557405844271892, time: 0.11211133003234863
Test Loss Energy: 9.789245132511603, Test Loss Force: 12.260165361838018, time: 9.686852931976318

Epoch 10, Batch 100/115, Loss: 0.9584445953369141, Variance: 0.08633542060852051

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6853769335320565, Training Loss Force: 3.5395984046964175, time: 1.9520225524902344
Validation Loss Energy: 1.956196023949796, Validation Loss Force: 3.652521716207746, time: 0.11670899391174316
Test Loss Energy: 10.029324813933696, Test Loss Force: 12.408804948895964, time: 9.598730087280273

Epoch 11, Batch 100/115, Loss: 0.43213868141174316, Variance: 0.09073507785797119

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.7166947564132462, Training Loss Force: 3.5542831850936616, time: 1.8824493885040283
Validation Loss Energy: 1.3426069956319193, Validation Loss Force: 3.707026892417096, time: 0.11564207077026367
Test Loss Energy: 9.823005771420815, Test Loss Force: 12.445027366587036, time: 9.515384197235107

Epoch 12, Batch 100/115, Loss: 0.9835153222084045, Variance: 0.08871391415596008

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6850184391114928, Training Loss Force: 3.553784547553285, time: 1.914675235748291
Validation Loss Energy: 1.9566603262305797, Validation Loss Force: 3.728729711840191, time: 0.11056303977966309
Test Loss Energy: 10.101511180567131, Test Loss Force: 12.565307759924114, time: 9.67415165901184

Epoch 13, Batch 100/115, Loss: 0.6473817825317383, Variance: 0.08977118879556656

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.7072416495596863, Training Loss Force: 3.55142801674689, time: 1.8466169834136963
Validation Loss Energy: 1.5849871854724782, Validation Loss Force: 3.7783949514936546, time: 0.11195158958435059
Test Loss Energy: 9.726629491470998, Test Loss Force: 12.371915339832036, time: 10.909540176391602

Epoch 14, Batch 100/115, Loss: 0.9209758043289185, Variance: 0.09286575019359589

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.7088739789808796, Training Loss Force: 3.560613750235033, time: 2.0661942958831787
Validation Loss Energy: 1.919581497162209, Validation Loss Force: 3.624424432697994, time: 0.1331787109375
Test Loss Energy: 9.842117778983697, Test Loss Force: 12.604306267303315, time: 11.92429494857788

Epoch 15, Batch 100/115, Loss: 0.4470558762550354, Variance: 0.08784499019384384

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7316961026308753, Training Loss Force: 3.5581551614459186, time: 2.0116944313049316
Validation Loss Energy: 1.6352367460763801, Validation Loss Force: 3.674593424182595, time: 0.13275957107543945
Test Loss Energy: 10.047773523919268, Test Loss Force: 12.929417303307007, time: 11.519026517868042

Epoch 16, Batch 100/115, Loss: 0.3452465534210205, Variance: 0.0852145403623581

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6989234941198075, Training Loss Force: 3.55306784721392, time: 1.9840998649597168
Validation Loss Energy: 1.657363750222468, Validation Loss Force: 3.6615343241122216, time: 0.12748289108276367
Test Loss Energy: 9.852387130770584, Test Loss Force: 12.63464169516077, time: 10.803428411483765

Epoch 17, Batch 100/115, Loss: 0.8009189963340759, Variance: 0.09163205325603485

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.7242449230644399, Training Loss Force: 3.5589118802714395, time: 1.9710454940795898
Validation Loss Energy: 2.001301147940756, Validation Loss Force: 3.7731375102891533, time: 0.12533903121948242
Test Loss Energy: 9.792135480616766, Test Loss Force: 12.459518581339454, time: 11.38935136795044

Epoch 18, Batch 100/115, Loss: 0.8064345717430115, Variance: 0.08851630985736847

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.7029133350597645, Training Loss Force: 3.5651793564668823, time: 1.911020278930664
Validation Loss Energy: 2.015310637037473, Validation Loss Force: 3.895226186547825, time: 0.13305878639221191
Test Loss Energy: 9.861680860609107, Test Loss Force: 12.58115480827189, time: 11.50356388092041

Epoch 19, Batch 100/115, Loss: 0.7295368313789368, Variance: 0.08910411596298218

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6963075053520702, Training Loss Force: 3.561720073173434, time: 2.0480411052703857
Validation Loss Energy: 1.3093001076725164, Validation Loss Force: 3.6480296630808478, time: 0.13313722610473633
Test Loss Energy: 10.013114472130786, Test Loss Force: 12.956859720356004, time: 11.711626052856445

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.039 MB of 0.056 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–„â–‚â–ˆâ–‡â–â–…â–ƒâ–‡â–‚â–‡â–ƒâ–ˆâ–â–ƒâ–‡â–ƒâ–‚â–„â–†
wandb:   test_error_force â–ƒâ–ƒâ–â–…â–†â–ƒâ–„â–ƒâ–†â–‚â–ƒâ–ƒâ–…â–ƒâ–…â–ˆâ–…â–„â–…â–ˆ
wandb:          test_loss â–â–ƒâ–ƒâ–…â–…â–„â–„â–†â–†â–…â–†â–‡â–ˆâ–…â–†â–‡â–‡â–…â–†â–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–…â–†â–„â–„â–ˆâ–ˆâ–â–‚â–…â–‡â–â–‡â–ƒâ–†â–„â–„â–‡â–‡â–
wandb:  valid_error_force â–ˆâ–‚â–â–ƒâ–†â–ƒâ–†â–ƒâ–„â–â–ƒâ–ƒâ–„â–…â–‚â–ƒâ–ƒâ–…â–‡â–ƒ
wandb:         valid_loss â–„â–ƒâ–„â–„â–„â–†â–‡â–‚â–ƒâ–ƒâ–…â–‚â–ˆâ–ƒâ–„â–ƒâ–ƒâ–…â–†â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 3665
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.01311
wandb:   test_error_force 12.95686
wandb:          test_loss 13.27251
wandb: train_error_energy 1.69631
wandb:  train_error_force 3.56172
wandb:         train_loss 0.63473
wandb: valid_error_energy 1.3093
wandb:  valid_error_force 3.64803
wandb:         valid_loss 0.48609
wandb: 
wandb: ğŸš€ View run al_63_31 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/u32y3nug
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_025733-u32y3nug/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.4676573276519775, Uncertainty Bias: -0.05270075798034668
2.2888184e-05 0.0008468628
2.5780208 5.3061633
(48745, 22, 3)
Found uncertainty sample 0 after 20 steps.
Found uncertainty sample 1 after 2921 steps.
Found uncertainty sample 2 after 22 steps.
Found uncertainty sample 3 after 611 steps.
Found uncertainty sample 4 after 1268 steps.
Found uncertainty sample 5 after 168 steps.
Found uncertainty sample 6 after 847 steps.
Found uncertainty sample 7 after 2353 steps.
Found uncertainty sample 8 after 20 steps.
Found uncertainty sample 9 after 1760 steps.
Found uncertainty sample 10 after 752 steps.
Found uncertainty sample 11 after 819 steps.
Found uncertainty sample 12 after 140 steps.
Found uncertainty sample 13 after 16 steps.
Found uncertainty sample 14 after 326 steps.
Found uncertainty sample 15 after 454 steps.
Found uncertainty sample 16 after 15 steps.
Found uncertainty sample 17 after 42 steps.
Found uncertainty sample 18 after 21 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 183 steps.
Found uncertainty sample 21 after 660 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 948 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 171 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1346 steps.
Found uncertainty sample 30 after 290 steps.
Found uncertainty sample 31 after 26 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 428 steps.
Found uncertainty sample 34 after 952 steps.
Found uncertainty sample 35 after 211 steps.
Found uncertainty sample 36 after 324 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 698 steps.
Found uncertainty sample 39 after 2214 steps.
Found uncertainty sample 40 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 92 steps.
Found uncertainty sample 43 after 2229 steps.
Found uncertainty sample 44 after 954 steps.
Found uncertainty sample 45 after 11 steps.
Found uncertainty sample 46 after 15 steps.
Found uncertainty sample 47 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 24 steps.
Found uncertainty sample 50 after 576 steps.
Found uncertainty sample 51 after 245 steps.
Found uncertainty sample 52 after 337 steps.
Found uncertainty sample 53 after 9 steps.
Found uncertainty sample 54 after 367 steps.
Found uncertainty sample 55 after 137 steps.
Found uncertainty sample 56 after 2004 steps.
Found uncertainty sample 57 after 219 steps.
Found uncertainty sample 58 after 553 steps.
Found uncertainty sample 59 after 17 steps.
Found uncertainty sample 60 after 2047 steps.
Found uncertainty sample 61 after 254 steps.
Found uncertainty sample 62 after 484 steps.
Found uncertainty sample 63 after 139 steps.
Found uncertainty sample 64 after 209 steps.
Found uncertainty sample 65 after 288 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 1568 steps.
Found uncertainty sample 68 after 143 steps.
Found uncertainty sample 69 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 3306 steps.
Found uncertainty sample 72 after 220 steps.
Found uncertainty sample 73 after 375 steps.
Found uncertainty sample 74 after 6 steps.
Found uncertainty sample 75 after 10 steps.
Found uncertainty sample 76 after 53 steps.
Found uncertainty sample 77 after 37 steps.
Found uncertainty sample 78 after 47 steps.
Found uncertainty sample 79 after 78 steps.
Found uncertainty sample 80 after 15 steps.
Found uncertainty sample 81 after 3 steps.
Found uncertainty sample 82 after 39 steps.
Found uncertainty sample 83 after 71 steps.
Found uncertainty sample 84 after 369 steps.
Found uncertainty sample 85 after 991 steps.
Found uncertainty sample 86 after 961 steps.
Found uncertainty sample 87 after 889 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1168 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1557 steps.
Found uncertainty sample 92 after 934 steps.
Found uncertainty sample 93 after 76 steps.
Found uncertainty sample 94 after 506 steps.
Found uncertainty sample 95 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 611 steps.
Found uncertainty sample 98 after 19 steps.
Found uncertainty sample 99 after 457 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_030928-gawn2tph
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_32
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/gawn2tph
Training model 32. Added 99 samples to the dataset.
Epoch 0, Batch 100/118, Loss: 1.778693675994873, Variance: 0.11514778435230255

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.081447099283464, Training Loss Force: 4.333602201075529, time: 1.9354350566864014
Validation Loss Energy: 1.2943838355905328, Validation Loss Force: 4.297156612865751, time: 0.12889838218688965
Test Loss Energy: 9.859458981601318, Test Loss Force: 12.526314326137175, time: 11.511845111846924

Epoch 1, Batch 100/118, Loss: 1.2346808910369873, Variance: 0.10090526938438416

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.9287495506032593, Training Loss Force: 4.3164509251818295, time: 1.9604113101959229
Validation Loss Energy: 1.5257678436631668, Validation Loss Force: 5.033377852827508, time: 0.13483858108520508
Test Loss Energy: 9.815868341612086, Test Loss Force: 12.56643856011685, time: 11.508573770523071

Epoch 2, Batch 100/118, Loss: 1.1515915393829346, Variance: 0.13646627962589264

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.637461884312412, Training Loss Force: 3.9957832746986015, time: 2.290856122970581
Validation Loss Energy: 1.968744024107216, Validation Loss Force: 3.7082138674109886, time: 0.12354445457458496
Test Loss Energy: 9.584925686908518, Test Loss Force: 11.714507401064745, time: 12.273282289505005

Epoch 3, Batch 100/118, Loss: 1.9269031286239624, Variance: 0.1559765487909317

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.130960377623478, Training Loss Force: 3.601841126006145, time: 2.0206449031829834
Validation Loss Energy: 5.444650140493689, Validation Loss Force: 3.6509662980003923, time: 0.12093853950500488
Test Loss Energy: 11.459741162261476, Test Loss Force: 12.389978186338045, time: 11.514113903045654

Epoch 4, Batch 100/118, Loss: 1.217353343963623, Variance: 0.1659390926361084

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.172579833772014, Training Loss Force: 3.5925087809333394, time: 2.1555287837982178
Validation Loss Energy: 2.2729433293703662, Validation Loss Force: 3.6716166345937347, time: 0.1300971508026123
Test Loss Energy: 9.775214835896415, Test Loss Force: 11.757155723881318, time: 11.524839639663696

Epoch 5, Batch 100/118, Loss: 1.6479175090789795, Variance: 0.16086867451667786

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.1323233562229955, Training Loss Force: 3.568014638965711, time: 2.041020393371582
Validation Loss Energy: 5.881656152908647, Validation Loss Force: 3.8070749601582996, time: 0.1366426944732666
Test Loss Energy: 10.930032276605482, Test Loss Force: 11.573444179495079, time: 11.365387439727783

Epoch 6, Batch 100/118, Loss: 1.2301403284072876, Variance: 0.16360123455524445

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.20241821733998, Training Loss Force: 3.5720868651431847, time: 2.197444438934326
Validation Loss Energy: 1.9356030617730537, Validation Loss Force: 3.6761246456760914, time: 0.13153409957885742
Test Loss Energy: 9.823690179432129, Test Loss Force: 11.797820239478739, time: 11.948024272918701

Epoch 7, Batch 100/118, Loss: 1.7408957481384277, Variance: 0.16984759271144867

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.112257127031575, Training Loss Force: 3.5828561413903293, time: 2.1147537231445312
Validation Loss Energy: 5.728189905505464, Validation Loss Force: 3.7256091522061108, time: 0.1322033405303955
Test Loss Energy: 11.65203110194559, Test Loss Force: 12.042286307220024, time: 11.890547752380371

Epoch 8, Batch 100/118, Loss: 1.3414723873138428, Variance: 0.1682814359664917

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.122911420139707, Training Loss Force: 3.5672361046023746, time: 2.1172070503234863
Validation Loss Energy: 2.66256156215641, Validation Loss Force: 3.7156637832493424, time: 0.1389923095703125
Test Loss Energy: 9.969260369075442, Test Loss Force: 11.71598381641456, time: 12.058213472366333

Epoch 9, Batch 100/118, Loss: 1.9260318279266357, Variance: 0.16456037759780884

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.0851367874163405, Training Loss Force: 3.5693828578382645, time: 2.092705011367798
Validation Loss Energy: 5.929555997350146, Validation Loss Force: 3.794853541414907, time: 0.12721943855285645
Test Loss Energy: 11.353556476641371, Test Loss Force: 11.594396068402359, time: 11.872766971588135

Epoch 10, Batch 100/118, Loss: 1.2568347454071045, Variance: 0.1640481799840927

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.156256737082545, Training Loss Force: 3.565546710018167, time: 2.162729263305664
Validation Loss Energy: 2.0338477321013886, Validation Loss Force: 3.627497192688749, time: 0.1397082805633545
Test Loss Energy: 10.117010981627967, Test Loss Force: 12.074662331089502, time: 12.014612436294556

Epoch 11, Batch 100/118, Loss: 1.7119755744934082, Variance: 0.17588721215724945

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.149948763604851, Training Loss Force: 3.561137315578092, time: 2.202454090118408
Validation Loss Energy: 5.586107840856986, Validation Loss Force: 3.8943081216591637, time: 0.1397078037261963
Test Loss Energy: 11.79137345708064, Test Loss Force: 12.170116793417774, time: 11.933433532714844

Epoch 12, Batch 100/118, Loss: 1.1709089279174805, Variance: 0.17471766471862793

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.171856930841653, Training Loss Force: 3.553783108473935, time: 2.1349003314971924
Validation Loss Energy: 2.478151948443275, Validation Loss Force: 3.7136789847055915, time: 0.14273333549499512
Test Loss Energy: 10.04974059947569, Test Loss Force: 11.671957733503685, time: 12.132858514785767

Epoch 13, Batch 100/118, Loss: 1.7055710554122925, Variance: 0.1643841564655304

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.109593965362232, Training Loss Force: 3.5647252080577734, time: 2.1816647052764893
Validation Loss Energy: 5.942760723041956, Validation Loss Force: 3.6074744119716784, time: 0.1294105052947998
Test Loss Energy: 11.214147885013153, Test Loss Force: 11.710561022967585, time: 11.926292896270752

Epoch 14, Batch 100/118, Loss: 1.163567066192627, Variance: 0.1701522171497345

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.175590247030918, Training Loss Force: 3.567476541991336, time: 2.1751296520233154
Validation Loss Energy: 1.8319414820202098, Validation Loss Force: 3.58147177711767, time: 0.14178848266601562
Test Loss Energy: 10.268209244686275, Test Loss Force: 12.097649336715689, time: 12.097108602523804

Epoch 15, Batch 100/118, Loss: 1.9251112937927246, Variance: 0.17714136838912964

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.196608049986922, Training Loss Force: 3.576978113728021, time: 2.0782487392425537
Validation Loss Energy: 5.419385356966235, Validation Loss Force: 3.7884100158930973, time: 0.13694047927856445
Test Loss Energy: 11.531610242849071, Test Loss Force: 12.365327187008223, time: 11.917166233062744

Epoch 16, Batch 100/118, Loss: 1.2009810209274292, Variance: 0.17714764177799225

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.173787278916614, Training Loss Force: 3.574315466273443, time: 2.0539016723632812
Validation Loss Energy: 2.640571276865922, Validation Loss Force: 3.6172131439610253, time: 0.13095998764038086
Test Loss Energy: 10.151698446465792, Test Loss Force: 11.780061701254525, time: 11.988362789154053

Epoch 17, Batch 100/118, Loss: 1.5156776905059814, Variance: 0.17220383882522583

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.132100709721389, Training Loss Force: 3.572346380371683, time: 2.2884907722473145
Validation Loss Energy: 6.18549933598441, Validation Loss Force: 3.7391776450593177, time: 0.11754631996154785
Test Loss Energy: 11.6526079899046, Test Loss Force: 11.785419647850333, time: 11.698904275894165

Epoch 18, Batch 100/118, Loss: 1.207135796546936, Variance: 0.16992175579071045

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.193007786160454, Training Loss Force: 3.572879293683822, time: 2.155646800994873
Validation Loss Energy: 2.1129440434487106, Validation Loss Force: 3.627799725835016, time: 0.13735699653625488
Test Loss Energy: 10.206799999430551, Test Loss Force: 12.22188054287992, time: 12.044068574905396

Epoch 19, Batch 100/118, Loss: 1.7253899574279785, Variance: 0.17911610007286072

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.094380006332949, Training Loss Force: 3.5622090429347355, time: 1.9839577674865723
Validation Loss Energy: 5.707156429535028, Validation Loss Force: 3.6674756564869155, time: 0.11957669258117676
Test Loss Energy: 11.647973641089791, Test Loss Force: 12.346435120602775, time: 10.970271587371826

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–â–‡â–‚â–…â–‚â–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–†â–ƒâ–‡â–ƒâ–ˆâ–ƒâ–ˆ
wandb:   test_error_force â–ˆâ–ˆâ–‚â–‡â–‚â–â–ƒâ–„â–‚â–â–…â–…â–‚â–‚â–…â–‡â–‚â–‚â–†â–†
wandb:          test_loss â–†â–ˆâ–ƒâ–…â–â–‚â–â–„â–â–‚â–‚â–ƒâ–â–‚â–‚â–ƒâ–â–‚â–â–ƒ
wandb: train_error_energy â–†â–â–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb:  train_error_force â–ˆâ–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–â–‚â–‡â–‚â–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–‡
wandb:  valid_error_force â–„â–ˆâ–‚â–â–â–‚â–â–‚â–‚â–‚â–â–ƒâ–‚â–â–â–‚â–â–‚â–â–
wandb:         valid_loss â–â–‚â–‚â–ˆâ–‚â–ˆâ–‚â–ˆâ–ƒâ–ˆâ–‚â–ˆâ–ƒâ–‡â–‚â–‡â–ƒâ–ˆâ–‚â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3754
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.64797
wandb:   test_error_force 12.34644
wandb:          test_loss 9.90274
wandb: train_error_energy 4.09438
wandb:  train_error_force 3.56221
wandb:         train_loss 1.45885
wandb: valid_error_energy 5.70716
wandb:  valid_error_force 3.66748
wandb:         valid_loss 2.03058
wandb: 
wandb: ğŸš€ View run al_63_32 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/gawn2tph
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_030928-gawn2tph/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.8416261672973633, Uncertainty Bias: -0.06337985396385193
0.00011444092 0.9410248
2.5411403 4.497304
(48745, 22, 3)
Found uncertainty sample 0 after 821 steps.
Found uncertainty sample 1 after 3863 steps.
Found uncertainty sample 2 after 141 steps.
Found uncertainty sample 3 after 147 steps.
Found uncertainty sample 4 after 1378 steps.
Found uncertainty sample 5 after 625 steps.
Found uncertainty sample 6 after 2292 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 16 steps.
Found uncertainty sample 9 after 12 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 3378 steps.
Found uncertainty sample 12 after 2402 steps.
Found uncertainty sample 13 after 1297 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 917 steps.
Found uncertainty sample 16 after 347 steps.
Found uncertainty sample 17 after 311 steps.
Found uncertainty sample 18 after 1761 steps.
Found uncertainty sample 19 after 450 steps.
Found uncertainty sample 20 after 404 steps.
Found uncertainty sample 21 after 13 steps.
Found uncertainty sample 22 after 131 steps.
Found uncertainty sample 23 after 1373 steps.
Found uncertainty sample 24 after 403 steps.
Found uncertainty sample 25 after 874 steps.
Found uncertainty sample 26 after 576 steps.
Found uncertainty sample 27 after 610 steps.
Found uncertainty sample 28 after 23 steps.
Found uncertainty sample 29 after 1311 steps.
Found uncertainty sample 30 after 2622 steps.
Found uncertainty sample 31 after 18 steps.
Found uncertainty sample 32 after 1751 steps.
Found uncertainty sample 33 after 2685 steps.
Found uncertainty sample 34 after 371 steps.
Found uncertainty sample 35 after 682 steps.
Found uncertainty sample 36 after 1896 steps.
Found uncertainty sample 37 after 261 steps.
Found uncertainty sample 38 after 597 steps.
Found uncertainty sample 39 after 51 steps.
Found uncertainty sample 40 after 21 steps.
Found uncertainty sample 41 after 1246 steps.
Found uncertainty sample 42 after 31 steps.
Found uncertainty sample 43 after 193 steps.
Found uncertainty sample 44 after 1290 steps.
Found uncertainty sample 45 after 2565 steps.
Found uncertainty sample 46 after 337 steps.
Found uncertainty sample 47 after 67 steps.
Found uncertainty sample 48 after 1340 steps.
Found uncertainty sample 49 after 890 steps.
Found uncertainty sample 50 after 259 steps.
Found uncertainty sample 51 after 1407 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 7 steps.
Found uncertainty sample 54 after 660 steps.
Found uncertainty sample 55 after 21 steps.
Found uncertainty sample 56 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1281 steps.
Found uncertainty sample 59 after 52 steps.
Found uncertainty sample 60 after 93 steps.
Found uncertainty sample 61 after 21 steps.
Found uncertainty sample 62 after 250 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 633 steps.
Found uncertainty sample 65 after 7 steps.
Found uncertainty sample 66 after 21 steps.
Found uncertainty sample 67 after 113 steps.
Found uncertainty sample 68 after 67 steps.
Found uncertainty sample 69 after 108 steps.
Found uncertainty sample 70 after 2343 steps.
Found uncertainty sample 71 after 91 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 524 steps.
Found uncertainty sample 74 after 89 steps.
Found uncertainty sample 75 after 973 steps.
Found uncertainty sample 76 after 207 steps.
Found uncertainty sample 77 after 331 steps.
Found uncertainty sample 78 after 288 steps.
Found uncertainty sample 79 after 2008 steps.
Found uncertainty sample 80 after 41 steps.
Found uncertainty sample 81 after 17 steps.
Found uncertainty sample 82 after 28 steps.
Found uncertainty sample 83 after 706 steps.
Found uncertainty sample 84 after 1284 steps.
Found uncertainty sample 85 after 454 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 61 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 670 steps.
Found uncertainty sample 90 after 308 steps.
Found uncertainty sample 91 after 1685 steps.
Found uncertainty sample 92 after 224 steps.
Found uncertainty sample 93 after 1123 steps.
Found uncertainty sample 94 after 168 steps.
Found uncertainty sample 95 after 641 steps.
Found uncertainty sample 96 after 421 steps.
Found uncertainty sample 97 after 3530 steps.
Found uncertainty sample 98 after 35 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_032346-0rfu0c8y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_33
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/0rfu0c8y
Training model 33. Added 99 samples to the dataset.
Epoch 0, Batch 100/121, Loss: 1.1860754489898682, Variance: 0.1253536194562912

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.339542798084307, Training Loss Force: 3.796751768893742, time: 2.048218011856079
Validation Loss Energy: 3.3445871360216572, Validation Loss Force: 3.5916878375328367, time: 0.12859416007995605
Test Loss Energy: 10.600122177240959, Test Loss Force: 13.10843845620209, time: 10.883918046951294

Epoch 1, Batch 100/121, Loss: 1.2690021991729736, Variance: 0.1259346753358841

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6522157120059533, Training Loss Force: 3.5396134226141642, time: 2.00160551071167
Validation Loss Energy: 2.0995363919419123, Validation Loss Force: 3.7275163876640307, time: 0.12320995330810547
Test Loss Energy: 10.304766365580472, Test Loss Force: 12.625975444643856, time: 11.020128965377808

Epoch 2, Batch 100/121, Loss: 0.7449287176132202, Variance: 0.12633022665977478

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.614840795234981, Training Loss Force: 3.5230944893543654, time: 2.3217480182647705
Validation Loss Energy: 1.8492679249377157, Validation Loss Force: 3.5860641798063333, time: 0.13245916366577148
Test Loss Energy: 10.389636586975298, Test Loss Force: 12.747925674709489, time: 11.27640700340271

Epoch 3, Batch 100/121, Loss: 0.9280146956443787, Variance: 0.13253706693649292

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6719546251399553, Training Loss Force: 3.521723283552538, time: 2.0252225399017334
Validation Loss Energy: 3.2989032949204318, Validation Loss Force: 3.6935632025085723, time: 0.13291096687316895
Test Loss Energy: 10.78941057880702, Test Loss Force: 12.676249959541899, time: 11.352125406265259

Epoch 4, Batch 100/121, Loss: 1.5123393535614014, Variance: 0.1392669677734375

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.7109354741168987, Training Loss Force: 3.531420451248058, time: 2.04009747505188
Validation Loss Energy: 1.6252463237290868, Validation Loss Force: 3.6130115256234654, time: 0.13632869720458984
Test Loss Energy: 9.990915006470503, Test Loss Force: 12.35467425206036, time: 11.400268077850342

Epoch 5, Batch 100/121, Loss: 0.9639023542404175, Variance: 0.13165336847305298

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6610276888758113, Training Loss Force: 3.517879447299635, time: 2.0738298892974854
Validation Loss Energy: 2.6129426376852996, Validation Loss Force: 3.733005107888102, time: 0.13356900215148926
Test Loss Energy: 10.24223363673703, Test Loss Force: 12.152570045028126, time: 11.966400623321533

Epoch 6, Batch 100/121, Loss: 0.8608222603797913, Variance: 0.12780240178108215

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.701074658400928, Training Loss Force: 3.5289535715497182, time: 2.128225326538086
Validation Loss Energy: 3.9726090588172336, Validation Loss Force: 3.7037693912582696, time: 0.1247105598449707
Test Loss Energy: 10.609275688471127, Test Loss Force: 12.17054019588424, time: 11.472564220428467

Epoch 7, Batch 100/121, Loss: 1.1427538394927979, Variance: 0.12856459617614746

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6842473430959464, Training Loss Force: 3.5300225398706457, time: 2.0373966693878174
Validation Loss Energy: 2.3895072130033252, Validation Loss Force: 3.6427862500325396, time: 0.13246822357177734
Test Loss Energy: 10.18247263746336, Test Loss Force: 12.077471123996903, time: 11.292619228363037

Epoch 8, Batch 100/121, Loss: 0.5427600145339966, Variance: 0.1268400400876999

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6541374157443425, Training Loss Force: 3.524004987172546, time: 2.0592081546783447
Validation Loss Energy: 2.017988641744022, Validation Loss Force: 3.618987000600535, time: 0.13103032112121582
Test Loss Energy: 10.10014320392557, Test Loss Force: 12.415803288597557, time: 11.502675771713257

Epoch 9, Batch 100/121, Loss: 0.9398187398910522, Variance: 0.12988311052322388

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6452718418589876, Training Loss Force: 3.5352022787444226, time: 1.9882926940917969
Validation Loss Energy: 3.1174321717030566, Validation Loss Force: 3.6454259927575143, time: 0.13501191139221191
Test Loss Energy: 10.657876267663035, Test Loss Force: 12.485406114897934, time: 11.42981743812561

Epoch 10, Batch 100/121, Loss: 1.2655607461929321, Variance: 0.12933409214019775

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.656564759707715, Training Loss Force: 3.528535763261359, time: 2.038080930709839
Validation Loss Energy: 1.9140336953914183, Validation Loss Force: 3.6359627767247287, time: 0.12344837188720703
Test Loss Energy: 10.13277526700188, Test Loss Force: 12.154905030635593, time: 11.445685148239136

Epoch 11, Batch 100/121, Loss: 0.8131941556930542, Variance: 0.13384073972702026

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6942360537125185, Training Loss Force: 3.5228934688020965, time: 2.1786916255950928
Validation Loss Energy: 2.8892691841608413, Validation Loss Force: 3.7040774169101365, time: 0.12491226196289062
Test Loss Energy: 10.040822837746516, Test Loss Force: 11.964624261468748, time: 11.358399391174316

Epoch 12, Batch 100/121, Loss: 0.9643988609313965, Variance: 0.13215942680835724

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6969559177648477, Training Loss Force: 3.5215642830604184, time: 2.0952541828155518
Validation Loss Energy: 3.781897240266904, Validation Loss Force: 3.5980779966826724, time: 0.13128352165222168
Test Loss Energy: 10.161566466845224, Test Loss Force: 11.884910012930495, time: 11.309504270553589

Epoch 13, Batch 100/121, Loss: 1.5539932250976562, Variance: 0.13045373558998108

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.722074839506618, Training Loss Force: 3.539054042755018, time: 2.08685302734375
Validation Loss Energy: 2.2886250266414763, Validation Loss Force: 3.6904952670159794, time: 0.1284620761871338
Test Loss Energy: 9.76733975333693, Test Loss Force: 11.996550787446827, time: 11.369046926498413

Epoch 14, Batch 100/121, Loss: 0.7838741540908813, Variance: 0.1287156343460083

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.643617378820649, Training Loss Force: 3.5425716926322677, time: 2.121312379837036
Validation Loss Energy: 2.2613977114848645, Validation Loss Force: 4.202323308076078, time: 0.14240026473999023
Test Loss Energy: 9.876586319247828, Test Loss Force: 12.451225169905776, time: 11.364703893661499

Epoch 15, Batch 100/121, Loss: 0.6650331616401672, Variance: 0.1021457388997078

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7690100047834343, Training Loss Force: 3.8463946845851456, time: 2.04535174369812
Validation Loss Energy: 1.6002181632807062, Validation Loss Force: 3.7514887994685187, time: 0.13155674934387207
Test Loss Energy: 9.75389437475969, Test Loss Force: 12.066896829729187, time: 11.500329971313477

Epoch 16, Batch 100/121, Loss: 0.4111488461494446, Variance: 0.09452776610851288

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.7188171846331513, Training Loss Force: 3.5724929161637813, time: 2.097972869873047
Validation Loss Energy: 1.5242894318687792, Validation Loss Force: 3.6410346603076236, time: 0.1404428482055664
Test Loss Energy: 9.67782701781474, Test Loss Force: 12.27485962753538, time: 11.338497877120972

Epoch 17, Batch 100/121, Loss: 0.6052460670471191, Variance: 0.09222295880317688

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.7065214256791135, Training Loss Force: 3.538071817868235, time: 2.0435311794281006
Validation Loss Energy: 2.07917856838327, Validation Loss Force: 3.799411801501195, time: 0.1326160430908203
Test Loss Energy: 9.533834421583219, Test Loss Force: 11.776099155745248, time: 12.130862951278687

Epoch 18, Batch 100/121, Loss: 0.6166183352470398, Variance: 0.09123973548412323

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.724968524683136, Training Loss Force: 3.527903922077501, time: 2.065725326538086
Validation Loss Energy: 1.8462365814081039, Validation Loss Force: 3.5427923651168456, time: 0.13367843627929688
Test Loss Energy: 9.639861690609415, Test Loss Force: 11.84752107938199, time: 11.335898637771606

Epoch 19, Batch 100/121, Loss: 0.5113086104393005, Variance: 0.09166327118873596

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.7128258259236722, Training Loss Force: 3.5419302877165797, time: 2.1494665145874023
Validation Loss Energy: 1.4488951435186028, Validation Loss Force: 3.699285873781965, time: 0.13613486289978027
Test Loss Energy: 9.48798025063203, Test Loss Force: 12.007038709402067, time: 11.462461709976196

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–…â–†â–ˆâ–„â–…â–‡â–…â–„â–‡â–„â–„â–…â–ƒâ–ƒâ–‚â–‚â–â–‚â–
wandb:   test_error_force â–ˆâ–…â–†â–†â–„â–ƒâ–ƒâ–ƒâ–„â–…â–ƒâ–‚â–‚â–‚â–…â–ƒâ–„â–â–â–‚
wandb:          test_loss â–…â–„â–…â–…â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–â–‚â–‡â–ˆâ–‡â–‡â–ˆ
wandb: train_error_energy â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–â–â–â–â–
wandb:  train_error_force â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–ˆâ–‚â–â–â–‚
wandb:         train_loss â–ˆâ–„â–„â–„â–…â–„â–…â–…â–„â–„â–„â–…â–„â–…â–„â–‚â–â–â–â–
wandb: valid_error_energy â–†â–ƒâ–‚â–†â–â–„â–ˆâ–„â–ƒâ–†â–‚â–…â–‡â–ƒâ–ƒâ–â–â–ƒâ–‚â–
wandb:  valid_error_force â–‚â–ƒâ–â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ˆâ–ƒâ–‚â–„â–â–ƒ
wandb:         valid_loss â–†â–ƒâ–‚â–†â–‚â–„â–ˆâ–ƒâ–ƒâ–…â–‚â–…â–‡â–ƒâ–„â–‚â–â–ƒâ–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 3843
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.48798
wandb:   test_error_force 12.00704
wandb:          test_loss 12.65671
wandb: train_error_energy 1.71283
wandb:  train_error_force 3.54193
wandb:         train_loss 0.63745
wandb: valid_error_energy 1.4489
wandb:  valid_error_force 3.69929
wandb:         valid_loss 0.59391
wandb: 
wandb: ğŸš€ View run al_63_33 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/0rfu0c8y
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_032346-0rfu0c8y/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.935281753540039, Uncertainty Bias: -0.10437136888504028
0.0001373291 0.0037193298
2.3890846 5.31752
(48745, 22, 3)
Found uncertainty sample 0 after 337 steps.
Found uncertainty sample 1 after 279 steps.
Found uncertainty sample 2 after 64 steps.
Found uncertainty sample 3 after 18 steps.
Found uncertainty sample 4 after 808 steps.
Found uncertainty sample 5 after 41 steps.
Found uncertainty sample 6 after 729 steps.
Found uncertainty sample 7 after 828 steps.
Found uncertainty sample 8 after 24 steps.
Found uncertainty sample 9 after 77 steps.
Found uncertainty sample 10 after 17 steps.
Found uncertainty sample 11 after 375 steps.
Found uncertainty sample 12 after 266 steps.
Found uncertainty sample 13 after 2839 steps.
Found uncertainty sample 14 after 565 steps.
Found uncertainty sample 15 after 486 steps.
Found uncertainty sample 16 after 63 steps.
Found uncertainty sample 17 after 1403 steps.
Found uncertainty sample 18 after 7 steps.
Found uncertainty sample 19 after 12 steps.
Found uncertainty sample 20 after 168 steps.
Found uncertainty sample 21 after 218 steps.
Found uncertainty sample 22 after 27 steps.
Found uncertainty sample 23 after 213 steps.
Found uncertainty sample 24 after 10 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 263 steps.
Found uncertainty sample 29 after 237 steps.
Found uncertainty sample 30 after 712 steps.
Found uncertainty sample 31 after 48 steps.
Found uncertainty sample 32 after 6 steps.
Found uncertainty sample 33 after 76 steps.
Found uncertainty sample 34 after 24 steps.
Found uncertainty sample 35 after 38 steps.
Found uncertainty sample 36 after 205 steps.
Found uncertainty sample 37 after 1482 steps.
Found uncertainty sample 38 after 596 steps.
Found uncertainty sample 39 after 227 steps.
Found uncertainty sample 40 after 222 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 16 steps.
Found uncertainty sample 43 after 1692 steps.
Found uncertainty sample 44 after 258 steps.
Found uncertainty sample 45 after 39 steps.
Found uncertainty sample 46 after 462 steps.
Found uncertainty sample 47 after 253 steps.
Found uncertainty sample 48 after 7 steps.
Found uncertainty sample 49 after 404 steps.
Found uncertainty sample 50 after 1483 steps.
Found uncertainty sample 51 after 1452 steps.
Found uncertainty sample 52 after 599 steps.
Found uncertainty sample 53 after 42 steps.
Found uncertainty sample 54 after 35 steps.
Found uncertainty sample 55 after 1745 steps.
Found uncertainty sample 56 after 54 steps.
Found uncertainty sample 57 after 78 steps.
Found uncertainty sample 58 after 455 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 30 steps.
Found uncertainty sample 62 after 466 steps.
Found uncertainty sample 63 after 588 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 91 steps.
Found uncertainty sample 67 after 274 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 400 steps.
Found uncertainty sample 70 after 13 steps.
Found uncertainty sample 71 after 95 steps.
Found uncertainty sample 72 after 304 steps.
Found uncertainty sample 73 after 49 steps.
Found uncertainty sample 74 after 69 steps.
Found uncertainty sample 75 after 484 steps.
Found uncertainty sample 76 after 2414 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 98 steps.
Found uncertainty sample 79 after 1907 steps.
Found uncertainty sample 80 after 1217 steps.
Found uncertainty sample 81 after 15 steps.
Found uncertainty sample 82 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 15 steps.
Found uncertainty sample 85 after 1432 steps.
Found uncertainty sample 86 after 506 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 801 steps.
Found uncertainty sample 89 after 68 steps.
Found uncertainty sample 90 after 549 steps.
Found uncertainty sample 91 after 466 steps.
Found uncertainty sample 92 after 25 steps.
Found uncertainty sample 93 after 67 steps.
Found uncertainty sample 94 after 614 steps.
Found uncertainty sample 95 after 2460 steps.
Found uncertainty sample 96 after 26 steps.
Found uncertainty sample 97 after 61 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 72 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_033514-je8c9xp4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_34
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/je8c9xp4
Training model 34. Added 99 samples to the dataset.
Epoch 0, Batch 100/123, Loss: 1.8348171710968018, Variance: 0.17707134783267975

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 6.7998012975031585, Training Loss Force: 4.345130216914417, time: 2.1071202754974365
Validation Loss Energy: 3.859433754247003, Validation Loss Force: 4.00115286818762, time: 0.1328413486480713
Test Loss Energy: 10.269606860806192, Test Loss Force: 11.435948541996947, time: 11.383607864379883

Epoch 1, Batch 100/123, Loss: 0.9898391962051392, Variance: 0.13057532906532288

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6565695128105156, Training Loss Force: 3.6156489058831105, time: 2.090752601623535
Validation Loss Energy: 2.812556620243724, Validation Loss Force: 3.5961271180721504, time: 0.13258028030395508
Test Loss Energy: 9.58645883975569, Test Loss Force: 11.350472012336473, time: 11.657855749130249

Epoch 2, Batch 100/123, Loss: 0.9191389083862305, Variance: 0.12404528260231018

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6685457194198525, Training Loss Force: 3.5242295934038954, time: 1.9859440326690674
Validation Loss Energy: 2.1276723456600073, Validation Loss Force: 3.6419736560960008, time: 0.13064908981323242
Test Loss Energy: 9.971406811040852, Test Loss Force: 11.738270801541457, time: 11.670447587966919

Epoch 3, Batch 100/123, Loss: 0.8843725919723511, Variance: 0.12773600220680237

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6565491449197767, Training Loss Force: 3.5122228059933587, time: 2.0339534282684326
Validation Loss Energy: 2.9766925051123283, Validation Loss Force: 3.606924344958189, time: 0.12606453895568848
Test Loss Energy: 9.97350593056662, Test Loss Force: 11.595915045035843, time: 11.442206144332886

Epoch 4, Batch 100/123, Loss: 0.7494276165962219, Variance: 0.12320176512002945

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.675853007523808, Training Loss Force: 3.522512169815492, time: 2.0899834632873535
Validation Loss Energy: 2.2537450182776184, Validation Loss Force: 3.6845764984480724, time: 0.13550066947937012
Test Loss Energy: 9.798499546673282, Test Loss Force: 11.687753435045899, time: 11.555946111679077

Epoch 5, Batch 100/123, Loss: 1.1119135618209839, Variance: 0.12903982400894165

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6492124201357607, Training Loss Force: 3.5226847058178574, time: 2.1791257858276367
Validation Loss Energy: 2.807049641149687, Validation Loss Force: 3.71330504898653, time: 0.1347370147705078
Test Loss Energy: 9.91193888166905, Test Loss Force: 11.764167652229851, time: 12.098785877227783

Epoch 6, Batch 100/123, Loss: 1.1273077726364136, Variance: 0.12438782304525375

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6576464267903392, Training Loss Force: 3.527259418076488, time: 2.076808452606201
Validation Loss Energy: 2.1282929987927566, Validation Loss Force: 3.6888832166225107, time: 0.140977144241333
Test Loss Energy: 9.967696382963501, Test Loss Force: 11.98000807405489, time: 10.807955503463745

Epoch 7, Batch 100/123, Loss: 0.9704267382621765, Variance: 0.13066516816616058

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.691223984484323, Training Loss Force: 3.534587615356342, time: 2.1440699100494385
Validation Loss Energy: 2.9963633943557837, Validation Loss Force: 3.619449632862724, time: 0.14706110954284668
Test Loss Energy: 10.072030792903579, Test Loss Force: 11.767129669489021, time: 11.738116264343262

Epoch 8, Batch 100/123, Loss: 0.802056074142456, Variance: 0.12530454993247986

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.647715027154422, Training Loss Force: 3.5156663571803577, time: 2.1105258464813232
Validation Loss Energy: 2.226850222378925, Validation Loss Force: 3.6371214727370886, time: 0.11680912971496582
Test Loss Energy: 9.826561021253786, Test Loss Force: 11.805547833457126, time: 10.084954023361206

Epoch 9, Batch 100/123, Loss: 1.1758551597595215, Variance: 0.1267472505569458

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7061781057953183, Training Loss Force: 3.5229545428060383, time: 2.025113344192505
Validation Loss Energy: 2.9826414082379844, Validation Loss Force: 3.6341997566503212, time: 0.11936426162719727
Test Loss Energy: 9.914759668399347, Test Loss Force: 11.581184490096728, time: 9.962981939315796

Epoch 10, Batch 100/123, Loss: 1.172349214553833, Variance: 0.12634794414043427

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6772860020536995, Training Loss Force: 3.5222875038885233, time: 2.0624587535858154
Validation Loss Energy: 2.387645995835564, Validation Loss Force: 3.755992282944908, time: 0.128082275390625
Test Loss Energy: 9.770926559235047, Test Loss Force: 11.742779667548977, time: 9.930402278900146

Epoch 11, Batch 100/123, Loss: 0.9729232788085938, Variance: 0.13240757584571838

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.7085787724599775, Training Loss Force: 3.5303800555542537, time: 1.9394357204437256
Validation Loss Energy: 3.379580705649939, Validation Loss Force: 3.6472751115683466, time: 0.1190800666809082
Test Loss Energy: 9.885577704023621, Test Loss Force: 11.456386100151343, time: 10.0710928440094

Epoch 12, Batch 100/123, Loss: 0.921740710735321, Variance: 0.1265932023525238

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.7179516671889976, Training Loss Force: 3.5291409687389415, time: 1.994673490524292
Validation Loss Energy: 1.8045174757846947, Validation Loss Force: 3.588865134992959, time: 0.11744952201843262
Test Loss Energy: 9.728241173080837, Test Loss Force: 11.920151874977783, time: 10.035904169082642

Epoch 13, Batch 100/123, Loss: 0.9870321750640869, Variance: 0.12891408801078796

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6692372428918274, Training Loss Force: 3.5300223583899806, time: 2.007840394973755
Validation Loss Energy: 2.9217871182246977, Validation Loss Force: 3.6386349549219594, time: 0.11960983276367188
Test Loss Energy: 10.010833472716042, Test Loss Force: 11.700550825423958, time: 10.21235966682434

Epoch 14, Batch 100/123, Loss: 1.1976711750030518, Variance: 0.12494789063930511

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.696525388362598, Training Loss Force: 3.541808348523671, time: 2.018634796142578
Validation Loss Energy: 2.1302883370094157, Validation Loss Force: 3.6423624364247913, time: 0.11975240707397461
Test Loss Energy: 9.837503859771576, Test Loss Force: 11.674970710520482, time: 10.005009412765503

Epoch 15, Batch 100/123, Loss: 1.0528572797775269, Variance: 0.1307588517665863

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6567926444927585, Training Loss Force: 3.5173683278920977, time: 2.032184362411499
Validation Loss Energy: 2.9003320553379717, Validation Loss Force: 3.731104849396754, time: 0.11675548553466797
Test Loss Energy: 9.946346636753765, Test Loss Force: 11.674137447602208, time: 9.934633255004883

Epoch 16, Batch 100/123, Loss: 0.9969465732574463, Variance: 0.1274462193250656

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6462292265460583, Training Loss Force: 3.523889237278231, time: 1.9833166599273682
Validation Loss Energy: 2.2800167859060054, Validation Loss Force: 3.6184516297506755, time: 0.11993598937988281
Test Loss Energy: 10.031460730993087, Test Loss Force: 11.90284495641379, time: 10.054314374923706

Epoch 17, Batch 100/123, Loss: 1.0886125564575195, Variance: 0.1343403458595276

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.673930538237734, Training Loss Force: 3.519277581854877, time: 2.008808135986328
Validation Loss Energy: 2.720455688543732, Validation Loss Force: 3.646582536301463, time: 0.12424612045288086
Test Loss Energy: 9.958010470971224, Test Loss Force: 11.58580727876886, time: 10.52622389793396

Epoch 18, Batch 100/123, Loss: 0.8789191246032715, Variance: 0.1285792738199234

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.7207476358539764, Training Loss Force: 3.5132797747917577, time: 2.1646292209625244
Validation Loss Energy: 1.8575131036307517, Validation Loss Force: 3.6174851714343004, time: 0.1346912384033203
Test Loss Energy: 9.724851281319374, Test Loss Force: 11.788212287335295, time: 11.981228113174438

Epoch 19, Batch 100/123, Loss: 0.9836902618408203, Variance: 0.13322198390960693

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6678385719961377, Training Loss Force: 3.5140068348111684, time: 2.1911697387695312
Validation Loss Energy: 2.668148182581641, Validation Loss Force: 3.649604973065238, time: 0.1271061897277832
Test Loss Energy: 9.663444400320342, Test Loss Force: 11.62877930889649, time: 11.049091339111328

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–â–…â–…â–ƒâ–„â–…â–†â–ƒâ–„â–ƒâ–„â–‚â–…â–„â–…â–†â–…â–‚â–‚
wandb:   test_error_force â–‚â–â–…â–„â–…â–†â–ˆâ–†â–†â–„â–…â–‚â–‡â–…â–…â–…â–‡â–„â–†â–„
wandb:          test_loss â–â–†â–ˆâ–‡â–‡â–‡â–‡â–†â–†â–…â–†â–…â–…â–…â–†â–…â–†â–…â–…â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–„â–‚â–…â–ƒâ–„â–‚â–…â–‚â–…â–ƒâ–†â–â–…â–‚â–…â–ƒâ–„â–â–„
wandb:  valid_error_force â–ˆâ–â–‚â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–„â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–â–‚
wandb:         valid_loss â–ˆâ–„â–‚â–…â–ƒâ–…â–‚â–…â–‚â–…â–ƒâ–‡â–â–…â–‚â–…â–ƒâ–„â–â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 3932
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.66344
wandb:   test_error_force 11.62878
wandb:          test_loss 9.51485
wandb: train_error_energy 2.66784
wandb:  train_error_force 3.51401
wandb:         train_loss 1.02757
wandb: valid_error_energy 2.66815
wandb:  valid_error_force 3.6496
wandb:         valid_loss 1.04861
wandb: 
wandb: ğŸš€ View run al_63_34 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/je8c9xp4
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_033514-je8c9xp4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.3544094562530518, Uncertainty Bias: -0.06753790378570557
0.00035858154 0.004917145
2.3922544 4.827666
(48745, 22, 3)
Found uncertainty sample 0 after 203 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 72 steps.
Found uncertainty sample 3 after 2736 steps.
Found uncertainty sample 4 after 34 steps.
Found uncertainty sample 5 after 10 steps.
Found uncertainty sample 6 after 222 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 773 steps.
Found uncertainty sample 10 after 27 steps.
Found uncertainty sample 11 after 2447 steps.
Found uncertainty sample 12 after 41 steps.
Found uncertainty sample 13 after 484 steps.
Found uncertainty sample 14 after 99 steps.
Found uncertainty sample 15 after 1204 steps.
Found uncertainty sample 16 after 1080 steps.
Found uncertainty sample 17 after 37 steps.
Found uncertainty sample 18 after 410 steps.
Found uncertainty sample 19 after 826 steps.
Found uncertainty sample 20 after 242 steps.
Found uncertainty sample 21 after 886 steps.
Found uncertainty sample 22 after 897 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 17 steps.
Found uncertainty sample 25 after 63 steps.
Found uncertainty sample 26 after 19 steps.
Found uncertainty sample 27 after 1594 steps.
Found uncertainty sample 28 after 69 steps.
Found uncertainty sample 29 after 1577 steps.
Found uncertainty sample 30 after 45 steps.
Found uncertainty sample 31 after 1035 steps.
Found uncertainty sample 32 after 770 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 63 steps.
Found uncertainty sample 36 after 896 steps.
Found uncertainty sample 37 after 73 steps.
Found uncertainty sample 38 after 176 steps.
Found uncertainty sample 39 after 513 steps.
Found uncertainty sample 40 after 217 steps.
Found uncertainty sample 41 after 174 steps.
Found uncertainty sample 42 after 41 steps.
Found uncertainty sample 43 after 134 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1757 steps.
Found uncertainty sample 46 after 61 steps.
Found uncertainty sample 47 after 156 steps.
Found uncertainty sample 48 after 882 steps.
Found uncertainty sample 49 after 221 steps.
Found uncertainty sample 50 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 13 steps.
Found uncertainty sample 53 after 1179 steps.
Found uncertainty sample 54 after 54 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 1443 steps.
Found uncertainty sample 57 after 132 steps.
Found uncertainty sample 58 after 348 steps.
Found uncertainty sample 59 after 1004 steps.
Found uncertainty sample 60 after 50 steps.
Found uncertainty sample 61 after 2267 steps.
Found uncertainty sample 62 after 27 steps.
Found uncertainty sample 63 after 506 steps.
Found uncertainty sample 64 after 1652 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 11 steps.
Found uncertainty sample 67 after 195 steps.
Found uncertainty sample 68 after 7 steps.
Found uncertainty sample 69 after 9 steps.
Found uncertainty sample 70 after 61 steps.
Found uncertainty sample 71 after 28 steps.
Found uncertainty sample 72 after 5 steps.
Found uncertainty sample 73 after 189 steps.
Found uncertainty sample 74 after 58 steps.
Found uncertainty sample 75 after 504 steps.
Found uncertainty sample 76 after 1901 steps.
Found uncertainty sample 77 after 1678 steps.
Found uncertainty sample 78 after 1077 steps.
Found uncertainty sample 79 after 162 steps.
Found uncertainty sample 80 after 142 steps.
Found uncertainty sample 81 after 207 steps.
Found uncertainty sample 82 after 1401 steps.
Found uncertainty sample 83 after 94 steps.
Found uncertainty sample 84 after 1600 steps.
Found uncertainty sample 85 after 9 steps.
Found uncertainty sample 86 after 84 steps.
Found uncertainty sample 87 after 2438 steps.
Found uncertainty sample 88 after 25 steps.
Found uncertainty sample 89 after 195 steps.
Found uncertainty sample 90 after 23 steps.
Found uncertainty sample 91 after 56 steps.
Found uncertainty sample 92 after 1620 steps.
Found uncertainty sample 93 after 1504 steps.
Found uncertainty sample 94 after 740 steps.
Found uncertainty sample 95 after 499 steps.
Found uncertainty sample 96 after 208 steps.
Found uncertainty sample 97 after 1810 steps.
Found uncertainty sample 98 after 354 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_034741-dxu5jlbm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_35
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/dxu5jlbm
Training model 35. Added 99 samples to the dataset.
Epoch 0, Batch 100/126, Loss: 1.1755791902542114, Variance: 0.12001334130764008

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.250183450142468, Training Loss Force: 3.7843853064247615, time: 2.110252857208252
Validation Loss Energy: 3.673814675956237, Validation Loss Force: 3.6220374936766966, time: 0.13090872764587402
Test Loss Energy: 10.34080117227513, Test Loss Force: 12.005552016297324, time: 11.41989016532898

Epoch 1, Batch 100/126, Loss: 1.3229129314422607, Variance: 0.1336987018585205

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6566614271081437, Training Loss Force: 3.5012296103454026, time: 2.1052474975585938
Validation Loss Energy: 3.143290739619651, Validation Loss Force: 3.6507960303268057, time: 0.1250147819519043
Test Loss Energy: 10.062691102288047, Test Loss Force: 11.868903444872895, time: 11.245476484298706

Epoch 2, Batch 100/126, Loss: 1.4416173696517944, Variance: 0.13123095035552979

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6707666109211456, Training Loss Force: 3.51300517477003, time: 2.3333253860473633
Validation Loss Energy: 3.206160111180535, Validation Loss Force: 3.6583728675783393, time: 0.128662109375
Test Loss Energy: 10.160837100650955, Test Loss Force: 11.830343392860742, time: 11.358599662780762

Epoch 3, Batch 100/126, Loss: 1.516897439956665, Variance: 0.13311165571212769

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6542371918885115, Training Loss Force: 3.5104555335244623, time: 2.260077476501465
Validation Loss Energy: 3.159141813887764, Validation Loss Force: 3.681610916539558, time: 0.1395416259765625
Test Loss Energy: 9.957969448552864, Test Loss Force: 11.581078107872102, time: 11.181916236877441

Epoch 4, Batch 100/126, Loss: 1.351720929145813, Variance: 0.13304206728935242

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6704897421611626, Training Loss Force: 3.509987917714721, time: 2.206468105316162
Validation Loss Energy: 3.0359213103603415, Validation Loss Force: 3.656794811341149, time: 0.14364242553710938
Test Loss Energy: 9.99185191429846, Test Loss Force: 11.713649861380524, time: 11.519060134887695

Epoch 5, Batch 100/126, Loss: 1.3521902561187744, Variance: 0.1343485713005066

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6496453361530277, Training Loss Force: 3.5047702317888456, time: 2.1682984828948975
Validation Loss Energy: 3.156561449601704, Validation Loss Force: 3.6870386996406936, time: 0.12790632247924805
Test Loss Energy: 10.20835915910132, Test Loss Force: 11.65701130840534, time: 11.31416392326355

Epoch 6, Batch 100/126, Loss: 1.3785309791564941, Variance: 0.13577306270599365

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.667557646334486, Training Loss Force: 3.517554291283611, time: 2.288252353668213
Validation Loss Energy: 2.975241147619171, Validation Loss Force: 3.595390825429616, time: 0.14531850814819336
Test Loss Energy: 10.139543280536865, Test Loss Force: 11.738459788190191, time: 11.549352884292603

Epoch 7, Batch 100/126, Loss: 1.6488127708435059, Variance: 0.1340794861316681

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6767741034240196, Training Loss Force: 3.5202009703292023, time: 2.2835581302642822
Validation Loss Energy: 3.3947752028297593, Validation Loss Force: 3.6058496694368927, time: 0.14023804664611816
Test Loss Energy: 10.17299386606715, Test Loss Force: 11.66425202405518, time: 11.373706102371216

Epoch 8, Batch 100/126, Loss: 1.6077543497085571, Variance: 0.13396410644054413

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6656923592695567, Training Loss Force: 3.5114128853796536, time: 2.1866495609283447
Validation Loss Energy: 3.035514271161586, Validation Loss Force: 3.6750111515003554, time: 0.1341078281402588
Test Loss Energy: 10.005868323820104, Test Loss Force: 11.621362915451169, time: 12.119823694229126

Epoch 9, Batch 100/126, Loss: 1.4450350999832153, Variance: 0.13429853320121765

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6770895513807518, Training Loss Force: 3.518855513221519, time: 2.277193546295166
Validation Loss Energy: 3.0088815568476495, Validation Loss Force: 3.57258623907247, time: 0.14259958267211914
Test Loss Energy: 10.151149248755752, Test Loss Force: 11.70049918017179, time: 11.474387884140015

Epoch 10, Batch 100/126, Loss: 1.3401861190795898, Variance: 0.13252690434455872

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.664869134648515, Training Loss Force: 3.5173459507205025, time: 2.0715816020965576
Validation Loss Energy: 3.271031129917212, Validation Loss Force: 3.638215594697129, time: 0.12929439544677734
Test Loss Energy: 9.913420673720967, Test Loss Force: 11.657705397278376, time: 11.589564561843872

Epoch 11, Batch 100/126, Loss: 1.5907586812973022, Variance: 0.13662151992321014

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6685313069374033, Training Loss Force: 3.52093016760173, time: 2.1032285690307617
Validation Loss Energy: 3.1215099117724328, Validation Loss Force: 3.6052253793747595, time: 0.1350717544555664
Test Loss Energy: 10.170534680040154, Test Loss Force: 11.785535109380243, time: 11.62735652923584

Epoch 12, Batch 100/126, Loss: 1.3824392557144165, Variance: 0.13184112310409546

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.654652120820616, Training Loss Force: 3.5066660280611965, time: 2.2707462310791016
Validation Loss Energy: 3.2788073302774, Validation Loss Force: 3.597427802776563, time: 0.1368093490600586
Test Loss Energy: 9.956414159361493, Test Loss Force: 11.507657151576167, time: 11.402217388153076

Epoch 13, Batch 100/126, Loss: 1.5847067832946777, Variance: 0.13521340489387512

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6873826438335087, Training Loss Force: 3.5026615620457706, time: 2.4841091632843018
Validation Loss Energy: 3.177391702864957, Validation Loss Force: 3.615718885545753, time: 0.13050532341003418
Test Loss Energy: 9.952600067721542, Test Loss Force: 11.570843720970158, time: 11.372754096984863

Epoch 14, Batch 100/126, Loss: 1.4948773384094238, Variance: 0.13473381102085114

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.7118229427153087, Training Loss Force: 3.5249085813270824, time: 2.2917516231536865
Validation Loss Energy: 3.594438321046385, Validation Loss Force: 3.761344358536429, time: 0.15052199363708496
Test Loss Energy: 10.174634967983623, Test Loss Force: 11.595410665611913, time: 11.402534246444702

Epoch 15, Batch 100/126, Loss: 1.4094390869140625, Variance: 0.1379488706588745

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.7158500308556315, Training Loss Force: 3.520057933678391, time: 2.3307061195373535
Validation Loss Energy: 3.15798170763748, Validation Loss Force: 3.627887160586317, time: 0.13132929801940918
Test Loss Energy: 9.963554624931767, Test Loss Force: 11.504753699799796, time: 11.561954498291016

Epoch 16, Batch 100/126, Loss: 1.4587438106536865, Variance: 0.13535109162330627

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6690683566419335, Training Loss Force: 3.5104820470577662, time: 2.312316656112671
Validation Loss Energy: 3.26671535653084, Validation Loss Force: 3.6948045087740713, time: 0.12858247756958008
Test Loss Energy: 10.093834266552681, Test Loss Force: 11.610161510256868, time: 11.474379539489746

Epoch 17, Batch 100/126, Loss: 1.286380410194397, Variance: 0.14024686813354492

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.697173506027626, Training Loss Force: 3.50928972446994, time: 2.2647180557250977
Validation Loss Energy: 3.0687558799497077, Validation Loss Force: 3.5444499884621803, time: 0.13625097274780273
Test Loss Energy: 9.925287167172208, Test Loss Force: 11.610333121717375, time: 11.615578889846802

Epoch 18, Batch 100/126, Loss: 1.3628268241882324, Variance: 0.13592037558555603

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6730293641787717, Training Loss Force: 3.5191998692884803, time: 2.158925771713257
Validation Loss Energy: 3.0607644327484564, Validation Loss Force: 3.553125104164479, time: 0.1434483528137207
Test Loss Energy: 9.994691810937868, Test Loss Force: 11.52322547877521, time: 11.602789402008057

Epoch 19, Batch 100/126, Loss: 1.2880699634552002, Variance: 0.13611355423927307

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.651858637420888, Training Loss Force: 3.5221250561863804, time: 2.1605255603790283
Validation Loss Energy: 2.810307364977659, Validation Loss Force: 3.608349872830418, time: 0.14056754112243652
Test Loss Energy: 10.042191952215754, Test Loss Force: 11.467550278795029, time: 11.657855987548828

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–ƒâ–…â–‚â–‚â–†â–…â–…â–ƒâ–…â–â–…â–‚â–‚â–…â–‚â–„â–â–‚â–ƒ
wandb:   test_error_force â–ˆâ–†â–†â–‚â–„â–ƒâ–…â–„â–ƒâ–„â–ƒâ–…â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–‚â–
wandb:          test_loss â–ˆâ–ƒâ–…â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–„â–â–‚â–ƒâ–‚â–„â–‚â–ƒâ–â–‚â–ƒ
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–„â–„â–„â–ƒâ–„â–‚â–†â–ƒâ–ƒâ–…â–„â–…â–„â–‡â–„â–…â–ƒâ–ƒâ–
wandb:  valid_error_force â–„â–„â–…â–…â–…â–†â–ƒâ–ƒâ–…â–‚â–„â–ƒâ–ƒâ–ƒâ–ˆâ–„â–†â–â–â–ƒ
wandb:         valid_loss â–ˆâ–„â–„â–„â–‚â–„â–‚â–„â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–‡â–ƒâ–„â–‚â–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 4021
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.04219
wandb:   test_error_force 11.46755
wandb:          test_loss 10.30146
wandb: train_error_energy 2.65186
wandb:  train_error_force 3.52213
wandb:         train_loss 1.0244
wandb: valid_error_energy 2.81031
wandb:  valid_error_force 3.60835
wandb:         valid_loss 1.09825
wandb: 
wandb: ğŸš€ View run al_63_35 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/dxu5jlbm
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_034741-dxu5jlbm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8070220947265625, Uncertainty Bias: -0.11363954842090607
4.5776367e-05 0.02119255
2.3477383 4.99641
(48745, 22, 3)
Found uncertainty sample 0 after 129 steps.
Found uncertainty sample 1 after 974 steps.
Found uncertainty sample 2 after 10 steps.
Found uncertainty sample 3 after 281 steps.
Found uncertainty sample 4 after 135 steps.
Found uncertainty sample 5 after 1133 steps.
Found uncertainty sample 6 after 466 steps.
Found uncertainty sample 7 after 729 steps.
Found uncertainty sample 8 after 764 steps.
Found uncertainty sample 9 after 449 steps.
Found uncertainty sample 10 after 16 steps.
Found uncertainty sample 11 after 421 steps.
Found uncertainty sample 12 after 702 steps.
Found uncertainty sample 13 after 286 steps.
Found uncertainty sample 14 after 667 steps.
Found uncertainty sample 15 after 185 steps.
Found uncertainty sample 16 after 4 steps.
Found uncertainty sample 17 after 1306 steps.
Found uncertainty sample 18 after 182 steps.
Found uncertainty sample 19 after 654 steps.
Found uncertainty sample 20 after 137 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 637 steps.
Found uncertainty sample 24 after 122 steps.
Found uncertainty sample 25 after 36 steps.
Found uncertainty sample 26 after 185 steps.
Found uncertainty sample 27 after 1050 steps.
Found uncertainty sample 28 after 64 steps.
Found uncertainty sample 29 after 1219 steps.
Found uncertainty sample 30 after 1281 steps.
Found uncertainty sample 31 after 147 steps.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 268 steps.
Found uncertainty sample 34 after 423 steps.
Found uncertainty sample 35 after 1089 steps.
Found uncertainty sample 36 after 39 steps.
Found uncertainty sample 37 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 860 steps.
Found uncertainty sample 40 after 3193 steps.
Found uncertainty sample 41 after 46 steps.
Found uncertainty sample 42 after 509 steps.
Found uncertainty sample 43 after 838 steps.
Found uncertainty sample 44 after 2290 steps.
Found uncertainty sample 45 after 1663 steps.
Found uncertainty sample 46 after 28 steps.
Found uncertainty sample 47 after 174 steps.
Found uncertainty sample 48 after 88 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 535 steps.
Found uncertainty sample 52 after 957 steps.
Found uncertainty sample 53 after 388 steps.
Found uncertainty sample 54 after 681 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 5 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 403 steps.
Found uncertainty sample 59 after 1767 steps.
Found uncertainty sample 60 after 153 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 54 steps.
Found uncertainty sample 65 after 372 steps.
Found uncertainty sample 66 after 184 steps.
Found uncertainty sample 67 after 303 steps.
Found uncertainty sample 68 after 169 steps.
Found uncertainty sample 69 after 127 steps.
Found uncertainty sample 70 after 566 steps.
Found uncertainty sample 71 after 1205 steps.
Found uncertainty sample 72 after 558 steps.
Found uncertainty sample 73 after 337 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 104 steps.
Found uncertainty sample 76 after 289 steps.
Found uncertainty sample 77 after 50 steps.
Found uncertainty sample 78 after 863 steps.
Found uncertainty sample 79 after 250 steps.
Found uncertainty sample 80 after 675 steps.
Found uncertainty sample 81 after 1334 steps.
Found uncertainty sample 82 after 219 steps.
Found uncertainty sample 83 after 855 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 177 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 50 steps.
Found uncertainty sample 88 after 621 steps.
Found uncertainty sample 89 after 965 steps.
Found uncertainty sample 90 after 43 steps.
Found uncertainty sample 91 after 2141 steps.
Found uncertainty sample 92 after 57 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 2045 steps.
Found uncertainty sample 95 after 1501 steps.
Found uncertainty sample 96 after 188 steps.
Found uncertainty sample 97 after 163 steps.
Found uncertainty sample 98 after 1345 steps.
Found uncertainty sample 99 after 797 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_040039-zxz1qqc7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_36
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zxz1qqc7
Training model 36. Added 98 samples to the dataset.
Epoch 0, Batch 100/129, Loss: 1.4729983806610107, Variance: 0.13353151082992554

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.0070121190457213, Training Loss Force: 3.6711353913336278, time: 2.147332191467285
Validation Loss Energy: 3.7457423025965118, Validation Loss Force: 3.5631521184747528, time: 0.13027143478393555
Test Loss Energy: 10.096500579186058, Test Loss Force: 11.531414270684513, time: 9.903525829315186

Epoch 1, Batch 100/129, Loss: 1.2056233882904053, Variance: 0.1314413845539093

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.713474738092345, Training Loss Force: 3.5075788259902656, time: 2.0770175457000732
Validation Loss Energy: 3.0941175987786877, Validation Loss Force: 3.687884693949122, time: 0.12017631530761719
Test Loss Energy: 10.053341712157884, Test Loss Force: 11.64061336026924, time: 9.833747625350952

Epoch 2, Batch 100/129, Loss: 1.4778298139572144, Variance: 0.13700397312641144

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.637831182853066, Training Loss Force: 3.5062100839286003, time: 2.083913564682007
Validation Loss Energy: 3.9401578512741064, Validation Loss Force: 3.7428476036994502, time: 0.12314629554748535
Test Loss Energy: 10.067815066746343, Test Loss Force: 11.416226318383197, time: 10.024235248565674

Epoch 3, Batch 100/129, Loss: 1.138582468032837, Variance: 0.12793070077896118

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6663941434961194, Training Loss Force: 3.5059431916546204, time: 2.0893404483795166
Validation Loss Energy: 3.454534480206421, Validation Loss Force: 3.686299512299262, time: 0.12833356857299805
Test Loss Energy: 10.124383237618503, Test Loss Force: 11.600728629689245, time: 9.861143350601196

Epoch 4, Batch 100/129, Loss: 1.1344350576400757, Variance: 0.12993890047073364

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6527013753677235, Training Loss Force: 3.503312504141581, time: 2.156437635421753
Validation Loss Energy: 3.7318242934072465, Validation Loss Force: 3.725750918229018, time: 0.12141847610473633
Test Loss Energy: 10.143309065796023, Test Loss Force: 11.418378859851074, time: 10.024994373321533

Epoch 5, Batch 100/129, Loss: 1.071797251701355, Variance: 0.132849782705307

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6715284469368044, Training Loss Force: 3.5219566348830504, time: 2.1000845432281494
Validation Loss Energy: 3.5773888414348893, Validation Loss Force: 3.602217044990226, time: 0.12232351303100586
Test Loss Energy: 10.185433829331139, Test Loss Force: 11.588835075524836, time: 9.931784391403198

Epoch 6, Batch 100/129, Loss: 1.4073457717895508, Variance: 0.13529238104820251

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6602426960274226, Training Loss Force: 3.5080066321075853, time: 2.159034252166748
Validation Loss Energy: 3.876588556364523, Validation Loss Force: 3.741023943394302, time: 0.12556028366088867
Test Loss Energy: 10.282471602188961, Test Loss Force: 11.512747396231443, time: 9.924705505371094

Epoch 7, Batch 100/129, Loss: 1.1291080713272095, Variance: 0.13299521803855896

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6854942911548023, Training Loss Force: 3.5169119031270197, time: 2.084737539291382
Validation Loss Energy: 2.941332347846801, Validation Loss Force: 3.6819473696190546, time: 0.12278223037719727
Test Loss Energy: 9.896236570597738, Test Loss Force: 11.5655725057554, time: 9.991605997085571

Epoch 8, Batch 100/129, Loss: 1.4731335639953613, Variance: 0.13627618551254272

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6737294934697178, Training Loss Force: 3.5251019211490333, time: 2.0399060249328613
Validation Loss Energy: 3.9640171471524632, Validation Loss Force: 3.666239842727016, time: 0.12088179588317871
Test Loss Energy: 10.18869641029987, Test Loss Force: 11.517417045683628, time: 10.594128131866455

Epoch 9, Batch 100/129, Loss: 1.301537036895752, Variance: 0.13109339773654938

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.680150569971548, Training Loss Force: 3.507953198417032, time: 2.1376357078552246
Validation Loss Energy: 2.977818394537188, Validation Loss Force: 3.6463913377281596, time: 0.12091994285583496
Test Loss Energy: 9.936811892629626, Test Loss Force: 11.619179218955775, time: 10.835264205932617

Epoch 10, Batch 100/129, Loss: 1.422541856765747, Variance: 0.13409271836280823

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6744863105113192, Training Loss Force: 3.516043666069642, time: 2.3148765563964844
Validation Loss Energy: 3.425819883601109, Validation Loss Force: 3.638299095626852, time: 0.14369750022888184
Test Loss Energy: 9.751295184632838, Test Loss Force: 11.462962175214152, time: 11.884621858596802

Epoch 11, Batch 100/129, Loss: 1.2410175800323486, Variance: 0.13089609146118164

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6885387807946115, Training Loss Force: 3.498180945227578, time: 2.25308895111084
Validation Loss Energy: 3.017746683554473, Validation Loss Force: 3.614464516023693, time: 0.13796019554138184
Test Loss Energy: 10.083629659025942, Test Loss Force: 11.52834210797557, time: 11.328332662582397

Epoch 12, Batch 100/129, Loss: 1.5501736402511597, Variance: 0.1365334838628769

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.697049637438028, Training Loss Force: 3.5090283054676807, time: 2.219970941543579
Validation Loss Energy: 3.914707064786755, Validation Loss Force: 3.7161947007162834, time: 0.13190460205078125
Test Loss Energy: 10.075949671944116, Test Loss Force: 11.322268271990442, time: 10.967563152313232

Epoch 13, Batch 100/129, Loss: 1.187052607536316, Variance: 0.1328916698694229

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.694928511004271, Training Loss Force: 3.504169650894597, time: 2.131967306137085
Validation Loss Energy: 2.9434785815213504, Validation Loss Force: 3.64859271912785, time: 0.13552594184875488
Test Loss Energy: 9.98491341314096, Test Loss Force: 11.69952691574772, time: 11.422295808792114

Epoch 14, Batch 100/129, Loss: 1.3840162754058838, Variance: 0.13408978283405304

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.66357545417922, Training Loss Force: 3.512543776599593, time: 2.3453125953674316
Validation Loss Energy: 3.9898080236486955, Validation Loss Force: 3.6562829691977097, time: 0.13325810432434082
Test Loss Energy: 10.11009724221214, Test Loss Force: 11.322516864021859, time: 11.448821306228638

Epoch 15, Batch 100/129, Loss: 1.3806381225585938, Variance: 0.13141727447509766

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.686964346814807, Training Loss Force: 3.5185736070139115, time: 2.2331924438476562
Validation Loss Energy: 2.924419958206148, Validation Loss Force: 3.6904505847406965, time: 0.13273072242736816
Test Loss Energy: 9.81814979802319, Test Loss Force: 11.545558951623898, time: 11.345136642456055

Epoch 16, Batch 100/129, Loss: 1.334774374961853, Variance: 0.13698440790176392

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.670840780747835, Training Loss Force: 3.5279666511956163, time: 2.1750729084014893
Validation Loss Energy: 4.029580444263831, Validation Loss Force: 3.6772932133539213, time: 0.13972067832946777
Test Loss Energy: 10.043168609400777, Test Loss Force: 11.487331287063302, time: 11.460476160049438

Epoch 17, Batch 100/129, Loss: 1.4146677255630493, Variance: 0.1286599189043045

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6456504817758124, Training Loss Force: 3.532101182253909, time: 2.2254247665405273
Validation Loss Energy: 3.2787676951694245, Validation Loss Force: 3.6240189521832598, time: 0.12848663330078125
Test Loss Energy: 9.99877220554989, Test Loss Force: 11.522708723143667, time: 11.277033805847168

Epoch 18, Batch 100/129, Loss: 1.632148265838623, Variance: 0.13556545972824097

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6631763887798408, Training Loss Force: 3.5201895411715545, time: 2.2190821170806885
Validation Loss Energy: 3.9640312495317054, Validation Loss Force: 3.645950304373125, time: 0.13517451286315918
Test Loss Energy: 10.076733122258897, Test Loss Force: 11.489328146658156, time: 11.483031749725342

Epoch 19, Batch 100/129, Loss: 1.166060209274292, Variance: 0.1270269900560379

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.669518668021891, Training Loss Force: 3.5095960254839262, time: 2.1977791786193848
Validation Loss Energy: 3.195725612333213, Validation Loss Force: 3.6853332024289838, time: 0.13720059394836426
Test Loss Energy: 10.047130035735291, Test Loss Force: 11.57472941181569, time: 11.31868290901184

wandb: - 0.039 MB of 0.059 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–…â–…â–†â–†â–‡â–ˆâ–ƒâ–‡â–ƒâ–â–…â–…â–„â–†â–‚â–…â–„â–…â–…
wandb:   test_error_force â–…â–‡â–ƒâ–†â–ƒâ–†â–…â–†â–…â–‡â–„â–…â–â–ˆâ–â–…â–„â–…â–„â–†
wandb:          test_loss â–†â–ˆâ–…â–ˆâ–†â–ˆâ–†â–†â–„â–†â–â–ˆâ–ƒâ–…â–„â–…â–ƒâ–‡â–†â–ˆ
wandb: train_error_energy â–ˆâ–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–‚â–‡â–„â–†â–…â–‡â–â–ˆâ–â–„â–‚â–‡â–â–ˆâ–â–ˆâ–ƒâ–ˆâ–ƒ
wandb:  valid_error_force â–â–†â–ˆâ–†â–‡â–ƒâ–ˆâ–†â–…â–„â–„â–ƒâ–‡â–„â–…â–†â–…â–ƒâ–„â–†
wandb:         valid_loss â–…â–‚â–ˆâ–…â–†â–…â–‡â–â–‡â–‚â–„â–â–‡â–â–‡â–â–ˆâ–ƒâ–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 4109
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.04713
wandb:   test_error_force 11.57473
wandb:          test_loss 10.12682
wandb: train_error_energy 2.66952
wandb:  train_error_force 3.5096
wandb:         train_loss 1.02583
wandb: valid_error_energy 3.19573
wandb:  valid_error_force 3.68533
wandb:         valid_loss 1.28797
wandb: 
wandb: ğŸš€ View run al_63_36 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zxz1qqc7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_040039-zxz1qqc7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.9719393253326416, Uncertainty Bias: -0.13828794658184052
3.0517578e-05 0.03283453
2.31876 4.9371905
(48745, 22, 3)
Found uncertainty sample 0 after 2978 steps.
Found uncertainty sample 1 after 776 steps.
Found uncertainty sample 2 after 61 steps.
Found uncertainty sample 3 after 1688 steps.
Found uncertainty sample 4 after 32 steps.
Found uncertainty sample 5 after 498 steps.
Found uncertainty sample 6 after 112 steps.
Found uncertainty sample 7 after 626 steps.
Found uncertainty sample 8 after 22 steps.
Found uncertainty sample 9 after 141 steps.
Found uncertainty sample 10 after 999 steps.
Found uncertainty sample 11 after 776 steps.
Found uncertainty sample 12 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 922 steps.
Found uncertainty sample 15 after 812 steps.
Found uncertainty sample 16 after 828 steps.
Found uncertainty sample 17 after 394 steps.
Found uncertainty sample 18 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 174 steps.
Found uncertainty sample 21 after 57 steps.
Found uncertainty sample 22 after 1062 steps.
Found uncertainty sample 23 after 1148 steps.
Found uncertainty sample 24 after 1703 steps.
Found uncertainty sample 25 after 1446 steps.
Found uncertainty sample 26 after 189 steps.
Found uncertainty sample 27 after 1085 steps.
Found uncertainty sample 28 after 1258 steps.
Found uncertainty sample 29 after 284 steps.
Found uncertainty sample 30 after 89 steps.
Found uncertainty sample 31 after 1724 steps.
Found uncertainty sample 32 after 1557 steps.
Found uncertainty sample 33 after 573 steps.
Found uncertainty sample 34 after 1037 steps.
Found uncertainty sample 35 after 93 steps.
Found uncertainty sample 36 after 322 steps.
Found uncertainty sample 37 after 1831 steps.
Found uncertainty sample 38 after 27 steps.
Found uncertainty sample 39 after 189 steps.
Found uncertainty sample 40 after 237 steps.
Found uncertainty sample 41 after 75 steps.
Found uncertainty sample 42 after 696 steps.
Found uncertainty sample 43 after 245 steps.
Found uncertainty sample 44 after 401 steps.
Found uncertainty sample 45 after 2276 steps.
Found uncertainty sample 46 after 835 steps.
Found uncertainty sample 47 after 1787 steps.
Found uncertainty sample 48 after 1216 steps.
Found uncertainty sample 49 after 737 steps.
Found uncertainty sample 50 after 98 steps.
Found uncertainty sample 51 after 136 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 48 steps.
Found uncertainty sample 54 after 508 steps.
Found uncertainty sample 55 after 1568 steps.
Found uncertainty sample 56 after 724 steps.
Found uncertainty sample 57 after 574 steps.
Found uncertainty sample 58 after 286 steps.
Found uncertainty sample 59 after 417 steps.
Found uncertainty sample 60 after 394 steps.
Found uncertainty sample 61 after 902 steps.
Found uncertainty sample 62 after 17 steps.
Found uncertainty sample 63 after 1224 steps.
Found uncertainty sample 64 after 2643 steps.
Found uncertainty sample 65 after 100 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 94 steps.
Found uncertainty sample 68 after 173 steps.
Found uncertainty sample 69 after 723 steps.
Found uncertainty sample 70 after 2396 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 97 steps.
Found uncertainty sample 73 after 67 steps.
Found uncertainty sample 74 after 87 steps.
Found uncertainty sample 75 after 8 steps.
Found uncertainty sample 76 after 897 steps.
Found uncertainty sample 77 after 404 steps.
Found uncertainty sample 78 after 41 steps.
Found uncertainty sample 79 after 280 steps.
Found uncertainty sample 80 after 237 steps.
Found uncertainty sample 81 after 9 steps.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 306 steps.
Found uncertainty sample 84 after 2758 steps.
Found uncertainty sample 85 after 78 steps.
Found uncertainty sample 86 after 373 steps.
Found uncertainty sample 87 after 24 steps.
Found uncertainty sample 88 after 181 steps.
Found uncertainty sample 89 after 3649 steps.
Found uncertainty sample 90 after 2510 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 178 steps.
Found uncertainty sample 93 after 1344 steps.
Found uncertainty sample 94 after 532 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 242 steps.
Found uncertainty sample 97 after 208 steps.
Found uncertainty sample 98 after 1795 steps.
Found uncertainty sample 99 after 560 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_041434-x88w5htz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_37
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/x88w5htz
Training model 37. Added 99 samples to the dataset.
Epoch 0, Batch 100/132, Loss: 0.7959308624267578, Variance: 0.11303497850894928

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9407178830607323, Training Loss Force: 3.790748109400838, time: 2.2956111431121826
Validation Loss Energy: 1.7445713789945996, Validation Loss Force: 3.7153104073147554, time: 0.14605998992919922
Test Loss Energy: 9.599062552277465, Test Loss Force: 11.554539377813455, time: 11.112155437469482

Epoch 1, Batch 100/132, Loss: 1.0573688745498657, Variance: 0.13255995512008667

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.604673829072596, Training Loss Force: 3.487350510902501, time: 2.2914681434631348
Validation Loss Energy: 1.8917263663486532, Validation Loss Force: 3.6105335891445325, time: 0.13199353218078613
Test Loss Energy: 9.54922929068339, Test Loss Force: 11.459913855590926, time: 11.19808578491211

Epoch 2, Batch 100/132, Loss: 0.8293477296829224, Variance: 0.12800800800323486

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.601126777896909, Training Loss Force: 3.4958018997804277, time: 2.350337505340576
Validation Loss Energy: 1.8593558763709257, Validation Loss Force: 3.6792396385815245, time: 0.1462099552154541
Test Loss Energy: 9.585473443681046, Test Loss Force: 11.522540361953206, time: 11.118506669998169

Epoch 3, Batch 100/132, Loss: 0.9340754151344299, Variance: 0.13347271084785461

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6606366666735752, Training Loss Force: 3.4995493675259937, time: 2.182755470275879
Validation Loss Energy: 1.631151991928908, Validation Loss Force: 3.6600463004330357, time: 0.13765525817871094
Test Loss Energy: 9.596393157250258, Test Loss Force: 11.652882784615203, time: 11.122972249984741

Epoch 4, Batch 100/132, Loss: 0.7889871597290039, Variance: 0.1359747052192688

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.621796305365544, Training Loss Force: 3.4988500463961207, time: 2.2397894859313965
Validation Loss Energy: 1.6770407999789605, Validation Loss Force: 3.609309205734124, time: 0.14011001586914062
Test Loss Energy: 9.529121082346151, Test Loss Force: 11.516454378183166, time: 11.3087739944458

Epoch 5, Batch 100/132, Loss: 0.87790846824646, Variance: 0.13349264860153198

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6891617988259027, Training Loss Force: 3.4972008743570058, time: 2.202493906021118
Validation Loss Energy: 1.7443118209934962, Validation Loss Force: 3.6479276806120127, time: 0.13257431983947754
Test Loss Energy: 9.44508120935198, Test Loss Force: 11.449158828588367, time: 11.146714210510254

Epoch 6, Batch 100/132, Loss: 0.7469086647033691, Variance: 0.1348663866519928

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6860678675539664, Training Loss Force: 3.508136506986113, time: 2.341829776763916
Validation Loss Energy: 1.6004073949432207, Validation Loss Force: 3.6881215396973404, time: 0.1323997974395752
Test Loss Energy: 9.512535434612296, Test Loss Force: 11.410284022663282, time: 10.458362102508545

Epoch 7, Batch 100/132, Loss: 0.8224397301673889, Variance: 0.13593940436840057

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.679985285343931, Training Loss Force: 3.4878198890276795, time: 2.2029812335968018
Validation Loss Energy: 1.5627990586458333, Validation Loss Force: 3.6728606128889845, time: 0.1330726146697998
Test Loss Energy: 9.581972581078418, Test Loss Force: 11.49039425854949, time: 11.759259700775146

Epoch 8, Batch 100/132, Loss: 0.8132667541503906, Variance: 0.135908305644989

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6674282021409326, Training Loss Force: 3.5074614399242714, time: 2.134967088699341
Validation Loss Energy: 1.7211059848353887, Validation Loss Force: 3.701941054901057, time: 0.12674641609191895
Test Loss Energy: 9.414614469652005, Test Loss Force: 11.545652183545043, time: 10.059423685073853

Epoch 9, Batch 100/132, Loss: 0.7964070439338684, Variance: 0.13710248470306396

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.668935987706821, Training Loss Force: 3.5072325394160324, time: 2.1498522758483887
Validation Loss Energy: 1.767274984894067, Validation Loss Force: 3.6501307691418226, time: 0.12315487861633301
Test Loss Energy: 9.482439855233407, Test Loss Force: 11.497834695599348, time: 9.975505352020264

Epoch 10, Batch 100/132, Loss: 0.8207429051399231, Variance: 0.13502460718154907

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.656543481462532, Training Loss Force: 3.512354002308533, time: 2.1536471843719482
Validation Loss Energy: 1.572773931264436, Validation Loss Force: 3.7120155719624623, time: 0.12598490715026855
Test Loss Energy: 9.619210258157446, Test Loss Force: 11.692642797157337, time: 9.95959186553955

Epoch 11, Batch 100/132, Loss: 0.646328330039978, Variance: 0.135934516787529

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6830064206740234, Training Loss Force: 3.5116874590890723, time: 2.1077260971069336
Validation Loss Energy: 1.8182935333934307, Validation Loss Force: 3.6228919494126703, time: 0.1240537166595459
Test Loss Energy: 9.704782020263952, Test Loss Force: 11.657039194509897, time: 10.153590202331543

Epoch 12, Batch 100/132, Loss: 0.7753614187240601, Variance: 0.13678431510925293

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.642567584199432, Training Loss Force: 3.5114379515635106, time: 2.2014331817626953
Validation Loss Energy: 1.5281995391827952, Validation Loss Force: 3.573180246130757, time: 0.13230180740356445
Test Loss Energy: 9.500989342243326, Test Loss Force: 11.533231754543932, time: 9.999992609024048

Epoch 13, Batch 100/132, Loss: 0.8176969885826111, Variance: 0.13524021208286285

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.7025431332012957, Training Loss Force: 3.521045846873862, time: 2.113395929336548
Validation Loss Energy: 1.4888008687291003, Validation Loss Force: 3.6217981243448243, time: 0.12203860282897949
Test Loss Energy: 9.405511302633343, Test Loss Force: 11.596093469472732, time: 10.861546516418457

Epoch 14, Batch 100/132, Loss: 0.8323656320571899, Variance: 0.1348056197166443

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6591228369898854, Training Loss Force: 3.512539344008543, time: 2.074920654296875
Validation Loss Energy: 1.6193677840535445, Validation Loss Force: 3.6150790086434053, time: 0.12080955505371094
Test Loss Energy: 9.593246942477704, Test Loss Force: 11.613022680543262, time: 9.97058629989624

Epoch 15, Batch 100/132, Loss: 0.9585961103439331, Variance: 0.1352056860923767

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6729911083021225, Training Loss Force: 3.5034163900471604, time: 2.2066357135772705
Validation Loss Energy: 1.7960789460499453, Validation Loss Force: 3.6067391584518256, time: 0.12699580192565918
Test Loss Energy: 9.456386077590064, Test Loss Force: 11.427026443465554, time: 9.94903564453125

Epoch 16, Batch 100/132, Loss: 0.9444372653961182, Variance: 0.13590706884860992

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.7042053246378965, Training Loss Force: 3.520586293181304, time: 2.1600496768951416
Validation Loss Energy: 1.9393531070826722, Validation Loss Force: 3.814580914398379, time: 0.12436556816101074
Test Loss Energy: 9.669643112968576, Test Loss Force: 11.726833539591912, time: 10.234493970870972

Epoch 17, Batch 100/132, Loss: 0.8543857336044312, Variance: 0.13796263933181763

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.661423769210124, Training Loss Force: 3.5191773946457454, time: 2.1099305152893066
Validation Loss Energy: 1.7089681041332436, Validation Loss Force: 3.58665217479768, time: 0.1250171661376953
Test Loss Energy: 9.50732213669162, Test Loss Force: 11.422642416739823, time: 10.508991241455078

Epoch 18, Batch 100/132, Loss: 0.7975194454193115, Variance: 0.12838128209114075

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6739972219691226, Training Loss Force: 3.5066326991008774, time: 2.307795763015747
Validation Loss Energy: 1.7072021522484144, Validation Loss Force: 3.618220424954154, time: 0.14081907272338867
Test Loss Energy: 9.505935619054787, Test Loss Force: 11.50752714543754, time: 11.935006618499756

Epoch 19, Batch 100/132, Loss: 0.8112508654594421, Variance: 0.13596214354038239

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.676029634598579, Training Loss Force: 3.5056036538856215, time: 2.262244701385498
Validation Loss Energy: 1.7966293841786787, Validation Loss Force: 3.7038607598311493, time: 0.1362471580505371
Test Loss Energy: 9.4806039096536, Test Loss Force: 11.605358394550743, time: 11.040544271469116

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–…â–…â–„â–‚â–„â–…â–â–ƒâ–†â–ˆâ–ƒâ–â–…â–‚â–‡â–ƒâ–ƒâ–ƒ
wandb:   test_error_force â–„â–‚â–ƒâ–†â–ƒâ–‚â–â–ƒâ–„â–ƒâ–‡â–†â–„â–…â–…â–â–ˆâ–â–ƒâ–…
wandb:          test_loss â–ˆâ–„â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–ƒâ–‚â–â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚
wandb: train_error_energy â–ˆâ–â–â–‚â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒ
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–‚â–â–â–â–â–â–‚â–â–‚â–â–â–‚â–â–â–
wandb: valid_error_energy â–…â–‡â–‡â–ƒâ–„â–…â–ƒâ–‚â–…â–…â–‚â–†â–‚â–â–ƒâ–†â–ˆâ–„â–„â–†
wandb:  valid_error_force â–…â–‚â–„â–„â–‚â–ƒâ–„â–„â–…â–ƒâ–…â–‚â–â–‚â–‚â–‚â–ˆâ–â–‚â–…
wandb:         valid_loss â–„â–„â–…â–ƒâ–‚â–„â–ƒâ–ƒâ–†â–…â–ƒâ–„â–â–‚â–ƒâ–„â–ˆâ–‚â–ƒâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 4198
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.4806
wandb:   test_error_force 11.60536
wandb:          test_loss 9.36644
wandb: train_error_energy 2.67603
wandb:  train_error_force 3.5056
wandb:         train_loss 1.02036
wandb: valid_error_energy 1.79663
wandb:  valid_error_force 3.70386
wandb:         valid_loss 0.81818
wandb: 
wandb: ğŸš€ View run al_63_37 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/x88w5htz
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_041434-x88w5htz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.945467472076416, Uncertainty Bias: -0.14096130430698395
2.4795532e-05 0.009602547
2.4135873 4.998014
(48745, 22, 3)
Found uncertainty sample 0 after 983 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 815 steps.
Found uncertainty sample 3 after 421 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 256 steps.
Found uncertainty sample 10 after 15 steps.
Found uncertainty sample 11 after 2446 steps.
Found uncertainty sample 12 after 51 steps.
Found uncertainty sample 13 after 44 steps.
Found uncertainty sample 14 after 766 steps.
Found uncertainty sample 15 after 618 steps.
Found uncertainty sample 16 after 435 steps.
Found uncertainty sample 17 after 138 steps.
Found uncertainty sample 18 after 1736 steps.
Found uncertainty sample 19 after 1696 steps.
Found uncertainty sample 20 after 17 steps.
Found uncertainty sample 21 after 667 steps.
Found uncertainty sample 22 after 256 steps.
Found uncertainty sample 23 after 15 steps.
Found uncertainty sample 24 after 3009 steps.
Found uncertainty sample 25 after 581 steps.
Found uncertainty sample 26 after 212 steps.
Found uncertainty sample 27 after 1879 steps.
Found uncertainty sample 28 after 430 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 265 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1899 steps.
Found uncertainty sample 34 after 696 steps.
Found uncertainty sample 35 after 328 steps.
Found uncertainty sample 36 after 245 steps.
Found uncertainty sample 37 after 33 steps.
Found uncertainty sample 38 after 967 steps.
Found uncertainty sample 39 after 859 steps.
Found uncertainty sample 40 after 387 steps.
Found uncertainty sample 41 after 170 steps.
Found uncertainty sample 42 after 671 steps.
Found uncertainty sample 43 after 45 steps.
Found uncertainty sample 44 after 19 steps.
Found uncertainty sample 45 after 952 steps.
Found uncertainty sample 46 after 2292 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 370 steps.
Found uncertainty sample 49 after 2824 steps.
Found uncertainty sample 50 after 624 steps.
Found uncertainty sample 51 after 625 steps.
Found uncertainty sample 52 after 476 steps.
Found uncertainty sample 53 after 22 steps.
Found uncertainty sample 54 after 245 steps.
Found uncertainty sample 55 after 53 steps.
Found uncertainty sample 56 after 1400 steps.
Found uncertainty sample 57 after 244 steps.
Found uncertainty sample 58 after 1555 steps.
Found uncertainty sample 59 after 622 steps.
Found uncertainty sample 60 after 110 steps.
Found uncertainty sample 61 after 864 steps.
Found uncertainty sample 62 after 510 steps.
Found uncertainty sample 63 after 25 steps.
Found uncertainty sample 64 after 1161 steps.
Found uncertainty sample 65 after 15 steps.
Found uncertainty sample 66 after 933 steps.
Found uncertainty sample 67 after 1059 steps.
Found uncertainty sample 68 after 3 steps.
Found uncertainty sample 69 after 164 steps.
Found uncertainty sample 70 after 36 steps.
Found uncertainty sample 71 after 1161 steps.
Found uncertainty sample 72 after 190 steps.
Found uncertainty sample 73 after 150 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 165 steps.
Found uncertainty sample 76 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 407 steps.
Found uncertainty sample 79 after 662 steps.
Found uncertainty sample 80 after 1955 steps.
Found uncertainty sample 81 after 732 steps.
Found uncertainty sample 82 after 446 steps.
Found uncertainty sample 83 after 204 steps.
Found uncertainty sample 84 after 698 steps.
Found uncertainty sample 85 after 12 steps.
Found uncertainty sample 86 after 14 steps.
Found uncertainty sample 87 after 666 steps.
Found uncertainty sample 88 after 908 steps.
Found uncertainty sample 89 after 23 steps.
Found uncertainty sample 90 after 238 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 858 steps.
Found uncertainty sample 93 after 14 steps.
Found uncertainty sample 94 after 1588 steps.
Found uncertainty sample 95 after 409 steps.
Found uncertainty sample 96 after 1028 steps.
Found uncertainty sample 97 after 548 steps.
Found uncertainty sample 98 after 53 steps.
Found uncertainty sample 99 after 1712 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_042709-cymtopmd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_38
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/cymtopmd
Training model 38. Added 100 samples to the dataset.
Epoch 0, Batch 100/134, Loss: 1.4841399192810059, Variance: 0.12220974266529083

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.5737008856441213, Training Loss Force: 3.8107732930513416, time: 2.285865545272827
Validation Loss Energy: 2.3738517164683857, Validation Loss Force: 3.5900682906114554, time: 0.1417992115020752
Test Loss Energy: 9.633751490695397, Test Loss Force: 11.563994007677339, time: 12.217867851257324

Epoch 1, Batch 100/134, Loss: 1.0379213094711304, Variance: 0.12865319848060608

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.612178883622574, Training Loss Force: 3.4841420714809566, time: 2.225088596343994
Validation Loss Energy: 1.5323735465462605, Validation Loss Force: 3.6094213567606226, time: 0.14945197105407715
Test Loss Energy: 9.376214133205256, Test Loss Force: 11.545360741109812, time: 11.75549578666687

Epoch 2, Batch 100/134, Loss: 0.9572338461875916, Variance: 0.13288867473602295

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6604290824558965, Training Loss Force: 3.4741981077769832, time: 2.1896350383758545
Validation Loss Energy: 3.9844633703620396, Validation Loss Force: 3.6348984924302425, time: 0.14778637886047363
Test Loss Energy: 10.049633881498632, Test Loss Force: 11.375398934878119, time: 11.634047269821167

Epoch 3, Batch 100/134, Loss: 1.4348891973495483, Variance: 0.1306159496307373

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6524274268159407, Training Loss Force: 3.4931471291288947, time: 2.18086838722229
Validation Loss Energy: 2.0678988916810543, Validation Loss Force: 3.6402504195756653, time: 0.14387011528015137
Test Loss Energy: 9.536991538983273, Test Loss Force: 11.596181264069807, time: 11.551668405532837

Epoch 4, Batch 100/134, Loss: 1.1706242561340332, Variance: 0.13289089500904083

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.677948248842623, Training Loss Force: 3.4966738129134005, time: 2.4199206829071045
Validation Loss Energy: 1.5513679298128629, Validation Loss Force: 3.658539869437785, time: 0.1350722312927246
Test Loss Energy: 9.419137306691175, Test Loss Force: 11.620045259704971, time: 11.462422132492065

Epoch 5, Batch 100/134, Loss: 0.9176865220069885, Variance: 0.13804903626441956

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.675614033942368, Training Loss Force: 3.5091617915501194, time: 2.397218704223633
Validation Loss Energy: 3.8619109484954492, Validation Loss Force: 3.63164952692201, time: 0.13749957084655762
Test Loss Energy: 10.10853777010508, Test Loss Force: 11.352003533053779, time: 11.683924674987793

Epoch 6, Batch 100/134, Loss: 1.21717369556427, Variance: 0.130540132522583

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6690969965838147, Training Loss Force: 3.4994548043155547, time: 2.4790263175964355
Validation Loss Energy: 1.987720519619096, Validation Loss Force: 3.6566727144261315, time: 0.15433406829833984
Test Loss Energy: 9.576330256838084, Test Loss Force: 11.557093371932718, time: 11.925028085708618

Epoch 7, Batch 100/134, Loss: 1.042726755142212, Variance: 0.1331178992986679

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6369974337503614, Training Loss Force: 3.5012482264830282, time: 2.3889636993408203
Validation Loss Energy: 1.78690635036412, Validation Loss Force: 3.6426039582250738, time: 0.14675450325012207
Test Loss Energy: 9.639928132284622, Test Loss Force: 11.563922669612536, time: 11.768956422805786

Epoch 8, Batch 100/134, Loss: 0.7937924861907959, Variance: 0.13813039660453796

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6844995772496496, Training Loss Force: 3.4866317767443262, time: 2.404262065887451
Validation Loss Energy: 4.2241914964515495, Validation Loss Force: 3.655652444399491, time: 0.14346075057983398
Test Loss Energy: 10.129636643964815, Test Loss Force: 11.50724311753513, time: 11.951026439666748

Epoch 9, Batch 100/134, Loss: 1.3961372375488281, Variance: 0.13257768750190735

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7010499907753815, Training Loss Force: 3.5031877528444313, time: 2.3989291191101074
Validation Loss Energy: 2.2173205767176047, Validation Loss Force: 3.6003074343947503, time: 0.14869427680969238
Test Loss Energy: 9.781545322046755, Test Loss Force: 11.681666574588956, time: 11.839084386825562

Epoch 10, Batch 100/134, Loss: 1.0214002132415771, Variance: 0.1394672989845276

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6975196828012695, Training Loss Force: 3.4986416847247384, time: 2.450373888015747
Validation Loss Energy: 1.805416391126805, Validation Loss Force: 3.566963620468647, time: 0.1465456485748291
Test Loss Energy: 9.442121812827052, Test Loss Force: 11.470374264499863, time: 11.93733263015747

Epoch 11, Batch 100/134, Loss: 0.7587382793426514, Variance: 0.13709744811058044

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6648801290101303, Training Loss Force: 3.5098725348468203, time: 2.3681881427764893
Validation Loss Energy: 3.915338257592411, Validation Loss Force: 3.667172158441016, time: 0.14441585540771484
Test Loss Energy: 10.204359177196157, Test Loss Force: 11.449232418992386, time: 11.878149509429932

Epoch 12, Batch 100/134, Loss: 1.288406252861023, Variance: 0.1329454779624939

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.678941162136806, Training Loss Force: 3.5036550805453013, time: 2.3640289306640625
Validation Loss Energy: 2.1086693896312867, Validation Loss Force: 3.628426762149949, time: 0.1502227783203125
Test Loss Energy: 9.67875952355529, Test Loss Force: 11.518221499178162, time: 12.00512170791626

Epoch 13, Batch 100/134, Loss: 1.0818586349487305, Variance: 0.136823371052742

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.709357813167568, Training Loss Force: 3.5049716700304514, time: 2.4156880378723145
Validation Loss Energy: 1.8986705263335573, Validation Loss Force: 3.634476460175661, time: 0.15430569648742676
Test Loss Energy: 9.67359255946988, Test Loss Force: 11.640757202205894, time: 11.907421827316284

Epoch 14, Batch 100/134, Loss: 0.8490113019943237, Variance: 0.13471217453479767

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.652796853207845, Training Loss Force: 3.4860133447640025, time: 2.256479263305664
Validation Loss Energy: 4.134934272414202, Validation Loss Force: 3.6377985353868065, time: 0.16861557960510254
Test Loss Energy: 10.248128344213299, Test Loss Force: 11.511024412691238, time: 12.129875898361206

Epoch 15, Batch 100/134, Loss: 1.782764196395874, Variance: 0.13197806477546692

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.706856238309353, Training Loss Force: 3.500620419134635, time: 2.3057913780212402
Validation Loss Energy: 2.2153400088876256, Validation Loss Force: 3.596226280065137, time: 0.14189410209655762
Test Loss Energy: 9.569915338278188, Test Loss Force: 11.570728562550078, time: 11.844194412231445

Epoch 16, Batch 100/134, Loss: 0.967032790184021, Variance: 0.13633286952972412

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6916573234733794, Training Loss Force: 3.4908170477323743, time: 2.3461451530456543
Validation Loss Energy: 1.8757110364738947, Validation Loss Force: 3.6059181030877405, time: 0.14721345901489258
Test Loss Energy: 9.532858393934001, Test Loss Force: 11.507832572894104, time: 11.911597967147827

Epoch 17, Batch 100/134, Loss: 0.9381265640258789, Variance: 0.13504938781261444

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.670661455047713, Training Loss Force: 3.499418555772649, time: 2.3665897846221924
Validation Loss Energy: 3.838023076743365, Validation Loss Force: 3.588740737163434, time: 0.154449462890625
Test Loss Energy: 10.268846843213051, Test Loss Force: 11.415479726224005, time: 12.551017761230469

Epoch 18, Batch 100/134, Loss: 1.1282461881637573, Variance: 0.13176962733268738

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.7035664587098687, Training Loss Force: 3.489375625603572, time: 2.3942973613739014
Validation Loss Energy: 2.1245196474458643, Validation Loss Force: 3.625145286946411, time: 0.14132022857666016
Test Loss Energy: 9.697414665326239, Test Loss Force: 11.558800966819907, time: 11.906399011611938

Epoch 19, Batch 100/134, Loss: 1.007856011390686, Variance: 0.13866540789604187

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6753066117681996, Training Loss Force: 3.498586088891788, time: 2.3954310417175293
Validation Loss Energy: 1.9152174787942189, Validation Loss Force: 3.631592746289799, time: 0.13698530197143555
Test Loss Energy: 9.516619200738296, Test Loss Force: 11.618728583236699, time: 11.847989559173584

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–â–†â–‚â–â–‡â–ƒâ–ƒâ–‡â–„â–‚â–‡â–ƒâ–ƒâ–ˆâ–ƒâ–‚â–ˆâ–„â–‚
wandb:   test_error_force â–†â–…â–â–†â–‡â–â–…â–…â–„â–ˆâ–„â–ƒâ–…â–‡â–„â–†â–„â–‚â–…â–‡
wandb:          test_loss â–ˆâ–ƒâ–„â–ƒâ–â–„â–…â–†â–„â–„â–‚â–‡â–…â–„â–†â–ƒâ–ƒâ–‡â–„â–ƒ
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–‚â–
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–â–‡â–‚â–â–‡â–‚â–‚â–ˆâ–ƒâ–‚â–‡â–‚â–‚â–ˆâ–ƒâ–‚â–‡â–ƒâ–‚
wandb:  valid_error_force â–ƒâ–„â–†â–†â–‡â–†â–‡â–†â–‡â–ƒâ–â–ˆâ–…â–†â–†â–ƒâ–„â–ƒâ–…â–†
wandb:         valid_loss â–ƒâ–â–‡â–‚â–â–‡â–‚â–‚â–ˆâ–‚â–‚â–‡â–‚â–‚â–ˆâ–‚â–‚â–‡â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 4288
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.51662
wandb:   test_error_force 11.61873
wandb:          test_loss 9.59184
wandb: train_error_energy 2.67531
wandb:  train_error_force 3.49859
wandb:         train_loss 1.01758
wandb: valid_error_energy 1.91522
wandb:  valid_error_force 3.63159
wandb:         valid_loss 0.82819
wandb: 
wandb: ğŸš€ View run al_63_38 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/cymtopmd
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_042709-cymtopmd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.058694362640381, Uncertainty Bias: -0.16510207951068878
0.00015258789 0.0028038025
2.2318692 4.9913383
(48745, 22, 3)
Found uncertainty sample 0 after 19 steps.
Found uncertainty sample 1 after 734 steps.
Found uncertainty sample 2 after 49 steps.
Found uncertainty sample 3 after 19 steps.
Found uncertainty sample 4 after 1015 steps.
Found uncertainty sample 5 after 75 steps.
Found uncertainty sample 6 after 587 steps.
Found uncertainty sample 7 after 802 steps.
Found uncertainty sample 8 after 702 steps.
Found uncertainty sample 9 after 589 steps.
Found uncertainty sample 10 after 513 steps.
Found uncertainty sample 11 after 1322 steps.
Found uncertainty sample 12 after 2941 steps.
Found uncertainty sample 13 after 3799 steps.
Found uncertainty sample 14 after 890 steps.
Found uncertainty sample 15 after 1713 steps.
Found uncertainty sample 16 after 817 steps.
Found uncertainty sample 17 after 118 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 137 steps.
Found uncertainty sample 20 after 1008 steps.
Found uncertainty sample 21 after 42 steps.
Found uncertainty sample 22 after 4 steps.
Found uncertainty sample 23 after 1147 steps.
Found uncertainty sample 24 after 440 steps.
Found uncertainty sample 25 after 3268 steps.
Found uncertainty sample 26 after 4 steps.
Found uncertainty sample 27 after 517 steps.
Found uncertainty sample 28 after 83 steps.
Found uncertainty sample 29 after 661 steps.
Found uncertainty sample 30 after 741 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1402 steps.
Found uncertainty sample 33 after 2075 steps.
Found uncertainty sample 34 after 1259 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 684 steps.
Found uncertainty sample 37 after 9 steps.
Found uncertainty sample 38 after 91 steps.
Found uncertainty sample 39 after 1213 steps.
Found uncertainty sample 40 after 1832 steps.
Found uncertainty sample 41 after 279 steps.
Found uncertainty sample 42 after 71 steps.
Found uncertainty sample 43 after 466 steps.
Found uncertainty sample 44 after 699 steps.
Found uncertainty sample 45 after 161 steps.
Found uncertainty sample 46 after 3174 steps.
Found uncertainty sample 47 after 316 steps.
Found uncertainty sample 48 after 2085 steps.
Found uncertainty sample 49 after 16 steps.
Found uncertainty sample 50 after 1710 steps.
Found uncertainty sample 51 after 325 steps.
Found uncertainty sample 52 after 71 steps.
Found uncertainty sample 53 after 661 steps.
Found uncertainty sample 54 after 1437 steps.
Found uncertainty sample 55 after 771 steps.
Found uncertainty sample 56 after 1350 steps.
Found uncertainty sample 57 after 1137 steps.
Found uncertainty sample 58 after 220 steps.
Found uncertainty sample 59 after 381 steps.
Found uncertainty sample 60 after 1464 steps.
Found uncertainty sample 61 after 766 steps.
Found uncertainty sample 62 after 747 steps.
Found uncertainty sample 63 after 39 steps.
Found uncertainty sample 64 after 1026 steps.
Found uncertainty sample 65 after 19 steps.
Found uncertainty sample 66 after 40 steps.
Found uncertainty sample 67 after 2977 steps.
Found uncertainty sample 68 after 731 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 85 steps.
Found uncertainty sample 72 after 34 steps.
Found uncertainty sample 73 after 14 steps.
Found uncertainty sample 74 after 2380 steps.
Found uncertainty sample 75 after 17 steps.
Found uncertainty sample 76 after 519 steps.
Found uncertainty sample 77 after 830 steps.
Found uncertainty sample 78 after 1182 steps.
Found uncertainty sample 79 after 28 steps.
Found uncertainty sample 80 after 130 steps.
Found uncertainty sample 81 after 3 steps.
Found uncertainty sample 82 after 209 steps.
Found uncertainty sample 83 after 341 steps.
Found uncertainty sample 84 after 910 steps.
Found uncertainty sample 85 after 607 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 200 steps.
Found uncertainty sample 88 after 16 steps.
Found uncertainty sample 89 after 810 steps.
Found uncertainty sample 90 after 1153 steps.
Found uncertainty sample 91 after 38 steps.
Found uncertainty sample 92 after 2663 steps.
Found uncertainty sample 93 after 321 steps.
Found uncertainty sample 94 after 528 steps.
Found uncertainty sample 95 after 2292 steps.
Found uncertainty sample 96 after 226 steps.
Found uncertainty sample 97 after 5 steps.
Found uncertainty sample 98 after 438 steps.
Found uncertainty sample 99 after 78 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_044143-oe53ankl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_39
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/oe53ankl
Training model 39. Added 100 samples to the dataset.
Epoch 0, Batch 100/137, Loss: 1.5191218852996826, Variance: 0.12540237605571747

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.2618463669337077, Training Loss Force: 3.936438190280086, time: 2.253213882446289
Validation Loss Energy: 1.3976902951671464, Validation Loss Force: 4.121287325798579, time: 0.1320512294769287
Test Loss Energy: 9.13774851163938, Test Loss Force: 11.98820594526947, time: 9.680684328079224

Epoch 1, Batch 100/137, Loss: 1.9868782758712769, Variance: 0.1025364100933075

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.805124848825562, Training Loss Force: 4.238174396630969, time: 2.2569167613983154
Validation Loss Energy: 3.8063434920139265, Validation Loss Force: 4.02578616201722, time: 0.12329673767089844
Test Loss Energy: 10.006200476237408, Test Loss Force: 11.52050201361717, time: 9.703968286514282

Epoch 2, Batch 100/137, Loss: 1.3051927089691162, Variance: 0.11937472224235535

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5849525078779543, Training Loss Force: 3.5871722974161226, time: 2.2984237670898438
Validation Loss Energy: 2.570280565340411, Validation Loss Force: 3.6091462861423538, time: 0.12674951553344727
Test Loss Energy: 9.547405642036313, Test Loss Force: 11.453364855230973, time: 10.422014236450195

Epoch 3, Batch 100/137, Loss: 0.5048602223396301, Variance: 0.12204007059335709

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.587452922292483, Training Loss Force: 3.4806864393734953, time: 2.3285446166992188
Validation Loss Energy: 3.833120941272716, Validation Loss Force: 3.6089682250486597, time: 0.13146209716796875
Test Loss Energy: 9.857675390084365, Test Loss Force: 11.366066621489578, time: 11.74941086769104

Epoch 4, Batch 100/137, Loss: 1.547853708267212, Variance: 0.12643325328826904

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.64782141270047, Training Loss Force: 3.476152903689017, time: 2.3862459659576416
Validation Loss Energy: 2.931522213284661, Validation Loss Force: 3.617298871582207, time: 0.14150595664978027
Test Loss Energy: 9.584899167129821, Test Loss Force: 11.38423517405213, time: 11.010218143463135

Epoch 5, Batch 100/137, Loss: 0.8924164772033691, Variance: 0.12753602862358093

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.624190929956969, Training Loss Force: 3.4847662517287183, time: 2.452658176422119
Validation Loss Energy: 1.8311639213626298, Validation Loss Force: 3.6269184755784827, time: 0.13136696815490723
Test Loss Energy: 9.525401407087305, Test Loss Force: 11.577180675017994, time: 11.049630403518677

Epoch 6, Batch 100/137, Loss: 0.8167585134506226, Variance: 0.12942321598529816

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.59258982147357, Training Loss Force: 3.4810480231853598, time: 2.2692131996154785
Validation Loss Energy: 3.122182298370086, Validation Loss Force: 3.614255503918715, time: 0.13255524635314941
Test Loss Energy: 10.023688094893643, Test Loss Force: 11.697360990814945, time: 12.117015361785889

Epoch 7, Batch 100/137, Loss: 1.5448793172836304, Variance: 0.1337563693523407

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6740038910848885, Training Loss Force: 3.4854473381345112, time: 2.327925682067871
Validation Loss Energy: 2.243673024799867, Validation Loss Force: 3.6040785590501834, time: 0.13681578636169434
Test Loss Energy: 9.813746642735437, Test Loss Force: 11.609958403353183, time: 11.261804819107056

Epoch 8, Batch 100/137, Loss: 0.9983459711074829, Variance: 0.13597673177719116

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.7075875578762276, Training Loss Force: 3.4899953115563855, time: 2.286912202835083
Validation Loss Energy: 2.2064606313554633, Validation Loss Force: 3.6441402268858787, time: 0.13840818405151367
Test Loss Energy: 9.579917902117272, Test Loss Force: 11.469810573476622, time: 11.412220239639282

Epoch 9, Batch 100/137, Loss: 0.5222693085670471, Variance: 0.12924206256866455

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6174106300836337, Training Loss Force: 3.4915834675459325, time: 2.3658812046051025
Validation Loss Energy: 3.8093014289484315, Validation Loss Force: 3.6300134282607814, time: 0.13596320152282715
Test Loss Energy: 10.10964327228491, Test Loss Force: 11.530789724533433, time: 11.266667127609253

Epoch 10, Batch 100/137, Loss: 1.3200783729553223, Variance: 0.12899643182754517

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6680591409284258, Training Loss Force: 3.4827507172579915, time: 2.3442933559417725
Validation Loss Energy: 2.8180542472336327, Validation Loss Force: 3.6439246831526204, time: 0.15633487701416016
Test Loss Energy: 9.752442767783826, Test Loss Force: 11.489949181398412, time: 11.329929828643799

Epoch 11, Batch 100/137, Loss: 0.9178369045257568, Variance: 0.12813512980937958

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6711185394159895, Training Loss Force: 3.4913655329636697, time: 2.6198580265045166
Validation Loss Energy: 1.5678792180986516, Validation Loss Force: 3.6120280702289507, time: 0.13562512397766113
Test Loss Energy: 9.485731208320052, Test Loss Force: 11.704483927310466, time: 11.279255867004395

Epoch 12, Batch 100/137, Loss: 0.8917423486709595, Variance: 0.1365269422531128

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.676901105980767, Training Loss Force: 3.493321460366821, time: 2.345212459564209
Validation Loss Energy: 3.311861822759894, Validation Loss Force: 3.6004469737261977, time: 0.14548134803771973
Test Loss Energy: 9.982332875926854, Test Loss Force: 11.652482828860062, time: 11.336328029632568

Epoch 13, Batch 100/137, Loss: 1.4615222215652466, Variance: 0.13559789955615997

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.649056010719339, Training Loss Force: 3.487000759362615, time: 2.2298452854156494
Validation Loss Energy: 2.1391778830992423, Validation Loss Force: 3.647509804848155, time: 0.14734601974487305
Test Loss Energy: 9.672739053478045, Test Loss Force: 11.560533435366246, time: 11.505455255508423

Epoch 14, Batch 100/137, Loss: 1.0339869260787964, Variance: 0.13730506598949432

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.690301199468294, Training Loss Force: 3.4957290030562924, time: 2.226422071456909
Validation Loss Energy: 2.2630481351944813, Validation Loss Force: 3.657977407175171, time: 0.13733410835266113
Test Loss Energy: 9.551517238308133, Test Loss Force: 11.507748308360766, time: 11.52546215057373

Epoch 15, Batch 100/137, Loss: 0.5244787335395813, Variance: 0.12895658612251282

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6892659464086477, Training Loss Force: 3.505334161730335, time: 2.2374343872070312
Validation Loss Energy: 3.7745301573508345, Validation Loss Force: 3.647385778133873, time: 0.1385335922241211
Test Loss Energy: 10.24246396290364, Test Loss Force: 11.352093719886916, time: 11.42915153503418

Epoch 16, Batch 100/137, Loss: 1.0956013202667236, Variance: 0.1290326714515686

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6711555528792026, Training Loss Force: 3.501516266802323, time: 2.32497239112854
Validation Loss Energy: 3.2637825069867765, Validation Loss Force: 3.600703933920599, time: 0.14383244514465332
Test Loss Energy: 9.670030095622899, Test Loss Force: 11.441389480064796, time: 11.320126295089722

Epoch 17, Batch 100/137, Loss: 1.054197907447815, Variance: 0.13186973333358765

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6796456055996267, Training Loss Force: 3.486789488601507, time: 2.361605644226074
Validation Loss Energy: 1.9020491966192925, Validation Loss Force: 3.6257002526811113, time: 0.1366727352142334
Test Loss Energy: 9.53662016415369, Test Loss Force: 11.603221120491952, time: 11.438294887542725

Epoch 18, Batch 100/137, Loss: 0.7287561893463135, Variance: 0.13256748020648956

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6632685320896505, Training Loss Force: 3.498023582599923, time: 2.3446156978607178
Validation Loss Energy: 3.3418131980247376, Validation Loss Force: 3.662628000733971, time: 0.14023923873901367
Test Loss Energy: 9.986698123832678, Test Loss Force: 11.632218878027835, time: 11.296116828918457

Epoch 19, Batch 100/137, Loss: 1.4659607410430908, Variance: 0.1330467164516449

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6589349882426436, Training Loss Force: 3.4912581284198647, time: 2.405928134918213
Validation Loss Energy: 2.1153278651019654, Validation Loss Force: 3.648284173914643, time: 0.13951730728149414
Test Loss Energy: 9.648032365877214, Test Loss Force: 11.55937314025522, time: 11.27842378616333

wandb: - 0.039 MB of 0.056 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‡â–„â–†â–„â–ƒâ–‡â–…â–„â–‡â–…â–ƒâ–†â–„â–„â–ˆâ–„â–„â–†â–„
wandb:   test_error_force â–ˆâ–ƒâ–‚â–â–â–ƒâ–…â–„â–‚â–ƒâ–ƒâ–…â–„â–ƒâ–ƒâ–â–‚â–„â–„â–ƒ
wandb:          test_loss â–†â–ˆâ–ƒâ–‚â–â–‚â–ƒâ–ƒâ–â–‚â–‚â–‚â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–‚
wandb: train_error_energy â–ˆâ–ƒâ–â–â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–…â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–ˆâ–„â–ˆâ–…â–‚â–†â–ƒâ–ƒâ–ˆâ–…â–â–‡â–ƒâ–ƒâ–ˆâ–†â–‚â–‡â–ƒ
wandb:  valid_error_force â–ˆâ–‡â–â–â–â–â–â–â–‚â–â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–‚
wandb:         valid_loss â–â–ˆâ–‚â–…â–ƒâ–‚â–„â–‚â–‚â–…â–ƒâ–â–„â–‚â–‚â–…â–„â–‚â–„â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 4378
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.64803
wandb:   test_error_force 11.55937
wandb:          test_loss 9.95539
wandb: train_error_energy 2.65893
wandb:  train_error_force 3.49126
wandb:         train_loss 1.00837
wandb: valid_error_energy 2.11533
wandb:  valid_error_force 3.64828
wandb:         valid_loss 0.88393
wandb: 
wandb: ğŸš€ View run al_63_39 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/oe53ankl
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_044143-oe53ankl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2121639251708984, Uncertainty Bias: -0.17576166987419128
0.00022888184 0.014435768
2.2165163 5.087469
(48745, 22, 3)
Found uncertainty sample 0 after 179 steps.
Found uncertainty sample 1 after 2050 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 124 steps.
Found uncertainty sample 4 after 3327 steps.
Found uncertainty sample 5 after 1255 steps.
Found uncertainty sample 6 after 32 steps.
Found uncertainty sample 7 after 1478 steps.
Found uncertainty sample 8 after 2555 steps.
Found uncertainty sample 9 after 1550 steps.
Found uncertainty sample 10 after 601 steps.
Found uncertainty sample 11 after 28 steps.
Found uncertainty sample 12 after 1900 steps.
Found uncertainty sample 13 after 1455 steps.
Found uncertainty sample 14 after 1608 steps.
Found uncertainty sample 15 after 16 steps.
Found uncertainty sample 16 after 270 steps.
Found uncertainty sample 17 after 292 steps.
Found uncertainty sample 18 after 1916 steps.
Found uncertainty sample 19 after 2387 steps.
Found uncertainty sample 20 after 943 steps.
Found uncertainty sample 21 after 425 steps.
Found uncertainty sample 22 after 605 steps.
Found uncertainty sample 23 after 19 steps.
Found uncertainty sample 24 after 100 steps.
Found uncertainty sample 25 after 1169 steps.
Found uncertainty sample 26 after 1235 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 308 steps.
Found uncertainty sample 29 after 431 steps.
Found uncertainty sample 30 after 207 steps.
Found uncertainty sample 31 after 65 steps.
Found uncertainty sample 32 after 1326 steps.
Found uncertainty sample 33 after 50 steps.
Found uncertainty sample 34 after 20 steps.
Found uncertainty sample 35 after 1704 steps.
Found uncertainty sample 36 after 13 steps.
Found uncertainty sample 37 after 607 steps.
Found uncertainty sample 38 after 49 steps.
Found uncertainty sample 39 after 404 steps.
Found uncertainty sample 40 after 33 steps.
Found uncertainty sample 41 after 794 steps.
Found uncertainty sample 42 after 477 steps.
Found uncertainty sample 43 after 1821 steps.
Found uncertainty sample 44 after 29 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 382 steps.
Found uncertainty sample 47 after 120 steps.
Found uncertainty sample 48 after 186 steps.
Found uncertainty sample 49 after 481 steps.
Found uncertainty sample 50 after 67 steps.
Found uncertainty sample 51 after 137 steps.
Found uncertainty sample 52 after 117 steps.
Found uncertainty sample 53 after 538 steps.
Found uncertainty sample 54 after 1466 steps.
Found uncertainty sample 55 after 450 steps.
Found uncertainty sample 56 after 338 steps.
Found uncertainty sample 57 after 36 steps.
Found uncertainty sample 58 after 1249 steps.
Found uncertainty sample 59 after 97 steps.
Found uncertainty sample 60 after 2278 steps.
Found uncertainty sample 61 after 104 steps.
Found uncertainty sample 62 after 211 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 2179 steps.
Found uncertainty sample 65 after 50 steps.
Found uncertainty sample 66 after 25 steps.
Found uncertainty sample 67 after 283 steps.
Found uncertainty sample 68 after 162 steps.
Found uncertainty sample 69 after 23 steps.
Found uncertainty sample 70 after 57 steps.
Found uncertainty sample 71 after 9 steps.
Found uncertainty sample 72 after 16 steps.
Found uncertainty sample 73 after 107 steps.
Found uncertainty sample 74 after 151 steps.
Found uncertainty sample 75 after 503 steps.
Found uncertainty sample 76 after 28 steps.
Found uncertainty sample 77 after 92 steps.
Found uncertainty sample 78 after 56 steps.
Found uncertainty sample 79 after 136 steps.
Found uncertainty sample 80 after 48 steps.
Found uncertainty sample 81 after 39 steps.
Found uncertainty sample 82 after 130 steps.
Found uncertainty sample 83 after 1022 steps.
Found uncertainty sample 84 after 1168 steps.
Found uncertainty sample 85 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 361 steps.
Found uncertainty sample 88 after 1501 steps.
Found uncertainty sample 89 after 1496 steps.
Found uncertainty sample 90 after 276 steps.
Found uncertainty sample 91 after 883 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 20 steps.
Found uncertainty sample 94 after 1409 steps.
Found uncertainty sample 95 after 829 steps.
Found uncertainty sample 96 after 1705 steps.
Found uncertainty sample 97 after 1696 steps.
Found uncertainty sample 98 after 1743 steps.
Found uncertainty sample 99 after 7 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_045502-slo34hms
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_40
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/slo34hms
Training model 40. Added 100 samples to the dataset.
Epoch 0, Batch 100/140, Loss: 0.5362535119056702, Variance: 0.1095813736319542

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.2216668286060317, Training Loss Force: 3.6543393739889183, time: 2.3386380672454834
Validation Loss Energy: 2.1239619354583636, Validation Loss Force: 3.605925154277362, time: 0.1410214900970459
Test Loss Energy: 9.150235256408541, Test Loss Force: 11.329215451415598, time: 11.416614294052124

Epoch 1, Batch 100/140, Loss: 0.4732920527458191, Variance: 0.09856383502483368

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6884052486806067, Training Loss Force: 3.49429738542672, time: 2.286942958831787
Validation Loss Energy: 2.0309796792787895, Validation Loss Force: 3.608347720362008, time: 0.146958589553833
Test Loss Energy: 9.059187134352875, Test Loss Force: 11.296213030093448, time: 11.549059629440308

Epoch 2, Batch 100/140, Loss: 0.5247674584388733, Variance: 0.0946883037686348

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7001903798376787, Training Loss Force: 3.487383494131217, time: 2.4087016582489014
Validation Loss Energy: 2.299239961720049, Validation Loss Force: 3.5658663811405638, time: 0.15626001358032227
Test Loss Energy: 9.39290748752055, Test Loss Force: 11.37518863727657, time: 11.488511323928833

Epoch 3, Batch 100/140, Loss: 0.6603152751922607, Variance: 0.09080126881599426

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6865005105631912, Training Loss Force: 3.5045191620769622, time: 2.292137861251831
Validation Loss Energy: 2.007764491016553, Validation Loss Force: 3.5917876675332607, time: 0.1523749828338623
Test Loss Energy: 9.270745509589405, Test Loss Force: 11.307066992145224, time: 11.431030750274658

Epoch 4, Batch 100/140, Loss: 0.3557168245315552, Variance: 0.09135499596595764

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.686007598213061, Training Loss Force: 3.4946879805090494, time: 2.469862699508667
Validation Loss Energy: 2.273841057143698, Validation Loss Force: 3.598452303516977, time: 0.13549184799194336
Test Loss Energy: 9.45675587624852, Test Loss Force: 11.543545169173955, time: 11.251747846603394

Epoch 5, Batch 100/140, Loss: 0.4198908805847168, Variance: 0.09116743505001068

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7041036016038325, Training Loss Force: 3.527212378340113, time: 2.5268659591674805
Validation Loss Energy: 2.230409737122678, Validation Loss Force: 3.6138166230320916, time: 0.15105271339416504
Test Loss Energy: 9.445787679095847, Test Loss Force: 11.41983288237914, time: 10.888420343399048

Epoch 6, Batch 100/140, Loss: 0.31895458698272705, Variance: 0.0880512073636055

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6869325093827119, Training Loss Force: 3.5109649535224134, time: 2.2746667861938477
Validation Loss Energy: 2.096988908466401, Validation Loss Force: 3.6161591766703007, time: 0.13223624229431152
Test Loss Energy: 9.315795684042419, Test Loss Force: 11.439960685601221, time: 10.092291831970215

Epoch 7, Batch 100/140, Loss: 0.32067424058914185, Variance: 0.08844541013240814

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6925394003876093, Training Loss Force: 3.510567815148182, time: 2.251642942428589
Validation Loss Energy: 1.9957618075975103, Validation Loss Force: 3.709125549117465, time: 0.1257925033569336
Test Loss Energy: 9.39271213090211, Test Loss Force: 11.37280650176891, time: 9.949204921722412

Epoch 8, Batch 100/140, Loss: 0.3020458221435547, Variance: 0.08868344128131866

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.721722354237902, Training Loss Force: 3.522982563196538, time: 2.2375712394714355
Validation Loss Energy: 1.973225463338159, Validation Loss Force: 3.6178000998296915, time: 0.12865948677062988
Test Loss Energy: 9.4141555240603, Test Loss Force: 11.488734415118934, time: 10.090615272521973

Epoch 9, Batch 100/140, Loss: 0.9625586867332458, Variance: 0.09075218439102173

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.697709135396517, Training Loss Force: 3.507408637792057, time: 2.296609401702881
Validation Loss Energy: 2.1640365923275713, Validation Loss Force: 3.706961130795998, time: 0.1269681453704834
Test Loss Energy: 9.33240588367268, Test Loss Force: 11.401098339821852, time: 9.88605260848999

Epoch 10, Batch 100/140, Loss: 0.6479395627975464, Variance: 0.08995112776756287

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6867369573625233, Training Loss Force: 3.508085518595306, time: 2.2476799488067627
Validation Loss Energy: 2.1256009288184896, Validation Loss Force: 3.684958376033339, time: 0.12901616096496582
Test Loss Energy: 9.350834266646583, Test Loss Force: 11.300476563666054, time: 9.96634292602539

Epoch 11, Batch 100/140, Loss: 0.5905831456184387, Variance: 0.09249302744865417

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.7048273860721768, Training Loss Force: 3.5093162332317633, time: 2.226301431655884
Validation Loss Energy: 1.931047696385663, Validation Loss Force: 3.638786147272045, time: 0.13422656059265137
Test Loss Energy: 9.454039951530461, Test Loss Force: 11.454236426568801, time: 10.06533670425415

Epoch 12, Batch 100/140, Loss: 0.562102198600769, Variance: 0.08979953825473785

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6964467190356547, Training Loss Force: 3.5063938454264054, time: 2.291374683380127
Validation Loss Energy: 1.9183601197852707, Validation Loss Force: 3.6325070258029784, time: 0.13249468803405762
Test Loss Energy: 9.223137162764667, Test Loss Force: 11.486291112047143, time: 9.881063461303711

Epoch 13, Batch 100/140, Loss: 0.5085219740867615, Variance: 0.08576314151287079

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6877329556570113, Training Loss Force: 3.5218224442223036, time: 2.3205554485321045
Validation Loss Energy: 2.0572742333831098, Validation Loss Force: 3.624945330053368, time: 0.1286487579345703
Test Loss Energy: 9.185510138774623, Test Loss Force: 11.500070862738301, time: 10.127248287200928

Epoch 14, Batch 100/140, Loss: 0.5265432596206665, Variance: 0.08817194402217865

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6900519458630174, Training Loss Force: 3.515780019525194, time: 2.3220276832580566
Validation Loss Energy: 2.0672861804145097, Validation Loss Force: 3.6347225095819113, time: 0.1295485496520996
Test Loss Energy: 9.324774795201625, Test Loss Force: 11.532377227305124, time: 10.6401948928833

Epoch 15, Batch 100/140, Loss: 0.8087940216064453, Variance: 0.08454892784357071

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6859688132240755, Training Loss Force: 3.515601012617795, time: 2.2466330528259277
Validation Loss Energy: 2.0798618657690735, Validation Loss Force: 3.5842696149362525, time: 0.1305530071258545
Test Loss Energy: 9.279010004345096, Test Loss Force: 11.504320677093585, time: 11.544277906417847

Epoch 16, Batch 100/140, Loss: 0.5227136611938477, Variance: 0.08711317181587219

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6736107464020615, Training Loss Force: 3.509504741749718, time: 2.668159008026123
Validation Loss Energy: 2.071670684343925, Validation Loss Force: 3.6689666490880484, time: 0.15041732788085938
Test Loss Energy: 9.430765583793715, Test Loss Force: 11.629265627637658, time: 11.575058221817017

Epoch 17, Batch 100/140, Loss: 0.8302644491195679, Variance: 0.08917362242937088

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.698574774822886, Training Loss Force: 3.5164451529405008, time: 2.330409049987793
Validation Loss Energy: 2.116881279287006, Validation Loss Force: 3.6013999234477203, time: 0.1370234489440918
Test Loss Energy: 9.281877717140638, Test Loss Force: 11.50639119704088, time: 10.95054316520691

Epoch 18, Batch 100/140, Loss: 0.4278913140296936, Variance: 0.08403190970420837

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.67340626177051, Training Loss Force: 3.520506063993926, time: 2.361734390258789
Validation Loss Energy: 2.011941021963672, Validation Loss Force: 3.681576625491085, time: 0.13354897499084473
Test Loss Energy: 9.470052292664557, Test Loss Force: 11.717472458748684, time: 11.078141450881958

Epoch 19, Batch 100/140, Loss: 1.041327953338623, Variance: 0.0885561853647232

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.677761807047333, Training Loss Force: 3.5077073023269967, time: 2.411865472793579
Validation Loss Energy: 2.1317716846365378, Validation Loss Force: 3.63981383960355, time: 0.14443469047546387
Test Loss Energy: 9.412171451037132, Test Loss Force: 11.66032392114929, time: 11.435442447662354

wandb: - 0.039 MB of 0.049 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–â–‡â–…â–ˆâ–ˆâ–…â–‡â–‡â–†â–†â–ˆâ–„â–ƒâ–†â–…â–‡â–…â–ˆâ–‡
wandb:   test_error_force â–‚â–â–‚â–â–…â–ƒâ–ƒâ–‚â–„â–ƒâ–â–„â–„â–„â–…â–„â–‡â–„â–ˆâ–‡
wandb:          test_loss â–â–ƒâ–„â–†â–†â–†â–…â–†â–†â–‡â–‡â–‡â–‡â–†â–‡â–‡â–ˆâ–‡â–‡â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–ƒâ–ˆâ–ƒâ–ˆâ–‡â–„â–‚â–‚â–†â–…â–â–â–„â–„â–„â–„â–…â–ƒâ–…
wandb:  valid_error_force â–ƒâ–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–ˆâ–„â–ˆâ–‡â–…â–„â–„â–„â–‚â–†â–ƒâ–‡â–…
wandb:         valid_loss â–ƒâ–‚â–‡â–‚â–ˆâ–‡â–„â–ƒâ–‚â–ˆâ–†â–â–â–„â–„â–„â–…â–…â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 4468
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.41217
wandb:   test_error_force 11.66032
wandb:          test_loss 12.55602
wandb: train_error_energy 1.67776
wandb:  train_error_force 3.50771
wandb:         train_loss 0.59824
wandb: valid_error_energy 2.13177
wandb:  valid_error_force 3.63981
wandb:         valid_loss 0.84993
wandb: 
wandb: ğŸš€ View run al_63_40 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/slo34hms
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_045502-slo34hms/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.342503547668457, Uncertainty Bias: -0.0593610554933548
3.33786e-05 0.018134117
2.3333848 5.1034503
(48745, 22, 3)
Found uncertainty sample 0 after 211 steps.
Found uncertainty sample 1 after 703 steps.
Found uncertainty sample 2 after 1376 steps.
Found uncertainty sample 3 after 1714 steps.
Found uncertainty sample 4 after 177 steps.
Found uncertainty sample 5 after 146 steps.
Found uncertainty sample 6 after 286 steps.
Found uncertainty sample 7 after 2180 steps.
Found uncertainty sample 8 after 2995 steps.
Found uncertainty sample 9 after 412 steps.
Found uncertainty sample 10 after 2208 steps.
Found uncertainty sample 11 after 337 steps.
Found uncertainty sample 12 after 249 steps.
Found uncertainty sample 13 after 867 steps.
Found uncertainty sample 14 after 1318 steps.
Found uncertainty sample 15 after 800 steps.
Found uncertainty sample 16 after 2399 steps.
Found uncertainty sample 17 after 811 steps.
Found uncertainty sample 18 after 222 steps.
Found uncertainty sample 19 after 275 steps.
Found uncertainty sample 20 after 123 steps.
Found uncertainty sample 21 after 112 steps.
Found uncertainty sample 22 after 299 steps.
Found uncertainty sample 23 after 1551 steps.
Found uncertainty sample 24 after 118 steps.
Found uncertainty sample 25 after 1526 steps.
Found uncertainty sample 26 after 1389 steps.
Found uncertainty sample 27 after 330 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 307 steps.
Found uncertainty sample 30 after 129 steps.
Found uncertainty sample 31 after 297 steps.
Found uncertainty sample 32 after 75 steps.
Found uncertainty sample 33 after 903 steps.
Found uncertainty sample 34 after 352 steps.
Found uncertainty sample 35 after 34 steps.
Found uncertainty sample 36 after 22 steps.
Found uncertainty sample 37 after 444 steps.
Found uncertainty sample 38 after 382 steps.
Found uncertainty sample 39 after 263 steps.
Found uncertainty sample 40 after 113 steps.
Found uncertainty sample 41 after 389 steps.
Found uncertainty sample 42 after 870 steps.
Found uncertainty sample 43 after 2543 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 52 steps.
Found uncertainty sample 46 after 20 steps.
Found uncertainty sample 47 after 1231 steps.
Found uncertainty sample 48 after 121 steps.
Found uncertainty sample 49 after 614 steps.
Found uncertainty sample 50 after 325 steps.
Found uncertainty sample 51 after 955 steps.
Found uncertainty sample 52 after 618 steps.
Found uncertainty sample 53 after 97 steps.
Found uncertainty sample 54 after 216 steps.
Found uncertainty sample 55 after 19 steps.
Found uncertainty sample 56 after 603 steps.
Found uncertainty sample 57 after 284 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 3558 steps.
Found uncertainty sample 60 after 1208 steps.
Found uncertainty sample 61 after 389 steps.
Found uncertainty sample 62 after 128 steps.
Found uncertainty sample 63 after 135 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 118 steps.
Found uncertainty sample 66 after 413 steps.
Found uncertainty sample 67 after 391 steps.
Found uncertainty sample 68 after 5 steps.
Found uncertainty sample 69 after 250 steps.
Found uncertainty sample 70 after 40 steps.
Found uncertainty sample 71 after 97 steps.
Found uncertainty sample 72 after 1153 steps.
Found uncertainty sample 73 after 7 steps.
Found uncertainty sample 74 after 1101 steps.
Found uncertainty sample 75 after 2989 steps.
Found uncertainty sample 76 after 112 steps.
Found uncertainty sample 77 after 56 steps.
Found uncertainty sample 78 after 669 steps.
Found uncertainty sample 79 after 887 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 661 steps.
Found uncertainty sample 82 after 67 steps.
Found uncertainty sample 83 after 563 steps.
Found uncertainty sample 84 after 1762 steps.
Found uncertainty sample 85 after 7 steps.
Found uncertainty sample 86 after 158 steps.
Found uncertainty sample 87 after 336 steps.
Found uncertainty sample 88 after 2431 steps.
Found uncertainty sample 89 after 217 steps.
Found uncertainty sample 90 after 730 steps.
Found uncertainty sample 91 after 194 steps.
Found uncertainty sample 92 after 841 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 162 steps.
Found uncertainty sample 95 after 2837 steps.
Found uncertainty sample 96 after 1490 steps.
Found uncertainty sample 97 after 566 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 242 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_050938-rs1ljcv8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_41
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/rs1ljcv8
Training model 41. Added 97 samples to the dataset.
Epoch 0, Batch 100/143, Loss: 0.86297607421875, Variance: 0.11152243614196777

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.261504140195332, Training Loss Force: 3.878193354699001, time: 2.6137099266052246
Validation Loss Energy: 3.248325557106956, Validation Loss Force: 3.6330658511362084, time: 0.1493535041809082
Test Loss Energy: 9.902574200813662, Test Loss Force: 11.694938350507252, time: 11.675419092178345

Epoch 1, Batch 100/143, Loss: 1.401533842086792, Variance: 0.12457738816738129

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6892399903421973, Training Loss Force: 3.5178087374595663, time: 2.5294888019561768
Validation Loss Energy: 2.2503530249546517, Validation Loss Force: 3.6976096204514204, time: 0.1539158821105957
Test Loss Energy: 9.683495003799786, Test Loss Force: 11.663176972755412, time: 11.764190912246704

Epoch 2, Batch 100/143, Loss: 0.9276660680770874, Variance: 0.12181168794631958

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.7065908427809027, Training Loss Force: 3.501283155267927, time: 2.5593199729919434
Validation Loss Energy: 2.390971039968655, Validation Loss Force: 3.657908157070788, time: 0.14282965660095215
Test Loss Energy: 9.722439305814545, Test Loss Force: 11.772746565146049, time: 11.677708387374878

Epoch 3, Batch 100/143, Loss: 0.7695953845977783, Variance: 0.12880447506904602

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.721982845157471, Training Loss Force: 3.505959194875396, time: 2.5609185695648193
Validation Loss Energy: 3.9675041459776925, Validation Loss Force: 3.598884058990931, time: 0.1559584140777588
Test Loss Energy: 10.47914734995197, Test Loss Force: 11.748089960615946, time: 11.61908507347107

Epoch 4, Batch 100/143, Loss: 1.1918320655822754, Variance: 0.12385734915733337

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.7023363073024775, Training Loss Force: 3.515324445826734, time: 2.6884520053863525
Validation Loss Energy: 2.678820863192057, Validation Loss Force: 3.6770887288549106, time: 0.14421701431274414
Test Loss Energy: 10.557714341765209, Test Loss Force: 11.890362557773301, time: 11.344536781311035

Epoch 5, Batch 100/143, Loss: 0.9100590348243713, Variance: 0.1307242512702942

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.684660238011946, Training Loss Force: 3.504779415883925, time: 2.437277317047119
Validation Loss Energy: 1.53229290320391, Validation Loss Force: 3.652106809365834, time: 0.15214228630065918
Test Loss Energy: 10.047292954938458, Test Loss Force: 12.077902523899713, time: 12.069614887237549

Epoch 6, Batch 100/143, Loss: 0.9381623268127441, Variance: 0.1345827579498291

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.7127980114923385, Training Loss Force: 3.506348833181104, time: 2.8674139976501465
Validation Loss Energy: 3.1475273170131217, Validation Loss Force: 3.6172692565857996, time: 0.16827678680419922
Test Loss Energy: 10.392699367269133, Test Loss Force: 11.96556837864748, time: 11.929619550704956

Epoch 7, Batch 100/143, Loss: 1.394912600517273, Variance: 0.1370513141155243

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.723721510469641, Training Loss Force: 3.508101834182029, time: 2.387723922729492
Validation Loss Energy: 2.0208363574512926, Validation Loss Force: 3.600713434470825, time: 0.1345672607421875
Test Loss Energy: 10.340705035162827, Test Loss Force: 12.185970151113288, time: 11.025294065475464

Epoch 8, Batch 100/143, Loss: 1.0301028490066528, Variance: 0.13583941757678986

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.685885543500997, Training Loss Force: 3.5029026497660367, time: 2.392392635345459
Validation Loss Energy: 2.620745781012144, Validation Loss Force: 3.620940105902516, time: 0.13929510116577148
Test Loss Energy: 10.292557693771325, Test Loss Force: 12.09260314557709, time: 11.242953300476074

Epoch 9, Batch 100/143, Loss: 0.7577587366104126, Variance: 0.13059884309768677

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.671448425057076, Training Loss Force: 3.5011508524242, time: 2.4833507537841797
Validation Loss Energy: 4.0358818326474895, Validation Loss Force: 3.6971075015273533, time: 0.13918685913085938
Test Loss Energy: 11.37062901669346, Test Loss Force: 12.467991746230059, time: 10.857106685638428

Epoch 10, Batch 100/143, Loss: 1.4699995517730713, Variance: 0.12905597686767578

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.671161994679068, Training Loss Force: 3.5145873611256517, time: 2.5259547233581543
Validation Loss Energy: 2.827127866484093, Validation Loss Force: 3.649796097415493, time: 0.1509091854095459
Test Loss Energy: 11.056099237157168, Test Loss Force: 12.245524140361283, time: 11.031496286392212

Epoch 11, Batch 100/143, Loss: 0.9829227924346924, Variance: 0.13043075799942017

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.669874139402661, Training Loss Force: 3.513975866494254, time: 2.4839179515838623
Validation Loss Energy: 1.7703062170891064, Validation Loss Force: 3.628277202706934, time: 0.14894485473632812
Test Loss Energy: 10.424733184801337, Test Loss Force: 12.480910968107738, time: 9.738173723220825

Epoch 12, Batch 100/143, Loss: 0.7313207983970642, Variance: 0.1314469575881958

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.686913453570747, Training Loss Force: 3.491171967349112, time: 2.459993600845337
Validation Loss Energy: 3.415384048738911, Validation Loss Force: 3.647280307181752, time: 0.14138579368591309
Test Loss Energy: 11.23774970463319, Test Loss Force: 12.503280123978513, time: 11.627300500869751

Epoch 13, Batch 100/143, Loss: 1.5326058864593506, Variance: 0.13114053010940552

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.702940107391326, Training Loss Force: 3.5278014840782705, time: 2.3683717250823975
Validation Loss Energy: 2.233501423990836, Validation Loss Force: 3.601577316533976, time: 0.11997199058532715
Test Loss Energy: 10.725367327762799, Test Loss Force: 12.209234339022904, time: 8.917361974716187

Epoch 14, Batch 100/143, Loss: 1.0195485353469849, Variance: 0.13548365235328674

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.7043103930727153, Training Loss Force: 3.4919216429782622, time: 2.3473966121673584
Validation Loss Energy: 2.592718118722019, Validation Loss Force: 3.6180223236373266, time: 0.15249395370483398
Test Loss Energy: 10.65087002845289, Test Loss Force: 12.1357439192799, time: 11.300978899002075

Epoch 15, Batch 100/143, Loss: 0.6098727583885193, Variance: 0.1322162002325058

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6924804796301784, Training Loss Force: 3.508971623465577, time: 2.7309281826019287
Validation Loss Energy: 3.8150848888986624, Validation Loss Force: 3.6078219985084723, time: 0.20830368995666504
Test Loss Energy: 11.439314394817101, Test Loss Force: 12.323890813881299, time: 11.232990026473999

Epoch 16, Batch 100/143, Loss: 1.1625659465789795, Variance: 0.12816908955574036

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.688952224727877, Training Loss Force: 3.485449286213915, time: 2.520033359527588
Validation Loss Energy: 2.8418806623014414, Validation Loss Force: 3.5800436514387375, time: 0.13312840461730957
Test Loss Energy: 10.891771781045605, Test Loss Force: 12.203097791869634, time: 10.042590618133545

Epoch 17, Batch 100/143, Loss: 0.7406217455863953, Variance: 0.12835930287837982

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6913343602722253, Training Loss Force: 3.48491978832766, time: 2.3517770767211914
Validation Loss Energy: 1.7379817082900126, Validation Loss Force: 3.5853171248217452, time: 0.13181495666503906
Test Loss Energy: 10.740860800822983, Test Loss Force: 12.408557373448287, time: 9.905988216400146

Epoch 18, Batch 100/143, Loss: 0.892869234085083, Variance: 0.13687632977962494

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.679711325930341, Training Loss Force: 3.493190368172207, time: 2.3180603981018066
Validation Loss Energy: 3.147635311548896, Validation Loss Force: 3.608236237342947, time: 0.1309058666229248
Test Loss Energy: 11.018575890719406, Test Loss Force: 12.316247787546082, time: 9.799858331680298

Epoch 19, Batch 100/143, Loss: 1.3661614656448364, Variance: 0.13310661911964417

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.688883521953597, Training Loss Force: 3.483185430807044, time: 2.337738037109375
Validation Loss Energy: 2.1163272784519376, Validation Loss Force: 3.6394110311185663, time: 0.1305701732635498
Test Loss Energy: 10.631182998222268, Test Loss Force: 12.33072886910365, time: 10.659468650817871

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.055 MB of 0.059 MB uploadedwandb: / 0.055 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–â–„â–„â–‚â–„â–„â–ƒâ–ˆâ–†â–„â–‡â–…â–…â–ˆâ–†â–…â–†â–…
wandb:   test_error_force â–â–â–‚â–‚â–ƒâ–„â–„â–…â–…â–ˆâ–†â–ˆâ–ˆâ–†â–…â–‡â–…â–‡â–†â–‡
wandb:          test_loss â–…â–ƒâ–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–…â–…â–„â–ˆâ–„â–ƒâ–†â–„â–…â–…â–„
wandb: train_error_energy â–ˆâ–â–â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–â–â–‚â–â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–ƒâ–ƒâ–ˆâ–„â–â–†â–‚â–„â–ˆâ–…â–‚â–†â–ƒâ–„â–‡â–…â–‚â–†â–ƒ
wandb:  valid_error_force â–„â–ˆâ–†â–‚â–‡â–…â–ƒâ–‚â–ƒâ–ˆâ–…â–„â–…â–‚â–ƒâ–ƒâ–â–â–ƒâ–…
wandb:         valid_loss â–†â–ƒâ–ƒâ–ˆâ–ƒâ–â–…â–‚â–ƒâ–ˆâ–„â–â–†â–‚â–ƒâ–‡â–„â–â–…â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 4555
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.63118
wandb:   test_error_force 12.33073
wandb:          test_loss 11.45213
wandb: train_error_energy 2.68888
wandb:  train_error_force 3.48319
wandb:         train_loss 1.01998
wandb: valid_error_energy 2.11633
wandb:  valid_error_force 3.63941
wandb:         valid_loss 0.88644
wandb: 
wandb: ğŸš€ View run al_63_41 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/rs1ljcv8
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_050938-rs1ljcv8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.879697322845459, Uncertainty Bias: -0.1324775665998459
7.8201294e-05 0.005668916
2.1924598 5.0093403
(48745, 22, 3)
Found uncertainty sample 0 after 28 steps.
Found uncertainty sample 1 after 769 steps.
Found uncertainty sample 2 after 124 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 895 steps.
Found uncertainty sample 5 after 97 steps.
Found uncertainty sample 6 after 63 steps.
Found uncertainty sample 7 after 370 steps.
Found uncertainty sample 8 after 682 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 687 steps.
Found uncertainty sample 11 after 8 steps.
Found uncertainty sample 12 after 907 steps.
Found uncertainty sample 13 after 332 steps.
Found uncertainty sample 14 after 357 steps.
Found uncertainty sample 15 after 16 steps.
Found uncertainty sample 16 after 472 steps.
Found uncertainty sample 17 after 376 steps.
Found uncertainty sample 18 after 137 steps.
Found uncertainty sample 19 after 15 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 111 steps.
Found uncertainty sample 22 after 3265 steps.
Found uncertainty sample 23 after 536 steps.
Found uncertainty sample 24 after 946 steps.
Found uncertainty sample 25 after 146 steps.
Found uncertainty sample 26 after 3721 steps.
Found uncertainty sample 27 after 3200 steps.
Found uncertainty sample 28 after 80 steps.
Found uncertainty sample 29 after 131 steps.
Found uncertainty sample 30 after 1382 steps.
Found uncertainty sample 31 after 298 steps.
Found uncertainty sample 32 after 305 steps.
Found uncertainty sample 33 after 330 steps.
Found uncertainty sample 34 after 14 steps.
Found uncertainty sample 35 after 1049 steps.
Found uncertainty sample 36 after 110 steps.
Found uncertainty sample 37 after 312 steps.
Found uncertainty sample 38 after 21 steps.
Found uncertainty sample 39 after 326 steps.
Found uncertainty sample 40 after 1701 steps.
Found uncertainty sample 41 after 12 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 2717 steps.
Found uncertainty sample 44 after 99 steps.
Found uncertainty sample 45 after 170 steps.
Found uncertainty sample 46 after 13 steps.
Found uncertainty sample 47 after 27 steps.
Found uncertainty sample 48 after 236 steps.
Found uncertainty sample 49 after 173 steps.
Found uncertainty sample 50 after 3313 steps.
Found uncertainty sample 51 after 772 steps.
Found uncertainty sample 52 after 185 steps.
Found uncertainty sample 53 after 1299 steps.
Found uncertainty sample 54 after 4 steps.
Found uncertainty sample 55 after 91 steps.
Found uncertainty sample 56 after 8 steps.
Found uncertainty sample 57 after 131 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 35 steps.
Found uncertainty sample 60 after 370 steps.
Found uncertainty sample 61 after 2739 steps.
Found uncertainty sample 62 after 238 steps.
Found uncertainty sample 63 after 169 steps.
Found uncertainty sample 64 after 343 steps.
Found uncertainty sample 65 after 19 steps.
Found uncertainty sample 66 after 1206 steps.
Found uncertainty sample 67 after 140 steps.
Found uncertainty sample 68 after 123 steps.
Found uncertainty sample 69 after 2117 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 459 steps.
Found uncertainty sample 72 after 714 steps.
Found uncertainty sample 73 after 713 steps.
Found uncertainty sample 74 after 13 steps.
Found uncertainty sample 75 after 5 steps.
Found uncertainty sample 76 after 29 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 19 steps.
Found uncertainty sample 79 after 117 steps.
Found uncertainty sample 80 after 67 steps.
Found uncertainty sample 81 after 74 steps.
Found uncertainty sample 82 after 347 steps.
Found uncertainty sample 83 after 2585 steps.
Found uncertainty sample 84 after 664 steps.
Found uncertainty sample 85 after 217 steps.
Found uncertainty sample 86 after 728 steps.
Found uncertainty sample 87 after 3170 steps.
Found uncertainty sample 88 after 790 steps.
Found uncertainty sample 89 after 87 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1040 steps.
Found uncertainty sample 92 after 756 steps.
Found uncertainty sample 93 after 99 steps.
Found uncertainty sample 94 after 1076 steps.
Found uncertainty sample 95 after 57 steps.
Found uncertainty sample 96 after 81 steps.
Found uncertainty sample 97 after 550 steps.
Found uncertainty sample 98 after 205 steps.
Found uncertainty sample 99 after 233 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_052224-bp0eno3l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_42
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/bp0eno3l
Training model 42. Added 100 samples to the dataset.
Epoch 0, Batch 100/146, Loss: 1.2957465648651123, Variance: 0.11515302211046219

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9594228621453618, Training Loss Force: 3.939763017191459, time: 2.4296557903289795
Validation Loss Energy: 2.9462422025145094, Validation Loss Force: 3.6230258477924218, time: 0.13503384590148926
Test Loss Energy: 11.195689340279337, Test Loss Force: 12.46605130236981, time: 9.864125967025757

Epoch 1, Batch 100/146, Loss: 0.7028605341911316, Variance: 0.12220903486013412

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6428194032807584, Training Loss Force: 3.473953246971441, time: 2.520658254623413
Validation Loss Energy: 2.3471368372396024, Validation Loss Force: 3.5809252728105143, time: 0.1573774814605713
Test Loss Energy: 10.553028776715227, Test Loss Force: 12.189768896069275, time: 12.178069829940796

Epoch 2, Batch 100/146, Loss: 0.8708468675613403, Variance: 0.13182029128074646

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6324994028909425, Training Loss Force: 3.47181410108966, time: 2.5839953422546387
Validation Loss Energy: 3.2341660261174083, Validation Loss Force: 3.6232165268043843, time: 0.15114903450012207
Test Loss Energy: 11.052212464442697, Test Loss Force: 12.61871623463103, time: 10.99226713180542

Epoch 3, Batch 100/146, Loss: 1.1997851133346558, Variance: 0.13642923533916473

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6655842777649945, Training Loss Force: 3.473708054688889, time: 2.34259295463562
Validation Loss Energy: 2.9316855039916745, Validation Loss Force: 3.574002785955814, time: 0.13831639289855957
Test Loss Energy: 11.148605186935098, Test Loss Force: 12.208321190342557, time: 11.15287971496582

Epoch 4, Batch 100/146, Loss: 0.8239611387252808, Variance: 0.13085795938968658

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.644374797594391, Training Loss Force: 3.4820346469154995, time: 2.3336522579193115
Validation Loss Energy: 2.349443244253122, Validation Loss Force: 3.613046633125473, time: 0.1475210189819336
Test Loss Energy: 10.843937109104042, Test Loss Force: 12.296832567560026, time: 11.341200828552246

Epoch 5, Batch 100/146, Loss: 0.7273434400558472, Variance: 0.13257482647895813

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.678978139544434, Training Loss Force: 3.490244403421136, time: 2.5191292762756348
Validation Loss Energy: 3.2772505760551702, Validation Loss Force: 3.5906057397066764, time: 0.14766168594360352
Test Loss Energy: 11.06569739185819, Test Loss Force: 12.538762939689452, time: 11.297940254211426

Epoch 6, Batch 100/146, Loss: 1.4077880382537842, Variance: 0.13497963547706604

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6874734326265757, Training Loss Force: 3.4822092546039225, time: 2.496670961380005
Validation Loss Energy: 2.889204684331571, Validation Loss Force: 3.583777145008509, time: 0.15912318229675293
Test Loss Energy: 10.698227003962206, Test Loss Force: 12.046161333262694, time: 11.437181949615479

Epoch 7, Batch 100/146, Loss: 0.9474600553512573, Variance: 0.1341065764427185

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6943261121956508, Training Loss Force: 3.4830791772031287, time: 2.51275897026062
Validation Loss Energy: 2.517914794285135, Validation Loss Force: 3.6495225112581964, time: 0.14720916748046875
Test Loss Energy: 10.68623300386265, Test Loss Force: 12.16817036663576, time: 11.346058130264282

Epoch 8, Batch 100/146, Loss: 0.6439065933227539, Variance: 0.13004067540168762

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.7046772733022815, Training Loss Force: 3.5075259464779833, time: 2.441438674926758
Validation Loss Energy: 3.1131014794839076, Validation Loss Force: 3.6259954377455634, time: 0.15384674072265625
Test Loss Energy: 11.001722443117224, Test Loss Force: 12.30928961689209, time: 11.470924615859985

Epoch 9, Batch 100/146, Loss: 1.3163177967071533, Variance: 0.13944299519062042

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.682886526002151, Training Loss Force: 3.5098499199519333, time: 2.458127737045288
Validation Loss Energy: 3.010352509225648, Validation Loss Force: 3.604582756219612, time: 0.15432405471801758
Test Loss Energy: 11.03301542757705, Test Loss Force: 12.34977408524921, time: 11.35497260093689

Epoch 10, Batch 100/146, Loss: 0.9983747601509094, Variance: 0.12932267785072327

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.665180026505956, Training Loss Force: 3.5183383316423877, time: 2.418088436126709
Validation Loss Energy: 2.5283703724782716, Validation Loss Force: 3.633644939361718, time: 0.15646839141845703
Test Loss Energy: 10.654868191699062, Test Loss Force: 12.0652011633797, time: 12.288496494293213

Epoch 11, Batch 100/146, Loss: 0.7449474334716797, Variance: 0.13342921435832977

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6977848907568958, Training Loss Force: 3.504198503821992, time: 2.4161996841430664
Validation Loss Energy: 3.1349016053237726, Validation Loss Force: 3.54944015246852, time: 0.15025877952575684
Test Loss Energy: 10.972512140303808, Test Loss Force: 12.505888071647902, time: 11.442437171936035

Epoch 12, Batch 100/146, Loss: 1.2886685132980347, Variance: 0.13822679221630096

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6949571397787677, Training Loss Force: 3.483965295169873, time: 2.4220056533813477
Validation Loss Energy: 2.931903200193343, Validation Loss Force: 3.6345939552616624, time: 0.1422414779663086
Test Loss Energy: 10.701294787899883, Test Loss Force: 12.19927095096382, time: 11.78954029083252

Epoch 13, Batch 100/146, Loss: 0.8295151591300964, Variance: 0.1299896538257599

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.678794632715829, Training Loss Force: 3.49203147583032, time: 2.5074384212493896
Validation Loss Energy: 2.4809422155108964, Validation Loss Force: 3.5730634190521933, time: 0.14555811882019043
Test Loss Energy: 10.954151751860097, Test Loss Force: 12.265636997937024, time: 11.407789707183838

Epoch 14, Batch 100/146, Loss: 0.8002370595932007, Variance: 0.13709712028503418

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6632256678003694, Training Loss Force: 3.5032768686013416, time: 2.559054374694824
Validation Loss Energy: 3.277820312176551, Validation Loss Force: 3.6299669646323314, time: 0.15395331382751465
Test Loss Energy: 11.118788221450695, Test Loss Force: 12.608598880766895, time: 11.35170841217041

Epoch 15, Batch 100/146, Loss: 1.5501372814178467, Variance: 0.1404494047164917

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6998619500752414, Training Loss Force: 3.4988293574376605, time: 2.7324326038360596
Validation Loss Energy: 2.547435527421755, Validation Loss Force: 3.6704695173769637, time: 0.15217924118041992
Test Loss Energy: 10.982310571329007, Test Loss Force: 12.354420605089587, time: 11.325323581695557

Epoch 16, Batch 100/146, Loss: 0.6760056018829346, Variance: 0.13477611541748047

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.710711829063505, Training Loss Force: 3.4975009819439573, time: 2.5562455654144287
Validation Loss Energy: 2.3858311680732034, Validation Loss Force: 3.5772807726760654, time: 0.15102171897888184
Test Loss Energy: 10.634345091286798, Test Loss Force: 12.332535217174154, time: 11.33547568321228

Epoch 17, Batch 100/146, Loss: 0.7552668452262878, Variance: 0.13746421039104462

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.716935095400771, Training Loss Force: 3.4792725459697067, time: 2.6843225955963135
Validation Loss Energy: 3.0066583538716585, Validation Loss Force: 3.6725519192296807, time: 0.21474623680114746
Test Loss Energy: 10.509624213520336, Test Loss Force: 12.187387366781277, time: 11.339890956878662

Epoch 18, Batch 100/146, Loss: 1.3843246698379517, Variance: 0.13593026995658875

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.682609122187432, Training Loss Force: 3.4918805000488566, time: 2.488976240158081
Validation Loss Energy: 2.9574728177815715, Validation Loss Force: 3.631530992713328, time: 0.15999674797058105
Test Loss Energy: 10.95883264901867, Test Loss Force: 12.44240267792846, time: 11.293898582458496

Epoch 19, Batch 100/146, Loss: 0.9541676044464111, Variance: 0.13382281363010406

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.7309733624510906, Training Loss Force: 3.4780080231412946, time: 2.5028820037841797
Validation Loss Energy: 2.2787166207629332, Validation Loss Force: 3.6235544643448545, time: 0.15489602088928223
Test Loss Energy: 10.674610812272897, Test Loss Force: 12.297308254140919, time: 11.494703531265259

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–â–‡â–ˆâ–„â–‡â–ƒâ–ƒâ–†â–†â–‚â–†â–ƒâ–†â–‡â–†â–‚â–â–†â–ƒ
wandb:   test_error_force â–†â–ƒâ–ˆâ–ƒâ–„â–‡â–â–‚â–„â–…â–â–‡â–ƒâ–„â–ˆâ–…â–…â–ƒâ–†â–„
wandb:          test_loss â–ˆâ–‚â–„â–„â–ƒâ–„â–‚â–â–ƒâ–„â–‚â–ƒâ–â–‚â–„â–ƒâ–‚â–â–ƒâ–‚
wandb: train_error_energy â–ˆâ–â–â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒ
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–â–ˆâ–†â–â–ˆâ–…â–ƒâ–‡â–†â–ƒâ–‡â–†â–‚â–ˆâ–ƒâ–‚â–†â–†â–
wandb:  valid_error_force â–…â–ƒâ–…â–‚â–…â–ƒâ–ƒâ–‡â–…â–„â–†â–â–†â–‚â–†â–ˆâ–ƒâ–ˆâ–†â–…
wandb:         valid_loss â–†â–â–ˆâ–…â–â–ˆâ–„â–‚â–‡â–…â–ƒâ–†â–…â–‚â–ˆâ–ƒâ–â–‡â–…â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 4645
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.67461
wandb:   test_error_force 12.29731
wandb:          test_loss 11.36313
wandb: train_error_energy 2.73097
wandb:  train_error_force 3.47801
wandb:         train_loss 1.02936
wandb: valid_error_energy 2.27872
wandb:  valid_error_force 3.62355
wandb:         valid_loss 0.89503
wandb: 
wandb: ğŸš€ View run al_63_42 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/bp0eno3l
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_052224-bp0eno3l/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8367016315460205, Uncertainty Bias: -0.141025573015213
3.4332275e-05 0.005633354
2.2348297 4.9393454
(48745, 22, 3)
Found uncertainty sample 0 after 2391 steps.
Found uncertainty sample 1 after 279 steps.
Found uncertainty sample 2 after 881 steps.
Found uncertainty sample 3 after 820 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 2203 steps.
Found uncertainty sample 6 after 53 steps.
Found uncertainty sample 7 after 109 steps.
Found uncertainty sample 8 after 568 steps.
Found uncertainty sample 9 after 114 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 669 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 249 steps.
Found uncertainty sample 14 after 3696 steps.
Found uncertainty sample 15 after 2093 steps.
Found uncertainty sample 16 after 69 steps.
Found uncertainty sample 17 after 374 steps.
Found uncertainty sample 18 after 968 steps.
Found uncertainty sample 19 after 3463 steps.
Found uncertainty sample 20 after 849 steps.
Found uncertainty sample 21 after 224 steps.
Found uncertainty sample 22 after 2548 steps.
Found uncertainty sample 23 after 1275 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 144 steps.
Found uncertainty sample 26 after 817 steps.
Found uncertainty sample 27 after 9 steps.
Found uncertainty sample 28 after 574 steps.
Found uncertainty sample 29 after 55 steps.
Found uncertainty sample 30 after 1252 steps.
Found uncertainty sample 31 after 755 steps.
Found uncertainty sample 32 after 957 steps.
Found uncertainty sample 33 after 521 steps.
Found uncertainty sample 34 after 497 steps.
Found uncertainty sample 35 after 179 steps.
Found uncertainty sample 36 after 688 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 283 steps.
Found uncertainty sample 39 after 179 steps.
Found uncertainty sample 40 after 298 steps.
Found uncertainty sample 41 after 645 steps.
Found uncertainty sample 42 after 189 steps.
Found uncertainty sample 43 after 149 steps.
Found uncertainty sample 44 after 495 steps.
Found uncertainty sample 45 after 246 steps.
Found uncertainty sample 46 after 488 steps.
Found uncertainty sample 47 after 10 steps.
Found uncertainty sample 48 after 48 steps.
Found uncertainty sample 49 after 798 steps.
Found uncertainty sample 50 after 12 steps.
Found uncertainty sample 51 after 141 steps.
Found uncertainty sample 52 after 418 steps.
Found uncertainty sample 53 after 161 steps.
Found uncertainty sample 54 after 1151 steps.
Found uncertainty sample 55 after 1662 steps.
Found uncertainty sample 56 after 592 steps.
Found uncertainty sample 57 after 21 steps.
Found uncertainty sample 58 after 202 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 2025 steps.
Found uncertainty sample 61 after 21 steps.
Found uncertainty sample 62 after 622 steps.
Found uncertainty sample 63 after 99 steps.
Found uncertainty sample 64 after 3299 steps.
Found uncertainty sample 65 after 3512 steps.
Found uncertainty sample 66 after 29 steps.
Found uncertainty sample 67 after 163 steps.
Found uncertainty sample 68 after 170 steps.
Found uncertainty sample 69 after 1359 steps.
Found uncertainty sample 70 after 26 steps.
Found uncertainty sample 71 after 103 steps.
Found uncertainty sample 72 after 414 steps.
Found uncertainty sample 73 after 96 steps.
Found uncertainty sample 74 after 75 steps.
Found uncertainty sample 75 after 1384 steps.
Found uncertainty sample 76 after 347 steps.
Found uncertainty sample 77 after 94 steps.
Found uncertainty sample 78 after 219 steps.
Found uncertainty sample 79 after 12 steps.
Found uncertainty sample 80 after 1982 steps.
Found uncertainty sample 81 after 3853 steps.
Found uncertainty sample 82 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 690 steps.
Found uncertainty sample 85 after 412 steps.
Found uncertainty sample 86 after 397 steps.
Found uncertainty sample 87 after 330 steps.
Found uncertainty sample 88 after 572 steps.
Found uncertainty sample 89 after 1274 steps.
Found uncertainty sample 90 after 1058 steps.
Found uncertainty sample 91 after 1366 steps.
Found uncertainty sample 92 after 490 steps.
Found uncertainty sample 93 after 225 steps.
Found uncertainty sample 94 after 811 steps.
Found uncertainty sample 95 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 922 steps.
Found uncertainty sample 98 after 1176 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_053643-zrduabfb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_43
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zrduabfb
Training model 43. Added 99 samples to the dataset.
Epoch 0, Batch 100/148, Loss: 0.6470334529876709, Variance: 0.11792324483394623

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.53900422891767, Training Loss Force: 3.7659569408671945, time: 2.639620304107666
Validation Loss Energy: 1.8028993391012378, Validation Loss Force: 3.604743968681264, time: 0.16025209426879883
Test Loss Energy: 10.25774988500368, Test Loss Force: 12.12314772388196, time: 11.374234676361084

Epoch 1, Batch 100/148, Loss: 0.5243508815765381, Variance: 0.1010369136929512

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6386759773547372, Training Loss Force: 3.4756658473508892, time: 2.425403118133545
Validation Loss Energy: 1.9906403430181223, Validation Loss Force: 3.6024659890953736, time: 0.13826465606689453
Test Loss Energy: 10.309856358421426, Test Loss Force: 12.102606031178919, time: 9.785695552825928

Epoch 2, Batch 100/148, Loss: 0.5352712869644165, Variance: 0.09927164763212204

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6955176096283975, Training Loss Force: 3.4834437769075075, time: 2.6313233375549316
Validation Loss Energy: 1.8455284935240532, Validation Loss Force: 3.5654149565587203, time: 0.1341876983642578
Test Loss Energy: 10.440540241304182, Test Loss Force: 12.049072488692024, time: 9.771502494812012

Epoch 3, Batch 100/148, Loss: 0.6685826778411865, Variance: 0.09600074589252472

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6656384990270277, Training Loss Force: 3.4919075831008692, time: 2.4210703372955322
Validation Loss Energy: 1.990310792682149, Validation Loss Force: 3.5747395239366404, time: 0.13196825981140137
Test Loss Energy: 10.621900814536737, Test Loss Force: 12.327130456430577, time: 9.824674844741821

Epoch 4, Batch 100/148, Loss: 0.8152292966842651, Variance: 0.09307513386011124

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6787489754892597, Training Loss Force: 3.487023565576695, time: 2.408350706100464
Validation Loss Energy: 1.9469657940667, Validation Loss Force: 3.5868903565607737, time: 0.13593459129333496
Test Loss Energy: 10.555467766300945, Test Loss Force: 12.166949332977602, time: 9.952901363372803

Epoch 5, Batch 100/148, Loss: 0.5708397626876831, Variance: 0.09191219508647919

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6625855448048439, Training Loss Force: 3.4886601762766585, time: 2.426896572113037
Validation Loss Energy: 2.114079581869894, Validation Loss Force: 3.566025292687708, time: 0.13115668296813965
Test Loss Energy: 10.549008742928857, Test Loss Force: 12.242114248014127, time: 9.845196962356567

Epoch 6, Batch 100/148, Loss: 0.9567686915397644, Variance: 0.1151248887181282

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.66998567327654, Training Loss Force: 3.48495117631294, time: 2.432666301727295
Validation Loss Energy: 2.0442742890825056, Validation Loss Force: 3.6485161951646994, time: 0.13425493240356445
Test Loss Energy: 10.434558522653177, Test Loss Force: 12.24297174106606, time: 10.004963397979736

Epoch 7, Batch 100/148, Loss: 0.6643341779708862, Variance: 0.09251514077186584

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.676274793292368, Training Loss Force: 3.4956400144786337, time: 2.424130439758301
Validation Loss Energy: 2.0850862777169286, Validation Loss Force: 3.6226350634413182, time: 0.14138007164001465
Test Loss Energy: 10.593883757204715, Test Loss Force: 12.30244427304408, time: 9.847588539123535

Epoch 8, Batch 100/148, Loss: 0.9196300506591797, Variance: 0.09314032644033432

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.684237613466169, Training Loss Force: 3.5027874578530787, time: 2.4619157314300537
Validation Loss Energy: 1.7416188603426457, Validation Loss Force: 3.6316747627585486, time: 0.1344287395477295
Test Loss Energy: 10.504598153537824, Test Loss Force: 12.384899002860026, time: 9.78892970085144

Epoch 9, Batch 100/148, Loss: 0.893481969833374, Variance: 0.09211185574531555

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6795984462448041, Training Loss Force: 3.4953798225725747, time: 2.4667534828186035
Validation Loss Energy: 1.9593584453833113, Validation Loss Force: 3.591999648560184, time: 0.13723421096801758
Test Loss Energy: 10.683932983140132, Test Loss Force: 12.424329954705149, time: 9.982717990875244

Epoch 10, Batch 100/148, Loss: 0.6488425731658936, Variance: 0.08874957263469696

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6745821670092524, Training Loss Force: 3.492244947920248, time: 2.4276764392852783
Validation Loss Energy: 1.946001747999406, Validation Loss Force: 3.5925634468376733, time: 0.13135170936584473
Test Loss Energy: 10.60726803350302, Test Loss Force: 12.47634893663565, time: 11.295036315917969

Epoch 11, Batch 100/148, Loss: 0.6094358563423157, Variance: 0.08915730565786362

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6770419306266844, Training Loss Force: 3.5054866381948027, time: 2.720564842224121
Validation Loss Energy: 2.049259405921537, Validation Loss Force: 3.616525244746206, time: 0.1676018238067627
Test Loss Energy: 10.885100888564073, Test Loss Force: 12.676874211909603, time: 12.135838985443115

Epoch 12, Batch 100/148, Loss: 0.753116250038147, Variance: 0.0884699821472168

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6683689425681965, Training Loss Force: 3.4896619843229644, time: 2.6582202911376953
Validation Loss Energy: 1.9253133720936333, Validation Loss Force: 3.6578683735167155, time: 0.15364885330200195
Test Loss Energy: 10.510961767261204, Test Loss Force: 12.48800380346531, time: 10.939886808395386

Epoch 13, Batch 100/148, Loss: 0.5714144706726074, Variance: 0.0871519073843956

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6919860739649406, Training Loss Force: 3.4984705530667215, time: 2.5585968494415283
Validation Loss Energy: 1.9840353081424664, Validation Loss Force: 3.642818017497359, time: 0.16815733909606934
Test Loss Energy: 10.571556547071477, Test Loss Force: 12.514876439315056, time: 11.907368898391724

Epoch 14, Batch 100/148, Loss: 0.7553638219833374, Variance: 0.09142957627773285

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.672753946500176, Training Loss Force: 3.490536585097052, time: 2.481060266494751
Validation Loss Energy: 1.943107902667404, Validation Loss Force: 3.5810763167145025, time: 0.1531982421875
Test Loss Energy: 10.440421044528497, Test Loss Force: 12.320221851228983, time: 11.60989260673523

Epoch 15, Batch 100/148, Loss: 0.552140474319458, Variance: 0.09094004333019257

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6623162971853436, Training Loss Force: 3.495984775524508, time: 2.3879971504211426
Validation Loss Energy: 1.8193951518813842, Validation Loss Force: 3.6276786680426802, time: 0.1530299186706543
Test Loss Energy: 10.308944995757479, Test Loss Force: 12.369196365362745, time: 11.785776853561401

Epoch 16, Batch 100/148, Loss: 0.7511411309242249, Variance: 0.0881633311510086

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6819598502805866, Training Loss Force: 3.5005394939677585, time: 2.4360392093658447
Validation Loss Energy: 1.982713618315288, Validation Loss Force: 3.644217344633418, time: 0.1555042266845703
Test Loss Energy: 10.50622090098815, Test Loss Force: 12.390062007864708, time: 11.649667978286743

Epoch 17, Batch 100/148, Loss: 0.6258836984634399, Variance: 0.08892568945884705

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6736212691526866, Training Loss Force: 3.4910165559637063, time: 2.4146525859832764
Validation Loss Energy: 1.959885577474771, Validation Loss Force: 3.5936884455859492, time: 0.14424490928649902
Test Loss Energy: 10.417535011782247, Test Loss Force: 12.415082502743152, time: 11.486403942108154

Epoch 18, Batch 100/148, Loss: 0.4972493648529053, Variance: 0.09209837019443512

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6952907925173666, Training Loss Force: 3.501975806248353, time: 2.771242380142212
Validation Loss Energy: 2.0752080063961342, Validation Loss Force: 3.604766384041954, time: 0.1465315818786621
Test Loss Energy: 10.733160855124517, Test Loss Force: 12.53390148699871, time: 11.468020915985107

Epoch 19, Batch 100/148, Loss: 0.5603634715080261, Variance: 0.08821950852870941

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6803414834053771, Training Loss Force: 3.492791793252221, time: 2.49515962600708
Validation Loss Energy: 1.906571829642232, Validation Loss Force: 3.6252227758091937, time: 0.15819931030273438
Test Loss Energy: 10.627678978614158, Test Loss Force: 12.472711609323358, time: 11.398711919784546

wandb: - 0.039 MB of 0.049 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–ƒâ–…â–„â–„â–ƒâ–…â–„â–†â–…â–ˆâ–„â–…â–ƒâ–‚â–„â–ƒâ–†â–…
wandb:   test_error_force â–‚â–‚â–â–„â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–ˆâ–†â–†â–„â–…â–…â–…â–†â–†
wandb:          test_loss â–â–ƒâ–…â–‡â–†â–†â–†â–†â–†â–†â–†â–ˆâ–†â–†â–…â–ˆâ–‡â–†â–ˆâ–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–â–â–‚â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–†â–ƒâ–†â–…â–ˆâ–‡â–‡â–â–…â–…â–‡â–„â–†â–…â–‚â–†â–…â–‡â–„
wandb:  valid_error_force â–„â–„â–â–‚â–ƒâ–â–‡â–…â–†â–ƒâ–ƒâ–…â–ˆâ–‡â–‚â–†â–‡â–ƒâ–„â–†
wandb:         valid_loss â–‚â–…â–‚â–…â–…â–ˆâ–ˆâ–‡â–â–„â–„â–†â–…â–†â–„â–ƒâ–‡â–…â–ˆâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 4734
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.62768
wandb:   test_error_force 12.47271
wandb:          test_loss 14.91053
wandb: train_error_energy 1.68034
wandb:  train_error_force 3.49279
wandb:         train_loss 0.58848
wandb: valid_error_energy 1.90657
wandb:  valid_error_force 3.62522
wandb:         valid_loss 0.74113
wandb: 
wandb: ğŸš€ View run al_63_43 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zrduabfb
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_053643-zrduabfb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.709197998046875, Uncertainty Bias: -0.08705738186836243
0.000102996826 0.0114097595
2.2286098 5.287783
(48745, 22, 3)
Found uncertainty sample 0 after 12 steps.
Found uncertainty sample 1 after 117 steps.
Found uncertainty sample 2 after 1426 steps.
Found uncertainty sample 3 after 1120 steps.
Found uncertainty sample 4 after 1443 steps.
Found uncertainty sample 5 after 1724 steps.
Found uncertainty sample 6 after 929 steps.
Found uncertainty sample 7 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1729 steps.
Found uncertainty sample 10 after 127 steps.
Found uncertainty sample 11 after 69 steps.
Found uncertainty sample 12 after 166 steps.
Found uncertainty sample 13 after 8 steps.
Found uncertainty sample 14 after 430 steps.
Found uncertainty sample 15 after 819 steps.
Found uncertainty sample 16 after 844 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 17 steps.
Found uncertainty sample 19 after 17 steps.
Found uncertainty sample 20 after 424 steps.
Found uncertainty sample 21 after 3637 steps.
Found uncertainty sample 22 after 117 steps.
Found uncertainty sample 23 after 54 steps.
Found uncertainty sample 24 after 942 steps.
Found uncertainty sample 25 after 108 steps.
Found uncertainty sample 26 after 540 steps.
Found uncertainty sample 27 after 150 steps.
Found uncertainty sample 28 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 448 steps.
Found uncertainty sample 31 after 147 steps.
Found uncertainty sample 32 after 2357 steps.
Found uncertainty sample 33 after 235 steps.
Found uncertainty sample 34 after 266 steps.
Found uncertainty sample 35 after 209 steps.
Found uncertainty sample 36 after 891 steps.
Found uncertainty sample 37 after 252 steps.
Found uncertainty sample 38 after 217 steps.
Found uncertainty sample 39 after 591 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 740 steps.
Found uncertainty sample 42 after 1068 steps.
Found uncertainty sample 43 after 20 steps.
Found uncertainty sample 44 after 1703 steps.
Found uncertainty sample 45 after 1021 steps.
Found uncertainty sample 46 after 58 steps.
Found uncertainty sample 47 after 57 steps.
Found uncertainty sample 48 after 20 steps.
Found uncertainty sample 49 after 104 steps.
Found uncertainty sample 50 after 126 steps.
Found uncertainty sample 51 after 899 steps.
Found uncertainty sample 52 after 230 steps.
Found uncertainty sample 53 after 86 steps.
Found uncertainty sample 54 after 63 steps.
Found uncertainty sample 55 after 561 steps.
Found uncertainty sample 56 after 1761 steps.
Found uncertainty sample 57 after 154 steps.
Found uncertainty sample 58 after 3 steps.
Found uncertainty sample 59 after 3019 steps.
Found uncertainty sample 60 after 704 steps.
Found uncertainty sample 61 after 210 steps.
Found uncertainty sample 62 after 19 steps.
Found uncertainty sample 63 after 1690 steps.
Found uncertainty sample 64 after 133 steps.
Found uncertainty sample 65 after 284 steps.
Found uncertainty sample 66 after 3048 steps.
Found uncertainty sample 67 after 770 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 2788 steps.
Found uncertainty sample 70 after 64 steps.
Found uncertainty sample 71 after 1426 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 127 steps.
Found uncertainty sample 75 after 1211 steps.
Found uncertainty sample 76 after 1696 steps.
Found uncertainty sample 77 after 14 steps.
Found uncertainty sample 78 after 50 steps.
Found uncertainty sample 79 after 452 steps.
Found uncertainty sample 80 after 107 steps.
Found uncertainty sample 81 after 1391 steps.
Found uncertainty sample 82 after 15 steps.
Found uncertainty sample 83 after 35 steps.
Found uncertainty sample 84 after 4 steps.
Found uncertainty sample 85 after 59 steps.
Found uncertainty sample 86 after 57 steps.
Found uncertainty sample 87 after 144 steps.
Found uncertainty sample 88 after 176 steps.
Found uncertainty sample 89 after 1217 steps.
Found uncertainty sample 90 after 1083 steps.
Found uncertainty sample 91 after 367 steps.
Found uncertainty sample 92 after 208 steps.
Found uncertainty sample 93 after 59 steps.
Found uncertainty sample 94 after 57 steps.
Found uncertainty sample 95 after 122 steps.
Found uncertainty sample 96 after 49 steps.
Found uncertainty sample 97 after 307 steps.
Found uncertainty sample 98 after 1221 steps.
Found uncertainty sample 99 after 413 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_054955-t6b36a00
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_44
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t6b36a00
Training model 44. Added 99 samples to the dataset.
Epoch 0, Batch 100/151, Loss: 2.0533151626586914, Variance: 0.10600627213716507

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.5934103433739537, Training Loss Force: 4.202131108720477, time: 2.7415781021118164
Validation Loss Energy: 3.016308166648731, Validation Loss Force: 3.6247809260006245, time: 0.150115966796875
Test Loss Energy: 11.089030898654807, Test Loss Force: 12.32125230341437, time: 11.537285089492798

Epoch 1, Batch 100/151, Loss: 0.5998788475990295, Variance: 0.1164463460445404

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6574765388819603, Training Loss Force: 3.4813856339355986, time: 2.7121682167053223
Validation Loss Energy: 1.9191604669187836, Validation Loss Force: 3.6405341742906265, time: 0.17097997665405273
Test Loss Energy: 10.359027959231282, Test Loss Force: 12.104274137849819, time: 11.046813249588013

Epoch 2, Batch 100/151, Loss: 1.0882235765457153, Variance: 0.12387791275978088

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6528812943353124, Training Loss Force: 3.487284701445297, time: 2.8483190536499023
Validation Loss Energy: 2.97531722624403, Validation Loss Force: 3.5987319759642618, time: 0.15271568298339844
Test Loss Energy: 10.774739288834931, Test Loss Force: 12.176150623643775, time: 12.111650943756104

Epoch 3, Batch 100/151, Loss: 1.3561811447143555, Variance: 0.12851497530937195

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6831698191966065, Training Loss Force: 3.4847338037095485, time: 2.6411831378936768
Validation Loss Energy: 1.6929851726192169, Validation Loss Force: 3.612283917077622, time: 0.14612770080566406
Test Loss Energy: 10.137941732121877, Test Loss Force: 11.990789033269397, time: 10.866012573242188

Epoch 4, Batch 100/151, Loss: 0.7222229242324829, Variance: 0.12754005193710327

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6878401847187456, Training Loss Force: 3.4758215035856503, time: 2.7497425079345703
Validation Loss Energy: 2.9267524175097415, Validation Loss Force: 3.5770595879911102, time: 0.159959077835083
Test Loss Energy: 10.750081252693779, Test Loss Force: 12.076957702573521, time: 10.684305429458618

Epoch 5, Batch 100/151, Loss: 0.8552365303039551, Variance: 0.1210174560546875

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.693061495642375, Training Loss Force: 3.478142739479512, time: 2.563457727432251
Validation Loss Energy: 3.8154040480078257, Validation Loss Force: 3.5966398508921738, time: 0.1521306037902832
Test Loss Energy: 11.229789434221185, Test Loss Force: 12.115635006795204, time: 10.836422204971313

Epoch 6, Batch 100/151, Loss: 1.4024702310562134, Variance: 0.12290147691965103

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6635520141792313, Training Loss Force: 3.4857564419921387, time: 2.531576633453369
Validation Loss Energy: 2.38343636140875, Validation Loss Force: 3.5750229075363578, time: 0.15317153930664062
Test Loss Energy: 10.603904325767827, Test Loss Force: 12.03129512967885, time: 11.091760635375977

Epoch 7, Batch 100/151, Loss: 0.6820862889289856, Variance: 0.12804853916168213

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.720976877630095, Training Loss Force: 3.487070682479568, time: 2.516446590423584
Validation Loss Energy: 2.0315233631100966, Validation Loss Force: 3.567598821318931, time: 0.14146780967712402
Test Loss Energy: 10.451429037670517, Test Loss Force: 12.22446435834344, time: 11.56637454032898

Epoch 8, Batch 100/151, Loss: 0.9418535232543945, Variance: 0.1278296411037445

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.656813324729335, Training Loss Force: 3.4922180543437347, time: 2.634467363357544
Validation Loss Energy: 2.99018803670872, Validation Loss Force: 3.6240127916702227, time: 0.14417290687561035
Test Loss Energy: 10.79514534688293, Test Loss Force: 12.287586043580436, time: 10.834964513778687

Epoch 9, Batch 100/151, Loss: 1.4628775119781494, Variance: 0.13354483246803284

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7069553757049953, Training Loss Force: 3.4947269428170395, time: 2.6727218627929688
Validation Loss Energy: 1.5026804303791683, Validation Loss Force: 3.5703392104978646, time: 0.15124249458312988
Test Loss Energy: 10.343079653546651, Test Loss Force: 12.252563204034637, time: 10.838146209716797

Epoch 10, Batch 100/151, Loss: 0.6244293451309204, Variance: 0.132405087351799

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.7072619454054045, Training Loss Force: 3.4663145161074445, time: 2.6851906776428223
Validation Loss Energy: 2.9167405447013905, Validation Loss Force: 3.551001582742814, time: 0.14124035835266113
Test Loss Energy: 10.811839615190944, Test Loss Force: 11.949787641404523, time: 11.286000490188599

Epoch 11, Batch 100/151, Loss: 0.8846462368965149, Variance: 0.12661774456501007

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6763213516458224, Training Loss Force: 3.4812093197870375, time: 2.8648970127105713
Validation Loss Energy: 3.9640873621564796, Validation Loss Force: 3.597911069046275, time: 0.16988253593444824
Test Loss Energy: 11.423213914007047, Test Loss Force: 11.926832154537653, time: 11.961454391479492

Epoch 12, Batch 100/151, Loss: 1.266039252281189, Variance: 0.12817823886871338

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.7286538093323847, Training Loss Force: 3.482701741304742, time: 2.7233965396881104
Validation Loss Energy: 2.196013440366823, Validation Loss Force: 3.572627210195321, time: 0.14160561561584473
Test Loss Energy: 10.581997668868631, Test Loss Force: 12.087030205508952, time: 11.402954339981079

Epoch 13, Batch 100/151, Loss: 0.8260358572006226, Variance: 0.13350394368171692

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.7560748075682566, Training Loss Force: 3.4747577201738564, time: 2.762622594833374
Validation Loss Energy: 2.3166859408573934, Validation Loss Force: 3.5575148037072992, time: 0.16311430931091309
Test Loss Energy: 10.764349341303904, Test Loss Force: 12.290148408194295, time: 11.079120874404907

Epoch 14, Batch 100/151, Loss: 1.0822267532348633, Variance: 0.13547849655151367

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6885350094109697, Training Loss Force: 3.4866091807671267, time: 2.4896178245544434
Validation Loss Energy: 3.2748269708478435, Validation Loss Force: 3.6844474987895324, time: 0.12949204444885254
Test Loss Energy: 10.817267617463804, Test Loss Force: 12.203401451962712, time: 11.52557921409607

Epoch 15, Batch 100/151, Loss: 1.2275387048721313, Variance: 0.13608095049858093

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.7054849962207927, Training Loss Force: 3.4845558674678703, time: 2.6806869506835938
Validation Loss Energy: 1.6333542640051826, Validation Loss Force: 3.644120038216647, time: 0.17786812782287598
Test Loss Energy: 10.41962210484252, Test Loss Force: 12.272636820545179, time: 10.66817569732666

Epoch 16, Batch 100/151, Loss: 0.8278523683547974, Variance: 0.13304173946380615

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.671496556591859, Training Loss Force: 3.502677664667123, time: 2.4312124252319336
Validation Loss Energy: 2.7432245736144805, Validation Loss Force: 3.551013903768889, time: 0.13098883628845215
Test Loss Energy: 10.919450143451181, Test Loss Force: 12.226198710833966, time: 9.765872478485107

Epoch 17, Batch 100/151, Loss: 0.786493718624115, Variance: 0.12705323100090027

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.679760584252675, Training Loss Force: 3.4761974192414007, time: 2.46492338180542
Validation Loss Energy: 4.064890723640669, Validation Loss Force: 3.560590013058659, time: 0.1316206455230713
Test Loss Energy: 11.123741008128707, Test Loss Force: 11.94766624097448, time: 9.881383419036865

Epoch 18, Batch 100/151, Loss: 1.2674413919448853, Variance: 0.13248030841350555

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.703237425473851, Training Loss Force: 3.479204470206198, time: 2.4907188415527344
Validation Loss Energy: 2.1931757447711195, Validation Loss Force: 3.6108104411159694, time: 0.13148784637451172
Test Loss Energy: 10.567476977850847, Test Loss Force: 12.260463663881913, time: 9.778316974639893

Epoch 19, Batch 100/151, Loss: 0.6205703020095825, Variance: 0.12931744754314423

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.7018552487774627, Training Loss Force: 3.474117353202771, time: 2.4937407970428467
Validation Loss Energy: 2.0344278594495786, Validation Loss Force: 3.6222782732464935, time: 0.13061070442199707
Test Loss Energy: 10.54542322874495, Test Loss Force: 12.306167226552343, time: 9.920129537582397

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–‚â–„â–â–„â–‡â–„â–ƒâ–…â–‚â–…â–ˆâ–ƒâ–„â–…â–ƒâ–…â–†â–ƒâ–ƒ
wandb:   test_error_force â–ˆâ–„â–…â–‚â–„â–„â–ƒâ–†â–‡â–‡â–â–â–„â–‡â–†â–‡â–†â–â–‡â–ˆ
wandb:          test_loss â–ˆâ–„â–…â–‚â–‚â–„â–ƒâ–‚â–„â–‚â–‚â–„â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–‚â–…â–‚â–…â–‡â–ƒâ–‚â–…â–â–…â–ˆâ–ƒâ–ƒâ–†â–â–„â–ˆâ–ƒâ–‚
wandb:  valid_error_force â–…â–†â–„â–„â–‚â–ƒâ–‚â–‚â–…â–‚â–â–ƒâ–‚â–â–ˆâ–†â–â–‚â–„â–…
wandb:         valid_loss â–…â–‚â–…â–â–„â–‡â–ƒâ–‚â–…â–â–„â–ˆâ–‚â–ƒâ–†â–â–„â–ˆâ–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 4823
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.54542
wandb:   test_error_force 12.30617
wandb:          test_loss 11.82494
wandb: train_error_energy 2.70186
wandb:  train_error_force 3.47412
wandb:         train_loss 1.01264
wandb: valid_error_energy 2.03443
wandb:  valid_error_force 3.62228
wandb:         valid_loss 0.85256
wandb: 
wandb: ğŸš€ View run al_63_44 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t6b36a00
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_054955-t6b36a00/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.833993673324585, Uncertainty Bias: -0.12465253472328186
6.484985e-05 0.038157463
2.16741 4.9351525
(48745, 22, 3)
Found uncertainty sample 0 after 283 steps.
Found uncertainty sample 1 after 7 steps.
Found uncertainty sample 2 after 376 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 161 steps.
Found uncertainty sample 5 after 155 steps.
Found uncertainty sample 6 after 2773 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 2305 steps.
Found uncertainty sample 9 after 568 steps.
Found uncertainty sample 10 after 89 steps.
Found uncertainty sample 11 after 1152 steps.
Found uncertainty sample 12 after 1087 steps.
Found uncertainty sample 13 after 1369 steps.
Found uncertainty sample 14 after 1591 steps.
Found uncertainty sample 15 after 2678 steps.
Found uncertainty sample 16 after 403 steps.
Found uncertainty sample 17 after 592 steps.
Found uncertainty sample 18 after 23 steps.
Found uncertainty sample 19 after 545 steps.
Found uncertainty sample 20 after 61 steps.
Found uncertainty sample 21 after 567 steps.
Found uncertainty sample 22 after 9 steps.
Found uncertainty sample 23 after 104 steps.
Found uncertainty sample 24 after 194 steps.
Found uncertainty sample 25 after 1149 steps.
Found uncertainty sample 26 after 801 steps.
Found uncertainty sample 27 after 1190 steps.
Found uncertainty sample 28 after 110 steps.
Found uncertainty sample 29 after 1144 steps.
Found uncertainty sample 30 after 705 steps.
Found uncertainty sample 31 after 35 steps.
Found uncertainty sample 32 after 449 steps.
Found uncertainty sample 33 after 2012 steps.
Found uncertainty sample 34 after 1088 steps.
Found uncertainty sample 35 after 183 steps.
Found uncertainty sample 36 after 108 steps.
Found uncertainty sample 37 after 405 steps.
Found uncertainty sample 38 after 717 steps.
Found uncertainty sample 39 after 1389 steps.
Found uncertainty sample 40 after 1774 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 812 steps.
Found uncertainty sample 43 after 46 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 140 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 118 steps.
Found uncertainty sample 48 after 146 steps.
Found uncertainty sample 49 after 974 steps.
Found uncertainty sample 50 after 53 steps.
Found uncertainty sample 51 after 420 steps.
Found uncertainty sample 52 after 188 steps.
Found uncertainty sample 53 after 1761 steps.
Found uncertainty sample 54 after 231 steps.
Found uncertainty sample 55 after 653 steps.
Found uncertainty sample 56 after 3943 steps.
Found uncertainty sample 57 after 1749 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 2296 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 99 steps.
Found uncertainty sample 62 after 772 steps.
Found uncertainty sample 63 after 1141 steps.
Found uncertainty sample 64 after 1106 steps.
Found uncertainty sample 65 after 15 steps.
Found uncertainty sample 66 after 381 steps.
Found uncertainty sample 67 after 167 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1566 steps.
Found uncertainty sample 70 after 2092 steps.
Found uncertainty sample 71 after 2435 steps.
Found uncertainty sample 72 after 680 steps.
Found uncertainty sample 73 after 893 steps.
Found uncertainty sample 74 after 27 steps.
Found uncertainty sample 75 after 2360 steps.
Found uncertainty sample 76 after 219 steps.
Found uncertainty sample 77 after 875 steps.
Found uncertainty sample 78 after 976 steps.
Found uncertainty sample 79 after 1649 steps.
Found uncertainty sample 80 after 127 steps.
Found uncertainty sample 81 after 147 steps.
Found uncertainty sample 82 after 1175 steps.
Found uncertainty sample 83 after 77 steps.
Found uncertainty sample 84 after 1403 steps.
Found uncertainty sample 85 after 421 steps.
Found uncertainty sample 86 after 1861 steps.
Found uncertainty sample 87 after 123 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 187 steps.
Found uncertainty sample 90 after 54 steps.
Found uncertainty sample 91 after 454 steps.
Found uncertainty sample 92 after 8 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 1138 steps.
Found uncertainty sample 95 after 99 steps.
Found uncertainty sample 96 after 638 steps.
Found uncertainty sample 97 after 516 steps.
Found uncertainty sample 98 after 234 steps.
Found uncertainty sample 99 after 1296 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_060451-uw5r09c8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_45
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/uw5r09c8
Training model 45. Added 98 samples to the dataset.
Epoch 0, Batch 100/154, Loss: 0.7602089047431946, Variance: 0.12331712245941162

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.385187961649088, Training Loss Force: 4.099113414131653, time: 2.717103958129883
Validation Loss Energy: 2.8618056912634486, Validation Loss Force: 4.153194181502961, time: 0.14914917945861816
Test Loss Energy: 10.802034091395674, Test Loss Force: 12.733146818052814, time: 11.572445631027222

Epoch 1, Batch 100/154, Loss: 1.2740082740783691, Variance: 0.11965961754322052

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.624398644734665, Training Loss Force: 3.478688760650605, time: 2.7397782802581787
Validation Loss Energy: 1.6612863270086562, Validation Loss Force: 3.5439952980661964, time: 0.16562366485595703
Test Loss Energy: 10.12503906936749, Test Loss Force: 12.022374290219068, time: 11.643994092941284

Epoch 2, Batch 100/154, Loss: 0.7502822279930115, Variance: 0.13180473446846008

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.649822197387763, Training Loss Force: 3.4438340382479415, time: 2.7309646606445312
Validation Loss Energy: 2.4525310427284346, Validation Loss Force: 3.551517804046123, time: 0.16555356979370117
Test Loss Energy: 10.531875678838132, Test Loss Force: 12.245336939728944, time: 11.55238389968872

Epoch 3, Batch 100/154, Loss: 0.8796353340148926, Variance: 0.1301850974559784

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.644852376039382, Training Loss Force: 3.4535435523823863, time: 2.6813511848449707
Validation Loss Energy: 3.584491022290353, Validation Loss Force: 3.5804726606598916, time: 0.16298556327819824
Test Loss Energy: 10.89624754745946, Test Loss Force: 11.976097075869989, time: 11.446009874343872

Epoch 4, Batch 100/154, Loss: 0.9585328698158264, Variance: 0.12701785564422607

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.661094422385075, Training Loss Force: 3.4532330445961, time: 2.8468868732452393
Validation Loss Energy: 1.4344263993084099, Validation Loss Force: 3.5556054157809682, time: 0.16261792182922363
Test Loss Energy: 10.399024057026836, Test Loss Force: 12.2550650352834, time: 11.55174446105957

Epoch 5, Batch 100/154, Loss: 0.6669120788574219, Variance: 0.13053308427333832

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6592834498835334, Training Loss Force: 3.4541119847807376, time: 2.494677782058716
Validation Loss Energy: 2.0947544978100643, Validation Loss Force: 3.566048747171079, time: 0.15390968322753906
Test Loss Energy: 10.39735156692144, Test Loss Force: 12.121947325008527, time: 11.53864073753357

Epoch 6, Batch 100/154, Loss: 1.1969578266143799, Variance: 0.1359248012304306

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.668415809206493, Training Loss Force: 3.4596870881022643, time: 2.772172689437866
Validation Loss Energy: 3.735887782651283, Validation Loss Force: 3.565397811034483, time: 0.14994597434997559
Test Loss Energy: 11.143233367235434, Test Loss Force: 11.981897721370494, time: 11.680335998535156

Epoch 7, Batch 100/154, Loss: 0.9974149465560913, Variance: 0.12894849479198456

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.7110706115483576, Training Loss Force: 3.4538983973549753, time: 2.5814247131347656
Validation Loss Energy: 1.5597948065289755, Validation Loss Force: 3.5641652630887246, time: 0.15314984321594238
Test Loss Energy: 10.469045813757846, Test Loss Force: 12.238396340118763, time: 11.562379598617554

Epoch 8, Batch 100/154, Loss: 0.9673541784286499, Variance: 0.132396399974823

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6831276392026373, Training Loss Force: 3.457302220578779, time: 2.6587131023406982
Validation Loss Energy: 2.1866309790109617, Validation Loss Force: 3.582996466982886, time: 0.1558668613433838
Test Loss Energy: 10.299501725517455, Test Loss Force: 12.049795270408096, time: 11.605620622634888

Epoch 9, Batch 100/154, Loss: 1.1104365587234497, Variance: 0.13547426462173462

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.671446812707788, Training Loss Force: 3.46455101942748, time: 2.5356268882751465
Validation Loss Energy: 3.9897560675146457, Validation Loss Force: 3.5490959801919644, time: 0.15485191345214844
Test Loss Energy: 10.806089882815781, Test Loss Force: 11.91503352504622, time: 11.402724027633667

Epoch 10, Batch 100/154, Loss: 1.3276548385620117, Variance: 0.12911811470985413

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.703059924190601, Training Loss Force: 3.4644269218639243, time: 2.655546188354492
Validation Loss Energy: 1.7903121491741345, Validation Loss Force: 3.550804594771717, time: 0.1632859706878662
Test Loss Energy: 10.336372663015721, Test Loss Force: 12.138709298726589, time: 11.74553918838501

Epoch 11, Batch 100/154, Loss: 0.7764605283737183, Variance: 0.1372607797384262

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.716623998121591, Training Loss Force: 3.453713843672397, time: 2.7025930881500244
Validation Loss Energy: 2.0368302368649935, Validation Loss Force: 3.619033093113742, time: 0.15777015686035156
Test Loss Energy: 10.301873415393981, Test Loss Force: 12.10599732190884, time: 11.530998229980469

Epoch 12, Batch 100/154, Loss: 0.9036785364151001, Variance: 0.1356711983680725

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6881964471077073, Training Loss Force: 3.4800272769393046, time: 2.606428384780884
Validation Loss Energy: 3.9048734015181945, Validation Loss Force: 3.5571217184416875, time: 0.16135621070861816
Test Loss Energy: 11.158358198117757, Test Loss Force: 11.974331232599518, time: 11.671067953109741

Epoch 13, Batch 100/154, Loss: 1.387500286102295, Variance: 0.12984409928321838

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6659968017314553, Training Loss Force: 3.4713932894813757, time: 2.823484420776367
Validation Loss Energy: 1.488777636914875, Validation Loss Force: 3.665622974969748, time: 0.15757131576538086
Test Loss Energy: 10.424481892760797, Test Loss Force: 12.12340460246987, time: 11.47146224975586

Epoch 14, Batch 100/154, Loss: 0.8693652153015137, Variance: 0.13394328951835632

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.666307621109955, Training Loss Force: 3.479199602710759, time: 2.6398892402648926
Validation Loss Energy: 2.1989451023421087, Validation Loss Force: 3.6990396759296176, time: 0.15395832061767578
Test Loss Energy: 10.538482190791315, Test Loss Force: 12.326169966657677, time: 11.728413105010986

Epoch 15, Batch 100/154, Loss: 0.9293320178985596, Variance: 0.13576188683509827

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6873885072936954, Training Loss Force: 3.4864798716501166, time: 2.707672595977783
Validation Loss Energy: 3.999244742053115, Validation Loss Force: 3.6090229274355092, time: 0.14959287643432617
Test Loss Energy: 11.132504962602592, Test Loss Force: 11.797039195389702, time: 11.46627688407898

Epoch 16, Batch 100/154, Loss: 1.1353809833526611, Variance: 0.1345457285642624

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6903761098312327, Training Loss Force: 3.4707977666342056, time: 2.5939414501190186
Validation Loss Energy: 1.5734939690593182, Validation Loss Force: 3.6770274206167315, time: 0.149674654006958
Test Loss Energy: 10.543234206842966, Test Loss Force: 12.219286580231865, time: 11.868793725967407

Epoch 17, Batch 100/154, Loss: 0.7818955183029175, Variance: 0.13499537110328674

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6872413497374525, Training Loss Force: 3.47281001758463, time: 2.6012444496154785
Validation Loss Energy: 2.182109682002898, Validation Loss Force: 3.6162540337264653, time: 0.15081167221069336
Test Loss Energy: 10.385826202601761, Test Loss Force: 12.05705587938938, time: 11.48821210861206

Epoch 18, Batch 100/154, Loss: 1.0686078071594238, Variance: 0.14186829328536987

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.690829442523788, Training Loss Force: 3.4714869522212126, time: 2.660487174987793
Validation Loss Energy: 4.016643812223718, Validation Loss Force: 3.5963750230546165, time: 0.1484229564666748
Test Loss Energy: 11.294210761153897, Test Loss Force: 11.80903129755539, time: 11.700232982635498

Epoch 19, Batch 100/154, Loss: 1.2507593631744385, Variance: 0.13110116124153137

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6839597682600043, Training Loss Force: 3.4626197911281023, time: 2.6280767917633057
Validation Loss Energy: 1.6117555795193328, Validation Loss Force: 3.542918098961246, time: 0.14762210845947266
Test Loss Energy: 10.111749591834998, Test Loss Force: 11.995334715463459, time: 12.280372619628906

wandb: - 0.039 MB of 0.059 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–â–ƒâ–†â–ƒâ–ƒâ–‡â–ƒâ–‚â–…â–‚â–‚â–‡â–ƒâ–„â–‡â–„â–ƒâ–ˆâ–
wandb:   test_error_force â–ˆâ–ƒâ–„â–‚â–„â–ƒâ–‚â–„â–ƒâ–‚â–„â–ƒâ–‚â–ƒâ–…â–â–„â–ƒâ–â–‚
wandb:          test_loss â–ˆâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–ƒâ–‚â–â–‚â–‚â–â–ƒâ–
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–‚â–„â–‡â–â–ƒâ–‡â–â–ƒâ–ˆâ–‚â–ƒâ–ˆâ–â–ƒâ–ˆâ–â–ƒâ–ˆâ–
wandb:  valid_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–
wandb:         valid_loss â–‡â–â–ƒâ–†â–â–‚â–‡â–â–ƒâ–ˆâ–‚â–‚â–‡â–â–ƒâ–ˆâ–‚â–ƒâ–ˆâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 4911
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.11175
wandb:   test_error_force 11.99533
wandb:          test_loss 11.1469
wandb: train_error_energy 2.68396
wandb:  train_error_force 3.46262
wandb:         train_loss 1.00574
wandb: valid_error_energy 1.61176
wandb:  valid_error_force 3.54292
wandb:         valid_loss 0.70453
wandb: 
wandb: ğŸš€ View run al_63_45 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/uw5r09c8
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_060451-uw5r09c8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.995635509490967, Uncertainty Bias: -0.1541818231344223
4.196167e-05 0.014539719
2.0719624 4.8962684
(48745, 22, 3)
Found uncertainty sample 0 after 33 steps.
Found uncertainty sample 1 after 170 steps.
Found uncertainty sample 2 after 1620 steps.
Found uncertainty sample 3 after 596 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 620 steps.
Found uncertainty sample 6 after 424 steps.
Found uncertainty sample 7 after 1068 steps.
Found uncertainty sample 8 after 593 steps.
Found uncertainty sample 9 after 3199 steps.
Found uncertainty sample 10 after 2487 steps.
Found uncertainty sample 11 after 1507 steps.
Found uncertainty sample 12 after 1080 steps.
Found uncertainty sample 13 after 138 steps.
Found uncertainty sample 14 after 210 steps.
Found uncertainty sample 15 after 684 steps.
Found uncertainty sample 16 after 1125 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1199 steps.
Found uncertainty sample 19 after 2347 steps.
Found uncertainty sample 20 after 493 steps.
Found uncertainty sample 21 after 979 steps.
Found uncertainty sample 22 after 52 steps.
Found uncertainty sample 23 after 289 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1854 steps.
Found uncertainty sample 26 after 3523 steps.
Found uncertainty sample 27 after 626 steps.
Found uncertainty sample 28 after 1201 steps.
Found uncertainty sample 29 after 3377 steps.
Found uncertainty sample 30 after 126 steps.
Found uncertainty sample 31 after 3109 steps.
Found uncertainty sample 32 after 452 steps.
Found uncertainty sample 33 after 290 steps.
Found uncertainty sample 34 after 45 steps.
Found uncertainty sample 35 after 469 steps.
Found uncertainty sample 36 after 1637 steps.
Found uncertainty sample 37 after 3669 steps.
Found uncertainty sample 38 after 1284 steps.
Found uncertainty sample 39 after 297 steps.
Found uncertainty sample 40 after 1240 steps.
Found uncertainty sample 41 after 24 steps.
Found uncertainty sample 42 after 378 steps.
Found uncertainty sample 43 after 3235 steps.
Found uncertainty sample 44 after 315 steps.
Found uncertainty sample 45 after 138 steps.
Found uncertainty sample 46 after 1031 steps.
Found uncertainty sample 47 after 223 steps.
Found uncertainty sample 48 after 642 steps.
Found uncertainty sample 49 after 2149 steps.
Found uncertainty sample 50 after 1255 steps.
Found uncertainty sample 51 after 1357 steps.
Found uncertainty sample 52 after 80 steps.
Found uncertainty sample 53 after 23 steps.
Found uncertainty sample 54 after 1353 steps.
Found uncertainty sample 55 after 978 steps.
Found uncertainty sample 56 after 1429 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 424 steps.
Found uncertainty sample 60 after 8 steps.
Found uncertainty sample 61 after 944 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1122 steps.
Found uncertainty sample 64 after 202 steps.
Found uncertainty sample 65 after 383 steps.
Found uncertainty sample 66 after 716 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 244 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 2182 steps.
Found uncertainty sample 71 after 156 steps.
Found uncertainty sample 72 after 291 steps.
Found uncertainty sample 73 after 1308 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 945 steps.
Found uncertainty sample 76 after 2245 steps.
Found uncertainty sample 77 after 62 steps.
Found uncertainty sample 78 after 396 steps.
Found uncertainty sample 79 after 105 steps.
Found uncertainty sample 80 after 691 steps.
Found uncertainty sample 81 after 249 steps.
Found uncertainty sample 82 after 2491 steps.
Found uncertainty sample 83 after 2215 steps.
Found uncertainty sample 84 after 230 steps.
Found uncertainty sample 85 after 619 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 330 steps.
Found uncertainty sample 88 after 943 steps.
Found uncertainty sample 89 after 434 steps.
Found uncertainty sample 90 after 1885 steps.
Found uncertainty sample 91 after 1138 steps.
Found uncertainty sample 92 after 2157 steps.
Found uncertainty sample 93 after 2142 steps.
Found uncertainty sample 94 after 1917 steps.
Found uncertainty sample 95 after 686 steps.
Found uncertainty sample 96 after 758 steps.
Found uncertainty sample 97 after 736 steps.
Found uncertainty sample 98 after 310 steps.
Found uncertainty sample 99 after 11 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_062129-t059xxwm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_46
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t059xxwm
Training model 46. Added 99 samples to the dataset.
Epoch 0, Batch 100/157, Loss: 0.7649059891700745, Variance: 0.11052429676055908

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.1310383793535217, Training Loss Force: 3.6017190910665327, time: 2.633847713470459
Validation Loss Energy: 1.6613484145888455, Validation Loss Force: 3.5862449129297773, time: 0.16620159149169922
Test Loss Energy: 10.445307701967588, Test Loss Force: 12.363066178852717, time: 11.551155805587769

Epoch 1, Batch 100/157, Loss: 0.5439242720603943, Variance: 0.09645947068929672

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6621116854185287, Training Loss Force: 3.4828758308469903, time: 2.6897594928741455
Validation Loss Energy: 1.599003577312719, Validation Loss Force: 3.551208665048527, time: 0.15727901458740234
Test Loss Energy: 10.231493998387485, Test Loss Force: 12.147374593281725, time: 11.708554983139038

Epoch 2, Batch 100/157, Loss: 0.5002120137214661, Variance: 0.09446106851100922

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6830249403787694, Training Loss Force: 3.477143729937112, time: 2.6785316467285156
Validation Loss Energy: 1.745906243412912, Validation Loss Force: 3.739251422769668, time: 0.15341854095458984
Test Loss Energy: 10.36546202966934, Test Loss Force: 12.287529548570953, time: 11.48359227180481

Epoch 3, Batch 100/157, Loss: 0.4431183934211731, Variance: 0.09067638963460922

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6648834425815415, Training Loss Force: 3.478880786127243, time: 2.7014565467834473
Validation Loss Energy: 2.1121442158765387, Validation Loss Force: 3.5766575959596625, time: 0.1603398323059082
Test Loss Energy: 10.526726397714006, Test Loss Force: 12.017976008004771, time: 11.486338376998901

Epoch 4, Batch 100/157, Loss: 0.760996401309967, Variance: 0.09219954907894135

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6685512558446378, Training Loss Force: 3.483813297735118, time: 2.922119140625
Validation Loss Energy: 1.4967768775362333, Validation Loss Force: 3.5762985048503295, time: 0.1570572853088379
Test Loss Energy: 10.041139727417962, Test Loss Force: 12.136810077803512, time: 11.405414342880249

Epoch 5, Batch 100/157, Loss: 0.5809252858161926, Variance: 0.08985869586467743

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6844331163677773, Training Loss Force: 3.488223949451777, time: 2.6922760009765625
Validation Loss Energy: 1.2816673120011697, Validation Loss Force: 3.8622772392405005, time: 0.15921664237976074
Test Loss Energy: 10.315429069991385, Test Loss Force: 12.527486862222192, time: 11.445718050003052

Epoch 6, Batch 100/157, Loss: 0.6789910197257996, Variance: 0.09218811243772507

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6633419186216003, Training Loss Force: 3.5379067700398465, time: 2.9121313095092773
Validation Loss Energy: 2.0533989737904315, Validation Loss Force: 3.573528723344134, time: 0.16115045547485352
Test Loss Energy: 10.222980468593697, Test Loss Force: 11.948206615856783, time: 11.486425161361694

Epoch 7, Batch 100/157, Loss: 0.5280210971832275, Variance: 0.0870363712310791

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6918820155394685, Training Loss Force: 3.47416313889899, time: 2.6783080101013184
Validation Loss Energy: 2.0005940249563374, Validation Loss Force: 3.564961008393221, time: 0.1624753475189209
Test Loss Energy: 10.207821101843424, Test Loss Force: 11.881493825225283, time: 11.498647451400757

Epoch 8, Batch 100/157, Loss: 0.6353899836540222, Variance: 0.08796361088752747

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6822165932297843, Training Loss Force: 3.480375135784863, time: 2.7903504371643066
Validation Loss Energy: 1.6855191132677827, Validation Loss Force: 3.6140895411819502, time: 0.17406320571899414
Test Loss Energy: 10.515360166056208, Test Loss Force: 12.53445126294904, time: 11.588413715362549

Epoch 9, Batch 100/157, Loss: 0.5508642196655273, Variance: 0.09215649962425232

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.674920155409568, Training Loss Force: 3.4934951039554636, time: 2.70880389213562
Validation Loss Energy: 1.4253248721916603, Validation Loss Force: 3.66523627288632, time: 0.1591026782989502
Test Loss Energy: 10.113289445741074, Test Loss Force: 12.326671375605251, time: 11.374032735824585

Epoch 10, Batch 100/157, Loss: 0.5206885933876038, Variance: 0.09086234867572784

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6933440129302868, Training Loss Force: 3.496001074999903, time: 2.6800498962402344
Validation Loss Energy: 1.8690605399809448, Validation Loss Force: 3.828969794608233, time: 0.158585786819458
Test Loss Energy: 10.032431596448703, Test Loss Force: 12.012080778246945, time: 11.679720878601074

Epoch 11, Batch 100/157, Loss: 0.43426692485809326, Variance: 0.08984240889549255

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.725598223756288, Training Loss Force: 3.5793230737539092, time: 2.7077131271362305
Validation Loss Energy: 1.9234926435989061, Validation Loss Force: 3.604439876850975, time: 0.16284418106079102
Test Loss Energy: 10.361343517830118, Test Loss Force: 12.061308006705845, time: 11.66280746459961

Epoch 12, Batch 100/157, Loss: 0.5826675891876221, Variance: 0.09081695973873138

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6691272023550536, Training Loss Force: 3.4864327293464683, time: 2.7160375118255615
Validation Loss Energy: 1.5144501647600463, Validation Loss Force: 3.664206873424696, time: 0.17216706275939941
Test Loss Energy: 10.328358686204691, Test Loss Force: 12.373712095642162, time: 12.115054368972778

Epoch 13, Batch 100/157, Loss: 0.608619749546051, Variance: 0.08926262706518173

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6962558382048745, Training Loss Force: 3.4930449846592064, time: 2.675661563873291
Validation Loss Energy: 1.5228279800410063, Validation Loss Force: 3.592198788999692, time: 0.16202950477600098
Test Loss Energy: 10.16430163204278, Test Loss Force: 12.316984269826564, time: 12.663645029067993

Epoch 14, Batch 100/157, Loss: 0.7821910977363586, Variance: 0.08756108582019806

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6874605991169642, Training Loss Force: 3.488396423015459, time: 2.7573020458221436
Validation Loss Energy: 2.0940190216210652, Validation Loss Force: 3.614904584479095, time: 0.16245007514953613
Test Loss Energy: 10.309100920118366, Test Loss Force: 12.077852538756778, time: 11.913037538528442

Epoch 15, Batch 100/157, Loss: 0.3681361675262451, Variance: 0.08981875330209732

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6672245306691087, Training Loss Force: 3.478829270131144, time: 2.747265100479126
Validation Loss Energy: 1.8963473820225685, Validation Loss Force: 3.5938701436090263, time: 0.1561119556427002
Test Loss Energy: 10.166833349920998, Test Loss Force: 12.170808569329626, time: 11.714672565460205

Epoch 16, Batch 100/157, Loss: 0.7590047121047974, Variance: 0.09035548567771912

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.706024416605803, Training Loss Force: 3.481851130129828, time: 2.7483394145965576
Validation Loss Energy: 1.4815869079623802, Validation Loss Force: 3.570624227004652, time: 0.17682266235351562
Test Loss Energy: 9.965579678404243, Test Loss Force: 12.120171516419418, time: 11.92962098121643

Epoch 17, Batch 100/157, Loss: 0.4564458727836609, Variance: 0.08722446858882904

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6707335843768456, Training Loss Force: 3.4714738799454063, time: 2.8971810340881348
Validation Loss Energy: 1.531093458737846, Validation Loss Force: 3.5982981422449463, time: 0.1596686840057373
Test Loss Energy: 10.18361115996198, Test Loss Force: 12.241540395816669, time: 11.800184488296509

Epoch 18, Batch 100/157, Loss: 0.3424198031425476, Variance: 0.08589781820774078

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6885072369998253, Training Loss Force: 3.485018550509307, time: 2.880913734436035
Validation Loss Energy: 2.157815364836446, Validation Loss Force: 3.5533172676374143, time: 0.16417622566223145
Test Loss Energy: 10.626229656620103, Test Loss Force: 12.19538763370144, time: 11.925968170166016

Epoch 19, Batch 100/157, Loss: 0.7083685398101807, Variance: 0.08657902479171753

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6627406839166676, Training Loss Force: 3.475935161236405, time: 2.7012503147125244
Validation Loss Energy: 1.9581774082333103, Validation Loss Force: 3.608165354331492, time: 0.15654516220092773
Test Loss Energy: 10.032755882000052, Test Loss Force: 12.173280607213506, time: 11.591526746749878

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–…â–‡â–‚â–…â–„â–„â–‡â–ƒâ–‚â–…â–…â–ƒâ–…â–ƒâ–â–ƒâ–ˆâ–‚
wandb:   test_error_force â–†â–„â–…â–‚â–„â–ˆâ–‚â–â–ˆâ–†â–‚â–ƒâ–†â–†â–ƒâ–„â–„â–…â–„â–„
wandb:          test_loss â–â–‚â–ƒâ–„â–ƒâ–„â–„â–â–ˆâ–†â–ƒâ–ƒâ–†â–ƒâ–…â–„â–ƒâ–„â–‡â–ƒ
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–‚â–â–â–
wandb:  train_error_force â–ˆâ–‚â–â–â–‚â–‚â–…â–â–â–‚â–‚â–‡â–‚â–‚â–‚â–â–‚â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–„â–…â–ˆâ–ƒâ–â–‡â–‡â–„â–‚â–†â–†â–ƒâ–ƒâ–‡â–†â–ƒâ–ƒâ–ˆâ–†
wandb:  valid_error_force â–‚â–â–…â–‚â–‚â–ˆâ–‚â–â–‚â–„â–‡â–‚â–„â–‚â–‚â–‚â–â–‚â–â–‚
wandb:         valid_loss â–ƒâ–‚â–„â–‡â–‚â–â–‡â–…â–„â–â–‡â–…â–‚â–‚â–ˆâ–…â–â–‚â–ˆâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 5000
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.03276
wandb:   test_error_force 12.17328
wandb:          test_loss 14.50577
wandb: train_error_energy 1.66274
wandb:  train_error_force 3.47594
wandb:         train_loss 0.57169
wandb: valid_error_energy 1.95818
wandb:  valid_error_force 3.60817
wandb:         valid_loss 0.77068
wandb: 
wandb: ğŸš€ View run al_63_46 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t059xxwm
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_062129-t059xxwm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 4.008344650268555, Uncertainty Bias: -0.09958702325820923
7.6293945e-06 0.004594803
2.1543102 5.31742
(48745, 22, 3)
Found uncertainty sample 0 after 1209 steps.
Found uncertainty sample 1 after 3496 steps.
Found uncertainty sample 2 after 1010 steps.
Found uncertainty sample 3 after 164 steps.
Found uncertainty sample 4 after 550 steps.
Found uncertainty sample 5 after 20 steps.
Found uncertainty sample 6 after 2297 steps.
Found uncertainty sample 7 after 92 steps.
Found uncertainty sample 8 after 190 steps.
Found uncertainty sample 9 after 12 steps.
Found uncertainty sample 10 after 242 steps.
Found uncertainty sample 11 after 1669 steps.
Found uncertainty sample 12 after 326 steps.
Found uncertainty sample 13 after 280 steps.
Found uncertainty sample 14 after 568 steps.
Found uncertainty sample 15 after 299 steps.
Found uncertainty sample 16 after 2261 steps.
Found uncertainty sample 17 after 322 steps.
Found uncertainty sample 18 after 298 steps.
Found uncertainty sample 19 after 1242 steps.
Found uncertainty sample 20 after 996 steps.
Found uncertainty sample 21 after 1703 steps.
Found uncertainty sample 22 after 2184 steps.
Found uncertainty sample 23 after 189 steps.
Found uncertainty sample 24 after 413 steps.
Found uncertainty sample 25 after 2034 steps.
Found uncertainty sample 26 after 1084 steps.
Found uncertainty sample 27 after 984 steps.
Found uncertainty sample 28 after 1682 steps.
Found uncertainty sample 29 after 1525 steps.
Found uncertainty sample 30 after 1199 steps.
Found uncertainty sample 31 after 1154 steps.
Found uncertainty sample 32 after 141 steps.
Found uncertainty sample 33 after 566 steps.
Found uncertainty sample 34 after 962 steps.
Found uncertainty sample 35 after 660 steps.
Found uncertainty sample 36 after 85 steps.
Found uncertainty sample 37 after 441 steps.
Found uncertainty sample 38 after 515 steps.
Found uncertainty sample 39 after 1751 steps.
Found uncertainty sample 40 after 3294 steps.
Found uncertainty sample 41 after 203 steps.
Found uncertainty sample 42 after 671 steps.
Found uncertainty sample 43 after 882 steps.
Found uncertainty sample 44 after 359 steps.
Found uncertainty sample 45 after 6 steps.
Found uncertainty sample 46 after 148 steps.
Found uncertainty sample 47 after 151 steps.
Found uncertainty sample 48 after 567 steps.
Found uncertainty sample 49 after 249 steps.
Found uncertainty sample 50 after 29 steps.
Found uncertainty sample 51 after 112 steps.
Found uncertainty sample 52 after 176 steps.
Found uncertainty sample 53 after 1826 steps.
Found uncertainty sample 54 after 203 steps.
Found uncertainty sample 55 after 254 steps.
Found uncertainty sample 56 after 6 steps.
Found uncertainty sample 57 after 3147 steps.
Found uncertainty sample 58 after 34 steps.
Found uncertainty sample 59 after 1090 steps.
Found uncertainty sample 60 after 12 steps.
Found uncertainty sample 61 after 44 steps.
Found uncertainty sample 62 after 145 steps.
Found uncertainty sample 63 after 343 steps.
Found uncertainty sample 64 after 352 steps.
Found uncertainty sample 65 after 429 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 132 steps.
Found uncertainty sample 68 after 123 steps.
Found uncertainty sample 69 after 40 steps.
Found uncertainty sample 70 after 39 steps.
Found uncertainty sample 71 after 51 steps.
Found uncertainty sample 72 after 130 steps.
Found uncertainty sample 73 after 388 steps.
Found uncertainty sample 74 after 619 steps.
Found uncertainty sample 75 after 512 steps.
Found uncertainty sample 76 after 634 steps.
Found uncertainty sample 77 after 342 steps.
Found uncertainty sample 78 after 1283 steps.
Found uncertainty sample 79 after 10 steps.
Found uncertainty sample 80 after 1367 steps.
Found uncertainty sample 81 after 1053 steps.
Found uncertainty sample 82 after 165 steps.
Found uncertainty sample 83 after 1232 steps.
Found uncertainty sample 84 after 98 steps.
Found uncertainty sample 85 after 1491 steps.
Found uncertainty sample 86 after 109 steps.
Found uncertainty sample 87 after 183 steps.
Found uncertainty sample 88 after 2029 steps.
Found uncertainty sample 89 after 215 steps.
Found uncertainty sample 90 after 384 steps.
Found uncertainty sample 91 after 598 steps.
Found uncertainty sample 92 after 128 steps.
Found uncertainty sample 93 after 44 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 491 steps.
Found uncertainty sample 96 after 768 steps.
Found uncertainty sample 97 after 206 steps.
Found uncertainty sample 98 after 95 steps.
Found uncertainty sample 99 after 900 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_063605-aa60dg8l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_47
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/aa60dg8l
Training model 47. Added 99 samples to the dataset.
Epoch 0, Batch 100/160, Loss: 1.5429253578186035, Variance: 0.11579383909702301

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1413083649223315, Training Loss Force: 3.7119616999459693, time: 4.07438063621521
Validation Loss Energy: 2.441466612867283, Validation Loss Force: 3.606822362489354, time: 0.14589428901672363
Test Loss Energy: 10.520464032001021, Test Loss Force: 11.948587596936433, time: 9.817535161972046

Epoch 1, Batch 100/160, Loss: 0.5684534907341003, Variance: 0.11806294322013855

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6790079913269835, Training Loss Force: 3.4941759416745315, time: 2.476506471633911
Validation Loss Energy: 2.5040535657280816, Validation Loss Force: 3.682753728819508, time: 0.14211726188659668
Test Loss Energy: 10.149510343219761, Test Loss Force: 11.899429398244385, time: 9.821768522262573

Epoch 2, Batch 100/160, Loss: 0.9063906669616699, Variance: 0.10729743540287018

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.3994193741042364, Training Loss Force: 4.001771562295778, time: 2.6029295921325684
Validation Loss Energy: 1.8317898814433573, Validation Loss Force: 3.5765360186059856, time: 0.14203619956970215
Test Loss Energy: 9.76641975903761, Test Loss Force: 11.626476406298957, time: 9.846163511276245

Epoch 3, Batch 100/160, Loss: 1.1022424697875977, Variance: 0.12127603590488434

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.693610404289371, Training Loss Force: 3.476794479994431, time: 2.41159987449646
Validation Loss Energy: 4.121802136863964, Validation Loss Force: 3.566594583470797, time: 0.1493375301361084
Test Loss Energy: 10.97030370538288, Test Loss Force: 11.603313848784449, time: 11.642659187316895

Epoch 4, Batch 100/160, Loss: 0.9230131506919861, Variance: 0.11847417056560516

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.7140053428515403, Training Loss Force: 3.452698030806098, time: 2.61775803565979
Validation Loss Energy: 1.7600229477033935, Validation Loss Force: 3.5447989110138494, time: 0.16203665733337402
Test Loss Energy: 9.94732073382651, Test Loss Force: 11.705444883406184, time: 11.836027383804321

Epoch 5, Batch 100/160, Loss: 0.8083321452140808, Variance: 0.13005055487155914

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6903316282533867, Training Loss Force: 3.4616598181891676, time: 2.568819522857666
Validation Loss Energy: 2.0488258690944154, Validation Loss Force: 3.5860661838286467, time: 0.150848388671875
Test Loss Energy: 9.980836711805662, Test Loss Force: 11.845201990719413, time: 11.524442195892334

Epoch 6, Batch 100/160, Loss: 0.9304421544075012, Variance: 0.13062533736228943

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.711363546003695, Training Loss Force: 3.46726112457236, time: 2.5834150314331055
Validation Loss Energy: 4.045523852004455, Validation Loss Force: 3.6058242109312086, time: 0.1485309600830078
Test Loss Energy: 11.209714852422406, Test Loss Force: 11.946537930945587, time: 11.03579330444336

Epoch 7, Batch 100/160, Loss: 1.1054298877716064, Variance: 0.1269703507423401

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.703636250978311, Training Loss Force: 3.4687943129766468, time: 2.480095863342285
Validation Loss Energy: 1.6681446109262807, Validation Loss Force: 3.565141667293618, time: 0.1456456184387207
Test Loss Energy: 10.083439026257661, Test Loss Force: 12.020824960633318, time: 11.206834554672241

Epoch 8, Batch 100/160, Loss: 0.8003274202346802, Variance: 0.13202780485153198

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6871195061387123, Training Loss Force: 3.46590781804593, time: 2.5999505519866943
Validation Loss Energy: 2.3030333532900746, Validation Loss Force: 3.6197405872134034, time: 0.15430068969726562
Test Loss Energy: 10.198298598693276, Test Loss Force: 12.062952293607164, time: 11.49001407623291

Epoch 9, Batch 100/160, Loss: 0.9434518814086914, Variance: 0.12435737997293472

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.695633784901595, Training Loss Force: 3.4963265971872035, time: 2.584691286087036
Validation Loss Energy: 3.857663089198585, Validation Loss Force: 3.590431315542366, time: 0.16801762580871582
Test Loss Energy: 11.259398447707818, Test Loss Force: 12.04109368661121, time: 11.350404977798462

Epoch 10, Batch 100/160, Loss: 1.0526015758514404, Variance: 0.1264687180519104

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.7115157654152156, Training Loss Force: 3.4601462107529017, time: 2.580446720123291
Validation Loss Energy: 1.7507496313405604, Validation Loss Force: 3.54763022342737, time: 0.15863418579101562
Test Loss Energy: 10.15501481310843, Test Loss Force: 12.006797816360384, time: 11.51918339729309

Epoch 11, Batch 100/160, Loss: 0.9014506936073303, Variance: 0.13286907970905304

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.72395480915254, Training Loss Force: 3.4719646108673046, time: 2.6188509464263916
Validation Loss Energy: 2.275428168592823, Validation Loss Force: 3.5395886756682855, time: 0.16345596313476562
Test Loss Energy: 10.09694458379465, Test Loss Force: 11.888680881108375, time: 11.424155473709106

Epoch 12, Batch 100/160, Loss: 1.064379096031189, Variance: 0.13120177388191223

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.709784908739202, Training Loss Force: 3.4590173060166913, time: 2.6166675090789795
Validation Loss Energy: 3.7323879432386384, Validation Loss Force: 3.6376473430367215, time: 0.1644580364227295
Test Loss Energy: 11.010607879348608, Test Loss Force: 11.900196000315727, time: 11.692701816558838

Epoch 13, Batch 100/160, Loss: 1.4213695526123047, Variance: 0.12509547173976898

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.695853559348626, Training Loss Force: 3.4779736870823883, time: 2.406677484512329
Validation Loss Energy: 1.752371590511023, Validation Loss Force: 3.628961539215419, time: 0.15515565872192383
Test Loss Energy: 10.126480212059304, Test Loss Force: 11.988893626866979, time: 11.585875749588013

Epoch 14, Batch 100/160, Loss: 0.7215881943702698, Variance: 0.13011392951011658

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.70072725602624, Training Loss Force: 3.481523482781816, time: 2.4706637859344482
Validation Loss Energy: 2.3645513458043226, Validation Loss Force: 3.585380956643578, time: 0.15848898887634277
Test Loss Energy: 10.330202148908336, Test Loss Force: 12.096456795381751, time: 11.419082403182983

Epoch 15, Batch 100/160, Loss: 1.0874369144439697, Variance: 0.13181939721107483

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.7383107037117886, Training Loss Force: 3.457237806569909, time: 2.8637187480926514
Validation Loss Energy: 3.7758902204941616, Validation Loss Force: 3.5484984846503913, time: 0.16405653953552246
Test Loss Energy: 10.85362314700348, Test Loss Force: 11.85502160212704, time: 11.356819152832031

Epoch 16, Batch 100/160, Loss: 1.4285258054733276, Variance: 0.12824106216430664

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.7138208830352126, Training Loss Force: 3.4614830815129984, time: 2.5095136165618896
Validation Loss Energy: 1.9335093037938949, Validation Loss Force: 3.6213451410789586, time: 0.17111611366271973
Test Loss Energy: 10.193575270821434, Test Loss Force: 11.935212703053239, time: 11.379161596298218

Epoch 17, Batch 100/160, Loss: 0.7318311929702759, Variance: 0.1297982782125473

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6894750200490023, Training Loss Force: 3.483094199287284, time: 2.5407543182373047
Validation Loss Energy: 2.1017107561518973, Validation Loss Force: 3.5613986731736205, time: 0.16068196296691895
Test Loss Energy: 10.223976725326791, Test Loss Force: 11.96152023958483, time: 11.522153854370117

Epoch 18, Batch 100/160, Loss: 1.0705468654632568, Variance: 0.13329117000102997

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6973208696944067, Training Loss Force: 3.4804498681862213, time: 2.555412769317627
Validation Loss Energy: 4.004979660129533, Validation Loss Force: 3.58204700574465, time: 0.16045832633972168
Test Loss Energy: 11.14876331700292, Test Loss Force: 11.632057381109265, time: 11.363229751586914

Epoch 19, Batch 100/160, Loss: 1.5626447200775146, Variance: 0.12769266963005066

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6908676534258085, Training Loss Force: 3.4811413226850703, time: 2.5195188522338867
Validation Loss Energy: 1.8705114150242863, Validation Loss Force: 3.581795834948043, time: 0.15874743461608887
Test Loss Energy: 9.930578704735405, Test Loss Force: 11.70330742748149, time: 11.57202434539795

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–ƒâ–â–‡â–‚â–‚â–ˆâ–‚â–ƒâ–ˆâ–ƒâ–ƒâ–‡â–ƒâ–„â–†â–ƒâ–ƒâ–‡â–‚
wandb:   test_error_force â–†â–…â–â–â–‚â–„â–†â–‡â–ˆâ–‡â–‡â–…â–…â–†â–ˆâ–…â–†â–†â–â–‚
wandb:          test_loss â–‡â–†â–†â–†â–ƒâ–ƒâ–‡â–„â–„â–ˆâ–„â–‚â–…â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–
wandb: train_error_energy â–ˆâ–„â–â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„
wandb:  train_error_force â–„â–‚â–ˆâ–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–ƒâ–â–ˆâ–â–‚â–ˆâ–â–ƒâ–‡â–â–ƒâ–‡â–â–ƒâ–‡â–‚â–‚â–ˆâ–‚
wandb:  valid_error_force â–„â–ˆâ–ƒâ–‚â–â–ƒâ–„â–‚â–…â–ƒâ–â–â–†â–…â–ƒâ–â–…â–‚â–ƒâ–ƒ
wandb:         valid_loss â–ƒâ–ƒâ–â–ˆâ–â–‚â–ˆâ–â–ƒâ–‡â–â–‚â–‡â–â–ƒâ–†â–‚â–‚â–‡â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 5089
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.93058
wandb:   test_error_force 11.70331
wandb:          test_loss 11.01888
wandb: train_error_energy 2.69087
wandb:  train_error_force 3.48114
wandb:         train_loss 1.01852
wandb: valid_error_energy 1.87051
wandb:  valid_error_force 3.5818
wandb:         valid_loss 0.78611
wandb: 
wandb: ğŸš€ View run al_63_47 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/aa60dg8l
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_063605-aa60dg8l/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.6868979930877686, Uncertainty Bias: -0.10573886334896088
0.00014209747 0.0011234283
2.1733923 4.874729
(48745, 22, 3)
Found uncertainty sample 0 after 801 steps.
Found uncertainty sample 1 after 1428 steps.
Found uncertainty sample 2 after 487 steps.
Found uncertainty sample 3 after 128 steps.
Found uncertainty sample 4 after 27 steps.
Found uncertainty sample 5 after 491 steps.
Found uncertainty sample 6 after 78 steps.
Found uncertainty sample 7 after 589 steps.
Found uncertainty sample 8 after 19 steps.
Found uncertainty sample 9 after 139 steps.
Found uncertainty sample 10 after 39 steps.
Found uncertainty sample 11 after 11 steps.
Found uncertainty sample 12 after 507 steps.
Found uncertainty sample 13 after 2913 steps.
Found uncertainty sample 14 after 69 steps.
Found uncertainty sample 15 after 22 steps.
Found uncertainty sample 16 after 2873 steps.
Found uncertainty sample 17 after 170 steps.
Found uncertainty sample 18 after 133 steps.
Found uncertainty sample 19 after 1117 steps.
Found uncertainty sample 20 after 2004 steps.
Found uncertainty sample 21 after 581 steps.
Found uncertainty sample 22 after 653 steps.
Found uncertainty sample 23 after 45 steps.
Found uncertainty sample 24 after 168 steps.
Found uncertainty sample 25 after 32 steps.
Found uncertainty sample 26 after 12 steps.
Found uncertainty sample 27 after 320 steps.
Found uncertainty sample 28 after 841 steps.
Found uncertainty sample 29 after 3334 steps.
Found uncertainty sample 30 after 16 steps.
Found uncertainty sample 31 after 1590 steps.
Found uncertainty sample 32 after 44 steps.
Found uncertainty sample 33 after 219 steps.
Found uncertainty sample 34 after 373 steps.
Found uncertainty sample 35 after 31 steps.
Found uncertainty sample 36 after 1038 steps.
Found uncertainty sample 37 after 435 steps.
Found uncertainty sample 38 after 3058 steps.
Found uncertainty sample 39 after 968 steps.
Found uncertainty sample 40 after 147 steps.
Found uncertainty sample 41 after 3351 steps.
Found uncertainty sample 42 after 1614 steps.
Found uncertainty sample 43 after 1452 steps.
Found uncertainty sample 44 after 642 steps.
Found uncertainty sample 45 after 1210 steps.
Found uncertainty sample 46 after 1898 steps.
Found uncertainty sample 47 after 243 steps.
Found uncertainty sample 48 after 891 steps.
Found uncertainty sample 49 after 126 steps.
Found uncertainty sample 50 after 350 steps.
Found uncertainty sample 51 after 821 steps.
Found uncertainty sample 52 after 947 steps.
Found uncertainty sample 53 after 529 steps.
Found uncertainty sample 54 after 956 steps.
Found uncertainty sample 55 after 727 steps.
Found uncertainty sample 56 after 14 steps.
Found uncertainty sample 57 after 688 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 1879 steps.
Found uncertainty sample 60 after 1027 steps.
Found uncertainty sample 61 after 522 steps.
Found uncertainty sample 62 after 44 steps.
Found uncertainty sample 63 after 411 steps.
Found uncertainty sample 64 after 516 steps.
Found uncertainty sample 65 after 682 steps.
Found uncertainty sample 66 after 392 steps.
Found uncertainty sample 67 after 2348 steps.
Found uncertainty sample 68 after 300 steps.
Found uncertainty sample 69 after 960 steps.
Found uncertainty sample 70 after 566 steps.
Found uncertainty sample 71 after 132 steps.
Found uncertainty sample 72 after 191 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 2175 steps.
Found uncertainty sample 75 after 38 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 320 steps.
Found uncertainty sample 78 after 942 steps.
Found uncertainty sample 79 after 128 steps.
Found uncertainty sample 80 after 202 steps.
Found uncertainty sample 81 after 2984 steps.
Found uncertainty sample 82 after 101 steps.
Found uncertainty sample 83 after 174 steps.
Found uncertainty sample 84 after 11 steps.
Found uncertainty sample 85 after 246 steps.
Found uncertainty sample 86 after 2327 steps.
Found uncertainty sample 87 after 312 steps.
Found uncertainty sample 88 after 1716 steps.
Found uncertainty sample 89 after 702 steps.
Found uncertainty sample 90 after 445 steps.
Found uncertainty sample 91 after 15 steps.
Found uncertainty sample 92 after 1125 steps.
Found uncertainty sample 93 after 1458 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 60 steps.
Found uncertainty sample 96 after 1025 steps.
Found uncertainty sample 97 after 153 steps.
Found uncertainty sample 98 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_065005-3gexqt2k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_48
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/3gexqt2k
Training model 48. Added 100 samples to the dataset.
Epoch 0, Batch 100/162, Loss: 2.3216824531555176, Variance: 0.1197802871465683

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.5545341531454087, Training Loss Force: 4.048010520687611, time: 2.7111518383026123
Validation Loss Energy: 1.2907125780470483, Validation Loss Force: 4.013927842427788, time: 0.16893935203552246
Test Loss Energy: 9.876196683216667, Test Loss Force: 12.311521325984897, time: 11.583835363388062

Epoch 1, Batch 100/162, Loss: 1.4263379573822021, Variance: 0.1151488870382309

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.582392905754068, Training Loss Force: 4.182523780667819, time: 2.5548460483551025
Validation Loss Energy: 3.3036845210448416, Validation Loss Force: 3.5530698079932397, time: 0.1754779815673828
Test Loss Energy: 10.383464109648846, Test Loss Force: 11.97516500847289, time: 12.58740520477295

Epoch 2, Batch 100/162, Loss: 1.5084912776947021, Variance: 0.12976914644241333

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.632908463208543, Training Loss Force: 3.4315177396352388, time: 2.6028964519500732
Validation Loss Energy: 3.4654319082081266, Validation Loss Force: 3.526968308038439, time: 0.1730046272277832
Test Loss Energy: 10.534770914006435, Test Loss Force: 11.888153428031377, time: 11.683183908462524

Epoch 3, Batch 100/162, Loss: 1.3527570962905884, Variance: 0.1314169019460678

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6323926680005134, Training Loss Force: 3.434942826816393, time: 2.666245460510254
Validation Loss Energy: 3.1429253395511836, Validation Loss Force: 3.519311681009615, time: 0.1585240364074707
Test Loss Energy: 10.43130145828339, Test Loss Force: 12.027617945755244, time: 11.805865287780762

Epoch 4, Batch 100/162, Loss: 1.4756308794021606, Variance: 0.1351899951696396

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6602740050482434, Training Loss Force: 3.435866998612556, time: 2.5953893661499023
Validation Loss Energy: 3.2067987262910562, Validation Loss Force: 3.55013458331162, time: 0.16904354095458984
Test Loss Energy: 10.368373021885686, Test Loss Force: 11.895611543379294, time: 11.47327995300293

Epoch 5, Batch 100/162, Loss: 1.3230173587799072, Variance: 0.1356876641511917

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.660200598453136, Training Loss Force: 3.436905253983484, time: 2.4818437099456787
Validation Loss Energy: 3.2644586789599104, Validation Loss Force: 3.545647948467272, time: 0.14676928520202637
Test Loss Energy: 10.384682140188055, Test Loss Force: 11.917430867341002, time: 11.631188869476318

Epoch 6, Batch 100/162, Loss: 1.375432014465332, Variance: 0.13281333446502686

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.685586534613487, Training Loss Force: 3.452598850308892, time: 2.9060933589935303
Validation Loss Energy: 3.327028486296473, Validation Loss Force: 3.534534605617065, time: 0.1716461181640625
Test Loss Energy: 10.568946706131445, Test Loss Force: 11.999045068929158, time: 10.712012529373169

Epoch 7, Batch 100/162, Loss: 1.3898900747299194, Variance: 0.1341361552476883

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.662270853154142, Training Loss Force: 3.4394889940437556, time: 2.4331209659576416
Validation Loss Energy: 3.1674838907708023, Validation Loss Force: 3.5820065541831396, time: 0.14348649978637695
Test Loss Energy: 10.327958198841538, Test Loss Force: 12.057945040423931, time: 9.92678689956665

Epoch 8, Batch 100/162, Loss: 1.3601707220077515, Variance: 0.13619522750377655

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.682431499060562, Training Loss Force: 3.4546622675551673, time: 2.5092861652374268
Validation Loss Energy: 3.3798744062534176, Validation Loss Force: 3.5281504585341694, time: 0.14863824844360352
Test Loss Energy: 10.445773385924026, Test Loss Force: 11.972249889299599, time: 10.109406232833862

Epoch 9, Batch 100/162, Loss: 1.2764638662338257, Variance: 0.1385500133037567

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.666826118890217, Training Loss Force: 3.445747350927714, time: 2.37949538230896
Validation Loss Energy: 3.461068706293672, Validation Loss Force: 3.5268435390936705, time: 0.14123797416687012
Test Loss Energy: 10.5010708877372, Test Loss Force: 12.052637927678587, time: 9.972286224365234

Epoch 10, Batch 100/162, Loss: 1.450143575668335, Variance: 0.13810476660728455

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.699291760067303, Training Loss Force: 3.436535635571752, time: 2.4582059383392334
Validation Loss Energy: 3.219874823439568, Validation Loss Force: 3.5934086989156153, time: 0.14647436141967773
Test Loss Energy: 10.208484147606416, Test Loss Force: 11.904748206366023, time: 10.13632321357727

Epoch 11, Batch 100/162, Loss: 1.4649779796600342, Variance: 0.13900360465049744

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6896610714085134, Training Loss Force: 3.4403774961670424, time: 2.4714787006378174
Validation Loss Energy: 3.271160797583191, Validation Loss Force: 3.544856433876548, time: 0.1421356201171875
Test Loss Energy: 10.307231357326561, Test Loss Force: 12.037031982217762, time: 9.954404354095459

Epoch 12, Batch 100/162, Loss: 1.4618055820465088, Variance: 0.14060065150260925

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.740542292835629, Training Loss Force: 3.450058648939154, time: 2.4625179767608643
Validation Loss Energy: 3.1795947171717174, Validation Loss Force: 3.5590321454906992, time: 0.1461036205291748
Test Loss Energy: 10.443898367155239, Test Loss Force: 12.030036533717642, time: 10.022916316986084

Epoch 13, Batch 100/162, Loss: 1.4879958629608154, Variance: 0.1419084519147873

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.712662087689391, Training Loss Force: 3.4385698350523795, time: 2.456169843673706
Validation Loss Energy: 3.2149053676637998, Validation Loss Force: 3.5673712534517925, time: 0.21062779426574707
Test Loss Energy: 10.294718796926157, Test Loss Force: 11.884565530937865, time: 10.03249454498291

Epoch 14, Batch 100/162, Loss: 1.6872425079345703, Variance: 0.13451646268367767

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.7054664352746696, Training Loss Force: 3.453502107699349, time: 2.4284956455230713
Validation Loss Energy: 3.58027864161074, Validation Loss Force: 3.552138382955824, time: 0.14289355278015137
Test Loss Energy: 10.728288509132735, Test Loss Force: 12.093667452799114, time: 9.990652084350586

Epoch 15, Batch 100/162, Loss: 1.3490148782730103, Variance: 0.13479894399642944

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.677423685775407, Training Loss Force: 3.4526908635226943, time: 2.4389827251434326
Validation Loss Energy: 3.255605353717822, Validation Loss Force: 3.576040060194956, time: 0.14986681938171387
Test Loss Energy: 10.303666202796917, Test Loss Force: 11.876533661320279, time: 10.258849143981934

Epoch 16, Batch 100/162, Loss: 1.3867448568344116, Variance: 0.13783210515975952

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.7104515150021764, Training Loss Force: 3.450702741435764, time: 2.703686475753784
Validation Loss Energy: 3.2176341780471454, Validation Loss Force: 3.538636319731152, time: 0.16775870323181152
Test Loss Energy: 10.315435425971405, Test Loss Force: 11.88744308780811, time: 12.071097373962402

Epoch 17, Batch 100/162, Loss: 1.3239988088607788, Variance: 0.13790282607078552

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.704221210307103, Training Loss Force: 3.4408012218067343, time: 2.7118618488311768
Validation Loss Energy: 3.388628792171666, Validation Loss Force: 3.5514417254212645, time: 0.17783880233764648
Test Loss Energy: 10.498283163294749, Test Loss Force: 11.88868426661204, time: 11.777275323867798

Epoch 18, Batch 100/162, Loss: 1.4191943407058716, Variance: 0.14108990132808685

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.695283982781028, Training Loss Force: 3.4523659713702735, time: 2.48903751373291
Validation Loss Energy: 3.367193546838928, Validation Loss Force: 3.5606397425012495, time: 0.14930272102355957
Test Loss Energy: 10.596236369814724, Test Loss Force: 12.084614755899118, time: 10.891326665878296

Epoch 19, Batch 100/162, Loss: 1.54611337184906, Variance: 0.14043468236923218

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.690437445871816, Training Loss Force: 3.4744412516729635, time: 2.5444529056549072
Validation Loss Energy: 3.6012437581533763, Validation Loss Force: 3.5157851955225077, time: 0.15375828742980957
Test Loss Energy: 10.82517201307567, Test Loss Force: 12.03148337953753, time: 10.995403289794922

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.040 MB of 0.059 MB uploadedwandb: / 0.040 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–…â–†â–…â–…â–…â–†â–„â–…â–†â–ƒâ–„â–…â–„â–‡â–„â–„â–†â–†â–ˆ
wandb:   test_error_force â–ˆâ–ƒâ–â–ƒâ–â–‚â–ƒâ–„â–ƒâ–„â–â–„â–ƒâ–â–„â–â–â–â–„â–ƒ
wandb:          test_loss â–‡â–ˆâ–†â–†â–…â–ƒâ–†â–…â–…â–„â–â–ƒâ–„â–ƒâ–†â–ƒâ–ƒâ–„â–„â–…
wandb: train_error_energy â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–‡â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆ
wandb:  valid_error_force â–ˆâ–‚â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–
wandb:         valid_loss â–â–‡â–ˆâ–†â–†â–†â–‡â–†â–‡â–‡â–†â–‡â–†â–†â–ˆâ–‡â–†â–‡â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5179
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.82517
wandb:   test_error_force 12.03148
wandb:          test_loss 11.58424
wandb: train_error_energy 2.69044
wandb:  train_error_force 3.47444
wandb:         train_loss 1.01354
wandb: valid_error_energy 3.60124
wandb:  valid_error_force 3.51579
wandb:         valid_loss 1.40982
wandb: 
wandb: ğŸš€ View run al_63_48 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/3gexqt2k
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_065005-3gexqt2k/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8785738945007324, Uncertainty Bias: -0.14290979504585266
4.196167e-05 0.081380844
2.0240915 4.91247
(48745, 22, 3)
Found uncertainty sample 0 after 1356 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 129 steps.
Found uncertainty sample 3 after 239 steps.
Found uncertainty sample 4 after 141 steps.
Found uncertainty sample 5 after 1627 steps.
Found uncertainty sample 6 after 235 steps.
Found uncertainty sample 7 after 456 steps.
Found uncertainty sample 8 after 621 steps.
Found uncertainty sample 9 after 1024 steps.
Found uncertainty sample 10 after 665 steps.
Found uncertainty sample 11 after 2010 steps.
Found uncertainty sample 12 after 8 steps.
Found uncertainty sample 13 after 1053 steps.
Found uncertainty sample 14 after 806 steps.
Found uncertainty sample 15 after 1426 steps.
Found uncertainty sample 16 after 169 steps.
Found uncertainty sample 17 after 1363 steps.
Found uncertainty sample 18 after 1688 steps.
Found uncertainty sample 19 after 487 steps.
Found uncertainty sample 20 after 13 steps.
Found uncertainty sample 21 after 1507 steps.
Found uncertainty sample 22 after 123 steps.
Found uncertainty sample 23 after 1415 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 1817 steps.
Found uncertainty sample 26 after 1895 steps.
Found uncertainty sample 27 after 1034 steps.
Found uncertainty sample 28 after 87 steps.
Found uncertainty sample 29 after 2053 steps.
Found uncertainty sample 30 after 1313 steps.
Found uncertainty sample 31 after 102 steps.
Found uncertainty sample 32 after 817 steps.
Found uncertainty sample 33 after 1239 steps.
Found uncertainty sample 34 after 19 steps.
Found uncertainty sample 35 after 1151 steps.
Found uncertainty sample 36 after 1200 steps.
Found uncertainty sample 37 after 134 steps.
Found uncertainty sample 38 after 1279 steps.
Found uncertainty sample 39 after 1572 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 508 steps.
Found uncertainty sample 42 after 1986 steps.
Found uncertainty sample 43 after 2222 steps.
Found uncertainty sample 44 after 3412 steps.
Found uncertainty sample 45 after 1898 steps.
Did not find any uncertainty samples for sample 46.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 45 steps.
Found uncertainty sample 49 after 317 steps.
Found uncertainty sample 50 after 482 steps.
Found uncertainty sample 51 after 1223 steps.
Found uncertainty sample 52 after 415 steps.
Found uncertainty sample 53 after 499 steps.
Found uncertainty sample 54 after 639 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 2148 steps.
Found uncertainty sample 57 after 533 steps.
Found uncertainty sample 58 after 216 steps.
Found uncertainty sample 59 after 727 steps.
Found uncertainty sample 60 after 2399 steps.
Found uncertainty sample 61 after 237 steps.
Found uncertainty sample 62 after 1161 steps.
Found uncertainty sample 63 after 170 steps.
Found uncertainty sample 64 after 52 steps.
Found uncertainty sample 65 after 2500 steps.
Found uncertainty sample 66 after 3072 steps.
Found uncertainty sample 67 after 1101 steps.
Found uncertainty sample 68 after 1173 steps.
Found uncertainty sample 69 after 1261 steps.
Found uncertainty sample 70 after 971 steps.
Found uncertainty sample 71 after 3317 steps.
Found uncertainty sample 72 after 441 steps.
Found uncertainty sample 73 after 965 steps.
Found uncertainty sample 74 after 17 steps.
Found uncertainty sample 75 after 2194 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 555 steps.
Found uncertainty sample 79 after 149 steps.
Found uncertainty sample 80 after 775 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 194 steps.
Found uncertainty sample 83 after 2856 steps.
Found uncertainty sample 84 after 2301 steps.
Found uncertainty sample 85 after 338 steps.
Found uncertainty sample 86 after 690 steps.
Found uncertainty sample 87 after 2305 steps.
Found uncertainty sample 88 after 899 steps.
Found uncertainty sample 89 after 2473 steps.
Found uncertainty sample 90 after 797 steps.
Found uncertainty sample 91 after 620 steps.
Found uncertainty sample 92 after 229 steps.
Found uncertainty sample 93 after 1312 steps.
Found uncertainty sample 94 after 96 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 1433 steps.
Found uncertainty sample 97 after 997 steps.
Found uncertainty sample 98 after 972 steps.
Found uncertainty sample 99 after 2606 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_070757-g4fia52x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_49
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/g4fia52x
Training model 49. Added 95 samples to the dataset.
Epoch 0, Batch 100/165, Loss: 0.8667289614677429, Variance: 0.13599100708961487

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.132970930901847, Training Loss Force: 3.5999701716092685, time: 2.65702748298645
Validation Loss Energy: 2.509600996383629, Validation Loss Force: 3.5341705415634728, time: 0.17166399955749512
Test Loss Energy: 10.288676302422186, Test Loss Force: 11.67867733806686, time: 12.107851505279541

Epoch 1, Batch 100/165, Loss: 1.0009963512420654, Variance: 0.1383754163980484

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.7081469450341014, Training Loss Force: 3.4452021194083926, time: 2.72859787940979
Validation Loss Energy: 1.7565264854394627, Validation Loss Force: 3.557461660004471, time: 0.16209125518798828
Test Loss Energy: 10.07929427443343, Test Loss Force: 11.918968678753131, time: 12.191196918487549

Epoch 2, Batch 100/165, Loss: 0.6801172494888306, Variance: 0.14088121056556702

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.704643280435432, Training Loss Force: 3.4466458197819505, time: 2.8806207180023193
Validation Loss Energy: 2.412240322564628, Validation Loss Force: 3.534480645460702, time: 0.17993664741516113
Test Loss Energy: 10.530956929620734, Test Loss Force: 11.903367536417337, time: 11.348384618759155

Epoch 3, Batch 100/165, Loss: 0.7879256010055542, Variance: 0.13538801670074463

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.691513453222008, Training Loss Force: 3.4566971441649197, time: 2.668958902359009
Validation Loss Energy: 1.658323380018368, Validation Loss Force: 3.57654344091085, time: 0.16739964485168457
Test Loss Energy: 10.211671622621969, Test Loss Force: 12.04654778897223, time: 11.066415786743164

Epoch 4, Batch 100/165, Loss: 0.8258062601089478, Variance: 0.14031334221363068

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.683410095207033, Training Loss Force: 3.45403145441165, time: 2.787583112716675
Validation Loss Energy: 2.455592075477504, Validation Loss Force: 3.5317481239249244, time: 0.17222833633422852
Test Loss Energy: 10.38700906773721, Test Loss Force: 11.849433539209889, time: 10.966274499893188

Epoch 5, Batch 100/165, Loss: 0.6953094005584717, Variance: 0.1373380422592163

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.686983225156927, Training Loss Force: 3.4516059099584893, time: 2.734243631362915
Validation Loss Energy: 1.7496619296742277, Validation Loss Force: 3.547201436875362, time: 0.1548478603363037
Test Loss Energy: 9.88832690553802, Test Loss Force: 11.71785524916087, time: 10.116830348968506

Epoch 6, Batch 100/165, Loss: 0.8562418222427368, Variance: 0.1405322551727295

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.74635258726691, Training Loss Force: 3.454684704957854, time: 2.6407694816589355
Validation Loss Energy: 2.3518422074083, Validation Loss Force: 3.5354479944536608, time: 0.21718358993530273
Test Loss Energy: 10.46828979721449, Test Loss Force: 11.806577716728674, time: 11.671603679656982

Epoch 7, Batch 100/165, Loss: 0.7083488702774048, Variance: 0.13461847603321075

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6760719950419483, Training Loss Force: 3.4596753021568944, time: 2.9094431400299072
Validation Loss Energy: 1.5875036437136565, Validation Loss Force: 3.543575195610879, time: 0.17908954620361328
Test Loss Energy: 10.126031213133322, Test Loss Force: 11.87330354108296, time: 9.035989046096802

Epoch 8, Batch 100/165, Loss: 0.7287824153900146, Variance: 0.13939842581748962

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.703765843894089, Training Loss Force: 3.4491849781304245, time: 2.5437257289886475
Validation Loss Energy: 2.2544272765407407, Validation Loss Force: 3.5004408420074777, time: 0.14144396781921387
Test Loss Energy: 10.321080939859817, Test Loss Force: 11.729982412404535, time: 9.1482093334198

Epoch 9, Batch 100/165, Loss: 0.4973489046096802, Variance: 0.13441510498523712

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7017230346397834, Training Loss Force: 3.4502149133475086, time: 2.598466634750366
Validation Loss Energy: 1.576854380475087, Validation Loss Force: 3.5522828210084825, time: 0.14022159576416016
Test Loss Energy: 9.880681104582033, Test Loss Force: 11.865302402139127, time: 8.882360219955444

Epoch 10, Batch 100/165, Loss: 0.8096938133239746, Variance: 0.14165270328521729

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.710512389735267, Training Loss Force: 3.450606900111999, time: 2.6372454166412354
Validation Loss Energy: 2.3340328322248007, Validation Loss Force: 3.5273263600250906, time: 0.1305408477783203
Test Loss Energy: 10.410073445645471, Test Loss Force: 11.812135967628024, time: 8.937448024749756

Epoch 11, Batch 100/165, Loss: 0.794184684753418, Variance: 0.13798919320106506

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6793525114186494, Training Loss Force: 3.453968945109594, time: 2.8062572479248047
Validation Loss Energy: 1.9103748330970571, Validation Loss Force: 3.560489737719943, time: 0.14214253425598145
Test Loss Energy: 9.829986619353477, Test Loss Force: 11.722123156841517, time: 8.931867837905884

Epoch 12, Batch 100/165, Loss: 0.8156983852386475, Variance: 0.13950903713703156

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.691888012414678, Training Loss Force: 3.4600264648951233, time: 2.942859411239624
Validation Loss Energy: 2.4476516937337194, Validation Loss Force: 3.657621730326076, time: 0.17294883728027344
Test Loss Energy: 10.431017896703796, Test Loss Force: 11.901797212978288, time: 12.05253791809082

Epoch 13, Batch 100/165, Loss: 0.8022398352622986, Variance: 0.14081405103206635

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.7100844108649786, Training Loss Force: 3.467759412111091, time: 2.8547844886779785
Validation Loss Energy: 1.7677937297810302, Validation Loss Force: 3.6190111973143924, time: 0.16148996353149414
Test Loss Energy: 10.036965998293777, Test Loss Force: 11.929675366732305, time: 11.0408616065979

Epoch 14, Batch 100/165, Loss: 0.7137697339057922, Variance: 0.13853541016578674

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6784907345910596, Training Loss Force: 3.4467720381004323, time: 2.779979944229126
Validation Loss Energy: 2.3029290699067317, Validation Loss Force: 3.5138237030160298, time: 0.14400482177734375
Test Loss Energy: 10.185815719740281, Test Loss Force: 11.750171807544508, time: 10.315750122070312

Epoch 15, Batch 100/165, Loss: 0.4769875407218933, Variance: 0.13060232996940613

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.682050344475266, Training Loss Force: 3.4481020139188594, time: 2.524672269821167
Validation Loss Energy: 1.6092578625979366, Validation Loss Force: 3.540935824001149, time: 0.14719820022583008
Test Loss Energy: 9.88380524829558, Test Loss Force: 11.660978960149485, time: 12.240902185440063

Epoch 16, Batch 100/165, Loss: 0.8396308422088623, Variance: 0.1404396891593933

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.706080715034174, Training Loss Force: 3.4650641437981835, time: 2.8917131423950195
Validation Loss Energy: 2.3676359814828607, Validation Loss Force: 3.537169226951683, time: 0.1789264678955078
Test Loss Energy: 9.935348874734387, Test Loss Force: 11.621765934947227, time: 12.09971308708191

Epoch 17, Batch 100/165, Loss: 0.9647220373153687, Variance: 0.13574767112731934

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6752120852614096, Training Loss Force: 3.465111936693959, time: 2.67785906791687
Validation Loss Energy: 2.020936113228668, Validation Loss Force: 3.678789178899313, time: 0.17422747611999512
Test Loss Energy: 10.095759265801174, Test Loss Force: 11.979892985583236, time: 11.863065004348755

Epoch 18, Batch 100/165, Loss: 0.8804607391357422, Variance: 0.13858133554458618

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.678232561861946, Training Loss Force: 3.468222991729284, time: 2.5699307918548584
Validation Loss Energy: 2.1107422909337767, Validation Loss Force: 3.5177138814414852, time: 0.16209959983825684
Test Loss Energy: 10.298652052699193, Test Loss Force: 11.754868149314833, time: 11.557837724685669

Epoch 19, Batch 100/165, Loss: 0.6521766781806946, Variance: 0.13698042929172516

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6850437627260413, Training Loss Force: 3.4511926252275673, time: 2.5251145362854004
Validation Loss Energy: 2.008768803087736, Validation Loss Force: 3.525767898672742, time: 0.15851569175720215
Test Loss Energy: 9.947281959084137, Test Loss Force: 11.675775467124849, time: 11.662113428115845

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.039 MB of 0.049 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–ƒâ–ˆâ–…â–‡â–‚â–‡â–„â–†â–‚â–‡â–â–‡â–ƒâ–…â–‚â–‚â–„â–†â–‚
wandb:   test_error_force â–‚â–†â–†â–ˆâ–…â–ƒâ–„â–…â–ƒâ–…â–„â–ƒâ–†â–†â–ƒâ–‚â–â–‡â–ƒâ–‚
wandb:          test_loss â–„â–†â–ˆâ–†â–ˆâ–ƒâ–…â–†â–…â–„â–†â–ƒâ–‡â–„â–„â–ƒâ–â–†â–†â–ƒ
wandb: train_error_energy â–ˆâ–‚â–â–â–â–â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–‚â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–â–‚â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‚â–‡â–‚â–ˆâ–‚â–‡â–â–†â–â–‡â–„â–ˆâ–‚â–†â–â–‡â–„â–…â–„
wandb:  valid_error_force â–‚â–ƒâ–‚â–„â–‚â–ƒâ–‚â–ƒâ–â–ƒâ–‚â–ƒâ–‡â–†â–‚â–ƒâ–‚â–ˆâ–‚â–‚
wandb:         valid_loss â–ˆâ–‚â–‡â–‚â–‡â–‚â–†â–â–…â–â–†â–ƒâ–ˆâ–ƒâ–†â–â–†â–…â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 5264
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.94728
wandb:   test_error_force 11.67578
wandb:          test_loss 10.69232
wandb: train_error_energy 2.68504
wandb:  train_error_force 3.45119
wandb:         train_loss 0.99739
wandb: valid_error_energy 2.00877
wandb:  valid_error_force 3.52577
wandb:         valid_loss 0.81333
wandb: 
wandb: ğŸš€ View run al_63_49 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/g4fia52x
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_070757-g4fia52x/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.0712127685546875, Uncertainty Bias: -0.1800529807806015
0.00015258789 0.013625383
2.0060275 4.89496
(48745, 22, 3)
Found uncertainty sample 0 after 358 steps.
Found uncertainty sample 1 after 41 steps.
Found uncertainty sample 2 after 139 steps.
Found uncertainty sample 3 after 132 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 777 steps.
Found uncertainty sample 6 after 13 steps.
Found uncertainty sample 7 after 924 steps.
Found uncertainty sample 8 after 1191 steps.
Found uncertainty sample 9 after 1546 steps.
Found uncertainty sample 10 after 769 steps.
Found uncertainty sample 11 after 206 steps.
Found uncertainty sample 12 after 504 steps.
Found uncertainty sample 13 after 1075 steps.
Found uncertainty sample 14 after 898 steps.
Found uncertainty sample 15 after 459 steps.
Found uncertainty sample 16 after 101 steps.
Found uncertainty sample 17 after 439 steps.
Found uncertainty sample 18 after 2560 steps.
Found uncertainty sample 19 after 304 steps.
Found uncertainty sample 20 after 285 steps.
Found uncertainty sample 21 after 1145 steps.
Found uncertainty sample 22 after 1677 steps.
Found uncertainty sample 23 after 1810 steps.
Found uncertainty sample 24 after 17 steps.
Found uncertainty sample 25 after 795 steps.
Found uncertainty sample 26 after 786 steps.
Found uncertainty sample 27 after 3080 steps.
Found uncertainty sample 28 after 59 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1220 steps.
Found uncertainty sample 31 after 2786 steps.
Found uncertainty sample 32 after 636 steps.
Found uncertainty sample 33 after 1444 steps.
Found uncertainty sample 34 after 472 steps.
Found uncertainty sample 35 after 264 steps.
Found uncertainty sample 36 after 381 steps.
Found uncertainty sample 37 after 534 steps.
Found uncertainty sample 38 after 159 steps.
Found uncertainty sample 39 after 599 steps.
Found uncertainty sample 40 after 69 steps.
Found uncertainty sample 41 after 2187 steps.
Found uncertainty sample 42 after 531 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 2676 steps.
Found uncertainty sample 45 after 17 steps.
Found uncertainty sample 46 after 476 steps.
Found uncertainty sample 47 after 3044 steps.
Found uncertainty sample 48 after 2399 steps.
Found uncertainty sample 49 after 514 steps.
Found uncertainty sample 50 after 567 steps.
Found uncertainty sample 51 after 64 steps.
Found uncertainty sample 52 after 2330 steps.
Found uncertainty sample 53 after 1998 steps.
Found uncertainty sample 54 after 495 steps.
Found uncertainty sample 55 after 17 steps.
Found uncertainty sample 56 after 630 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 461 steps.
Found uncertainty sample 59 after 386 steps.
Found uncertainty sample 60 after 270 steps.
Found uncertainty sample 61 after 3392 steps.
Found uncertainty sample 62 after 1135 steps.
Found uncertainty sample 63 after 392 steps.
Found uncertainty sample 64 after 1086 steps.
Found uncertainty sample 65 after 598 steps.
Found uncertainty sample 66 after 281 steps.
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 299 steps.
Found uncertainty sample 70 after 2027 steps.
Found uncertainty sample 71 after 1383 steps.
Found uncertainty sample 72 after 1235 steps.
Found uncertainty sample 73 after 279 steps.
Found uncertainty sample 74 after 169 steps.
Found uncertainty sample 75 after 1921 steps.
Found uncertainty sample 76 after 47 steps.
Found uncertainty sample 77 after 1083 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 2953 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 1361 steps.
Found uncertainty sample 82 after 2261 steps.
Found uncertainty sample 83 after 197 steps.
Found uncertainty sample 84 after 589 steps.
Found uncertainty sample 85 after 1741 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 723 steps.
Found uncertainty sample 88 after 1020 steps.
Found uncertainty sample 89 after 1598 steps.
Found uncertainty sample 90 after 3151 steps.
Found uncertainty sample 91 after 115 steps.
Found uncertainty sample 92 after 78 steps.
Found uncertainty sample 93 after 172 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 500 steps.
Found uncertainty sample 96 after 7 steps.
Found uncertainty sample 97 after 16 steps.
Found uncertainty sample 98 after 10 steps.
Found uncertainty sample 99 after 14 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_072358-mcl3wwzq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_50
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/mcl3wwzq
Training model 50. Added 97 samples to the dataset.
Epoch 0, Batch 100/168, Loss: 1.275357723236084, Variance: 0.1408129632472992

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.036008414421453, Training Loss Force: 3.5985445640918505, time: 2.633302688598633
Validation Loss Energy: 3.202861564675809, Validation Loss Force: 3.529382115803345, time: 0.16753864288330078
Test Loss Energy: 10.47172864354546, Test Loss Force: 11.941407871648048, time: 11.461538791656494

Epoch 1, Batch 100/168, Loss: 1.6228084564208984, Variance: 0.137534499168396

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6716954319666057, Training Loss Force: 3.436542725738292, time: 2.690556764602661
Validation Loss Energy: 3.0659270168967923, Validation Loss Force: 3.550788043031869, time: 0.18178725242614746
Test Loss Energy: 10.464748644320325, Test Loss Force: 12.007350012526144, time: 11.589887857437134

Epoch 2, Batch 100/168, Loss: 1.4162895679473877, Variance: 0.14173559844493866

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6780832338719414, Training Loss Force: 3.444074449955777, time: 2.795297384262085
Validation Loss Energy: 3.472763735176091, Validation Loss Force: 3.563047909314459, time: 0.16770482063293457
Test Loss Energy: 10.445343720742104, Test Loss Force: 11.687794956429673, time: 11.48904538154602

Epoch 3, Batch 100/168, Loss: 1.4769630432128906, Variance: 0.14026972651481628

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.7094491387437, Training Loss Force: 3.441428660703163, time: 2.6215498447418213
Validation Loss Energy: 3.2348622383136756, Validation Loss Force: 3.585582917558934, time: 0.17561006546020508
Test Loss Energy: 10.46690068215956, Test Loss Force: 11.853412727780569, time: 11.543163299560547

Epoch 4, Batch 100/168, Loss: 1.2778054475784302, Variance: 0.1413264125585556

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6624944194564435, Training Loss Force: 3.4870492452111828, time: 2.812302589416504
Validation Loss Energy: 3.810385159681662, Validation Loss Force: 3.5683193044521047, time: 0.14878392219543457
Test Loss Energy: 10.647273807497065, Test Loss Force: 11.855320706246179, time: 11.49418592453003

Epoch 5, Batch 100/168, Loss: 1.4132835865020752, Variance: 0.13959160447120667

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.7049050347158237, Training Loss Force: 3.453084389806737, time: 2.76892352104187
Validation Loss Energy: 3.2035784108795995, Validation Loss Force: 3.5384193657599696, time: 0.17993831634521484
Test Loss Energy: 10.273479583214133, Test Loss Force: 11.701236035510522, time: 11.00846242904663

Epoch 6, Batch 100/168, Loss: 1.234107255935669, Variance: 0.1428351253271103

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.7188469019762866, Training Loss Force: 3.443640310066386, time: 2.773181200027466
Validation Loss Energy: 3.3345988661427746, Validation Loss Force: 3.526035769655293, time: 0.14707708358764648
Test Loss Energy: 10.445392404844377, Test Loss Force: 11.911486533790365, time: 9.91032862663269

Epoch 7, Batch 100/168, Loss: 1.3488898277282715, Variance: 0.13893041014671326

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.683748454517567, Training Loss Force: 3.4436547623542864, time: 2.5482895374298096
Validation Loss Energy: 3.36743454907787, Validation Loss Force: 3.5240640343681022, time: 0.14714908599853516
Test Loss Energy: 10.461614950315454, Test Loss Force: 11.90545251703524, time: 10.77005648612976

Epoch 8, Batch 100/168, Loss: 1.4289778470993042, Variance: 0.13762223720550537

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.702555101185921, Training Loss Force: 3.4490651308726346, time: 2.5747530460357666
Validation Loss Energy: 3.6683363469589754, Validation Loss Force: 3.536218355475522, time: 0.1513080596923828
Test Loss Energy: 10.88389771506987, Test Loss Force: 12.055189706764466, time: 10.134094715118408

Epoch 9, Batch 100/168, Loss: 1.5025677680969238, Variance: 0.14005142450332642

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7058392192601546, Training Loss Force: 3.446708591491402, time: 2.542900323867798
Validation Loss Energy: 3.461712071679306, Validation Loss Force: 3.579130039932996, time: 0.14774775505065918
Test Loss Energy: 10.38595678698068, Test Loss Force: 11.72107383257166, time: 9.923905611038208

Epoch 10, Batch 100/168, Loss: 1.2862697839736938, Variance: 0.14221131801605225

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6874138643451477, Training Loss Force: 3.453439413227922, time: 2.5222690105438232
Validation Loss Energy: 3.2281749643001234, Validation Loss Force: 3.5994936528894326, time: 0.14571857452392578
Test Loss Energy: 10.503667825524841, Test Loss Force: 11.873507496694172, time: 10.054107904434204

Epoch 11, Batch 100/168, Loss: 1.3503612279891968, Variance: 0.14474090933799744

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.705746368877126, Training Loss Force: 3.4350693491825073, time: 2.532005786895752
Validation Loss Energy: 3.1789425934663433, Validation Loss Force: 3.5473377846157184, time: 0.15057969093322754
Test Loss Energy: 10.380388043817911, Test Loss Force: 11.865688316914195, time: 9.933141946792603

Epoch 12, Batch 100/168, Loss: 1.3917529582977295, Variance: 0.14115488529205322

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.7189326461651135, Training Loss Force: 3.4482718760165008, time: 2.602963924407959
Validation Loss Energy: 3.4461160278063296, Validation Loss Force: 3.518449357630485, time: 0.14745378494262695
Test Loss Energy: 10.449829844185762, Test Loss Force: 11.898301260754938, time: 9.915576457977295

Epoch 13, Batch 100/168, Loss: 1.3520337343215942, Variance: 0.14212742447853088

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6887853929613725, Training Loss Force: 3.437024454530567, time: 2.5638086795806885
Validation Loss Energy: 3.2428441849647, Validation Loss Force: 3.6651015580530335, time: 0.15208220481872559
Test Loss Energy: 10.441752839182554, Test Loss Force: 11.86435872796707, time: 10.045760869979858

Epoch 14, Batch 100/168, Loss: 1.3602519035339355, Variance: 0.14143669605255127

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.695026766045852, Training Loss Force: 3.451848740397874, time: 2.6455318927764893
Validation Loss Energy: 3.2650686439736556, Validation Loss Force: 3.5697806820568285, time: 0.14613604545593262
Test Loss Energy: 10.179579561369708, Test Loss Force: 11.662925578123852, time: 9.896658897399902

Epoch 15, Batch 100/168, Loss: 1.5748441219329834, Variance: 0.14375561475753784

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.731834772440077, Training Loss Force: 3.446683150216069, time: 2.713132858276367
Validation Loss Energy: 3.350745570813631, Validation Loss Force: 3.5294418653575015, time: 0.17926812171936035
Test Loss Energy: 10.625236626840916, Test Loss Force: 11.813674138108723, time: 12.481787919998169

Epoch 16, Batch 100/168, Loss: 1.4946335554122925, Variance: 0.1418880671262741

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.7034334163185045, Training Loss Force: 3.4526518960785313, time: 2.834963798522949
Validation Loss Energy: 3.2369798961144975, Validation Loss Force: 3.5295876394250354, time: 0.17229056358337402
Test Loss Energy: 10.40975009273976, Test Loss Force: 11.763609295939654, time: 11.491463661193848

Epoch 17, Batch 100/168, Loss: 1.6566994190216064, Variance: 0.14320288598537445

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.7036745318801367, Training Loss Force: 3.453639857872428, time: 2.567486047744751
Validation Loss Energy: 3.2994052653984007, Validation Loss Force: 3.522608743053367, time: 0.15686607360839844
Test Loss Energy: 10.48173816025777, Test Loss Force: 11.77896293743943, time: 10.96998643875122

Epoch 18, Batch 100/168, Loss: 1.2335349321365356, Variance: 0.14267385005950928

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.7036514074242497, Training Loss Force: 3.4415170497508036, time: 2.5866575241088867
Validation Loss Energy: 3.2589465784755167, Validation Loss Force: 3.614983260910656, time: 0.15721702575683594
Test Loss Energy: 10.473538050693655, Test Loss Force: 11.822835712060428, time: 10.845661163330078

Epoch 19, Batch 100/168, Loss: 1.4779434204101562, Variance: 0.14234493672847748

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6964291131322025, Training Loss Force: 3.4474977375716285, time: 2.600193500518799
Validation Loss Energy: 3.517630751627005, Validation Loss Force: 3.524568379983629, time: 0.17079448699951172
Test Loss Energy: 10.801352835726611, Test Loss Force: 11.946359716353962, time: 11.12597131729126

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–„â–„â–„â–†â–‚â–„â–„â–ˆâ–ƒâ–„â–ƒâ–„â–„â–â–…â–ƒâ–„â–„â–‡
wandb:   test_error_force â–†â–‡â–â–„â–„â–‚â–…â–…â–ˆâ–‚â–…â–…â–…â–…â–â–„â–ƒâ–ƒâ–„â–†
wandb:          test_loss â–…â–†â–…â–†â–†â–ƒâ–„â–†â–ˆâ–ƒâ–…â–ƒâ–‚â–…â–â–„â–„â–„â–„â–†
wandb: train_error_energy â–ˆâ–â–â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–ƒâ–‚â–â–â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–â–…â–ƒâ–ˆâ–‚â–„â–„â–‡â–…â–ƒâ–‚â–…â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–…
wandb:  valid_error_force â–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–â–â–‚â–„â–…â–‚â–â–ˆâ–ƒâ–‚â–‚â–â–†â–
wandb:         valid_loss â–‚â–â–…â–ƒâ–ˆâ–‚â–ƒâ–„â–†â–„â–ƒâ–‚â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 5351
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.80135
wandb:   test_error_force 11.94636
wandb:          test_loss 11.34638
wandb: train_error_energy 2.69643
wandb:  train_error_force 3.4475
wandb:         train_loss 1.00579
wandb: valid_error_energy 3.51763
wandb:  valid_error_force 3.52457
wandb:         valid_loss 1.36792
wandb: 
wandb: ğŸš€ View run al_63_50 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/mcl3wwzq
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_072358-mcl3wwzq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.219425916671753, Uncertainty Bias: -0.19468657672405243
9.918213e-05 0.022900581
1.9796242 4.962249
(48745, 22, 3)
Found uncertainty sample 0 after 1133 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1494 steps.
Found uncertainty sample 3 after 2244 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1559 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 3415 steps.
Found uncertainty sample 8 after 13 steps.
Found uncertainty sample 9 after 213 steps.
Found uncertainty sample 10 after 365 steps.
Found uncertainty sample 11 after 581 steps.
Found uncertainty sample 12 after 1103 steps.
Found uncertainty sample 13 after 380 steps.
Found uncertainty sample 14 after 2728 steps.
Found uncertainty sample 15 after 132 steps.
Found uncertainty sample 16 after 433 steps.
Found uncertainty sample 17 after 1836 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 748 steps.
Found uncertainty sample 20 after 375 steps.
Found uncertainty sample 21 after 169 steps.
Found uncertainty sample 22 after 2197 steps.
Found uncertainty sample 23 after 1801 steps.
Found uncertainty sample 24 after 237 steps.
Found uncertainty sample 25 after 2195 steps.
Found uncertainty sample 26 after 40 steps.
Found uncertainty sample 27 after 304 steps.
Found uncertainty sample 28 after 38 steps.
Found uncertainty sample 29 after 106 steps.
Found uncertainty sample 30 after 10 steps.
Found uncertainty sample 31 after 979 steps.
Found uncertainty sample 32 after 619 steps.
Found uncertainty sample 33 after 159 steps.
Found uncertainty sample 34 after 488 steps.
Found uncertainty sample 35 after 234 steps.
Found uncertainty sample 36 after 491 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 711 steps.
Found uncertainty sample 39 after 1012 steps.
Found uncertainty sample 40 after 1089 steps.
Found uncertainty sample 41 after 15 steps.
Found uncertainty sample 42 after 183 steps.
Found uncertainty sample 43 after 72 steps.
Found uncertainty sample 44 after 1828 steps.
Found uncertainty sample 45 after 697 steps.
Found uncertainty sample 46 after 1991 steps.
Found uncertainty sample 47 after 55 steps.
Found uncertainty sample 48 after 6 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 132 steps.
Found uncertainty sample 51 after 32 steps.
Found uncertainty sample 52 after 356 steps.
Found uncertainty sample 53 after 744 steps.
Found uncertainty sample 54 after 1004 steps.
Found uncertainty sample 55 after 1553 steps.
Found uncertainty sample 56 after 225 steps.
Found uncertainty sample 57 after 29 steps.
Found uncertainty sample 58 after 1089 steps.
Found uncertainty sample 59 after 421 steps.
Found uncertainty sample 60 after 92 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 63 steps.
Found uncertainty sample 63 after 137 steps.
Found uncertainty sample 64 after 2525 steps.
Found uncertainty sample 65 after 179 steps.
Found uncertainty sample 66 after 116 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 71 steps.
Found uncertainty sample 69 after 19 steps.
Found uncertainty sample 70 after 590 steps.
Found uncertainty sample 71 after 1054 steps.
Found uncertainty sample 72 after 74 steps.
Found uncertainty sample 73 after 326 steps.
Found uncertainty sample 74 after 1446 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 737 steps.
Found uncertainty sample 77 after 2338 steps.
Found uncertainty sample 78 after 2307 steps.
Found uncertainty sample 79 after 1639 steps.
Found uncertainty sample 80 after 1667 steps.
Found uncertainty sample 81 after 417 steps.
Found uncertainty sample 82 after 434 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 283 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 344 steps.
Found uncertainty sample 87 after 128 steps.
Found uncertainty sample 88 after 678 steps.
Found uncertainty sample 89 after 380 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 947 steps.
Found uncertainty sample 92 after 54 steps.
Found uncertainty sample 93 after 419 steps.
Found uncertainty sample 94 after 60 steps.
Found uncertainty sample 95 after 375 steps.
Found uncertainty sample 96 after 2490 steps.
Found uncertainty sample 97 after 127 steps.
Found uncertainty sample 98 after 3197 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_073923-muaf6vls
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_51
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/muaf6vls
Training model 51. Added 95 samples to the dataset.
Epoch 0, Batch 100/170, Loss: 0.548428475856781, Variance: 0.1197420135140419

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.3793776786528715, Training Loss Force: 3.5970087813581197, time: 2.8072359561920166
Validation Loss Energy: 1.4274457958066222, Validation Loss Force: 3.549646344097649, time: 0.1815938949584961
Test Loss Energy: 9.80105456203952, Test Loss Force: 11.75327517657648, time: 11.562195539474487

Epoch 1, Batch 100/170, Loss: 0.5979670882225037, Variance: 0.09960266947746277

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6684876520654053, Training Loss Force: 3.4357597207577215, time: 2.7305102348327637
Validation Loss Energy: 1.7465014380399773, Validation Loss Force: 3.5076509431756686, time: 0.1769256591796875
Test Loss Energy: 9.944639051193032, Test Loss Force: 11.614751707627198, time: 11.943177461624146

Epoch 2, Batch 100/170, Loss: 0.5467948317527771, Variance: 0.09594671428203583

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6841258807172292, Training Loss Force: 3.439779672944455, time: 2.6036689281463623
Validation Loss Energy: 1.529084277922615, Validation Loss Force: 3.55861983645549, time: 0.17864155769348145
Test Loss Energy: 10.015500776564446, Test Loss Force: 11.944024143179753, time: 11.813117265701294

Epoch 3, Batch 100/170, Loss: 0.5713420510292053, Variance: 0.09690803289413452

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6799993846187162, Training Loss Force: 3.448515555929889, time: 2.707946538925171
Validation Loss Energy: 1.773732983910362, Validation Loss Force: 3.545460015708109, time: 0.16829800605773926
Test Loss Energy: 10.178906638889414, Test Loss Force: 11.75583767176073, time: 11.859299898147583

Epoch 4, Batch 100/170, Loss: 0.7146531939506531, Variance: 0.09554527699947357

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6841730616896147, Training Loss Force: 3.441600682176948, time: 2.690027952194214
Validation Loss Energy: 1.4045702893264986, Validation Loss Force: 3.5393855271841503, time: 0.16935038566589355
Test Loss Energy: 9.970144712995276, Test Loss Force: 11.98132185932169, time: 11.636695861816406

Epoch 5, Batch 100/170, Loss: 0.4721555709838867, Variance: 0.08943658322095871

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6947829616075487, Training Loss Force: 3.4517158446530027, time: 2.7077219486236572
Validation Loss Energy: 2.1079951474574172, Validation Loss Force: 3.5253892126785673, time: 0.17988038063049316
Test Loss Energy: 10.150368759590876, Test Loss Force: 11.880046601683913, time: 11.558508157730103

Epoch 6, Batch 100/170, Loss: 0.7226908802986145, Variance: 0.09347245842218399

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.686379816401421, Training Loss Force: 3.45190596480907, time: 2.8652145862579346
Validation Loss Energy: 1.4087903892031708, Validation Loss Force: 3.547668299851324, time: 0.1738746166229248
Test Loss Energy: 9.915649076584991, Test Loss Force: 11.88942458206609, time: 11.60365080833435

Epoch 7, Batch 100/170, Loss: 0.611627459526062, Variance: 0.08787848055362701

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6700421057005816, Training Loss Force: 3.47525657476998, time: 2.6576311588287354
Validation Loss Energy: 1.9085456431747847, Validation Loss Force: 3.505752540260536, time: 0.17422127723693848
Test Loss Energy: 10.033745162543775, Test Loss Force: 11.856635372863106, time: 11.63271951675415

Epoch 8, Batch 100/170, Loss: 0.8426762819290161, Variance: 0.09330913424491882

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6811673838353138, Training Loss Force: 3.462744385487105, time: 2.972142457962036
Validation Loss Energy: 1.5297103493379154, Validation Loss Force: 3.5583185651010503, time: 0.17672038078308105
Test Loss Energy: 10.10983951957993, Test Loss Force: 12.091111952457146, time: 11.584306478500366

Epoch 9, Batch 100/170, Loss: 0.5286040902137756, Variance: 0.08963116258382797

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6902872027857951, Training Loss Force: 3.4476345798620294, time: 2.738332986831665
Validation Loss Energy: 2.0155334175179735, Validation Loss Force: 3.554194138734078, time: 0.1772618293762207
Test Loss Energy: 10.46082721449397, Test Loss Force: 12.015953225524425, time: 11.628620624542236

Epoch 10, Batch 100/170, Loss: 0.57936692237854, Variance: 0.09249572455883026

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6891694681817992, Training Loss Force: 3.47912612966646, time: 2.62145733833313
Validation Loss Energy: 1.3662234336951176, Validation Loss Force: 3.691240243755205, time: 0.17578911781311035
Test Loss Energy: 9.976843710753094, Test Loss Force: 12.08001984400509, time: 11.79777193069458

Epoch 11, Batch 100/170, Loss: 0.40464240312576294, Variance: 0.09066949784755707

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6850636662333078, Training Loss Force: 3.451887750472933, time: 2.5281589031219482
Validation Loss Energy: 1.9243407161526964, Validation Loss Force: 3.569127296559628, time: 0.1509549617767334
Test Loss Energy: 9.984276584046835, Test Loss Force: 11.678595605666425, time: 11.596283197402954

Epoch 12, Batch 100/170, Loss: 0.7049214243888855, Variance: 0.08826526254415512

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.687624094535957, Training Loss Force: 3.447420664233174, time: 2.820631504058838
Validation Loss Energy: 1.5197452370502091, Validation Loss Force: 3.5114360959872943, time: 0.18996000289916992
Test Loss Energy: 10.061676379171333, Test Loss Force: 12.027355331396667, time: 10.995112657546997

Epoch 13, Batch 100/170, Loss: 0.7173047065734863, Variance: 0.09048207104206085

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6769257763021763, Training Loss Force: 3.4646581914312544, time: 2.5768449306488037
Validation Loss Energy: 1.9826865269025333, Validation Loss Force: 3.539771190144897, time: 0.15228748321533203
Test Loss Energy: 10.208579920986919, Test Loss Force: 11.904066652250354, time: 10.012471437454224

Epoch 14, Batch 100/170, Loss: 0.507021963596344, Variance: 0.0865948498249054

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.690174298722807, Training Loss Force: 3.4555068156784308, time: 2.527909517288208
Validation Loss Energy: 1.3716804977889427, Validation Loss Force: 3.5226334593285653, time: 0.15381145477294922
Test Loss Energy: 9.782872229637674, Test Loss Force: 11.868333607714904, time: 10.215160846710205

Epoch 15, Batch 100/170, Loss: 0.8279123306274414, Variance: 0.08731557428836823

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6936497943944666, Training Loss Force: 3.451411550237452, time: 2.648547410964966
Validation Loss Energy: 2.08734829934736, Validation Loss Force: 3.562476094947716, time: 0.15897893905639648
Test Loss Energy: 10.248605592975636, Test Loss Force: 11.834450386526342, time: 10.116843461990356

Epoch 16, Batch 100/170, Loss: 0.559861421585083, Variance: 0.09191592037677765

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6605802630125, Training Loss Force: 3.451464271742115, time: 2.5825436115264893
Validation Loss Energy: 1.4354409170487814, Validation Loss Force: 3.55059498888235, time: 0.1578531265258789
Test Loss Energy: 9.828082222247113, Test Loss Force: 11.876832928652407, time: 10.04860258102417

Epoch 17, Batch 100/170, Loss: 0.35747772455215454, Variance: 0.08921229839324951

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.686837770865121, Training Loss Force: 3.441063681476262, time: 2.7992470264434814
Validation Loss Energy: 1.923801151700589, Validation Loss Force: 3.495023450133152, time: 0.15304207801818848
Test Loss Energy: 10.214832811020228, Test Loss Force: 11.680269625310672, time: 10.023009777069092

Epoch 18, Batch 100/170, Loss: 0.8879271149635315, Variance: 0.09196770936250687

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.652650578883903, Training Loss Force: 3.4573301969922374, time: 2.6043598651885986
Validation Loss Energy: 1.4722862775176468, Validation Loss Force: 3.506242857719869, time: 0.15529489517211914
Test Loss Energy: 9.950593788405751, Test Loss Force: 12.15863630334294, time: 10.966966152191162

Epoch 19, Batch 100/170, Loss: 0.3599891662597656, Variance: 0.08833381533622742

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6637130816745793, Training Loss Force: 3.4494906201904514, time: 2.5965750217437744
Validation Loss Energy: 2.04361734087897, Validation Loss Force: 3.5550517118380944, time: 0.1560840606689453
Test Loss Energy: 10.116679533358122, Test Loss Force: 11.768922848603124, time: 10.538437604904175

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.054 MB uploadedwandb: | 0.039 MB of 0.054 MB uploadedwandb: / 0.057 MB of 0.057 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–ƒâ–ƒâ–…â–ƒâ–…â–‚â–„â–„â–ˆâ–ƒâ–ƒâ–„â–…â–â–†â–â–…â–ƒâ–„
wandb:   test_error_force â–ƒâ–â–…â–ƒâ–†â–„â–…â–„â–‡â–†â–‡â–‚â–†â–…â–„â–„â–„â–‚â–ˆâ–ƒ
wandb:          test_loss â–â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–…â–ˆâ–ˆâ–„â–†â–‡â–‡â–‡â–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–…â–ƒâ–…â–â–ˆâ–â–†â–ƒâ–‡â–â–†â–‚â–‡â–â–ˆâ–‚â–†â–‚â–‡
wandb:  valid_error_force â–ƒâ–â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–ƒâ–ˆâ–„â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–â–ƒ
wandb:         valid_loss â–ƒâ–ƒâ–ƒâ–„â–‚â–ˆâ–‚â–…â–ƒâ–‡â–‚â–†â–ƒâ–‡â–â–ˆâ–‚â–†â–‚â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5436
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.11668
wandb:   test_error_force 11.76892
wandb:          test_loss 14.118
wandb: train_error_energy 1.66371
wandb:  train_error_force 3.44949
wandb:         train_loss 0.56167
wandb: valid_error_energy 2.04362
wandb:  valid_error_force 3.55505
wandb:         valid_loss 0.79663
wandb: 
wandb: ğŸš€ View run al_63_51 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/muaf6vls
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_073923-muaf6vls/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.8615996837615967, Uncertainty Bias: -0.1003967672586441
1.5258789e-05 0.0057189465
2.1181664 5.0182242
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 846 steps.
Found uncertainty sample 2 after 253 steps.
Found uncertainty sample 3 after 426 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 528 steps.
Found uncertainty sample 6 after 496 steps.
Found uncertainty sample 7 after 51 steps.
Found uncertainty sample 8 after 101 steps.
Found uncertainty sample 9 after 580 steps.
Found uncertainty sample 10 after 16 steps.
Found uncertainty sample 11 after 53 steps.
Found uncertainty sample 12 after 571 steps.
Found uncertainty sample 13 after 407 steps.
Found uncertainty sample 14 after 105 steps.
Found uncertainty sample 15 after 471 steps.
Found uncertainty sample 16 after 942 steps.
Found uncertainty sample 17 after 103 steps.
Found uncertainty sample 18 after 827 steps.
Found uncertainty sample 19 after 2326 steps.
Found uncertainty sample 20 after 608 steps.
Found uncertainty sample 21 after 284 steps.
Found uncertainty sample 22 after 2845 steps.
Found uncertainty sample 23 after 1865 steps.
Found uncertainty sample 24 after 3854 steps.
Found uncertainty sample 25 after 161 steps.
Found uncertainty sample 26 after 118 steps.
Found uncertainty sample 27 after 273 steps.
Found uncertainty sample 28 after 139 steps.
Found uncertainty sample 29 after 215 steps.
Found uncertainty sample 30 after 907 steps.
Found uncertainty sample 31 after 100 steps.
Found uncertainty sample 32 after 406 steps.
Found uncertainty sample 33 after 97 steps.
Found uncertainty sample 34 after 915 steps.
Found uncertainty sample 35 after 2961 steps.
Found uncertainty sample 36 after 31 steps.
Found uncertainty sample 37 after 119 steps.
Found uncertainty sample 38 after 548 steps.
Found uncertainty sample 39 after 1972 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 2625 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 556 steps.
Found uncertainty sample 44 after 665 steps.
Found uncertainty sample 45 after 2269 steps.
Found uncertainty sample 46 after 1817 steps.
Found uncertainty sample 47 after 373 steps.
Found uncertainty sample 48 after 3000 steps.
Found uncertainty sample 49 after 1283 steps.
Found uncertainty sample 50 after 5 steps.
Found uncertainty sample 51 after 45 steps.
Found uncertainty sample 52 after 220 steps.
Found uncertainty sample 53 after 2313 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 2141 steps.
Found uncertainty sample 56 after 105 steps.
Found uncertainty sample 57 after 687 steps.
Found uncertainty sample 58 after 1099 steps.
Found uncertainty sample 59 after 361 steps.
Found uncertainty sample 60 after 750 steps.
Found uncertainty sample 61 after 10 steps.
Found uncertainty sample 62 after 11 steps.
Found uncertainty sample 63 after 734 steps.
Found uncertainty sample 64 after 1257 steps.
Found uncertainty sample 65 after 133 steps.
Found uncertainty sample 66 after 493 steps.
Found uncertainty sample 67 after 131 steps.
Found uncertainty sample 68 after 3961 steps.
Found uncertainty sample 69 after 2800 steps.
Found uncertainty sample 70 after 15 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 88 steps.
Found uncertainty sample 73 after 17 steps.
Found uncertainty sample 74 after 366 steps.
Found uncertainty sample 75 after 402 steps.
Found uncertainty sample 76 after 240 steps.
Found uncertainty sample 77 after 1271 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 155 steps.
Found uncertainty sample 80 after 2386 steps.
Found uncertainty sample 81 after 2183 steps.
Found uncertainty sample 82 after 165 steps.
Found uncertainty sample 83 after 51 steps.
Found uncertainty sample 84 after 12 steps.
Found uncertainty sample 85 after 153 steps.
Found uncertainty sample 86 after 797 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 375 steps.
Found uncertainty sample 89 after 444 steps.
Found uncertainty sample 90 after 220 steps.
Found uncertainty sample 91 after 2989 steps.
Found uncertainty sample 92 after 119 steps.
Found uncertainty sample 93 after 1324 steps.
Found uncertainty sample 94 after 3506 steps.
Found uncertainty sample 95 after 1501 steps.
Found uncertainty sample 96 after 1566 steps.
Found uncertainty sample 97 after 273 steps.
Found uncertainty sample 98 after 1207 steps.
Found uncertainty sample 99 after 403 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_075540-67n5269k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_52
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/67n5269k
Training model 52. Added 96 samples to the dataset.
Epoch 0, Batch 100/173, Loss: 0.8014395236968994, Variance: 0.09145833551883698

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.118588285414681, Training Loss Force: 3.5367036369224563, time: 2.801703453063965
Validation Loss Energy: 1.5909049149177035, Validation Loss Force: 3.599092328865422, time: 0.17488694190979004
Test Loss Energy: 9.785342985133802, Test Loss Force: 11.822447871372118, time: 11.452277183532715

Epoch 1, Batch 100/173, Loss: 0.8036409616470337, Variance: 0.09126483649015427

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.665360715671416, Training Loss Force: 3.4614534884456396, time: 2.6872406005859375
Validation Loss Energy: 1.4253464710988153, Validation Loss Force: 3.529609609447434, time: 0.174302339553833
Test Loss Energy: 9.839342803807108, Test Loss Force: 11.9160394579778, time: 11.728343725204468

Epoch 2, Batch 100/173, Loss: 0.5119813084602356, Variance: 0.08952440321445465

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.642036470797353, Training Loss Force: 3.4620232447316637, time: 2.6759657859802246
Validation Loss Energy: 1.9794542615802453, Validation Loss Force: 3.535512197390736, time: 0.17499971389770508
Test Loss Energy: 10.146877862306882, Test Loss Force: 11.830563161478738, time: 11.768500566482544

Epoch 3, Batch 100/173, Loss: 0.7483938336372375, Variance: 0.0885283499956131

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6809064727853849, Training Loss Force: 3.456507363162139, time: 2.6845290660858154
Validation Loss Energy: 2.2013963927465787, Validation Loss Force: 3.6610791883779457, time: 0.1686549186706543
Test Loss Energy: 10.200947899870227, Test Loss Force: 11.913877883836923, time: 11.738641738891602

Epoch 4, Batch 100/173, Loss: 0.51682049036026, Variance: 0.0963626503944397

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.705477874674741, Training Loss Force: 3.4744479462471345, time: 2.763704538345337
Validation Loss Energy: 1.632707966809127, Validation Loss Force: 3.5458308340591276, time: 0.16667509078979492
Test Loss Energy: 9.894425857745652, Test Loss Force: 11.995213168420763, time: 11.543551206588745

Epoch 5, Batch 100/173, Loss: 0.5614299178123474, Variance: 0.09059643745422363

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6948482629536281, Training Loss Force: 3.4593627551629713, time: 2.8096601963043213
Validation Loss Energy: 1.5485361720610504, Validation Loss Force: 3.6318459211087184, time: 0.18151473999023438
Test Loss Energy: 10.210474856403291, Test Loss Force: 12.241782277462725, time: 11.489439964294434

Epoch 6, Batch 100/173, Loss: 0.5036165714263916, Variance: 0.08834139257669449

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.660810319796005, Training Loss Force: 3.4657856505385736, time: 2.940223217010498
Validation Loss Energy: 2.044311845945119, Validation Loss Force: 3.532674798687227, time: 0.1790788173675537
Test Loss Energy: 10.10469377318277, Test Loss Force: 11.831479532449764, time: 11.50133204460144

Epoch 7, Batch 100/173, Loss: 0.5389675498008728, Variance: 0.115513876080513

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6884398359240922, Training Loss Force: 3.4534214223592423, time: 2.731849193572998
Validation Loss Energy: 1.9253139287664982, Validation Loss Force: 3.561349389069321, time: 0.17943549156188965
Test Loss Energy: 10.161681205574764, Test Loss Force: 11.92752148225471, time: 11.485753536224365

Epoch 8, Batch 100/173, Loss: 0.7520678043365479, Variance: 0.09209839254617691

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6766859532052603, Training Loss Force: 3.444321613956556, time: 2.7042410373687744
Validation Loss Energy: 1.47612000931461, Validation Loss Force: 3.5704326573161054, time: 0.17626357078552246
Test Loss Energy: 9.775284359965456, Test Loss Force: 11.948235607146424, time: 11.63106083869934

Epoch 9, Batch 100/173, Loss: 0.7062419652938843, Variance: 0.09042097628116608

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6789321722981494, Training Loss Force: 3.4471438823130507, time: 2.7363669872283936
Validation Loss Energy: 1.5049766688350241, Validation Loss Force: 3.546518436381753, time: 0.17774510383605957
Test Loss Energy: 9.973158916441086, Test Loss Force: 12.202287168924206, time: 11.493857383728027

Epoch 10, Batch 100/173, Loss: 0.4848356246948242, Variance: 0.08646377921104431

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6618309482813576, Training Loss Force: 3.442030699824594, time: 2.671159267425537
Validation Loss Energy: 1.9594654750194538, Validation Loss Force: 3.5728794201146084, time: 0.1760106086730957
Test Loss Energy: 10.655472499439808, Test Loss Force: 12.213755791083303, time: 11.76232099533081

Epoch 11, Batch 100/173, Loss: 0.41621559858322144, Variance: 0.08876746147871017

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.690981139579993, Training Loss Force: 3.4448009035011933, time: 2.63318133354187
Validation Loss Energy: 1.8296013035195573, Validation Loss Force: 3.5190263676106324, time: 0.16809558868408203
Test Loss Energy: 10.471453503755972, Test Loss Force: 12.104691659985539, time: 11.685679197311401

Epoch 12, Batch 100/173, Loss: 0.5935127139091492, Variance: 0.09020616114139557

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6623001437058975, Training Loss Force: 3.4442653352952513, time: 2.6837289333343506
Validation Loss Energy: 1.6619234576582043, Validation Loss Force: 3.511501486493149, time: 0.17182683944702148
Test Loss Energy: 9.988533705680684, Test Loss Force: 12.081602781476384, time: 11.719817161560059

Epoch 13, Batch 100/173, Loss: 0.7179676294326782, Variance: 0.08570711314678192

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6728770446204595, Training Loss Force: 3.4497969980578294, time: 2.7438628673553467
Validation Loss Energy: 1.640562324039535, Validation Loss Force: 3.587011043237867, time: 0.16841650009155273
Test Loss Energy: 10.124804420388351, Test Loss Force: 12.12232457550562, time: 12.315669059753418

Epoch 14, Batch 100/173, Loss: 0.17499572038650513, Variance: 0.08227305114269257

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6635064409329128, Training Loss Force: 3.46091519929996, time: 2.7703568935394287
Validation Loss Energy: 1.8236257172079149, Validation Loss Force: 3.5515774793772317, time: 0.17933940887451172
Test Loss Energy: 10.281384067427336, Test Loss Force: 12.122796685390503, time: 11.616735935211182

Epoch 15, Batch 100/173, Loss: 0.4473206400871277, Variance: 0.08658457547426224

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.693703263131659, Training Loss Force: 3.4769910981856564, time: 2.762162208557129
Validation Loss Energy: 2.1074370643546736, Validation Loss Force: 3.603902873094046, time: 0.17557907104492188
Test Loss Energy: 10.233233121360445, Test Loss Force: 11.961436590597614, time: 10.545926570892334

Epoch 16, Batch 100/173, Loss: 0.4883165955543518, Variance: 0.0880320817232132

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6785094207664515, Training Loss Force: 3.44610183575878, time: 2.688938617706299
Validation Loss Energy: 1.5944816865202658, Validation Loss Force: 3.517979896847821, time: 0.18848323822021484
Test Loss Energy: 9.949443015606684, Test Loss Force: 12.010549961677098, time: 12.433934211730957

Epoch 17, Batch 100/173, Loss: 0.5483767986297607, Variance: 0.08890016376972198

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6635417893380524, Training Loss Force: 3.45303127255085, time: 2.580383539199829
Validation Loss Energy: 1.4613601994191647, Validation Loss Force: 3.488284294208599, time: 0.1551048755645752
Test Loss Energy: 10.112650578816229, Test Loss Force: 12.151849846740475, time: 9.929821729660034

Epoch 18, Batch 100/173, Loss: 0.559388279914856, Variance: 0.08770236372947693

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6892493695542385, Training Loss Force: 3.449959555544091, time: 2.6633718013763428
Validation Loss Energy: 2.059692865646178, Validation Loss Force: 3.5369465578057544, time: 0.15242362022399902
Test Loss Energy: 9.872484562387484, Test Loss Force: 11.824390073269393, time: 10.111192226409912

Epoch 19, Batch 100/173, Loss: 0.5075164437294006, Variance: 0.09027041494846344

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6793250301752298, Training Loss Force: 3.45529319721938, time: 2.6020476818084717
Validation Loss Energy: 2.0324408342143676, Validation Loss Force: 3.521166811887387, time: 0.15654683113098145
Test Loss Energy: 9.661259346825545, Test Loss Force: 11.715400549159389, time: 9.97724723815918

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–„â–…â–ƒâ–…â–„â–…â–‚â–ƒâ–ˆâ–‡â–ƒâ–„â–…â–…â–ƒâ–„â–‚â–
wandb:   test_error_force â–‚â–„â–ƒâ–„â–…â–ˆâ–ƒâ–„â–„â–‡â–ˆâ–†â–†â–†â–†â–„â–…â–‡â–‚â–
wandb:          test_loss â–â–‚â–„â–…â–…â–…â–ƒâ–…â–‚â–„â–ˆâ–‡â–†â–…â–†â–…â–„â–‡â–‚â–‚
wandb: train_error_energy â–ˆâ–â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–â–â–â–â–â–‚â–‚â–„â–â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–â–†â–ˆâ–ƒâ–‚â–‡â–†â–â–‚â–†â–…â–ƒâ–ƒâ–…â–‡â–ƒâ–â–‡â–†
wandb:  valid_error_force â–…â–ƒâ–ƒâ–ˆâ–ƒâ–‡â–ƒâ–„â–„â–ƒâ–„â–‚â–‚â–…â–„â–†â–‚â–â–ƒâ–‚
wandb:         valid_loss â–ƒâ–â–…â–ˆâ–ƒâ–ƒâ–†â–…â–‚â–‚â–…â–ƒâ–ƒâ–ƒâ–„â–‡â–‚â–â–†â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 5522
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.66126
wandb:   test_error_force 11.7154
wandb:          test_loss 13.90157
wandb: train_error_energy 1.67933
wandb:  train_error_force 3.45529
wandb:         train_loss 0.57352
wandb: valid_error_energy 2.03244
wandb:  valid_error_force 3.52117
wandb:         valid_loss 0.82276
wandb: 
wandb: ğŸš€ View run al_63_52 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/67n5269k
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_075540-67n5269k/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.741396427154541, Uncertainty Bias: -0.08551467955112457
1.5258789e-05 0.002243042
2.038674 5.0916085
(48745, 22, 3)
Found uncertainty sample 0 after 1762 steps.
Found uncertainty sample 1 after 1365 steps.
Found uncertainty sample 2 after 461 steps.
Found uncertainty sample 3 after 109 steps.
Found uncertainty sample 4 after 3813 steps.
Found uncertainty sample 5 after 2110 steps.
Found uncertainty sample 6 after 25 steps.
Found uncertainty sample 7 after 184 steps.
Found uncertainty sample 8 after 2527 steps.
Found uncertainty sample 9 after 2389 steps.
Found uncertainty sample 10 after 18 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 408 steps.
Found uncertainty sample 13 after 126 steps.
Found uncertainty sample 14 after 141 steps.
Found uncertainty sample 15 after 542 steps.
Found uncertainty sample 16 after 134 steps.
Found uncertainty sample 17 after 723 steps.
Found uncertainty sample 18 after 971 steps.
Found uncertainty sample 19 after 627 steps.
Found uncertainty sample 20 after 261 steps.
Found uncertainty sample 21 after 97 steps.
Found uncertainty sample 22 after 168 steps.
Found uncertainty sample 23 after 2168 steps.
Found uncertainty sample 24 after 139 steps.
Found uncertainty sample 25 after 2062 steps.
Found uncertainty sample 26 after 827 steps.
Found uncertainty sample 27 after 1755 steps.
Found uncertainty sample 28 after 348 steps.
Found uncertainty sample 29 after 1652 steps.
Found uncertainty sample 30 after 78 steps.
Found uncertainty sample 31 after 1553 steps.
Found uncertainty sample 32 after 660 steps.
Found uncertainty sample 33 after 974 steps.
Found uncertainty sample 34 after 2215 steps.
Found uncertainty sample 35 after 810 steps.
Found uncertainty sample 36 after 295 steps.
Found uncertainty sample 37 after 133 steps.
Found uncertainty sample 38 after 213 steps.
Found uncertainty sample 39 after 28 steps.
Found uncertainty sample 40 after 2146 steps.
Found uncertainty sample 41 after 13 steps.
Found uncertainty sample 42 after 77 steps.
Found uncertainty sample 43 after 84 steps.
Found uncertainty sample 44 after 112 steps.
Found uncertainty sample 45 after 29 steps.
Found uncertainty sample 46 after 527 steps.
Found uncertainty sample 47 after 615 steps.
Found uncertainty sample 48 after 768 steps.
Found uncertainty sample 49 after 270 steps.
Found uncertainty sample 50 after 1242 steps.
Found uncertainty sample 51 after 942 steps.
Found uncertainty sample 52 after 1125 steps.
Found uncertainty sample 53 after 856 steps.
Found uncertainty sample 54 after 930 steps.
Found uncertainty sample 55 after 114 steps.
Found uncertainty sample 56 after 754 steps.
Found uncertainty sample 57 after 1550 steps.
Found uncertainty sample 58 after 15 steps.
Found uncertainty sample 59 after 43 steps.
Found uncertainty sample 60 after 858 steps.
Found uncertainty sample 61 after 108 steps.
Found uncertainty sample 62 after 2619 steps.
Found uncertainty sample 63 after 103 steps.
Found uncertainty sample 64 after 2329 steps.
Found uncertainty sample 65 after 2529 steps.
Found uncertainty sample 66 after 3471 steps.
Found uncertainty sample 67 after 195 steps.
Found uncertainty sample 68 after 730 steps.
Found uncertainty sample 69 after 1231 steps.
Found uncertainty sample 70 after 51 steps.
Found uncertainty sample 71 after 846 steps.
Found uncertainty sample 72 after 47 steps.
Found uncertainty sample 73 after 492 steps.
Found uncertainty sample 74 after 3902 steps.
Found uncertainty sample 75 after 76 steps.
Found uncertainty sample 76 after 186 steps.
Found uncertainty sample 77 after 3843 steps.
Found uncertainty sample 78 after 238 steps.
Found uncertainty sample 79 after 928 steps.
Found uncertainty sample 80 after 224 steps.
Found uncertainty sample 81 after 114 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 1379 steps.
Found uncertainty sample 84 after 5 steps.
Found uncertainty sample 85 after 587 steps.
Found uncertainty sample 86 after 32 steps.
Found uncertainty sample 87 after 775 steps.
Found uncertainty sample 88 after 2608 steps.
Found uncertainty sample 89 after 1764 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 329 steps.
Found uncertainty sample 92 after 593 steps.
Found uncertainty sample 93 after 39 steps.
Found uncertainty sample 94 after 328 steps.
Found uncertainty sample 95 after 32 steps.
Found uncertainty sample 96 after 43 steps.
Found uncertainty sample 97 after 1024 steps.
Found uncertainty sample 98 after 567 steps.
Found uncertainty sample 99 after 2112 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_081059-aqz8jscv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_53
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/aqz8jscv
Training model 53. Added 100 samples to the dataset.
Epoch 0, Batch 100/176, Loss: 0.8684443235397339, Variance: 0.1107533648610115

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.012517071775994, Training Loss Force: 3.7646167017746524, time: 2.7233965396881104
Validation Loss Energy: 2.5780825210984775, Validation Loss Force: 3.5181897996293814, time: 0.1685471534729004
Test Loss Energy: 10.215701943489716, Test Loss Force: 11.446148551580688, time: 10.811068296432495

Epoch 1, Batch 100/176, Loss: 0.5165573358535767, Variance: 0.1185334175825119

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6969669919318835, Training Loss Force: 3.4508085974511884, time: 2.8251876831054688
Validation Loss Energy: 3.108275051522874, Validation Loss Force: 3.5895462056873035, time: 0.17667031288146973
Test Loss Energy: 10.274313713492848, Test Loss Force: 11.795913095730976, time: 11.60333800315857

Epoch 2, Batch 100/176, Loss: 1.5488635301589966, Variance: 0.12940877676010132

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6874785459666657, Training Loss Force: 3.4489640479967876, time: 2.7453560829162598
Validation Loss Energy: 2.9924230411494457, Validation Loss Force: 3.5375220833249297, time: 0.16740131378173828
Test Loss Energy: 10.415438983976768, Test Loss Force: 11.576812643723022, time: 11.449572801589966

Epoch 3, Batch 100/176, Loss: 0.8612110018730164, Variance: 0.11932609975337982

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6895661982566303, Training Loss Force: 3.4312784903535247, time: 2.757620334625244
Validation Loss Energy: 2.396136073477596, Validation Loss Force: 3.5456950801031604, time: 0.16799426078796387
Test Loss Energy: 10.099894607230922, Test Loss Force: 11.507104116029334, time: 11.33876347541809

Epoch 4, Batch 100/176, Loss: 0.5477741956710815, Variance: 0.1284891664981842

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.7350251886447094, Training Loss Force: 3.4504321486915206, time: 2.987720251083374
Validation Loss Energy: 3.470342950444613, Validation Loss Force: 3.5520787818477637, time: 0.1771378517150879
Test Loss Energy: 10.270416126752215, Test Loss Force: 11.757685517898869, time: 11.329282760620117

Epoch 5, Batch 100/176, Loss: 1.6600980758666992, Variance: 0.15135493874549866

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.680628074024911, Training Loss Force: 3.4627583864050266, time: 2.786181688308716
Validation Loss Energy: 2.7215957545708824, Validation Loss Force: 3.606158066204003, time: 0.1848757266998291
Test Loss Energy: 10.45450544391711, Test Loss Force: 11.741316220636747, time: 11.307514429092407

Epoch 6, Batch 100/176, Loss: 0.6703792810440063, Variance: 0.1207546666264534

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.748562719311003, Training Loss Force: 3.4505985647792925, time: 3.1179215908050537
Validation Loss Energy: 2.3005620968855194, Validation Loss Force: 3.558079641107469, time: 0.17387628555297852
Test Loss Energy: 10.386498677412149, Test Loss Force: 11.639893936077463, time: 12.181398868560791

Epoch 7, Batch 100/176, Loss: 0.5828897356987, Variance: 0.12353436648845673

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.713687094113372, Training Loss Force: 3.4442820445933133, time: 2.8423635959625244
Validation Loss Energy: 3.2101180552494015, Validation Loss Force: 3.585010212515608, time: 0.1991419792175293
Test Loss Energy: 9.971494689576035, Test Loss Force: 11.521688380836991, time: 11.329365730285645

Epoch 8, Batch 100/176, Loss: 1.4722836017608643, Variance: 0.1347966194152832

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6955815160396948, Training Loss Force: 3.439289623953263, time: 2.946380138397217
Validation Loss Energy: 2.7163051356636383, Validation Loss Force: 3.5169773661298045, time: 0.17568397521972656
Test Loss Energy: 10.99673677135404, Test Loss Force: 11.810953615447675, time: 11.646762609481812

Epoch 9, Batch 100/176, Loss: 0.8951236009597778, Variance: 0.12441158294677734

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7041380214125135, Training Loss Force: 3.4352545300859947, time: 2.7306644916534424
Validation Loss Energy: 2.4199351002404548, Validation Loss Force: 3.567105163822409, time: 0.1843245029449463
Test Loss Energy: 10.127304223132857, Test Loss Force: 11.69345200714462, time: 11.542097330093384

Epoch 10, Batch 100/176, Loss: 0.606509804725647, Variance: 0.13259993493556976

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.7068796441279086, Training Loss Force: 3.44015976563409, time: 2.7016756534576416
Validation Loss Energy: 3.345020153287438, Validation Loss Force: 3.5072797537094065, time: 0.17250394821166992
Test Loss Energy: 10.538149390185298, Test Loss Force: 11.916525025939224, time: 11.598774671554565

Epoch 11, Batch 100/176, Loss: 1.3056286573410034, Variance: 0.13865573704242706

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.7152166850192687, Training Loss Force: 3.448421882785925, time: 2.845950126647949
Validation Loss Energy: 2.7615000695434553, Validation Loss Force: 3.550777874474205, time: 0.1678156852722168
Test Loss Energy: 10.182295294895411, Test Loss Force: 11.870453972908292, time: 11.323489904403687

Epoch 12, Batch 100/176, Loss: 0.8462183475494385, Variance: 0.13639920949935913

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.7382888794153817, Training Loss Force: 3.4491235815836863, time: 2.768928289413452
Validation Loss Energy: 2.3759721947450565, Validation Loss Force: 3.494950563099385, time: 0.18409395217895508
Test Loss Energy: 10.4262030389891, Test Loss Force: 11.618020705197914, time: 11.511521577835083

Epoch 13, Batch 100/176, Loss: 0.8089411854743958, Variance: 0.13465717434883118

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.715420047105529, Training Loss Force: 3.4390415388955584, time: 2.825026512145996
Validation Loss Energy: 3.237811194259294, Validation Loss Force: 3.5982218409509414, time: 0.1802690029144287
Test Loss Energy: 10.261387510762576, Test Loss Force: 11.774502214599705, time: 11.311436653137207

Epoch 14, Batch 100/176, Loss: 1.3417586088180542, Variance: 0.13930584490299225

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.704616858126592, Training Loss Force: 3.452830901407486, time: 2.8404314517974854
Validation Loss Energy: 2.6876584531478342, Validation Loss Force: 3.5325327357721257, time: 0.18617463111877441
Test Loss Energy: 10.480415388950256, Test Loss Force: 11.679669369915212, time: 11.43711233139038

Epoch 15, Batch 100/176, Loss: 0.7979453206062317, Variance: 0.12854084372520447

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6973340069842417, Training Loss Force: 3.443395262189325, time: 2.8088152408599854
Validation Loss Energy: 2.232869414731645, Validation Loss Force: 3.49400429345224, time: 0.18449187278747559
Test Loss Energy: 10.343102232897532, Test Loss Force: 11.698790531395032, time: 11.457412719726562

Epoch 16, Batch 100/176, Loss: 0.7781358957290649, Variance: 0.12975174188613892

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6973520950568513, Training Loss Force: 3.419641666679784, time: 2.8015189170837402
Validation Loss Energy: 3.349845245233596, Validation Loss Force: 3.5655700827303822, time: 0.1778090000152588
Test Loss Energy: 10.31933360027879, Test Loss Force: 11.83828240827867, time: 11.611629962921143

Epoch 17, Batch 100/176, Loss: 1.288432240486145, Variance: 0.13475283980369568

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.70560560591968, Training Loss Force: 3.4273574170057937, time: 2.711041212081909
Validation Loss Energy: 2.6967419440637634, Validation Loss Force: 3.599024785891157, time: 0.17969942092895508
Test Loss Energy: 10.551835335378247, Test Loss Force: 11.72009646657595, time: 11.56487512588501

Epoch 18, Batch 100/176, Loss: 0.8795030117034912, Variance: 0.1364608108997345

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6954750561497947, Training Loss Force: 3.4313762889197146, time: 2.777442455291748
Validation Loss Energy: 2.482970286092007, Validation Loss Force: 3.505119269209518, time: 0.16690373420715332
Test Loss Energy: 10.338635788276825, Test Loss Force: 11.759253085370059, time: 11.586402416229248

Epoch 19, Batch 100/176, Loss: 0.9497908353805542, Variance: 0.13106392323970795

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.685057996345129, Training Loss Force: 3.4380116586710057, time: 2.8733625411987305
Validation Loss Energy: 3.575647346044588, Validation Loss Force: 3.523760313653057, time: 0.17375946044921875
Test Loss Energy: 10.282675856246776, Test Loss Force: 11.756841203129792, time: 11.320334672927856

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–ƒâ–„â–‚â–ƒâ–„â–„â–â–ˆâ–‚â–…â–‚â–„â–ƒâ–„â–„â–ƒâ–…â–„â–ƒ
wandb:   test_error_force â–â–†â–ƒâ–‚â–†â–…â–„â–‚â–†â–…â–ˆâ–‡â–„â–†â–„â–…â–‡â–…â–†â–†
wandb:          test_loss â–„â–‡â–…â–‚â–…â–†â–„â–â–ˆâ–ƒâ–†â–ƒâ–ƒâ–ƒâ–…â–„â–ƒâ–„â–ƒâ–
wandb: train_error_energy â–ˆâ–â–â–â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–†â–…â–‚â–‡â–„â–â–†â–„â–‚â–‡â–„â–‚â–†â–ƒâ–â–‡â–ƒâ–‚â–ˆ
wandb:  valid_error_force â–ƒâ–‡â–„â–„â–…â–ˆâ–…â–‡â–‚â–†â–‚â–…â–â–ˆâ–ƒâ–â–…â–ˆâ–‚â–ƒ
wandb:         valid_loss â–ƒâ–†â–…â–‚â–ˆâ–„â–‚â–†â–ƒâ–‚â–‡â–„â–‚â–‡â–ƒâ–â–‡â–„â–‚â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5612
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.28268
wandb:   test_error_force 11.75684
wandb:          test_loss 11.12661
wandb: train_error_energy 2.68506
wandb:  train_error_force 3.43801
wandb:         train_loss 0.992
wandb: valid_error_energy 3.57565
wandb:  valid_error_force 3.52376
wandb:         valid_loss 1.3801
wandb: 
wandb: ğŸš€ View run al_63_53 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/aqz8jscv
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_081059-aqz8jscv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.65446138381958, Uncertainty Bias: -0.11560724675655365
3.4332275e-05 0.04406953
2.082309 4.772107
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 25 steps.
Found uncertainty sample 2 after 1377 steps.
Found uncertainty sample 3 after 717 steps.
Found uncertainty sample 4 after 85 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 848 steps.
Found uncertainty sample 7 after 16 steps.
Found uncertainty sample 8 after 932 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 2391 steps.
Found uncertainty sample 11 after 717 steps.
Found uncertainty sample 12 after 579 steps.
Found uncertainty sample 13 after 7 steps.
Found uncertainty sample 14 after 987 steps.
Found uncertainty sample 15 after 367 steps.
Found uncertainty sample 16 after 931 steps.
Found uncertainty sample 17 after 258 steps.
Found uncertainty sample 18 after 30 steps.
Found uncertainty sample 19 after 1609 steps.
Found uncertainty sample 20 after 893 steps.
Found uncertainty sample 21 after 785 steps.
Found uncertainty sample 22 after 111 steps.
Found uncertainty sample 23 after 2361 steps.
Found uncertainty sample 24 after 860 steps.
Found uncertainty sample 25 after 2257 steps.
Found uncertainty sample 26 after 9 steps.
Found uncertainty sample 27 after 3560 steps.
Found uncertainty sample 28 after 228 steps.
Found uncertainty sample 29 after 98 steps.
Found uncertainty sample 30 after 2087 steps.
Found uncertainty sample 31 after 1990 steps.
Found uncertainty sample 32 after 1190 steps.
Found uncertainty sample 33 after 224 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 883 steps.
Found uncertainty sample 36 after 575 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 527 steps.
Found uncertainty sample 39 after 2279 steps.
Found uncertainty sample 40 after 459 steps.
Found uncertainty sample 41 after 1001 steps.
Found uncertainty sample 42 after 1906 steps.
Found uncertainty sample 43 after 14 steps.
Found uncertainty sample 44 after 545 steps.
Found uncertainty sample 45 after 1438 steps.
Found uncertainty sample 46 after 1442 steps.
Found uncertainty sample 47 after 1486 steps.
Found uncertainty sample 48 after 301 steps.
Found uncertainty sample 49 after 817 steps.
Found uncertainty sample 50 after 2184 steps.
Found uncertainty sample 51 after 1336 steps.
Found uncertainty sample 52 after 2124 steps.
Found uncertainty sample 53 after 1895 steps.
Found uncertainty sample 54 after 1079 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 1741 steps.
Found uncertainty sample 57 after 1311 steps.
Found uncertainty sample 58 after 582 steps.
Found uncertainty sample 59 after 2284 steps.
Found uncertainty sample 60 after 593 steps.
Found uncertainty sample 61 after 3284 steps.
Found uncertainty sample 62 after 2116 steps.
Found uncertainty sample 63 after 2759 steps.
Found uncertainty sample 64 after 180 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 685 steps.
Found uncertainty sample 67 after 1408 steps.
Found uncertainty sample 68 after 700 steps.
Found uncertainty sample 69 after 183 steps.
Found uncertainty sample 70 after 1082 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 591 steps.
Found uncertainty sample 73 after 171 steps.
Found uncertainty sample 74 after 3692 steps.
Found uncertainty sample 75 after 120 steps.
Found uncertainty sample 76 after 1482 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 2184 steps.
Found uncertainty sample 79 after 1442 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 156 steps.
Found uncertainty sample 82 after 634 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 493 steps.
Found uncertainty sample 85 after 80 steps.
Found uncertainty sample 86 after 1822 steps.
Found uncertainty sample 87 after 20 steps.
Found uncertainty sample 88 after 295 steps.
Found uncertainty sample 89 after 683 steps.
Found uncertainty sample 90 after 1428 steps.
Found uncertainty sample 91 after 2142 steps.
Found uncertainty sample 92 after 24 steps.
Found uncertainty sample 93 after 1187 steps.
Found uncertainty sample 94 after 72 steps.
Found uncertainty sample 95 after 1388 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 3797 steps.
Found uncertainty sample 98 after 1119 steps.
Found uncertainty sample 99 after 1576 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_082915-m1ng6rwe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_54
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/m1ng6rwe
Training model 54. Added 95 samples to the dataset.
Epoch 0, Batch 100/179, Loss: 0.657721996307373, Variance: 0.12005063146352768

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7831662536722366, Training Loss Force: 3.597655501259681, time: 2.8465123176574707
Validation Loss Energy: 1.9240220394933496, Validation Loss Force: 3.499012419436351, time: 0.1683669090270996
Test Loss Energy: 9.849102296876945, Test Loss Force: 11.645240327193825, time: 10.180853366851807

Epoch 1, Batch 100/179, Loss: 0.7523912191390991, Variance: 0.1364617645740509

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6819943838739215, Training Loss Force: 3.413650312352443, time: 2.8249447345733643
Validation Loss Energy: 3.493599852060187, Validation Loss Force: 3.53170737547122, time: 0.15819120407104492
Test Loss Energy: 10.326013362220337, Test Loss Force: 11.719286370575311, time: 9.90942096710205

Epoch 2, Batch 100/179, Loss: 1.4215989112854004, Variance: 0.1373719722032547

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.661425544187988, Training Loss Force: 3.4236080703385254, time: 3.064133882522583
Validation Loss Energy: 2.2302402859026267, Validation Loss Force: 3.5381577295135043, time: 0.17695975303649902
Test Loss Energy: 9.741995677054591, Test Loss Force: 11.582428597407961, time: 11.317031383514404

Epoch 3, Batch 100/179, Loss: 0.9083905220031738, Variance: 0.1364288628101349

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6635440035148044, Training Loss Force: 3.421188227700715, time: 2.9259562492370605
Validation Loss Energy: 2.1827969132102796, Validation Loss Force: 3.5049933869190197, time: 0.18520402908325195
Test Loss Energy: 10.360532297991485, Test Loss Force: 11.737799372487896, time: 12.019034147262573

Epoch 4, Batch 100/179, Loss: 0.47978270053863525, Variance: 0.12981794774532318

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6570266660736968, Training Loss Force: 3.4348818427838155, time: 3.2929131984710693
Validation Loss Energy: 4.141294297006287, Validation Loss Force: 3.539915257082313, time: 0.19333767890930176
Test Loss Energy: 11.066815125685551, Test Loss Force: 11.498023159966527, time: 11.760315418243408

Epoch 5, Batch 100/179, Loss: 1.3281323909759521, Variance: 0.1292850524187088

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6914435331877002, Training Loss Force: 3.4244390218326592, time: 2.998854160308838
Validation Loss Energy: 2.7616609851129126, Validation Loss Force: 3.586969626776724, time: 0.2069861888885498
Test Loss Energy: 10.130852874497943, Test Loss Force: 11.52607876820597, time: 11.627207517623901

Epoch 6, Batch 100/179, Loss: 0.8518599271774292, Variance: 0.1310042142868042

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.7202587785275685, Training Loss Force: 3.436717365507822, time: 3.0346038341522217
Validation Loss Energy: 1.9758494707768204, Validation Loss Force: 3.54112769058189, time: 0.1787569522857666
Test Loss Energy: 9.9367033932994, Test Loss Force: 11.719991077618923, time: 11.532381057739258

Epoch 7, Batch 100/179, Loss: 0.8294675350189209, Variance: 0.13632726669311523

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.68046114757711, Training Loss Force: 3.427369177370315, time: 2.7608165740966797
Validation Loss Energy: 3.7057248716978073, Validation Loss Force: 3.8035517404853327, time: 0.1730821132659912
Test Loss Energy: 10.59012050407846, Test Loss Force: 12.045238596985198, time: 11.357123851776123

Epoch 8, Batch 100/179, Loss: 1.3675898313522339, Variance: 0.14401063323020935

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.7235227623855147, Training Loss Force: 3.5461692919902448, time: 2.8156893253326416
Validation Loss Energy: 1.9496320344200395, Validation Loss Force: 3.5132129215487495, time: 0.17490553855895996
Test Loss Energy: 9.887913006730603, Test Loss Force: 11.746571804710621, time: 11.510109186172485

Epoch 9, Batch 100/179, Loss: 1.0478556156158447, Variance: 0.14061692357063293

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.682859803681072, Training Loss Force: 3.4549508574110726, time: 2.8013153076171875
Validation Loss Energy: 2.4773109270791056, Validation Loss Force: 3.5163633188483843, time: 0.1767110824584961
Test Loss Energy: 10.38144572459268, Test Loss Force: 11.664858439575069, time: 11.331674575805664

Epoch 10, Batch 100/179, Loss: 0.6144776344299316, Variance: 0.12899315357208252

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.708289708612521, Training Loss Force: 3.5032212129450486, time: 2.759413957595825
Validation Loss Energy: 4.075657222184571, Validation Loss Force: 3.4863525651446263, time: 0.18862295150756836
Test Loss Energy: 10.677575315032687, Test Loss Force: 11.288666527918476, time: 11.431415319442749

Epoch 11, Batch 100/179, Loss: 1.1608917713165283, Variance: 0.13229675590991974

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6785952852489996, Training Loss Force: 3.425211172500766, time: 2.792879581451416
Validation Loss Energy: 2.7143702150085747, Validation Loss Force: 3.758150392307241, time: 0.19289922714233398
Test Loss Energy: 10.469670611723249, Test Loss Force: 11.678808468043478, time: 11.361605644226074

Epoch 12, Batch 100/179, Loss: 0.892223060131073, Variance: 0.13548316061496735

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.678225457920809, Training Loss Force: 3.460360361738856, time: 2.779768943786621
Validation Loss Energy: 1.7568183675782496, Validation Loss Force: 3.5602790927078956, time: 0.18137359619140625
Test Loss Energy: 9.681248829373144, Test Loss Force: 11.461888532209299, time: 11.511276721954346

Epoch 13, Batch 100/179, Loss: 0.6694191694259644, Variance: 0.13812923431396484

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6961739190117133, Training Loss Force: 3.4196036045896623, time: 2.724731922149658
Validation Loss Energy: 3.217160931889, Validation Loss Force: 3.5994876408224923, time: 0.17257332801818848
Test Loss Energy: 10.162025193265203, Test Loss Force: 11.649488612010261, time: 11.492019653320312

Epoch 14, Batch 100/179, Loss: 1.0617311000823975, Variance: 0.137617290019989

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.717329824034943, Training Loss Force: 3.465648503918337, time: 2.752828359603882
Validation Loss Energy: 2.020846006447616, Validation Loss Force: 3.508707211380901, time: 0.17266011238098145
Test Loss Energy: 9.908126440811879, Test Loss Force: 11.69531989583751, time: 11.673442602157593

Epoch 15, Batch 100/179, Loss: 0.9334335923194885, Variance: 0.13799236714839935

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6841328902656607, Training Loss Force: 3.4537875703507, time: 2.837364912033081
Validation Loss Energy: 2.555336051388419, Validation Loss Force: 3.58569417790885, time: 0.17235994338989258
Test Loss Energy: 10.341741185813058, Test Loss Force: 11.60727628703948, time: 11.439181804656982

Epoch 16, Batch 100/179, Loss: 0.6005887389183044, Variance: 0.13646528124809265

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.703197937606456, Training Loss Force: 3.446846654617933, time: 2.823150396347046
Validation Loss Energy: 4.106707989245908, Validation Loss Force: 3.506752547394898, time: 0.1830909252166748
Test Loss Energy: 10.872476074934752, Test Loss Force: 11.34798383501984, time: 11.54822564125061

Epoch 17, Batch 100/179, Loss: 1.379968285560608, Variance: 0.1343001425266266

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.692012300470236, Training Loss Force: 3.4159833488542706, time: 2.9108941555023193
Validation Loss Energy: 3.083389399327887, Validation Loss Force: 3.6700344960888693, time: 0.18139362335205078
Test Loss Energy: 10.57229340707344, Test Loss Force: 11.662655847679066, time: 11.387748718261719

Epoch 18, Batch 100/179, Loss: 0.9042664766311646, Variance: 0.1375236213207245

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.7051721266081175, Training Loss Force: 3.4411574765989035, time: 2.8456218242645264
Validation Loss Energy: 1.6642492739989443, Validation Loss Force: 3.498601149523823, time: 1.0802381038665771
Test Loss Energy: 9.92244768089688, Test Loss Force: 11.618044403723564, time: 11.538270950317383

Epoch 19, Batch 100/179, Loss: 0.6919008493423462, Variance: 0.14197546243667603

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.691704212827611, Training Loss Force: 3.4230282150032956, time: 2.8760266304016113
Validation Loss Energy: 3.2782709203048594, Validation Loss Force: 3.5093903603206074, time: 0.17862629890441895
Test Loss Energy: 10.213453227603711, Test Loss Force: 11.62239268214201, time: 11.48642086982727

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–â–„â–ˆâ–ƒâ–‚â–†â–‚â–…â–†â–…â–â–ƒâ–‚â–„â–‡â–†â–‚â–„
wandb:   test_error_force â–„â–…â–„â–…â–ƒâ–ƒâ–…â–ˆâ–…â–„â–â–…â–ƒâ–„â–…â–„â–‚â–„â–„â–„
wandb:          test_loss â–…â–…â–‚â–†â–ˆâ–ƒâ–ƒâ–‡â–ƒâ–„â–ƒâ–„â–â–ƒâ–ƒâ–„â–…â–„â–ƒâ–ƒ
wandb: train_error_energy â–ˆâ–‚â–â–â–â–ƒâ–…â–‚â–…â–‚â–„â–‚â–‚â–ƒâ–„â–ƒâ–„â–ƒâ–„â–ƒ
wandb:  train_error_force â–ˆâ–â–â–â–‚â–â–‚â–‚â–†â–ƒâ–„â–â–ƒâ–â–ƒâ–ƒâ–‚â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–‚â–â–ƒâ–â–ƒâ–â–‚â–â–‚â–â–‚â–â–‚â–
wandb: valid_error_energy â–‚â–†â–ƒâ–‚â–ˆâ–„â–‚â–‡â–‚â–ƒâ–ˆâ–„â–â–…â–‚â–„â–ˆâ–…â–â–†
wandb:  valid_error_force â–â–‚â–‚â–â–‚â–ƒâ–‚â–ˆâ–‚â–‚â–â–‡â–ƒâ–ƒâ–â–ƒâ–â–…â–â–‚
wandb:         valid_loss â–â–†â–‚â–‚â–ˆâ–„â–‚â–‡â–‚â–ƒâ–‡â–„â–â–…â–‚â–ƒâ–ˆâ–…â–â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 5697
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.21345
wandb:   test_error_force 11.62239
wandb:          test_loss 10.96415
wandb: train_error_energy 2.6917
wandb:  train_error_force 3.42303
wandb:         train_loss 0.99149
wandb: valid_error_energy 3.27827
wandb:  valid_error_force 3.50939
wandb:         valid_loss 1.25126
wandb: 
wandb: ğŸš€ View run al_63_54 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/m1ng6rwe
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_082915-m1ng6rwe/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.965052843093872, Uncertainty Bias: -0.16272173821926117
9.536743e-06 0.010508537
1.9632607 4.908562
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 90 steps.
Found uncertainty sample 2 after 529 steps.
Found uncertainty sample 3 after 3871 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 37 steps.
Found uncertainty sample 6 after 2164 steps.
Found uncertainty sample 7 after 192 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 359 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 51 steps.
Found uncertainty sample 12 after 1411 steps.
Found uncertainty sample 13 after 42 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 68 steps.
Found uncertainty sample 16 after 1038 steps.
Found uncertainty sample 17 after 11 steps.
Found uncertainty sample 18 after 198 steps.
Found uncertainty sample 19 after 1141 steps.
Found uncertainty sample 20 after 1009 steps.
Found uncertainty sample 21 after 16 steps.
Found uncertainty sample 22 after 36 steps.
Found uncertainty sample 23 after 2226 steps.
Found uncertainty sample 24 after 1587 steps.
Found uncertainty sample 25 after 1669 steps.
Found uncertainty sample 26 after 645 steps.
Found uncertainty sample 27 after 1462 steps.
Found uncertainty sample 28 after 1692 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 176 steps.
Found uncertainty sample 31 after 203 steps.
Found uncertainty sample 32 after 26 steps.
Found uncertainty sample 33 after 759 steps.
Found uncertainty sample 34 after 1885 steps.
Found uncertainty sample 35 after 1780 steps.
Found uncertainty sample 36 after 1768 steps.
Found uncertainty sample 37 after 2998 steps.
Found uncertainty sample 38 after 1292 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 616 steps.
Found uncertainty sample 41 after 1660 steps.
Found uncertainty sample 42 after 16 steps.
Found uncertainty sample 43 after 1383 steps.
Found uncertainty sample 44 after 190 steps.
Found uncertainty sample 45 after 4 steps.
Found uncertainty sample 46 after 856 steps.
Found uncertainty sample 47 after 981 steps.
Found uncertainty sample 48 after 3059 steps.
Found uncertainty sample 49 after 81 steps.
Found uncertainty sample 50 after 1290 steps.
Found uncertainty sample 51 after 164 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 507 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 3737 steps.
Found uncertainty sample 56 after 489 steps.
Found uncertainty sample 57 after 47 steps.
Found uncertainty sample 58 after 370 steps.
Found uncertainty sample 59 after 483 steps.
Found uncertainty sample 60 after 1642 steps.
Found uncertainty sample 61 after 1000 steps.
Found uncertainty sample 62 after 2439 steps.
Found uncertainty sample 63 after 1070 steps.
Found uncertainty sample 64 after 2426 steps.
Found uncertainty sample 65 after 3001 steps.
Found uncertainty sample 66 after 2907 steps.
Found uncertainty sample 67 after 1676 steps.
Did not find any uncertainty samples for sample 68.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 2273 steps.
Found uncertainty sample 71 after 555 steps.
Found uncertainty sample 72 after 1305 steps.
Found uncertainty sample 73 after 1460 steps.
Found uncertainty sample 74 after 618 steps.
Found uncertainty sample 75 after 1096 steps.
Found uncertainty sample 76 after 255 steps.
Found uncertainty sample 77 after 795 steps.
Found uncertainty sample 78 after 1260 steps.
Found uncertainty sample 79 after 2221 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 2943 steps.
Found uncertainty sample 82 after 271 steps.
Found uncertainty sample 83 after 938 steps.
Found uncertainty sample 84 after 323 steps.
Found uncertainty sample 85 after 938 steps.
Found uncertainty sample 86 after 80 steps.
Found uncertainty sample 87 after 349 steps.
Found uncertainty sample 88 after 262 steps.
Found uncertainty sample 89 after 18 steps.
Found uncertainty sample 90 after 136 steps.
Found uncertainty sample 91 after 3849 steps.
Found uncertainty sample 92 after 2025 steps.
Found uncertainty sample 93 after 2114 steps.
Found uncertainty sample 94 after 3109 steps.
Found uncertainty sample 95 after 3665 steps.
Found uncertainty sample 96 after 622 steps.
Found uncertainty sample 97 after 1926 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 287 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_084915-psz7qfsq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_55
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/psz7qfsq
Training model 55. Added 91 samples to the dataset.
Epoch 0, Batch 100/181, Loss: 1.0569472312927246, Variance: 0.1283472776412964

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.2083981334443035, Training Loss Force: 3.658088624467517, time: 3.0493907928466797
Validation Loss Energy: 3.4982223150835505, Validation Loss Force: 3.4892705700789612, time: 0.18774890899658203
Test Loss Energy: 10.797629970819685, Test Loss Force: 11.613501332937286, time: 11.809882402420044

Epoch 1, Batch 100/181, Loss: 1.2207694053649902, Variance: 0.13336537778377533

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.644785868039148, Training Loss Force: 3.395779760944198, time: 2.8797264099121094
Validation Loss Energy: 2.285861034533075, Validation Loss Force: 3.4780977745667916, time: 0.19840312004089355
Test Loss Energy: 10.119061146584226, Test Loss Force: 11.459160730249048, time: 12.107059717178345

Epoch 2, Batch 100/181, Loss: 0.6023012399673462, Variance: 0.13694173097610474

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6857964316236638, Training Loss Force: 3.406693848534711, time: 2.771836996078491
Validation Loss Energy: 2.0022309400711014, Validation Loss Force: 3.521826061005431, time: 0.1977214813232422
Test Loss Energy: 9.836223747675515, Test Loss Force: 11.627078540975372, time: 12.164737701416016

Epoch 3, Batch 100/181, Loss: 0.8336613774299622, Variance: 0.13934184610843658

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6876512625337012, Training Loss Force: 3.407733014683349, time: 2.8997154235839844
Validation Loss Energy: 3.3383454229691067, Validation Loss Force: 3.5148591887676424, time: 0.1775650978088379
Test Loss Energy: 10.080443988679697, Test Loss Force: 11.609997238717524, time: 12.118600606918335

Epoch 4, Batch 100/181, Loss: 1.3281961679458618, Variance: 0.13833096623420715

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.669425738194888, Training Loss Force: 3.4065217646518295, time: 2.7931857109069824
Validation Loss Energy: 1.7558394769012267, Validation Loss Force: 3.478105567986899, time: 0.1737213134765625
Test Loss Energy: 9.838785342856724, Test Loss Force: 11.605277911895463, time: 12.054121732711792

Epoch 5, Batch 100/181, Loss: 0.7244418859481812, Variance: 0.1352497935295105

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6902921745792456, Training Loss Force: 3.4238117531855417, time: 3.0066421031951904
Validation Loss Energy: 2.6650216851036688, Validation Loss Force: 3.4865000463422793, time: 0.17590022087097168
Test Loss Energy: 10.405349049894374, Test Loss Force: 11.57341896542027, time: 11.93553638458252

Epoch 6, Batch 100/181, Loss: 0.8668941259384155, Variance: 0.13661667704582214

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.69664023634323, Training Loss Force: 3.4138664166244688, time: 3.005399703979492
Validation Loss Energy: 3.9913089621383464, Validation Loss Force: 3.482051562144934, time: 0.18870115280151367
Test Loss Energy: 10.78004191555476, Test Loss Force: 11.527552516189267, time: 11.75741195678711

Epoch 7, Batch 100/181, Loss: 1.026456356048584, Variance: 0.13489095866680145

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.707697009953389, Training Loss Force: 3.42011769173806, time: 3.0525307655334473
Validation Loss Energy: 2.39497878770316, Validation Loss Force: 3.4804798148668787, time: 0.195448637008667
Test Loss Energy: 10.130129868946923, Test Loss Force: 11.550989868414378, time: 11.924598693847656

Epoch 8, Batch 100/181, Loss: 0.8239655494689941, Variance: 0.1390305757522583

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.72274767947286, Training Loss Force: 3.4098612162577986, time: 3.009639024734497
Validation Loss Energy: 2.2326969203665996, Validation Loss Force: 3.5045683857424925, time: 0.18586397171020508
Test Loss Energy: 9.992357049165355, Test Loss Force: 11.610227536904763, time: 11.726085186004639

Epoch 9, Batch 100/181, Loss: 1.1243834495544434, Variance: 0.1425919383764267

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.707507250938164, Training Loss Force: 3.4218849404758793, time: 3.0460891723632812
Validation Loss Energy: 3.173279745970157, Validation Loss Force: 3.5087472176041192, time: 0.19511198997497559
Test Loss Energy: 10.191830878400621, Test Loss Force: 11.647013025409734, time: 11.9619140625

Epoch 10, Batch 100/181, Loss: 1.4341232776641846, Variance: 0.14163798093795776

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.7032104634795773, Training Loss Force: 3.4150864922275512, time: 3.078507661819458
Validation Loss Energy: 1.580503630884649, Validation Loss Force: 3.4766775165310153, time: 0.19752025604248047
Test Loss Energy: 9.628473956806445, Test Loss Force: 11.515631910638149, time: 11.756496667861938

Epoch 11, Batch 100/181, Loss: 0.7936316132545471, Variance: 0.14012983441352844

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6777102212727564, Training Loss Force: 3.4156674987703077, time: 3.0201573371887207
Validation Loss Energy: 2.791371951923203, Validation Loss Force: 3.4770534562723903, time: 0.1936054229736328
Test Loss Energy: 10.249728862054859, Test Loss Force: 11.317612019992838, time: 12.86387825012207

Epoch 12, Batch 100/181, Loss: 1.0524839162826538, Variance: 0.13840925693511963

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.693419090330751, Training Loss Force: 3.419084859973027, time: 3.0197391510009766
Validation Loss Energy: 3.7852277769055944, Validation Loss Force: 3.5096384879722193, time: 0.19770264625549316
Test Loss Energy: 10.723319457114805, Test Loss Force: 11.560695725033609, time: 11.849564552307129

Epoch 13, Batch 100/181, Loss: 1.0940344333648682, Variance: 0.13706780970096588

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.708269735295284, Training Loss Force: 3.4236588446837515, time: 2.9366776943206787
Validation Loss Energy: 2.2618768212638525, Validation Loss Force: 3.494802859232586, time: 0.2011399269104004
Test Loss Energy: 10.104289657828089, Test Loss Force: 11.525695828570484, time: 11.794416666030884

Epoch 14, Batch 100/181, Loss: 0.7735617160797119, Variance: 0.13334861397743225

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.7070694624835934, Training Loss Force: 3.4335567982347435, time: 2.810734748840332
Validation Loss Energy: 2.2284239736792943, Validation Loss Force: 3.4708413582162483, time: 0.17633700370788574
Test Loss Energy: 9.829722433487001, Test Loss Force: 11.453910410959796, time: 12.153766870498657

Epoch 15, Batch 100/181, Loss: 0.8559396862983704, Variance: 0.14362144470214844

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6979182994206905, Training Loss Force: 3.4093048029686135, time: 3.021804094314575
Validation Loss Energy: 3.343879048137607, Validation Loss Force: 3.47664226058291, time: 0.20284128189086914
Test Loss Energy: 10.244381211866887, Test Loss Force: 11.606583806980172, time: 10.317363262176514

Epoch 16, Batch 100/181, Loss: 1.2722246646881104, Variance: 0.14137619733810425

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.727704507488628, Training Loss Force: 3.4097010322569536, time: 2.844870090484619
Validation Loss Energy: 1.585380456295711, Validation Loss Force: 3.5333014638869167, time: 0.16433310508728027
Test Loss Energy: 9.718199125688695, Test Loss Force: 11.68176514590637, time: 9.911100149154663

Epoch 17, Batch 100/181, Loss: 0.7633849382400513, Variance: 0.14474990963935852

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.71892131872175, Training Loss Force: 3.421416861636165, time: 2.7318108081817627
Validation Loss Energy: 2.668679545386873, Validation Loss Force: 3.506989244697019, time: 0.1605062484741211
Test Loss Energy: 10.33214571713147, Test Loss Force: 11.579482761550599, time: 9.878868103027344

Epoch 18, Batch 100/181, Loss: 0.7842286825180054, Variance: 0.13956373929977417

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.7217317681024835, Training Loss Force: 3.4296179963311295, time: 2.7009499073028564
Validation Loss Energy: 3.8586949380208146, Validation Loss Force: 3.488072387404724, time: 0.1690378189086914
Test Loss Energy: 10.727455157613125, Test Loss Force: 11.461449675804621, time: 10.119925022125244

Epoch 19, Batch 100/181, Loss: 1.4048030376434326, Variance: 0.1375470757484436

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.7098122001453975, Training Loss Force: 3.4251254224126733, time: 2.7491583824157715
Validation Loss Energy: 2.3775950817036264, Validation Loss Force: 3.52514739398609, time: 0.159379243850708
Test Loss Energy: 10.270959024440968, Test Loss Force: 11.598469575426346, time: 9.904858827590942

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–„â–‚â–„â–‚â–†â–ˆâ–„â–ƒâ–„â–â–…â–ˆâ–„â–‚â–…â–‚â–…â–ˆâ–…
wandb:   test_error_force â–‡â–„â–‡â–‡â–‡â–†â–…â–…â–‡â–‡â–…â–â–†â–…â–„â–‡â–ˆâ–†â–„â–†
wandb:          test_loss â–ˆâ–„â–‚â–ƒâ–ƒâ–…â–…â–ƒâ–‚â–ƒâ–â–ƒâ–…â–ƒâ–â–„â–‚â–ƒâ–…â–„
wandb: train_error_energy â–ˆâ–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–ƒâ–‚â–†â–‚â–„â–ˆâ–ƒâ–ƒâ–†â–â–…â–‡â–ƒâ–ƒâ–†â–â–„â–ˆâ–ƒ
wandb:  valid_error_force â–ƒâ–‚â–‡â–†â–‚â–ƒâ–‚â–‚â–…â–…â–‚â–‚â–…â–„â–â–‚â–ˆâ–…â–ƒâ–‡
wandb:         valid_loss â–†â–‚â–‚â–†â–â–ƒâ–ˆâ–ƒâ–‚â–…â–â–„â–‡â–‚â–‚â–†â–â–„â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5778
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.27096
wandb:   test_error_force 11.59847
wandb:          test_loss 10.88923
wandb: train_error_energy 2.70981
wandb:  train_error_force 3.42513
wandb:         train_loss 0.99966
wandb: valid_error_energy 2.3776
wandb:  valid_error_force 3.52515
wandb:         valid_loss 0.89489
wandb: 
wandb: ğŸš€ View run al_63_55 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/psz7qfsq
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_084915-psz7qfsq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.975375175476074, Uncertainty Bias: -0.17614588141441345
6.866455e-05 0.00029182434
2.0009325 5.0452833
(48745, 22, 3)
Found uncertainty sample 0 after 14 steps.
Found uncertainty sample 1 after 53 steps.
Found uncertainty sample 2 after 1439 steps.
Found uncertainty sample 3 after 251 steps.
Found uncertainty sample 4 after 1853 steps.
Found uncertainty sample 5 after 968 steps.
Found uncertainty sample 6 after 215 steps.
Found uncertainty sample 7 after 342 steps.
Found uncertainty sample 8 after 656 steps.
Found uncertainty sample 9 after 1254 steps.
Found uncertainty sample 10 after 3 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 227 steps.
Found uncertainty sample 13 after 83 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 336 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 2487 steps.
Found uncertainty sample 18 after 1430 steps.
Found uncertainty sample 19 after 1162 steps.
Found uncertainty sample 20 after 1301 steps.
Found uncertainty sample 21 after 3675 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 429 steps.
Found uncertainty sample 24 after 1796 steps.
Found uncertainty sample 25 after 2353 steps.
Found uncertainty sample 26 after 1136 steps.
Found uncertainty sample 27 after 179 steps.
Found uncertainty sample 28 after 1265 steps.
Found uncertainty sample 29 after 1574 steps.
Found uncertainty sample 30 after 1337 steps.
Found uncertainty sample 31 after 1507 steps.
Found uncertainty sample 32 after 854 steps.
Found uncertainty sample 33 after 3303 steps.
Found uncertainty sample 34 after 843 steps.
Found uncertainty sample 35 after 134 steps.
Found uncertainty sample 36 after 409 steps.
Found uncertainty sample 37 after 1601 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 777 steps.
Found uncertainty sample 40 after 349 steps.
Found uncertainty sample 41 after 1290 steps.
Found uncertainty sample 42 after 583 steps.
Found uncertainty sample 43 after 1111 steps.
Found uncertainty sample 44 after 3981 steps.
Found uncertainty sample 45 after 513 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 665 steps.
Found uncertainty sample 48 after 513 steps.
Found uncertainty sample 49 after 1147 steps.
Found uncertainty sample 50 after 249 steps.
Found uncertainty sample 51 after 968 steps.
Found uncertainty sample 52 after 1731 steps.
Found uncertainty sample 53 after 3205 steps.
Found uncertainty sample 54 after 49 steps.
Found uncertainty sample 55 after 732 steps.
Found uncertainty sample 56 after 2979 steps.
Found uncertainty sample 57 after 3327 steps.
Found uncertainty sample 58 after 867 steps.
Found uncertainty sample 59 after 541 steps.
Found uncertainty sample 60 after 2592 steps.
Found uncertainty sample 61 after 2607 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 19 steps.
Found uncertainty sample 64 after 14 steps.
Found uncertainty sample 65 after 42 steps.
Found uncertainty sample 66 after 88 steps.
Found uncertainty sample 67 after 1729 steps.
Found uncertainty sample 68 after 1413 steps.
Found uncertainty sample 69 after 95 steps.
Found uncertainty sample 70 after 1246 steps.
Found uncertainty sample 71 after 49 steps.
Found uncertainty sample 72 after 1683 steps.
Found uncertainty sample 73 after 364 steps.
Found uncertainty sample 74 after 18 steps.
Found uncertainty sample 75 after 181 steps.
Found uncertainty sample 76 after 49 steps.
Found uncertainty sample 77 after 160 steps.
Found uncertainty sample 78 after 1389 steps.
Found uncertainty sample 79 after 402 steps.
Found uncertainty sample 80 after 1352 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 174 steps.
Found uncertainty sample 83 after 337 steps.
Found uncertainty sample 84 after 1740 steps.
Found uncertainty sample 85 after 976 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 1726 steps.
Found uncertainty sample 88 after 1505 steps.
Found uncertainty sample 89 after 365 steps.
Found uncertainty sample 90 after 2267 steps.
Found uncertainty sample 91 after 171 steps.
Found uncertainty sample 92 after 1384 steps.
Found uncertainty sample 93 after 3662 steps.
Found uncertainty sample 94 after 141 steps.
Found uncertainty sample 95 after 1181 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1403 steps.
Found uncertainty sample 98 after 3655 steps.
Found uncertainty sample 99 after 33 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_090804-hbqeh5kq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_56
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hbqeh5kq
Training model 56. Added 94 samples to the dataset.
Epoch 0, Batch 100/184, Loss: 0.338966429233551, Variance: 0.12027272582054138

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.44258092256262, Training Loss Force: 3.9268424013325536, time: 2.901404857635498
Validation Loss Energy: 6.126836295924675, Validation Loss Force: 3.5899245205662345, time: 0.17243576049804688
Test Loss Energy: 11.476995227797197, Test Loss Force: 11.57121188749875, time: 11.742659568786621

Epoch 1, Batch 100/184, Loss: 2.118175983428955, Variance: 0.16972094774246216

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.074750019677724, Training Loss Force: 3.4576074309988174, time: 3.25909161567688
Validation Loss Energy: 4.6550846470264, Validation Loss Force: 3.71116937354065, time: 0.2073066234588623
Test Loss Energy: 10.582743678124299, Test Loss Force: 11.15583761602845, time: 11.532168626785278

Epoch 2, Batch 100/184, Loss: 1.7818189859390259, Variance: 0.18469956517219543

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.143519467901534, Training Loss Force: 3.4484126873973096, time: 2.9658963680267334
Validation Loss Energy: 5.213857393226542, Validation Loss Force: 3.454090218148771, time: 0.19104671478271484
Test Loss Energy: 11.028164000826912, Test Loss Force: 11.430574176174687, time: 10.647608757019043

Epoch 3, Batch 100/184, Loss: 1.9801068305969238, Variance: 0.1909286081790924

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.187440534590508, Training Loss Force: 3.452770937343584, time: 2.8557729721069336
Validation Loss Energy: 4.532826816439873, Validation Loss Force: 3.2484264262300613, time: 0.19418001174926758
Test Loss Energy: 10.792748918580871, Test Loss Force: 11.324179897100494, time: 10.645479440689087

Epoch 4, Batch 100/184, Loss: 2.185899019241333, Variance: 0.1946408748626709

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.202127386657919, Training Loss Force: 3.446717309933542, time: 3.249110221862793
Validation Loss Energy: 4.644078779150628, Validation Loss Force: 3.7160242289293133, time: 0.17931008338928223
Test Loss Energy: 10.85035533855595, Test Loss Force: 11.411471816830923, time: 10.822010278701782

Epoch 5, Batch 100/184, Loss: 2.0063743591308594, Variance: 0.19472119212150574

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.172146665253525, Training Loss Force: 3.460113476633721, time: 2.9894447326660156
Validation Loss Energy: 4.878127209111909, Validation Loss Force: 3.5202932066048644, time: 0.17285656929016113
Test Loss Energy: 11.09199604027686, Test Loss Force: 11.570485473127924, time: 10.915245771408081

Epoch 6, Batch 100/184, Loss: 1.93463134765625, Variance: 0.19587959349155426

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.170961521430987, Training Loss Force: 3.4580242761111033, time: 2.9338414669036865
Validation Loss Energy: 5.392827385262283, Validation Loss Force: 3.7134104828271273, time: 0.21792316436767578
Test Loss Energy: 11.095794743263792, Test Loss Force: 11.579455598246412, time: 11.75943899154663

Epoch 7, Batch 100/184, Loss: 1.9142849445343018, Variance: 0.19710394740104675

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.159993679757595, Training Loss Force: 3.481486913126088, time: 3.1231439113616943
Validation Loss Energy: 4.6515860694055835, Validation Loss Force: 3.460806253593672, time: 0.17405962944030762
Test Loss Energy: 11.01724086911144, Test Loss Force: 11.45404833059665, time: 10.784616947174072

Epoch 8, Batch 100/184, Loss: 1.8918747901916504, Variance: 0.19751904904842377

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.199894284989588, Training Loss Force: 3.4633544514660257, time: 3.1124181747436523
Validation Loss Energy: 4.701746413905017, Validation Loss Force: 3.584326061564745, time: 0.18321990966796875
Test Loss Energy: 11.298521663798427, Test Loss Force: 11.763675779583917, time: 10.899484872817993

Epoch 9, Batch 100/184, Loss: 1.9294111728668213, Variance: 0.19339197874069214

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.185754475951513, Training Loss Force: 3.4511823914533672, time: 3.150108575820923
Validation Loss Energy: 5.493042083747689, Validation Loss Force: 3.682421450786497, time: 0.2146286964416504
Test Loss Energy: 11.207740959226372, Test Loss Force: 11.61622560017057, time: 12.153905630111694

Epoch 10, Batch 100/184, Loss: 1.9719562530517578, Variance: 0.20088699460029602

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.198862992235144, Training Loss Force: 3.4588531886303038, time: 3.265840768814087
Validation Loss Energy: 4.653095098207536, Validation Loss Force: 3.2405103154231822, time: 0.20113492012023926
Test Loss Energy: 10.986324962609267, Test Loss Force: 11.478046132901413, time: 11.723068714141846

Epoch 11, Batch 100/184, Loss: 1.787449836730957, Variance: 0.19815322756767273

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.171318574431879, Training Loss Force: 3.4727653765542703, time: 3.0205326080322266
Validation Loss Energy: 4.980303177434648, Validation Loss Force: 3.6129585303667344, time: 0.18954038619995117
Test Loss Energy: 11.079644020833248, Test Loss Force: 11.700685098416377, time: 11.3612961769104

Epoch 12, Batch 100/184, Loss: 1.9147998094558716, Variance: 0.1966087520122528

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.179626354820733, Training Loss Force: 3.4795180858495374, time: 2.8663928508758545
Validation Loss Energy: 5.300955431667353, Validation Loss Force: 3.198234351377333, time: 0.18466544151306152
Test Loss Energy: 11.433442938196576, Test Loss Force: 11.604453299265478, time: 11.404753684997559

Epoch 13, Batch 100/184, Loss: 1.8202409744262695, Variance: 0.19860033690929413

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.157134422164567, Training Loss Force: 3.4677206049568237, time: 2.979893922805786
Validation Loss Energy: 4.584107520747042, Validation Loss Force: 3.6184798345091598, time: 0.17852306365966797
Test Loss Energy: 11.060859054789573, Test Loss Force: 11.665913173189628, time: 11.20002794265747

Epoch 14, Batch 100/184, Loss: 1.845965027809143, Variance: 0.19685426354408264

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.190622931100296, Training Loss Force: 3.46170411851829, time: 2.9293370246887207
Validation Loss Energy: 4.914362270563311, Validation Loss Force: 3.5002676239677077, time: 0.18585753440856934
Test Loss Energy: 10.650571647134118, Test Loss Force: 11.399973228767495, time: 11.307102918624878

Epoch 15, Batch 100/184, Loss: 1.8543701171875, Variance: 0.20160195231437683

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.195177712586258, Training Loss Force: 3.4859454053334167, time: 2.851569414138794
Validation Loss Energy: 5.114091039066462, Validation Loss Force: 3.4686225529558885, time: 0.1847984790802002
Test Loss Energy: 11.011819409347725, Test Loss Force: 11.501576208755791, time: 11.241593599319458

Epoch 16, Batch 100/184, Loss: 1.836911678314209, Variance: 0.19580310583114624

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.167971338010631, Training Loss Force: 3.4437742676286454, time: 2.915067672729492
Validation Loss Energy: 5.019873488688329, Validation Loss Force: 3.7911222374250704, time: 0.18387866020202637
Test Loss Energy: 10.856229024199553, Test Loss Force: 11.409942618401256, time: 11.061200380325317

Epoch 17, Batch 100/184, Loss: 1.7086327075958252, Variance: 0.19771184027194977

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.141397891406114, Training Loss Force: 3.4950343399452235, time: 2.801788091659546
Validation Loss Energy: 5.132707181677181, Validation Loss Force: 3.655348111344294, time: 0.16677474975585938
Test Loss Energy: 10.90098703341697, Test Loss Force: 11.417734700162239, time: 11.737104415893555

Epoch 18, Batch 100/184, Loss: 1.8267457485198975, Variance: 0.19289323687553406

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.167887189041083, Training Loss Force: 3.469187304838198, time: 3.1626532077789307
Validation Loss Energy: 5.126702351484374, Validation Loss Force: 3.54338076833496, time: 0.20331335067749023
Test Loss Energy: 10.964208549042997, Test Loss Force: 11.431967702643487, time: 10.444525957107544

Epoch 19, Batch 100/184, Loss: 1.9147253036499023, Variance: 0.20075172185897827

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.185523185637131, Training Loss Force: 3.471803626157464, time: 2.7163193225860596
Validation Loss Energy: 4.469525874962657, Validation Loss Force: 3.5453806712691103, time: 0.16904640197753906
Test Loss Energy: 10.850892519126948, Test Loss Force: 11.27932503729686, time: 9.851866245269775

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.039 MB of 0.056 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–â–„â–ƒâ–ƒâ–…â–…â–„â–‡â–†â–„â–…â–ˆâ–…â–‚â–„â–ƒâ–ƒâ–„â–ƒ
wandb:   test_error_force â–†â–â–„â–ƒâ–„â–†â–†â–„â–ˆâ–†â–…â–‡â–†â–‡â–„â–…â–„â–„â–„â–‚
wandb:          test_loss â–ˆâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–
wandb: train_error_energy â–â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–‚â–â–‚â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‚â–„â–â–‚â–ƒâ–…â–‚â–‚â–…â–‚â–ƒâ–…â–â–ƒâ–„â–ƒâ–„â–„â–
wandb:  valid_error_force â–†â–‡â–„â–‚â–‡â–…â–‡â–„â–†â–‡â–â–†â–â–†â–…â–„â–ˆâ–†â–…â–…
wandb:         valid_loss â–ˆâ–‚â–‚â–â–‚â–‚â–ƒâ–â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 5862
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.85089
wandb:   test_error_force 11.27933
wandb:          test_loss 8.84488
wandb: train_error_energy 4.18552
wandb:  train_error_force 3.4718
wandb:         train_loss 1.43825
wandb: valid_error_energy 4.46953
wandb:  valid_error_force 3.54538
wandb:         valid_loss 1.48394
wandb: 
wandb: ğŸš€ View run al_63_56 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hbqeh5kq
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_090804-hbqeh5kq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.4728078842163086, Uncertainty Bias: -0.23502296209335327
8.392334e-05 0.78004026
2.129716 4.920703
(48745, 22, 3)
Found uncertainty sample 0 after 739 steps.
Found uncertainty sample 1 after 505 steps.
Found uncertainty sample 2 after 2344 steps.
Found uncertainty sample 3 after 2588 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 1545 steps.
Found uncertainty sample 6 after 1197 steps.
Found uncertainty sample 7 after 282 steps.
Found uncertainty sample 8 after 1254 steps.
Found uncertainty sample 9 after 1343 steps.
Found uncertainty sample 10 after 1458 steps.
Found uncertainty sample 11 after 2143 steps.
Found uncertainty sample 12 after 2837 steps.
Found uncertainty sample 13 after 3629 steps.
Found uncertainty sample 14 after 416 steps.
Found uncertainty sample 15 after 48 steps.
Found uncertainty sample 16 after 99 steps.
Found uncertainty sample 17 after 1576 steps.
Found uncertainty sample 18 after 864 steps.
Found uncertainty sample 19 after 79 steps.
Found uncertainty sample 20 after 363 steps.
Found uncertainty sample 21 after 28 steps.
Found uncertainty sample 22 after 134 steps.
Found uncertainty sample 23 after 887 steps.
Found uncertainty sample 24 after 1468 steps.
Found uncertainty sample 25 after 3428 steps.
Found uncertainty sample 26 after 711 steps.
Found uncertainty sample 27 after 16 steps.
Found uncertainty sample 28 after 949 steps.
Found uncertainty sample 29 after 103 steps.
Found uncertainty sample 30 after 3518 steps.
Found uncertainty sample 31 after 1324 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1105 steps.
Found uncertainty sample 34 after 408 steps.
Found uncertainty sample 35 after 744 steps.
Found uncertainty sample 36 after 218 steps.
Found uncertainty sample 37 after 2305 steps.
Found uncertainty sample 38 after 1657 steps.
Found uncertainty sample 39 after 1339 steps.
Found uncertainty sample 40 after 549 steps.
Found uncertainty sample 41 after 1922 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 157 steps.
Found uncertainty sample 44 after 2853 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 688 steps.
Found uncertainty sample 47 after 1182 steps.
Found uncertainty sample 48 after 997 steps.
Found uncertainty sample 49 after 350 steps.
Found uncertainty sample 50 after 571 steps.
Found uncertainty sample 51 after 919 steps.
Found uncertainty sample 52 after 391 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 12 steps.
Found uncertainty sample 56 after 253 steps.
Found uncertainty sample 57 after 410 steps.
Found uncertainty sample 58 after 65 steps.
Found uncertainty sample 59 after 1498 steps.
Found uncertainty sample 60 after 1961 steps.
Found uncertainty sample 61 after 2425 steps.
Found uncertainty sample 62 after 1578 steps.
Found uncertainty sample 63 after 1538 steps.
Found uncertainty sample 64 after 1620 steps.
Found uncertainty sample 65 after 1648 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 118 steps.
Found uncertainty sample 68 after 1218 steps.
Found uncertainty sample 69 after 1298 steps.
Found uncertainty sample 70 after 607 steps.
Found uncertainty sample 71 after 245 steps.
Found uncertainty sample 72 after 1440 steps.
Found uncertainty sample 73 after 1155 steps.
Found uncertainty sample 74 after 1415 steps.
Found uncertainty sample 75 after 3059 steps.
Found uncertainty sample 76 after 1076 steps.
Found uncertainty sample 77 after 18 steps.
Found uncertainty sample 78 after 1422 steps.
Found uncertainty sample 79 after 347 steps.
Found uncertainty sample 80 after 2549 steps.
Found uncertainty sample 81 after 814 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 755 steps.
Found uncertainty sample 84 after 93 steps.
Found uncertainty sample 85 after 278 steps.
Found uncertainty sample 86 after 1457 steps.
Found uncertainty sample 87 after 150 steps.
Found uncertainty sample 88 after 1903 steps.
Found uncertainty sample 89 after 2490 steps.
Found uncertainty sample 90 after 22 steps.
Found uncertainty sample 91 after 2053 steps.
Found uncertainty sample 92 after 209 steps.
Found uncertainty sample 93 after 2408 steps.
Found uncertainty sample 94 after 1176 steps.
Found uncertainty sample 95 after 1949 steps.
Found uncertainty sample 96 after 651 steps.
Found uncertainty sample 97 after 165 steps.
Found uncertainty sample 98 after 469 steps.
Found uncertainty sample 99 after 48 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_092621-lkyy5kr3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_57
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/lkyy5kr3
Training model 57. Added 96 samples to the dataset.
Epoch 0, Batch 100/186, Loss: 0.6747602224349976, Variance: 0.14200201630592346

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.2424797558111638, Training Loss Force: 3.5815019439983593, time: 2.943345785140991
Validation Loss Energy: 1.438350803837171, Validation Loss Force: 3.5039316447639495, time: 0.19762158393859863
Test Loss Energy: 9.592160788031622, Test Loss Force: 11.51966329938515, time: 11.796226739883423

Epoch 1, Batch 100/186, Loss: 0.7355811595916748, Variance: 0.10510522872209549

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6502378233785946, Training Loss Force: 3.4187192506553528, time: 2.8222923278808594
Validation Loss Energy: 1.6586457956637901, Validation Loss Force: 3.5574850217090312, time: 0.18251299858093262
Test Loss Energy: 9.716687624209996, Test Loss Force: 11.26618048823319, time: 11.915246486663818

Epoch 2, Batch 100/186, Loss: 0.551428496837616, Variance: 0.09855231642723083

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6833323274714636, Training Loss Force: 3.4221257426128693, time: 2.8692286014556885
Validation Loss Energy: 1.5346901406732065, Validation Loss Force: 3.5276181864965914, time: 0.18069696426391602
Test Loss Energy: 9.616110055457415, Test Loss Force: 11.386166725136228, time: 12.542565822601318

Epoch 3, Batch 100/186, Loss: 0.5198467373847961, Variance: 0.0979027971625328

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6812877835635656, Training Loss Force: 3.4120689852115706, time: 2.9632952213287354
Validation Loss Energy: 1.801861914272479, Validation Loss Force: 3.4812886419836993, time: 0.20313358306884766
Test Loss Energy: 9.631840891495546, Test Loss Force: 11.388028155258079, time: 11.715906143188477

Epoch 4, Batch 100/186, Loss: 0.45231395959854126, Variance: 0.09650494158267975

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6605895760967126, Training Loss Force: 3.4044423359143767, time: 2.9596872329711914
Validation Loss Energy: 1.4899855242527225, Validation Loss Force: 3.4013161832144365, time: 0.1881561279296875
Test Loss Energy: 9.562011157174817, Test Loss Force: 11.424478548848759, time: 11.47750186920166

Epoch 5, Batch 100/186, Loss: 0.7480646371841431, Variance: 0.09566628932952881

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.682152503628879, Training Loss Force: 3.4101858842159185, time: 3.0625269412994385
Validation Loss Energy: 2.3854562860102724, Validation Loss Force: 3.670252526294257, time: 0.19928216934204102
Test Loss Energy: 9.971748168670594, Test Loss Force: 11.558658113487626, time: 11.662568807601929

Epoch 6, Batch 100/186, Loss: 0.7688584327697754, Variance: 0.09601545333862305

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6581955529642807, Training Loss Force: 3.4174444099376187, time: 2.938833475112915
Validation Loss Energy: 1.3119597328966455, Validation Loss Force: 3.5614702984155606, time: 0.19704604148864746
Test Loss Energy: 9.644792975142256, Test Loss Force: 11.49123933342476, time: 11.527973651885986

Epoch 7, Batch 100/186, Loss: 0.5412631034851074, Variance: 0.09661982953548431

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.684412164188219, Training Loss Force: 3.4225413390108463, time: 2.9776406288146973
Validation Loss Energy: 1.9129840330375145, Validation Loss Force: 3.5793723967425763, time: 0.2134242057800293
Test Loss Energy: 10.098525693280497, Test Loss Force: 11.716145252711165, time: 11.833812236785889

Epoch 8, Batch 100/186, Loss: 0.3427262306213379, Variance: 0.09476013481616974

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6931090460988178, Training Loss Force: 3.4187754267288613, time: 2.9189765453338623
Validation Loss Energy: 1.3366130344767972, Validation Loss Force: 3.496187768541446, time: 0.1851189136505127
Test Loss Energy: 9.630878648348848, Test Loss Force: 11.619620947909013, time: 11.684908390045166

Epoch 9, Batch 100/186, Loss: 0.5741069912910461, Variance: 0.09495817124843597

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6603141313703356, Training Loss Force: 3.406231020403571, time: 2.9083895683288574
Validation Loss Energy: 1.7444531045665876, Validation Loss Force: 3.60443837383424, time: 0.18573498725891113
Test Loss Energy: 10.036943516870688, Test Loss Force: 11.582505200257387, time: 11.562004327774048

Epoch 10, Batch 100/186, Loss: 0.40304869413375854, Variance: 0.09464612603187561

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6660169958640543, Training Loss Force: 3.421854062311919, time: 3.279480218887329
Validation Loss Energy: 1.4494112813209825, Validation Loss Force: 3.5872592265567653, time: 0.18627548217773438
Test Loss Energy: 9.462759423470672, Test Loss Force: 11.545620566857934, time: 11.440397500991821

Epoch 11, Batch 100/186, Loss: 0.7480238676071167, Variance: 0.09165351837873459

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6613000923913146, Training Loss Force: 3.426545575516293, time: 3.0088202953338623
Validation Loss Energy: 1.8734679973881532, Validation Loss Force: 3.4883995811589026, time: 0.19197511672973633
Test Loss Energy: 10.11866200223079, Test Loss Force: 11.73388416340794, time: 11.55599308013916

Epoch 12, Batch 100/186, Loss: 0.7494663000106812, Variance: 0.09427382051944733

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6721041486956874, Training Loss Force: 3.4145966894417645, time: 3.1763627529144287
Validation Loss Energy: 1.4258438954150559, Validation Loss Force: 3.592754923747409, time: 0.19252824783325195
Test Loss Energy: 9.81340807371615, Test Loss Force: 11.688374726263703, time: 11.514912605285645

Epoch 13, Batch 100/186, Loss: 0.6224384903907776, Variance: 0.09198048710823059

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6604138835562732, Training Loss Force: 3.415441917576787, time: 3.028284788131714
Validation Loss Energy: 1.7265920608633523, Validation Loss Force: 3.5665612944334804, time: 0.20035552978515625
Test Loss Energy: 9.787074133986563, Test Loss Force: 11.734783397015375, time: 11.763277769088745

Epoch 14, Batch 100/186, Loss: 0.5045151710510254, Variance: 0.09454745054244995

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6720227430706562, Training Loss Force: 3.418630211728484, time: 3.1020045280456543
Validation Loss Energy: 1.6555104470872675, Validation Loss Force: 3.4460251416832666, time: 0.2017371654510498
Test Loss Energy: 9.469047315694754, Test Loss Force: 11.636893162728835, time: 11.872138500213623

Epoch 15, Batch 100/186, Loss: 0.4324469566345215, Variance: 0.09521947056055069

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6759362999607155, Training Loss Force: 3.4108141739426574, time: 2.9266409873962402
Validation Loss Energy: 2.0579633572782323, Validation Loss Force: 3.597845530094324, time: 0.19674158096313477
Test Loss Energy: 9.803121141558998, Test Loss Force: 11.680697746964057, time: 11.682669639587402

Epoch 16, Batch 100/186, Loss: 0.7303110361099243, Variance: 0.09456299990415573

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6814131708342146, Training Loss Force: 3.4181044635377376, time: 3.029928684234619
Validation Loss Energy: 1.5363484969711472, Validation Loss Force: 3.5263788100303572, time: 0.265643835067749
Test Loss Energy: 9.481138725967908, Test Loss Force: 11.61163819607392, time: 11.646656274795532

Epoch 17, Batch 100/186, Loss: 1.003735065460205, Variance: 0.0931454449892044

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.668818185092712, Training Loss Force: 3.415625862642805, time: 3.011312246322632
Validation Loss Energy: 1.672439703915725, Validation Loss Force: 3.5484440420442116, time: 0.18737339973449707
Test Loss Energy: 9.869434023067642, Test Loss Force: 11.763592093290873, time: 11.568822145462036

Epoch 18, Batch 100/186, Loss: 0.7999323606491089, Variance: 0.09361647069454193

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.670881607393603, Training Loss Force: 3.429544780465843, time: 3.018087863922119
Validation Loss Energy: 1.5836005875892614, Validation Loss Force: 3.503779728739141, time: 0.19887137413024902
Test Loss Energy: 9.797880349244931, Test Loss Force: 11.884901148133709, time: 12.638649940490723

Epoch 19, Batch 100/186, Loss: 0.5661947727203369, Variance: 0.0875566154718399

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.682081154927652, Training Loss Force: 3.4187875969145467, time: 2.973846197128296
Validation Loss Energy: 1.9201527270224437, Validation Loss Force: 3.4747467894782, time: 0.19409656524658203
Test Loss Energy: 10.08587699925963, Test Loss Force: 11.80176861343756, time: 10.986880540847778

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–ƒâ–ƒâ–‚â–†â–ƒâ–ˆâ–ƒâ–‡â–â–ˆâ–…â–„â–â–…â–â–…â–…â–ˆ
wandb:   test_error_force â–„â–â–‚â–‚â–ƒâ–„â–„â–†â–…â–…â–„â–†â–†â–†â–…â–†â–…â–‡â–ˆâ–‡
wandb:          test_loss â–â–„â–„â–„â–„â–…â–…â–‡â–…â–†â–…â–‡â–‡â–…â–…â–†â–…â–‡â–‡â–ˆ
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–ƒâ–‚â–„â–‚â–ˆâ–â–…â–â–„â–‚â–…â–‚â–„â–ƒâ–†â–‚â–ƒâ–ƒâ–…
wandb:  valid_error_force â–„â–…â–„â–ƒâ–â–ˆâ–…â–†â–ƒâ–†â–†â–ƒâ–†â–…â–‚â–†â–„â–…â–„â–ƒ
wandb:         valid_loss â–‚â–ƒâ–‚â–ƒâ–â–ˆâ–â–„â–â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–†â–ƒâ–ƒâ–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 5948
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.08588
wandb:   test_error_force 11.80177
wandb:          test_loss 14.38627
wandb: train_error_energy 1.68208
wandb:  train_error_force 3.41879
wandb:         train_loss 0.55938
wandb: valid_error_energy 1.92015
wandb:  valid_error_force 3.47475
wandb:         valid_loss 0.71141
wandb: 
wandb: ğŸš€ View run al_63_57 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/lkyy5kr3
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_092621-lkyy5kr3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.6326258182525635, Uncertainty Bias: -0.09411852061748505
4.863739e-05 0.013776779
2.1062787 5.152048
(48745, 22, 3)
Found uncertainty sample 0 after 1322 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 2 steps.
Found uncertainty sample 3 after 2629 steps.
Found uncertainty sample 4 after 200 steps.
Found uncertainty sample 5 after 1569 steps.
Found uncertainty sample 6 after 2652 steps.
Found uncertainty sample 7 after 220 steps.
Found uncertainty sample 8 after 307 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 1583 steps.
Found uncertainty sample 11 after 366 steps.
Found uncertainty sample 12 after 2467 steps.
Found uncertainty sample 13 after 1283 steps.
Found uncertainty sample 14 after 1530 steps.
Found uncertainty sample 15 after 1513 steps.
Found uncertainty sample 16 after 3694 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 2 steps.
Found uncertainty sample 19 after 353 steps.
Found uncertainty sample 20 after 1202 steps.
Found uncertainty sample 21 after 614 steps.
Found uncertainty sample 22 after 100 steps.
Found uncertainty sample 23 after 173 steps.
Found uncertainty sample 24 after 1243 steps.
Found uncertainty sample 25 after 38 steps.
Found uncertainty sample 26 after 1742 steps.
Found uncertainty sample 27 after 274 steps.
Found uncertainty sample 28 after 176 steps.
Found uncertainty sample 29 after 376 steps.
Found uncertainty sample 30 after 1221 steps.
Found uncertainty sample 31 after 1351 steps.
Found uncertainty sample 32 after 369 steps.
Found uncertainty sample 33 after 819 steps.
Found uncertainty sample 34 after 1661 steps.
Found uncertainty sample 35 after 107 steps.
Found uncertainty sample 36 after 708 steps.
Found uncertainty sample 37 after 478 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 310 steps.
Found uncertainty sample 40 after 400 steps.
Found uncertainty sample 41 after 632 steps.
Found uncertainty sample 42 after 2983 steps.
Found uncertainty sample 43 after 1371 steps.
Found uncertainty sample 44 after 1611 steps.
Found uncertainty sample 45 after 107 steps.
Found uncertainty sample 46 after 1445 steps.
Found uncertainty sample 47 after 1062 steps.
Found uncertainty sample 48 after 2674 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 130 steps.
Found uncertainty sample 51 after 733 steps.
Found uncertainty sample 52 after 335 steps.
Found uncertainty sample 53 after 2688 steps.
Found uncertainty sample 54 after 3222 steps.
Found uncertainty sample 55 after 779 steps.
Found uncertainty sample 56 after 2574 steps.
Found uncertainty sample 57 after 29 steps.
Found uncertainty sample 58 after 4 steps.
Found uncertainty sample 59 after 1205 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 270 steps.
Found uncertainty sample 62 after 891 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 1202 steps.
Found uncertainty sample 65 after 12 steps.
Found uncertainty sample 66 after 1179 steps.
Found uncertainty sample 67 after 1127 steps.
Found uncertainty sample 68 after 3402 steps.
Found uncertainty sample 69 after 1564 steps.
Found uncertainty sample 70 after 15 steps.
Found uncertainty sample 71 after 444 steps.
Found uncertainty sample 72 after 983 steps.
Found uncertainty sample 73 after 1891 steps.
Found uncertainty sample 74 after 51 steps.
Found uncertainty sample 75 after 940 steps.
Found uncertainty sample 76 after 10 steps.
Found uncertainty sample 77 after 169 steps.
Found uncertainty sample 78 after 1849 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 3777 steps.
Found uncertainty sample 81 after 360 steps.
Found uncertainty sample 82 after 1163 steps.
Found uncertainty sample 83 after 718 steps.
Found uncertainty sample 84 after 262 steps.
Found uncertainty sample 85 after 117 steps.
Found uncertainty sample 86 after 1238 steps.
Found uncertainty sample 87 after 100 steps.
Found uncertainty sample 88 after 2344 steps.
Found uncertainty sample 89 after 585 steps.
Found uncertainty sample 90 after 224 steps.
Found uncertainty sample 91 after 1405 steps.
Found uncertainty sample 92 after 642 steps.
Found uncertainty sample 93 after 818 steps.
Found uncertainty sample 94 after 990 steps.
Found uncertainty sample 95 after 2549 steps.
Found uncertainty sample 96 after 3172 steps.
Found uncertainty sample 97 after 2235 steps.
Found uncertainty sample 98 after 348 steps.
Found uncertainty sample 99 after 1028 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_094541-0pxttqew
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_58
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/0pxttqew
Training model 58. Added 93 samples to the dataset.
Epoch 0, Batch 100/189, Loss: 0.606494665145874, Variance: 0.09726844727993011

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7028625710485303, Training Loss Force: 3.5858594207998573, time: 3.0559964179992676
Validation Loss Energy: 1.6456109943639667, Validation Loss Force: 3.4754398471948997, time: 0.19033575057983398
Test Loss Energy: 9.540202693871558, Test Loss Force: 11.62940006384306, time: 11.55434775352478

Epoch 1, Batch 100/189, Loss: 0.5020127296447754, Variance: 0.08770821988582611

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.627127432084813, Training Loss Force: 3.4005969548042287, time: 3.1550605297088623
Validation Loss Energy: 1.3270225491102927, Validation Loss Force: 3.4676619025930693, time: 0.2023606300354004
Test Loss Energy: 9.563291614349767, Test Loss Force: 11.798322291951804, time: 11.778444766998291

Epoch 2, Batch 100/189, Loss: 0.32581663131713867, Variance: 0.08721672743558884

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.636670202560699, Training Loss Force: 3.402592257676578, time: 3.106708288192749
Validation Loss Energy: 1.9294969542303733, Validation Loss Force: 3.483350613942277, time: 0.1937847137451172
Test Loss Energy: 9.739100017532524, Test Loss Force: 11.56295900884447, time: 11.642052173614502

Epoch 3, Batch 100/189, Loss: 0.6419448852539062, Variance: 0.08991078287363052

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6534958387116048, Training Loss Force: 3.401695352247499, time: 3.2146902084350586
Validation Loss Energy: 1.7877476407868407, Validation Loss Force: 3.519089457201978, time: 0.19129061698913574
Test Loss Energy: 9.859156487315285, Test Loss Force: 11.681556154008717, time: 11.856389284133911

Epoch 4, Batch 100/189, Loss: 0.6440858244895935, Variance: 0.09144386649131775

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6748180747402104, Training Loss Force: 3.4127047934329, time: 3.0930676460266113
Validation Loss Energy: 1.4465023037649387, Validation Loss Force: 3.4692347261053307, time: 0.20183467864990234
Test Loss Energy: 9.979079602761585, Test Loss Force: 11.875278926512014, time: 11.634595394134521

Epoch 5, Batch 100/189, Loss: 0.503308892250061, Variance: 0.09146624058485031

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6563192354726166, Training Loss Force: 3.422972951359352, time: 3.1706745624542236
Validation Loss Energy: 1.5259138589556087, Validation Loss Force: 3.470155574358315, time: 0.20906805992126465
Test Loss Energy: 9.498041692390201, Test Loss Force: 11.659194948941995, time: 11.751639127731323

Epoch 6, Batch 100/189, Loss: 0.6389056444168091, Variance: 0.09174175560474396

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6823682635609938, Training Loss Force: 3.4145558491118595, time: 2.9805524349212646
Validation Loss Energy: 1.9937434045684856, Validation Loss Force: 3.5147642760441906, time: 0.2026076316833496
Test Loss Energy: 9.68050767453802, Test Loss Force: 11.606056759604025, time: 11.673476457595825

Epoch 7, Batch 100/189, Loss: 0.4113645553588867, Variance: 0.09162142872810364

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6533134903532447, Training Loss Force: 3.4021875559147827, time: 3.065554141998291
Validation Loss Energy: 1.7636628552867075, Validation Loss Force: 3.5040108036453272, time: 0.19403076171875
Test Loss Energy: 9.819940305107545, Test Loss Force: 11.46645023725541, time: 11.684572696685791

Epoch 8, Batch 100/189, Loss: 0.6563168168067932, Variance: 0.0890723392367363

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6857438986542048, Training Loss Force: 3.4166570977282125, time: 3.1391091346740723
Validation Loss Energy: 1.4772471512534153, Validation Loss Force: 3.6044072558210964, time: 0.19098782539367676
Test Loss Energy: 9.806130984886451, Test Loss Force: 11.920802787021032, time: 11.703170776367188

Epoch 9, Batch 100/189, Loss: 0.5238608121871948, Variance: 0.09393906593322754

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.664242099693314, Training Loss Force: 3.4181338112285586, time: 3.2238388061523438
Validation Loss Energy: 1.32386813448815, Validation Loss Force: 3.528317200612945, time: 0.19124221801757812
Test Loss Energy: 9.542525382546293, Test Loss Force: 11.73663611156374, time: 11.760321140289307

Epoch 10, Batch 100/189, Loss: 0.5206904411315918, Variance: 0.09051832556724548

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6548821570802457, Training Loss Force: 3.4152268961602057, time: 3.1909408569335938
Validation Loss Energy: 2.0448364544529243, Validation Loss Force: 3.518737083278548, time: 0.1860368251800537
Test Loss Energy: 9.995248301936787, Test Loss Force: 11.807896827402061, time: 11.582417964935303

Epoch 11, Batch 100/189, Loss: 0.21857261657714844, Variance: 0.08631168305873871

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6768282310583662, Training Loss Force: 3.4190371858443056, time: 3.0904300212860107
Validation Loss Energy: 1.8802914706959641, Validation Loss Force: 3.568470571025238, time: 0.19163155555725098
Test Loss Energy: 9.786734315222121, Test Loss Force: 11.699039123426449, time: 11.69614291191101

Epoch 12, Batch 100/189, Loss: 0.5573727488517761, Variance: 0.09184051305055618

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6738720092064496, Training Loss Force: 3.4218099362724472, time: 3.185788154602051
Validation Loss Energy: 1.599116563348515, Validation Loss Force: 3.4669812586812547, time: 0.1885511875152588
Test Loss Energy: 9.575138023277207, Test Loss Force: 11.839555073199316, time: 11.725505113601685

Epoch 13, Batch 100/189, Loss: 0.6812233328819275, Variance: 0.09140875935554504

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.657522542777105, Training Loss Force: 3.4247206602791533, time: 3.030040979385376
Validation Loss Energy: 1.4162495829226058, Validation Loss Force: 3.577966519422475, time: 0.19270682334899902
Test Loss Energy: 9.68197873650188, Test Loss Force: 11.862509011874424, time: 11.727519035339355

Epoch 14, Batch 100/189, Loss: 0.7509821653366089, Variance: 0.09021218866109848

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6669663038296045, Training Loss Force: 3.419676590311058, time: 3.3925185203552246
Validation Loss Energy: 1.9109489483783075, Validation Loss Force: 3.5058621862591437, time: 0.20066070556640625
Test Loss Energy: 9.873245573248226, Test Loss Force: 11.802050524234375, time: 11.683596134185791

Epoch 15, Batch 100/189, Loss: 0.4799801707267761, Variance: 0.08959609270095825

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6834448188104505, Training Loss Force: 3.41508980854565, time: 3.0017664432525635
Validation Loss Energy: 2.0053427692190855, Validation Loss Force: 3.5948893745129866, time: 0.19123172760009766
Test Loss Energy: 9.81002987733098, Test Loss Force: 11.76195261417781, time: 12.384219884872437

Epoch 16, Batch 100/189, Loss: 0.6756027340888977, Variance: 0.09101662039756775

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6620420001138914, Training Loss Force: 3.406561449482673, time: 3.199841260910034
Validation Loss Energy: 1.4670617635131, Validation Loss Force: 3.521369366587133, time: 0.1934804916381836
Test Loss Energy: 9.771582917928821, Test Loss Force: 11.865461133763382, time: 11.776280879974365

Epoch 17, Batch 100/189, Loss: 0.6076837182044983, Variance: 0.08753374963998795

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6887772741870413, Training Loss Force: 3.412574659222043, time: 3.144805908203125
Validation Loss Energy: 1.5690026210513817, Validation Loss Force: 3.45259477216472, time: 0.21056914329528809
Test Loss Energy: 9.766320907926504, Test Loss Force: 11.659078531450305, time: 11.185441970825195

Epoch 18, Batch 100/189, Loss: 0.5185618996620178, Variance: 0.08962197601795197

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6759915941557135, Training Loss Force: 3.404353523495122, time: 3.121819496154785
Validation Loss Energy: 2.046547833841239, Validation Loss Force: 3.46482303798435, time: 0.15610885620117188
Test Loss Energy: 9.697065216995593, Test Loss Force: 11.531179561083405, time: 10.476387023925781

Epoch 19, Batch 100/189, Loss: 0.4281063675880432, Variance: 0.09035121649503708

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6663724172365038, Training Loss Force: 3.4037490792624236, time: 3.282392978668213
Validation Loss Energy: 2.0093146579432157, Validation Loss Force: 3.430586098113931, time: 0.2061750888824463
Test Loss Energy: 10.276240741245603, Test Loss Force: 11.771490132922722, time: 10.268743753433228

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–ƒâ–„â–…â–â–ƒâ–„â–„â–â–…â–„â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–ˆ
wandb:   test_error_force â–„â–†â–‚â–„â–‡â–„â–ƒâ–â–ˆâ–…â–†â–…â–‡â–‡â–†â–†â–‡â–„â–‚â–†
wandb:          test_loss â–‚â–„â–ƒâ–…â–ˆâ–‚â–ƒâ–…â–ˆâ–â–†â–„â–„â–…â–…â–†â–ˆâ–…â–â–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–â–‡â–…â–‚â–ƒâ–‡â–…â–‚â–â–ˆâ–†â–„â–‚â–‡â–ˆâ–‚â–ƒâ–ˆâ–ˆ
wandb:  valid_error_force â–ƒâ–‚â–ƒâ–…â–ƒâ–ƒâ–„â–„â–ˆâ–…â–…â–‡â–‚â–‡â–„â–ˆâ–…â–‚â–‚â–
wandb:         valid_loss â–„â–â–†â–…â–â–ƒâ–‡â–„â–ƒâ–‚â–‡â–†â–ƒâ–‚â–†â–ˆâ–ƒâ–ƒâ–‡â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 6031
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.27624
wandb:   test_error_force 11.77149
wandb:          test_loss 14.18311
wandb: train_error_energy 1.66637
wandb:  train_error_force 3.40375
wandb:         train_loss 0.55014
wandb: valid_error_energy 2.00931
wandb:  valid_error_force 3.43059
wandb:         valid_loss 0.74323
wandb: 
wandb: ğŸš€ View run al_63_58 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/0pxttqew
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_094541-0pxttqew/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.3346898555755615, Uncertainty Bias: -0.0703197568655014
7.6293945e-05 0.004463196
2.0623212 5.2196736
(48745, 22, 3)
Found uncertainty sample 0 after 511 steps.
Found uncertainty sample 1 after 404 steps.
Found uncertainty sample 2 after 105 steps.
Found uncertainty sample 3 after 451 steps.
Found uncertainty sample 4 after 7 steps.
Found uncertainty sample 5 after 176 steps.
Found uncertainty sample 6 after 118 steps.
Found uncertainty sample 7 after 865 steps.
Found uncertainty sample 8 after 12 steps.
Found uncertainty sample 9 after 1431 steps.
Found uncertainty sample 10 after 343 steps.
Found uncertainty sample 11 after 3564 steps.
Found uncertainty sample 12 after 451 steps.
Found uncertainty sample 13 after 2504 steps.
Found uncertainty sample 14 after 2373 steps.
Found uncertainty sample 15 after 1623 steps.
Found uncertainty sample 16 after 808 steps.
Found uncertainty sample 17 after 3 steps.
Found uncertainty sample 18 after 1396 steps.
Found uncertainty sample 19 after 10 steps.
Found uncertainty sample 20 after 1177 steps.
Found uncertainty sample 21 after 167 steps.
Found uncertainty sample 22 after 297 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 3356 steps.
Found uncertainty sample 25 after 1534 steps.
Found uncertainty sample 26 after 1792 steps.
Found uncertainty sample 27 after 3458 steps.
Found uncertainty sample 28 after 307 steps.
Found uncertainty sample 29 after 485 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 893 steps.
Found uncertainty sample 32 after 1402 steps.
Found uncertainty sample 33 after 1 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 1710 steps.
Found uncertainty sample 36 after 56 steps.
Found uncertainty sample 37 after 229 steps.
Found uncertainty sample 38 after 18 steps.
Found uncertainty sample 39 after 43 steps.
Found uncertainty sample 40 after 943 steps.
Found uncertainty sample 41 after 527 steps.
Found uncertainty sample 42 after 336 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 707 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 792 steps.
Found uncertainty sample 48 after 782 steps.
Found uncertainty sample 49 after 2399 steps.
Found uncertainty sample 50 after 3188 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 44 steps.
Found uncertainty sample 54 after 531 steps.
Found uncertainty sample 55 after 10 steps.
Found uncertainty sample 56 after 3178 steps.
Found uncertainty sample 57 after 776 steps.
Found uncertainty sample 58 after 1734 steps.
Found uncertainty sample 59 after 72 steps.
Found uncertainty sample 60 after 1103 steps.
Found uncertainty sample 61 after 164 steps.
Found uncertainty sample 62 after 66 steps.
Found uncertainty sample 63 after 380 steps.
Found uncertainty sample 64 after 9 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 3205 steps.
Found uncertainty sample 67 after 261 steps.
Found uncertainty sample 68 after 405 steps.
Found uncertainty sample 69 after 736 steps.
Found uncertainty sample 70 after 530 steps.
Found uncertainty sample 71 after 316 steps.
Found uncertainty sample 72 after 2677 steps.
Found uncertainty sample 73 after 734 steps.
Found uncertainty sample 74 after 154 steps.
Found uncertainty sample 75 after 862 steps.
Found uncertainty sample 76 after 3445 steps.
Found uncertainty sample 77 after 2463 steps.
Found uncertainty sample 78 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 3839 steps.
Found uncertainty sample 81 after 5 steps.
Found uncertainty sample 82 after 140 steps.
Found uncertainty sample 83 after 1907 steps.
Found uncertainty sample 84 after 1375 steps.
Found uncertainty sample 85 after 2972 steps.
Found uncertainty sample 86 after 230 steps.
Found uncertainty sample 87 after 376 steps.
Found uncertainty sample 88 after 1010 steps.
Found uncertainty sample 89 after 431 steps.
Found uncertainty sample 90 after 2012 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 37 steps.
Found uncertainty sample 93 after 2782 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 0 steps.
Found uncertainty sample 96 after 1026 steps.
Found uncertainty sample 97 after 3023 steps.
Found uncertainty sample 98 after 19 steps.
Found uncertainty sample 99 after 59 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_100443-u3ytg58u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_59
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/u3ytg58u
Training model 59. Added 91 samples to the dataset.
Epoch 0, Batch 100/191, Loss: 0.6199756860733032, Variance: 0.09072083234786987

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.0816132142690824, Training Loss Force: 3.5059923844644785, time: 3.042893409729004
Validation Loss Energy: 1.6317024115018794, Validation Loss Force: 3.4549541187676844, time: 0.1849985122680664
Test Loss Energy: 9.538993546782047, Test Loss Force: 11.617553202582489, time: 11.267775774002075

Epoch 1, Batch 100/191, Loss: 0.5544372797012329, Variance: 0.09380146861076355

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6413515000213243, Training Loss Force: 3.399524511488844, time: 3.043217658996582
Validation Loss Energy: 1.9145125175560924, Validation Loss Force: 3.4636557506541097, time: 0.206465482711792
Test Loss Energy: 10.057560049722904, Test Loss Force: 11.696908545763334, time: 11.33953332901001

Epoch 2, Batch 100/191, Loss: 0.6871637105941772, Variance: 0.09454626590013504

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.672663937720435, Training Loss Force: 3.4175093946486035, time: 3.0025181770324707
Validation Loss Energy: 2.0647515931937006, Validation Loss Force: 3.4989373984899355, time: 0.20152902603149414
Test Loss Energy: 9.919703795462615, Test Loss Force: 11.734649926098209, time: 11.314084529876709

Epoch 3, Batch 100/191, Loss: 0.22621124982833862, Variance: 0.09171642363071442

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6655391044568484, Training Loss Force: 3.4255194963170124, time: 2.959052801132202
Validation Loss Energy: 1.3274624041744298, Validation Loss Force: 3.502668832370107, time: 0.19309306144714355
Test Loss Energy: 9.581519702346585, Test Loss Force: 11.76370707450828, time: 11.570121765136719

Epoch 4, Batch 100/191, Loss: 0.468356728553772, Variance: 0.08691508322954178

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6444999420682405, Training Loss Force: 3.407918328054424, time: 2.992643117904663
Validation Loss Energy: 1.7188671115182523, Validation Loss Force: 3.5004334568141378, time: 0.18555569648742676
Test Loss Energy: 9.688625580715655, Test Loss Force: 11.755287961095592, time: 11.279305219650269

Epoch 5, Batch 100/191, Loss: 0.41680270433425903, Variance: 0.09186825901269913

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.666805702517624, Training Loss Force: 3.399529761117975, time: 2.991389513015747
Validation Loss Energy: 1.838136026325348, Validation Loss Force: 3.435197464791522, time: 0.19213318824768066
Test Loss Energy: 9.788944391599369, Test Loss Force: 11.726068159286358, time: 11.517130851745605

Epoch 6, Batch 100/191, Loss: 0.5702953934669495, Variance: 0.09382683783769608

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6802921722958408, Training Loss Force: 3.4162533902764625, time: 3.2331478595733643
Validation Loss Energy: 2.1001092268755737, Validation Loss Force: 3.517039064038989, time: 0.19496870040893555
Test Loss Energy: 10.076246705589943, Test Loss Force: 11.680257948587286, time: 11.365094184875488

Epoch 7, Batch 100/191, Loss: 0.45150530338287354, Variance: 0.08782543241977692

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6718155927563987, Training Loss Force: 3.415965727387678, time: 3.149531602859497
Validation Loss Energy: 1.3543752826633488, Validation Loss Force: 3.441761973882619, time: 0.19789719581604004
Test Loss Energy: 9.38035100883708, Test Loss Force: 11.644692682338775, time: 11.238710165023804

Epoch 8, Batch 100/191, Loss: 0.3825117349624634, Variance: 0.08565764129161835

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6544205024567522, Training Loss Force: 3.430672797143835, time: 3.2286691665649414
Validation Loss Energy: 1.4652779331514292, Validation Loss Force: 3.474335352563812, time: 0.19609475135803223
Test Loss Energy: 9.751898634339602, Test Loss Force: 11.869483032841023, time: 11.304260969161987

Epoch 9, Batch 100/191, Loss: 0.5018116235733032, Variance: 0.09143441915512085

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6732047395795724, Training Loss Force: 3.41386023322898, time: 3.117373466491699
Validation Loss Energy: 1.8980334142397717, Validation Loss Force: 3.439793355963472, time: 0.19961214065551758
Test Loss Energy: 9.976640585196497, Test Loss Force: 11.665072010762806, time: 11.384300708770752

Epoch 10, Batch 100/191, Loss: 0.8100075125694275, Variance: 0.09067974984645844

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6310173077952073, Training Loss Force: 3.4212540216756713, time: 3.0700571537017822
Validation Loss Energy: 1.69926126022117, Validation Loss Force: 3.5248516893602977, time: 0.2780420780181885
Test Loss Energy: 10.05184439621933, Test Loss Force: 11.962238390715306, time: 11.447296857833862

Epoch 11, Batch 100/191, Loss: 0.4682707190513611, Variance: 0.09178771078586578

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6753034583591668, Training Loss Force: 3.4187153007945263, time: 2.960744857788086
Validation Loss Energy: 1.4972896845793053, Validation Loss Force: 3.450355833902416, time: 0.18876409530639648
Test Loss Energy: 9.646062058703656, Test Loss Force: 11.785067639407412, time: 11.574745893478394

Epoch 12, Batch 100/191, Loss: 0.5199626684188843, Variance: 0.0841442346572876

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6616643343088282, Training Loss Force: 3.4673582524093507, time: 2.9415531158447266
Validation Loss Energy: 1.664722714992592, Validation Loss Force: 3.5188443541395986, time: 0.1877455711364746
Test Loss Energy: 9.48919169669304, Test Loss Force: 11.628376569194126, time: 12.582881927490234

Epoch 13, Batch 100/191, Loss: 0.5527570843696594, Variance: 0.08959692716598511

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6733858051523263, Training Loss Force: 3.4168631155225584, time: 3.0695104598999023
Validation Loss Energy: 1.8272030825948664, Validation Loss Force: 3.4988108667477613, time: 0.1852710247039795
Test Loss Energy: 9.712247279677452, Test Loss Force: 11.673185841279004, time: 11.419450759887695

Epoch 14, Batch 100/191, Loss: 0.5048177242279053, Variance: 0.09412805736064911

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6706712505490005, Training Loss Force: 3.4112774751524513, time: 3.0753040313720703
Validation Loss Energy: 1.8825041896663353, Validation Loss Force: 3.490550954779571, time: 0.20415997505187988
Test Loss Energy: 10.236675343868626, Test Loss Force: 11.864645637744502, time: 11.510937690734863

Epoch 15, Batch 100/191, Loss: 0.4494364261627197, Variance: 0.08718402683734894

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.666861988264744, Training Loss Force: 3.4168063675329434, time: 3.0994904041290283
Validation Loss Energy: 1.6902783972034379, Validation Loss Force: 3.558210032448172, time: 0.201218843460083
Test Loss Energy: 9.47079671175941, Test Loss Force: 11.79973130966464, time: 10.593000173568726

Epoch 16, Batch 100/191, Loss: 0.25888895988464355, Variance: 0.08817167580127716

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.657575069653892, Training Loss Force: 3.4146050195530546, time: 2.987933874130249
Validation Loss Energy: 1.6260430246553355, Validation Loss Force: 3.432258120730703, time: 0.19962787628173828
Test Loss Energy: 9.484826975732, Test Loss Force: 11.784619983491147, time: 12.215198755264282

Epoch 17, Batch 100/191, Loss: 0.685138463973999, Variance: 0.08787888288497925

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6379192358369399, Training Loss Force: 3.409747865192895, time: 3.141649007797241
Validation Loss Energy: 1.8074118035665128, Validation Loss Force: 3.5314849475502, time: 0.17143487930297852
Test Loss Energy: 9.995622847761537, Test Loss Force: 11.797706946349939, time: 9.927603721618652

Epoch 18, Batch 100/191, Loss: 0.5925363302230835, Variance: 0.08786945044994354

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6552207421417156, Training Loss Force: 3.417056032106846, time: 2.935931444168091
Validation Loss Energy: 2.114822981369336, Validation Loss Force: 3.4697190314977235, time: 0.1692523956298828
Test Loss Energy: 10.062391797650093, Test Loss Force: 11.594213442396875, time: 10.145622968673706

Epoch 19, Batch 100/191, Loss: 0.29558998346328735, Variance: 0.08758086711168289

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6478178860478148, Training Loss Force: 3.411807276164842, time: 2.875025510787964
Validation Loss Energy: 1.5730463762423599, Validation Loss Force: 3.48654513684433, time: 0.16936230659484863
Test Loss Energy: 9.356640274859508, Test Loss Force: 11.564580464580589, time: 10.062697649002075

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.039 MB of 0.056 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‡â–…â–ƒâ–„â–„â–‡â–â–„â–†â–‡â–ƒâ–‚â–„â–ˆâ–‚â–‚â–†â–‡â–
wandb:   test_error_force â–‚â–ƒâ–„â–…â–„â–„â–ƒâ–‚â–†â–ƒâ–ˆâ–…â–‚â–ƒâ–†â–…â–…â–…â–‚â–
wandb:          test_loss â–ƒâ–†â–„â–‚â–…â–„â–…â–â–„â–„â–†â–ƒâ–ƒâ–ƒâ–ˆâ–‚â–‚â–‡â–ƒâ–
wandb: train_error_energy â–ˆâ–â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–â–
wandb:  train_error_force â–ˆâ–â–‚â–ƒâ–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–…â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–†â–ˆâ–â–„â–†â–ˆâ–â–‚â–†â–„â–ƒâ–„â–…â–†â–„â–„â–…â–ˆâ–ƒ
wandb:  valid_error_force â–‚â–ƒâ–…â–…â–…â–â–†â–‚â–ƒâ–â–†â–‚â–†â–…â–„â–ˆâ–â–‡â–ƒâ–„
wandb:         valid_loss â–„â–†â–ˆâ–â–…â–…â–ˆâ–‚â–‚â–†â–„â–‚â–…â–…â–†â–†â–„â–…â–ˆâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 6112
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.35664
wandb:   test_error_force 11.56458
wandb:          test_loss 13.20816
wandb: train_error_energy 1.64782
wandb:  train_error_force 3.41181
wandb:         train_loss 0.53841
wandb: valid_error_energy 1.57305
wandb:  valid_error_force 3.48655
wandb:         valid_loss 0.57795
wandb: 
wandb: ğŸš€ View run al_63_59 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/u3ytg58u
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_100443-u3ytg58u/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.5052316188812256, Uncertainty Bias: -0.06967015564441681
0.0 0.0020694733
2.0643404 5.2429614
(48745, 22, 3)
Found uncertainty sample 0 after 660 steps.
Found uncertainty sample 1 after 163 steps.
Found uncertainty sample 2 after 2269 steps.
Found uncertainty sample 3 after 173 steps.
Found uncertainty sample 4 after 3284 steps.
Found uncertainty sample 5 after 104 steps.
Found uncertainty sample 6 after 3062 steps.
Found uncertainty sample 7 after 174 steps.
Found uncertainty sample 8 after 2912 steps.
Found uncertainty sample 9 after 169 steps.
Found uncertainty sample 10 after 50 steps.
Found uncertainty sample 11 after 561 steps.
Found uncertainty sample 12 after 1109 steps.
Found uncertainty sample 13 after 2444 steps.
Found uncertainty sample 14 after 2639 steps.
Found uncertainty sample 15 after 989 steps.
Found uncertainty sample 16 after 1865 steps.
Found uncertainty sample 17 after 1115 steps.
Found uncertainty sample 18 after 1256 steps.
Found uncertainty sample 19 after 2590 steps.
Found uncertainty sample 20 after 2464 steps.
Found uncertainty sample 21 after 1893 steps.
Found uncertainty sample 22 after 3186 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 901 steps.
Found uncertainty sample 25 after 383 steps.
Found uncertainty sample 26 after 1770 steps.
Found uncertainty sample 27 after 7 steps.
Found uncertainty sample 28 after 380 steps.
Found uncertainty sample 29 after 919 steps.
Found uncertainty sample 30 after 1450 steps.
Found uncertainty sample 31 after 995 steps.
Found uncertainty sample 32 after 1669 steps.
Found uncertainty sample 33 after 1699 steps.
Found uncertainty sample 34 after 2578 steps.
Found uncertainty sample 35 after 3278 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 485 steps.
Found uncertainty sample 38 after 951 steps.
Found uncertainty sample 39 after 411 steps.
Found uncertainty sample 40 after 2741 steps.
Found uncertainty sample 41 after 2635 steps.
Found uncertainty sample 42 after 318 steps.
Found uncertainty sample 43 after 382 steps.
Found uncertainty sample 44 after 205 steps.
Found uncertainty sample 45 after 14 steps.
Found uncertainty sample 46 after 2348 steps.
Found uncertainty sample 47 after 121 steps.
Found uncertainty sample 48 after 1099 steps.
Found uncertainty sample 49 after 9 steps.
Found uncertainty sample 50 after 1089 steps.
Found uncertainty sample 51 after 427 steps.
Found uncertainty sample 52 after 3844 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 850 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 2105 steps.
Found uncertainty sample 57 after 1057 steps.
Found uncertainty sample 58 after 3063 steps.
Found uncertainty sample 59 after 1152 steps.
Found uncertainty sample 60 after 75 steps.
Found uncertainty sample 61 after 1368 steps.
Found uncertainty sample 62 after 495 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 404 steps.
Found uncertainty sample 65 after 1336 steps.
Found uncertainty sample 66 after 1837 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 2877 steps.
Found uncertainty sample 69 after 12 steps.
Found uncertainty sample 70 after 403 steps.
Found uncertainty sample 71 after 3709 steps.
Found uncertainty sample 72 after 1146 steps.
Found uncertainty sample 73 after 815 steps.
Found uncertainty sample 74 after 73 steps.
Found uncertainty sample 75 after 119 steps.
Found uncertainty sample 76 after 791 steps.
Found uncertainty sample 77 after 2179 steps.
Found uncertainty sample 78 after 3615 steps.
Found uncertainty sample 79 after 132 steps.
Found uncertainty sample 80 after 1798 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 3571 steps.
Found uncertainty sample 83 after 736 steps.
Found uncertainty sample 84 after 60 steps.
Found uncertainty sample 85 after 2870 steps.
Found uncertainty sample 86 after 1475 steps.
Found uncertainty sample 87 after 2089 steps.
Found uncertainty sample 88 after 924 steps.
Found uncertainty sample 89 after 1230 steps.
Found uncertainty sample 90 after 3015 steps.
Found uncertainty sample 91 after 731 steps.
Found uncertainty sample 92 after 423 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 872 steps.
Found uncertainty sample 95 after 1901 steps.
Found uncertainty sample 96 after 208 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1750 steps.
Found uncertainty sample 99 after 3409 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_102505-0sxwr6o7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_60
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/0sxwr6o7
Training model 60. Added 96 samples to the dataset.
Epoch 0, Batch 100/194, Loss: 0.2732813358306885, Variance: 0.09837248921394348

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1139778300627476, Training Loss Force: 3.9727901772118632, time: 3.0061445236206055
Validation Loss Energy: 2.1395670345432753, Validation Loss Force: 3.4903457094943118, time: 0.19053363800048828
Test Loss Energy: 10.088958521067546, Test Loss Force: 11.51218965548737, time: 11.660221338272095

Epoch 1, Batch 100/194, Loss: 0.255729615688324, Variance: 0.08470098674297333

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6501117317788527, Training Loss Force: 3.3901652888579186, time: 3.0960593223571777
Validation Loss Energy: 1.564691802053506, Validation Loss Force: 3.40981357182676, time: 0.18920302391052246
Test Loss Energy: 9.417012497441327, Test Loss Force: 11.647035553465104, time: 11.621413230895996

Epoch 2, Batch 100/194, Loss: 0.4535256028175354, Variance: 0.09339317679405212

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6603588545398769, Training Loss Force: 3.3925030887776773, time: 3.1535420417785645
Validation Loss Energy: 2.045185098668161, Validation Loss Force: 3.5131439683365606, time: 0.20404696464538574
Test Loss Energy: 9.763735921992708, Test Loss Force: 11.49333487159128, time: 11.647661209106445

Epoch 3, Batch 100/194, Loss: 0.32827943563461304, Variance: 0.08642561733722687

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6581751301913095, Training Loss Force: 3.400620786323925, time: 3.2574050426483154
Validation Loss Energy: 1.5934798609212124, Validation Loss Force: 3.4934107502880076, time: 0.20583844184875488
Test Loss Energy: 9.762295222307571, Test Loss Force: 11.723163322531217, time: 11.95417308807373

Epoch 4, Batch 100/194, Loss: 0.6535184383392334, Variance: 0.09433044493198395

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.659132035350916, Training Loss Force: 3.4043024312682895, time: 3.3214495182037354
Validation Loss Energy: 1.8825335819935978, Validation Loss Force: 3.4872621871615035, time: 0.21069574356079102
Test Loss Energy: 10.059934612147991, Test Loss Force: 11.695617531039773, time: 11.8019437789917

Epoch 5, Batch 100/194, Loss: 0.636750340461731, Variance: 0.08832695335149765

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6751570577419563, Training Loss Force: 3.399084992461076, time: 3.2848405838012695
Validation Loss Energy: 1.4124913730776365, Validation Loss Force: 3.4812734448144895, time: 0.20978808403015137
Test Loss Energy: 9.363803338433577, Test Loss Force: 11.799405536543496, time: 12.074518203735352

Epoch 6, Batch 100/194, Loss: 0.5769706964492798, Variance: 0.0870630219578743

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6466596331858554, Training Loss Force: 3.4160978770488475, time: 3.1362674236297607
Validation Loss Energy: 1.975860400120733, Validation Loss Force: 3.5311826185173167, time: 0.2028353214263916
Test Loss Energy: 9.641516754231077, Test Loss Force: 11.586737061184149, time: 11.907808065414429

Epoch 7, Batch 100/194, Loss: 0.3584604859352112, Variance: 0.08912268280982971

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6720393818072228, Training Loss Force: 3.404890238789049, time: 3.2730729579925537
Validation Loss Energy: 1.4324740919042016, Validation Loss Force: 3.4964561405295758, time: 0.20214247703552246
Test Loss Energy: 9.523605970970758, Test Loss Force: 11.637056487632856, time: 11.79782247543335

Epoch 8, Batch 100/194, Loss: 0.6973061561584473, Variance: 0.09107914566993713

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6740716262870405, Training Loss Force: 3.4128017389672722, time: 3.208763837814331
Validation Loss Energy: 1.9988623455642194, Validation Loss Force: 3.399779320769243, time: 0.20733046531677246
Test Loss Energy: 9.925022813429518, Test Loss Force: 11.606693425485165, time: 11.820831775665283

Epoch 9, Batch 100/194, Loss: 0.1863613724708557, Variance: 0.08571146428585052

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6712716065403832, Training Loss Force: 3.4009917072098768, time: 3.1482245922088623
Validation Loss Energy: 1.6569825684782828, Validation Loss Force: 3.528156656158731, time: 0.20299458503723145
Test Loss Energy: 9.525405007099465, Test Loss Force: 11.834083092021425, time: 11.951199293136597

Epoch 10, Batch 100/194, Loss: 0.5418083071708679, Variance: 0.08789043128490448

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6512851172594658, Training Loss Force: 3.405047605042928, time: 3.3770751953125
Validation Loss Energy: 2.0204713284980684, Validation Loss Force: 3.4662135511333823, time: 0.21629977226257324
Test Loss Energy: 9.893255891099365, Test Loss Force: 11.592279545708998, time: 11.911194324493408

Epoch 11, Batch 100/194, Loss: 0.4603719711303711, Variance: 0.0899721309542656

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.67315253757942, Training Loss Force: 3.402155464545486, time: 3.2094976902008057
Validation Loss Energy: 1.3687836742571142, Validation Loss Force: 3.5640040506270725, time: 0.21039557456970215
Test Loss Energy: 9.41998543598091, Test Loss Force: 11.69336401669196, time: 13.014773607254028

Epoch 12, Batch 100/194, Loss: 0.4826539158821106, Variance: 0.08950039744377136

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6723160720374544, Training Loss Force: 3.4149029425894706, time: 3.0959713459014893
Validation Loss Energy: 1.8745455768862276, Validation Loss Force: 3.494707130055599, time: 0.19330883026123047
Test Loss Energy: 10.011841033114258, Test Loss Force: 11.849932024313393, time: 12.02177906036377

Epoch 13, Batch 100/194, Loss: 0.536913275718689, Variance: 0.08377806842327118

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6595456650972797, Training Loss Force: 3.4278828103206034, time: 3.4278104305267334
Validation Loss Energy: 1.6389689964100866, Validation Loss Force: 3.510826873204036, time: 0.20694589614868164
Test Loss Energy: 9.467288429960737, Test Loss Force: 11.828320055456244, time: 11.521882057189941

Epoch 14, Batch 100/194, Loss: 0.8077313303947449, Variance: 0.08945947885513306

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.67740500750494, Training Loss Force: 3.3916691216638735, time: 3.194824457168579
Validation Loss Energy: 1.9468355047826893, Validation Loss Force: 3.4969768523273275, time: 0.20995759963989258
Test Loss Energy: 9.90283879704422, Test Loss Force: 11.786998784790658, time: 11.057395696640015

Epoch 15, Batch 100/194, Loss: 0.38289791345596313, Variance: 0.09032642841339111

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6564054344423438, Training Loss Force: 3.4122893944791794, time: 3.1967170238494873
Validation Loss Energy: 1.4951631942356949, Validation Loss Force: 3.458244111066054, time: 0.1957554817199707
Test Loss Energy: 9.34889316527116, Test Loss Force: 11.73730691227521, time: 11.247546672821045

Epoch 16, Batch 100/194, Loss: 0.8430207967758179, Variance: 0.09175869077444077

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6688123888082433, Training Loss Force: 3.407763883974029, time: 3.1877641677856445
Validation Loss Energy: 2.126073061630905, Validation Loss Force: 3.45990878560101, time: 0.20493841171264648
Test Loss Energy: 9.977285276493346, Test Loss Force: 11.778999263987002, time: 11.081594467163086

Epoch 17, Batch 100/194, Loss: 0.2609841227531433, Variance: 0.08903299272060394

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6626727123614422, Training Loss Force: 3.412699249180074, time: 3.109706401824951
Validation Loss Energy: 1.6228938149244787, Validation Loss Force: 3.4159831215202674, time: 0.19372010231018066
Test Loss Energy: 9.545332953687907, Test Loss Force: 11.814057189661685, time: 11.2677001953125

Epoch 18, Batch 100/194, Loss: 0.4966188073158264, Variance: 0.08756060898303986

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6554286001072318, Training Loss Force: 3.4145021932201978, time: 3.1698601245880127
Validation Loss Energy: 2.190821575225798, Validation Loss Force: 3.4995130538994452, time: 0.1832897663116455
Test Loss Energy: 9.846612510746587, Test Loss Force: 11.659294937880778, time: 10.997014045715332

Epoch 19, Batch 100/194, Loss: 0.28983962535858154, Variance: 0.08766411989927292

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.667609415304031, Training Loss Force: 3.4189139875107455, time: 3.3585803508758545
Validation Loss Energy: 1.3340030319996263, Validation Loss Force: 3.510564902953852, time: 0.21098875999450684
Test Loss Energy: 9.501695327011463, Test Loss Force: 11.752846122277452, time: 11.487037181854248

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‚â–…â–…â–ˆâ–â–„â–ƒâ–†â–ƒâ–†â–‚â–‡â–‚â–†â–â–‡â–ƒâ–†â–‚
wandb:   test_error_force â–â–„â–â–†â–…â–‡â–ƒâ–„â–ƒâ–ˆâ–ƒâ–…â–ˆâ–ˆâ–‡â–†â–‡â–‡â–„â–†
wandb:          test_loss â–…â–ƒâ–„â–ˆâ–‡â–†â–‚â–ƒâ–…â–ƒâ–†â–ƒâ–‡â–„â–‡â–â–ˆâ–ƒâ–„â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–ƒâ–‡â–ƒâ–…â–‚â–†â–‚â–†â–„â–‡â–â–…â–ƒâ–†â–‚â–‡â–ƒâ–ˆâ–
wandb:  valid_error_force â–…â–â–†â–…â–…â–„â–‡â–…â–â–†â–„â–ˆâ–…â–†â–…â–ƒâ–„â–‚â–…â–†
wandb:         valid_loss â–‡â–ƒâ–‡â–„â–…â–‚â–†â–‚â–…â–…â–†â–‚â–…â–„â–†â–‚â–ˆâ–ƒâ–ˆâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 6198
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.5017
wandb:   test_error_force 11.75285
wandb:          test_loss 13.39192
wandb: train_error_energy 1.66761
wandb:  train_error_force 3.41891
wandb:         train_loss 0.5527
wandb: valid_error_energy 1.334
wandb:  valid_error_force 3.51056
wandb:         valid_loss 0.43646
wandb: 
wandb: ğŸš€ View run al_63_60 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/0sxwr6o7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_102505-0sxwr6o7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.4722704887390137, Uncertainty Bias: -0.06925366818904877
0.00010681152 9.918213e-05
2.1016812 5.283861
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 15 steps.
Found uncertainty sample 2 after 76 steps.
Found uncertainty sample 3 after 456 steps.
Found uncertainty sample 4 after 1726 steps.
Found uncertainty sample 5 after 802 steps.
Found uncertainty sample 6 after 284 steps.
Found uncertainty sample 7 after 108 steps.
Found uncertainty sample 8 after 2315 steps.
Found uncertainty sample 9 after 1067 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 130 steps.
Found uncertainty sample 12 after 823 steps.
Found uncertainty sample 13 after 221 steps.
Found uncertainty sample 14 after 21 steps.
Found uncertainty sample 15 after 52 steps.
Found uncertainty sample 16 after 3266 steps.
Found uncertainty sample 17 after 6 steps.
Found uncertainty sample 18 after 1843 steps.
Found uncertainty sample 19 after 1768 steps.
Found uncertainty sample 20 after 1039 steps.
Found uncertainty sample 21 after 796 steps.
Found uncertainty sample 22 after 1681 steps.
Found uncertainty sample 23 after 2719 steps.
Found uncertainty sample 24 after 659 steps.
Found uncertainty sample 25 after 3819 steps.
Found uncertainty sample 26 after 373 steps.
Found uncertainty sample 27 after 1954 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 379 steps.
Found uncertainty sample 31 after 2152 steps.
Found uncertainty sample 32 after 70 steps.
Found uncertainty sample 33 after 729 steps.
Found uncertainty sample 34 after 682 steps.
Found uncertainty sample 35 after 203 steps.
Found uncertainty sample 36 after 733 steps.
Did not find any uncertainty samples for sample 37.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 23 steps.
Found uncertainty sample 40 after 385 steps.
Found uncertainty sample 41 after 2949 steps.
Found uncertainty sample 42 after 401 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 500 steps.
Found uncertainty sample 45 after 3119 steps.
Found uncertainty sample 46 after 5 steps.
Found uncertainty sample 47 after 2161 steps.
Found uncertainty sample 48 after 183 steps.
Found uncertainty sample 49 after 1341 steps.
Found uncertainty sample 50 after 1850 steps.
Found uncertainty sample 51 after 227 steps.
Found uncertainty sample 52 after 520 steps.
Found uncertainty sample 53 after 2643 steps.
Found uncertainty sample 54 after 2909 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 40 steps.
Found uncertainty sample 57 after 31 steps.
Found uncertainty sample 58 after 87 steps.
Found uncertainty sample 59 after 874 steps.
Found uncertainty sample 60 after 1873 steps.
Found uncertainty sample 61 after 1228 steps.
Found uncertainty sample 62 after 467 steps.
Found uncertainty sample 63 after 5 steps.
Found uncertainty sample 64 after 2261 steps.
Found uncertainty sample 65 after 1764 steps.
Found uncertainty sample 66 after 300 steps.
Found uncertainty sample 67 after 301 steps.
Found uncertainty sample 68 after 1470 steps.
Found uncertainty sample 69 after 809 steps.
Found uncertainty sample 70 after 843 steps.
Found uncertainty sample 71 after 741 steps.
Found uncertainty sample 72 after 1536 steps.
Found uncertainty sample 73 after 212 steps.
Found uncertainty sample 74 after 7 steps.
Found uncertainty sample 75 after 837 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1093 steps.
Found uncertainty sample 78 after 546 steps.
Found uncertainty sample 79 after 2067 steps.
Found uncertainty sample 80 after 236 steps.
Found uncertainty sample 81 after 116 steps.
Found uncertainty sample 82 after 111 steps.
Found uncertainty sample 83 after 2091 steps.
Found uncertainty sample 84 after 825 steps.
Found uncertainty sample 85 after 1068 steps.
Found uncertainty sample 86 after 856 steps.
Found uncertainty sample 87 after 1661 steps.
Found uncertainty sample 88 after 526 steps.
Found uncertainty sample 89 after 2683 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 1086 steps.
Found uncertainty sample 92 after 1613 steps.
Did not find any uncertainty samples for sample 93.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 2129 steps.
Found uncertainty sample 96 after 1606 steps.
Found uncertainty sample 97 after 764 steps.
Found uncertainty sample 98 after 1013 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_104413-yrekh8cp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_61
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/yrekh8cp
Training model 61. Added 91 samples to the dataset.
Epoch 0, Batch 100/197, Loss: 1.0199458599090576, Variance: 0.09491878002882004

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.3244680087952903, Training Loss Force: 3.9790677966731134, time: 3.115139961242676
Validation Loss Energy: 1.4193641954349139, Validation Loss Force: 3.4544946966523487, time: 0.17699837684631348
Test Loss Energy: 9.36224726581987, Test Loss Force: 11.619361490555917, time: 9.793373107910156

Epoch 1, Batch 100/197, Loss: 0.3417942523956299, Variance: 0.08671625703573227

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6326533463690442, Training Loss Force: 3.3922540768253846, time: 2.9709742069244385
Validation Loss Energy: 1.9819283570164115, Validation Loss Force: 3.4630452475232625, time: 0.17130136489868164
Test Loss Energy: 9.849124643066418, Test Loss Force: 11.579044917066438, time: 9.75740909576416

Epoch 2, Batch 100/197, Loss: 0.30772435665130615, Variance: 0.0887753963470459

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6715621697566871, Training Loss Force: 3.373029653536651, time: 3.2293975353240967
Validation Loss Energy: 1.8818975275782548, Validation Loss Force: 3.4404217839605664, time: 0.17289280891418457
Test Loss Energy: 9.748909892119695, Test Loss Force: 11.59758545067824, time: 9.798861980438232

Epoch 3, Batch 100/197, Loss: 0.5278396606445312, Variance: 0.09190618991851807

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6461701643445763, Training Loss Force: 3.3772580954058102, time: 2.894815683364868
Validation Loss Energy: 1.5323845130016975, Validation Loss Force: 3.428307023738074, time: 0.1752309799194336
Test Loss Energy: 9.54487260446452, Test Loss Force: 11.727729271114981, time: 9.766635417938232

Epoch 4, Batch 100/197, Loss: 0.7809768915176392, Variance: 0.08605640381574631

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.666574495330803, Training Loss Force: 3.4064224162446495, time: 2.9800937175750732
Validation Loss Energy: 1.6627364505436295, Validation Loss Force: 3.4566161769401758, time: 0.18117618560791016
Test Loss Energy: 9.315483112520274, Test Loss Force: 11.544541230051465, time: 9.94845724105835

Epoch 5, Batch 100/197, Loss: 0.6015138030052185, Variance: 0.08913315832614899

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6493517616983133, Training Loss Force: 3.4032695239956325, time: 3.0525898933410645
Validation Loss Energy: 2.0303793818171303, Validation Loss Force: 3.4384628521492666, time: 0.18767714500427246
Test Loss Energy: 9.953646761930704, Test Loss Force: 11.73544368214728, time: 9.735781192779541

Epoch 6, Batch 100/197, Loss: 0.21380466222763062, Variance: 0.08605218678712845

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6388484823863543, Training Loss Force: 3.3878990322877116, time: 2.993844509124756
Validation Loss Energy: 2.0199205563656157, Validation Loss Force: 3.574991548298306, time: 0.17092180252075195
Test Loss Energy: 9.641606493394306, Test Loss Force: 11.572774404693812, time: 10.95367169380188

Epoch 7, Batch 100/197, Loss: 0.8041815757751465, Variance: 0.0885525494813919

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6397370453086293, Training Loss Force: 3.407576798654553, time: 3.0290706157684326
Validation Loss Energy: 1.662391369034728, Validation Loss Force: 3.464448397146326, time: 0.17734742164611816
Test Loss Energy: 9.539704824314175, Test Loss Force: 11.887176229911883, time: 9.765957355499268

Epoch 8, Batch 100/197, Loss: 0.5184593200683594, Variance: 0.09021418541669846

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6638341736873088, Training Loss Force: 3.391960438388419, time: 3.0013022422790527
Validation Loss Energy: 1.6266231891150502, Validation Loss Force: 3.453546682763561, time: 0.1698927879333496
Test Loss Energy: 9.39229687043206, Test Loss Force: 11.627634159950622, time: 9.784811973571777

Epoch 9, Batch 100/197, Loss: 0.6488491296768188, Variance: 0.09006445109844208

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6740887349541667, Training Loss Force: 3.4120044943700094, time: 3.1955392360687256
Validation Loss Energy: 2.1919097036746606, Validation Loss Force: 3.5534356719557616, time: 0.169785737991333
Test Loss Energy: 9.70874436991765, Test Loss Force: 11.658236627570204, time: 9.792575597763062

Epoch 10, Batch 100/197, Loss: 0.875074028968811, Variance: 0.08947983384132385

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6840171899098622, Training Loss Force: 3.387862269272629, time: 3.134723663330078
Validation Loss Energy: 1.843837274971326, Validation Loss Force: 3.4734222420636485, time: 0.17061519622802734
Test Loss Energy: 9.767055911963649, Test Loss Force: 11.722238408526161, time: 9.978571891784668

Epoch 11, Batch 100/197, Loss: 0.6909933686256409, Variance: 0.08901724219322205

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.649430769119459, Training Loss Force: 3.3896675678504904, time: 3.088343381881714
Validation Loss Energy: 1.5687317440353603, Validation Loss Force: 3.4279183547438565, time: 0.17112421989440918
Test Loss Energy: 9.564291657624148, Test Loss Force: 11.807618740452359, time: 9.94567084312439

Epoch 12, Batch 100/197, Loss: 0.6156184673309326, Variance: 0.09127369523048401

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6527836702553294, Training Loss Force: 3.3954400597440264, time: 2.9481937885284424
Validation Loss Energy: 1.2921353596664784, Validation Loss Force: 3.5289461852829045, time: 0.17441248893737793
Test Loss Energy: 9.52238552021612, Test Loss Force: 11.771095180811427, time: 9.801730394363403

Epoch 13, Batch 100/197, Loss: 0.6086033582687378, Variance: 0.08515669405460358

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6610791912020655, Training Loss Force: 3.4076235851721033, time: 3.0741348266601562
Validation Loss Energy: 2.0159198041534445, Validation Loss Force: 3.5160187383450614, time: 0.17088913917541504
Test Loss Energy: 10.010106041761471, Test Loss Force: 11.832323143071383, time: 10.007756471633911

Epoch 14, Batch 100/197, Loss: 0.9221025109291077, Variance: 0.08977068960666656

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6577227075796066, Training Loss Force: 3.3980962053859827, time: 2.995619058609009
Validation Loss Energy: 1.815174133494485, Validation Loss Force: 3.511189100237174, time: 0.17415237426757812
Test Loss Energy: 9.721549976903779, Test Loss Force: 11.87803643789904, time: 9.776334285736084

Epoch 15, Batch 100/197, Loss: 0.8269261121749878, Variance: 0.09082713723182678

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6552902003160268, Training Loss Force: 3.394623730918751, time: 2.99635648727417
Validation Loss Energy: 1.4100029897046071, Validation Loss Force: 3.490383841585552, time: 0.17151594161987305
Test Loss Energy: 9.44562576714997, Test Loss Force: 11.847439580142966, time: 9.842667818069458

Epoch 16, Batch 100/197, Loss: 0.6259635090827942, Variance: 0.09235794842243195

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.670936347949355, Training Loss Force: 3.4214955033552275, time: 3.277970790863037
Validation Loss Energy: 1.4065279315123282, Validation Loss Force: 3.452522404692293, time: 0.17709112167358398
Test Loss Energy: 9.224209570014052, Test Loss Force: 11.759300724909219, time: 9.803580522537231

Epoch 17, Batch 100/197, Loss: 0.7435011863708496, Variance: 0.0933637022972107

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6491863513015956, Training Loss Force: 3.405198003823202, time: 2.969111442565918
Validation Loss Energy: 1.6434040369577467, Validation Loss Force: 3.429291110028502, time: 0.1764392852783203
Test Loss Energy: 9.451797433074718, Test Loss Force: 11.715505164020534, time: 9.830779314041138

Epoch 18, Batch 100/197, Loss: 0.39501094818115234, Variance: 0.08562357723712921

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6506298710103613, Training Loss Force: 3.390854688276346, time: 3.05233097076416
Validation Loss Energy: 1.8933619825600658, Validation Loss Force: 3.5219008778384624, time: 0.1749119758605957
Test Loss Energy: 9.451528346155417, Test Loss Force: 11.551981098187484, time: 9.969102382659912

Epoch 19, Batch 100/197, Loss: 0.6259576082229614, Variance: 0.09115298092365265

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.680029969369507, Training Loss Force: 3.387116886002087, time: 3.108227252960205
Validation Loss Energy: 1.4493325677778972, Validation Loss Force: 3.508967697170112, time: 0.17013168334960938
Test Loss Energy: 9.396295877722535, Test Loss Force: 11.67441604284378, time: 9.803433895111084

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‡â–†â–„â–‚â–‡â–…â–„â–‚â–…â–†â–„â–„â–ˆâ–…â–ƒâ–â–ƒâ–ƒâ–ƒ
wandb:   test_error_force â–ƒâ–‚â–‚â–…â–â–…â–‚â–ˆâ–ƒâ–ƒâ–…â–†â–†â–‡â–ˆâ–‡â–…â–„â–â–„
wandb:          test_loss â–ƒâ–†â–„â–…â–„â–ˆâ–„â–†â–â–„â–…â–„â–„â–‡â–‡â–…â–â–ƒâ–‚â–‚
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–†â–†â–ƒâ–„â–‡â–‡â–„â–„â–ˆâ–…â–ƒâ–â–‡â–…â–‚â–‚â–„â–†â–‚
wandb:  valid_error_force â–‚â–ƒâ–‚â–â–‚â–‚â–ˆâ–ƒâ–‚â–‡â–ƒâ–â–†â–…â–…â–„â–‚â–â–…â–…
wandb:         valid_loss â–‚â–†â–„â–‚â–ƒâ–†â–‡â–„â–ƒâ–ˆâ–„â–ƒâ–â–†â–„â–â–‚â–‚â–…â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 6279
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.3963
wandb:   test_error_force 11.67442
wandb:          test_loss 13.18825
wandb: train_error_energy 1.68003
wandb:  train_error_force 3.38712
wandb:         train_loss 0.54482
wandb: valid_error_energy 1.44933
wandb:  valid_error_force 3.50897
wandb:         valid_loss 0.48734
wandb: 
wandb: ğŸš€ View run al_63_61 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/yrekh8cp
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_104413-yrekh8cp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.502312660217285, Uncertainty Bias: -0.079350546002388
8.392334e-05 0.00057792664
2.0027013 5.560793
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 1639 steps.
Found uncertainty sample 2 after 2590 steps.
Found uncertainty sample 3 after 42 steps.
Found uncertainty sample 4 after 530 steps.
Found uncertainty sample 5 after 896 steps.
Found uncertainty sample 6 after 103 steps.
Found uncertainty sample 7 after 2366 steps.
Found uncertainty sample 8 after 462 steps.
Found uncertainty sample 9 after 82 steps.
Found uncertainty sample 10 after 1894 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 285 steps.
Found uncertainty sample 13 after 175 steps.
Found uncertainty sample 14 after 838 steps.
Found uncertainty sample 15 after 1475 steps.
Found uncertainty sample 16 after 1621 steps.
Found uncertainty sample 17 after 1286 steps.
Found uncertainty sample 18 after 2021 steps.
Found uncertainty sample 19 after 26 steps.
Found uncertainty sample 20 after 3921 steps.
Found uncertainty sample 21 after 299 steps.
Found uncertainty sample 22 after 16 steps.
Found uncertainty sample 23 after 1085 steps.
Found uncertainty sample 24 after 2392 steps.
Found uncertainty sample 25 after 20 steps.
Found uncertainty sample 26 after 3131 steps.
Found uncertainty sample 27 after 240 steps.
Found uncertainty sample 28 after 2088 steps.
Found uncertainty sample 29 after 1092 steps.
Found uncertainty sample 30 after 1428 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 332 steps.
Found uncertainty sample 33 after 932 steps.
Found uncertainty sample 34 after 1435 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 314 steps.
Found uncertainty sample 37 after 825 steps.
Found uncertainty sample 38 after 98 steps.
Found uncertainty sample 39 after 2618 steps.
Found uncertainty sample 40 after 2168 steps.
Found uncertainty sample 41 after 918 steps.
Found uncertainty sample 42 after 255 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1500 steps.
Found uncertainty sample 45 after 17 steps.
Found uncertainty sample 46 after 1005 steps.
Found uncertainty sample 47 after 119 steps.
Found uncertainty sample 48 after 2219 steps.
Found uncertainty sample 49 after 1794 steps.
Found uncertainty sample 50 after 1894 steps.
Found uncertainty sample 51 after 841 steps.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 53 steps.
Found uncertainty sample 54 after 624 steps.
Found uncertainty sample 55 after 919 steps.
Found uncertainty sample 56 after 3184 steps.
Found uncertainty sample 57 after 80 steps.
Found uncertainty sample 58 after 1402 steps.
Found uncertainty sample 59 after 3406 steps.
Found uncertainty sample 60 after 278 steps.
Found uncertainty sample 61 after 1172 steps.
Found uncertainty sample 62 after 2515 steps.
Found uncertainty sample 63 after 115 steps.
Found uncertainty sample 64 after 2432 steps.
Found uncertainty sample 65 after 1453 steps.
Found uncertainty sample 66 after 212 steps.
Found uncertainty sample 67 after 654 steps.
Found uncertainty sample 68 after 407 steps.
Found uncertainty sample 69 after 398 steps.
Found uncertainty sample 70 after 1978 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 1961 steps.
Found uncertainty sample 73 after 473 steps.
Found uncertainty sample 74 after 711 steps.
Found uncertainty sample 75 after 84 steps.
Found uncertainty sample 76 after 1413 steps.
Found uncertainty sample 77 after 665 steps.
Found uncertainty sample 78 after 346 steps.
Found uncertainty sample 79 after 73 steps.
Found uncertainty sample 80 after 289 steps.
Found uncertainty sample 81 after 525 steps.
Found uncertainty sample 82 after 1132 steps.
Found uncertainty sample 83 after 36 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 370 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 340 steps.
Found uncertainty sample 88 after 166 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1766 steps.
Found uncertainty sample 91 after 822 steps.
Found uncertainty sample 92 after 32 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 327 steps.
Found uncertainty sample 95 after 11 steps.
Found uncertainty sample 96 after 3127 steps.
Found uncertainty sample 97 after 260 steps.
Found uncertainty sample 98 after 3528 steps.
Found uncertainty sample 99 after 253 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_110153-0enfkf46
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_62
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/0enfkf46
Training model 62. Added 94 samples to the dataset.
Epoch 0, Batch 100/199, Loss: 2.360090732574463, Variance: 0.15429720282554626

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.370541556659916, Training Loss Force: 4.0033243462772505, time: 3.0609261989593506
Validation Loss Energy: 6.340822681176155, Validation Loss Force: 3.572286229509857, time: 0.17306113243103027
Test Loss Energy: 10.89453434737199, Test Loss Force: 10.939138735863246, time: 9.92708134651184

Epoch 1, Batch 100/199, Loss: 1.304099678993225, Variance: 0.16641409695148468

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.268810336949922, Training Loss Force: 3.4961806646683793, time: 3.0561678409576416
Validation Loss Energy: 4.128517764082098, Validation Loss Force: 3.5011873588749625, time: 0.1713237762451172
Test Loss Energy: 10.585418482496204, Test Loss Force: 10.99978286661742, time: 9.902889966964722

Epoch 2, Batch 100/199, Loss: 0.9206953048706055, Variance: 0.17406728863716125

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.270193803291852, Training Loss Force: 3.4616773263410177, time: 3.3053884506225586
Validation Loss Energy: 2.140521116166289, Validation Loss Force: 3.5322672399271307, time: 0.18530058860778809
Test Loss Energy: 9.548410376518248, Test Loss Force: 11.170338073299131, time: 9.941561937332153

Epoch 3, Batch 100/199, Loss: 1.8819924592971802, Variance: 0.18207745254039764

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.2341780312268025, Training Loss Force: 3.4724301693358894, time: 2.9612605571746826
Validation Loss Energy: 5.483045797779553, Validation Loss Force: 3.551534133116825, time: 0.18025422096252441
Test Loss Energy: 10.418096347875254, Test Loss Force: 11.089363596669806, time: 10.85804533958435

Epoch 4, Batch 100/199, Loss: 1.76047682762146, Variance: 0.18340903520584106

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.2461688536907785, Training Loss Force: 3.4420246571775936, time: 3.068854331970215
Validation Loss Energy: 5.698626048777583, Validation Loss Force: 3.475342318108984, time: 0.18023014068603516
Test Loss Energy: 10.777909192806673, Test Loss Force: 11.223359901104157, time: 10.111450910568237

Epoch 5, Batch 100/199, Loss: 1.272907018661499, Variance: 0.18742111325263977

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.265705177106515, Training Loss Force: 3.4466393241699884, time: 3.001091480255127
Validation Loss Energy: 3.174453397926818, Validation Loss Force: 3.4729945503016078, time: 0.17184948921203613
Test Loss Energy: 9.934672203018634, Test Loss Force: 11.217903084744332, time: 9.92128849029541

Epoch 6, Batch 100/199, Loss: 0.8383089303970337, Variance: 0.171565979719162

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.242052633819423, Training Loss Force: 3.453724058879221, time: 2.992856502532959
Validation Loss Energy: 2.1521199519776673, Validation Loss Force: 3.5842489067056813, time: 0.1788034439086914
Test Loss Energy: 9.417251007573407, Test Loss Force: 11.100018653386494, time: 10.059105157852173

Epoch 7, Batch 100/199, Loss: 1.6175981760025024, Variance: 0.17583464086055756

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.268238014024754, Training Loss Force: 3.421858128048151, time: 3.0914933681488037
Validation Loss Energy: 5.690997292503524, Validation Loss Force: 3.552244392025048, time: 0.17815923690795898
Test Loss Energy: 10.728379013065386, Test Loss Force: 10.84497216389565, time: 9.957385778427124

Epoch 8, Batch 100/199, Loss: 2.0403084754943848, Variance: 0.16959601640701294

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.232555650961758, Training Loss Force: 3.4513550549811245, time: 3.0762505531311035
Validation Loss Energy: 5.996102675632979, Validation Loss Force: 3.467668972338453, time: 0.17564654350280762
Test Loss Energy: 11.952904030169591, Test Loss Force: 11.044479680291339, time: 10.111530780792236

Epoch 9, Batch 100/199, Loss: 1.2624831199645996, Variance: 0.17642901837825775

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.234866515274488, Training Loss Force: 3.4391418078597056, time: 3.08962345123291
Validation Loss Energy: 3.7515152414295976, Validation Loss Force: 3.530358186004519, time: 0.17244887351989746
Test Loss Energy: 10.210408540635246, Test Loss Force: 11.106405946982607, time: 9.887207508087158

Epoch 10, Batch 100/199, Loss: 0.8847097754478455, Variance: 0.17835094034671783

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.210362526574164, Training Loss Force: 3.4615599875336964, time: 3.107120990753174
Validation Loss Energy: 1.9055646414279142, Validation Loss Force: 3.4494517971699494, time: 0.1760718822479248
Test Loss Energy: 9.397164963559742, Test Loss Force: 11.161988857013743, time: 9.90577483177185

Epoch 11, Batch 100/199, Loss: 1.8168823719024658, Variance: 0.1855740249156952

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.2425569839107125, Training Loss Force: 3.4985245729704713, time: 3.2381491661071777
Validation Loss Energy: 5.379664521369907, Validation Loss Force: 3.521983655093462, time: 0.17543244361877441
Test Loss Energy: 10.6273961678143, Test Loss Force: 11.29558572797451, time: 9.962781190872192

Epoch 12, Batch 100/199, Loss: 1.9032647609710693, Variance: 0.1901385486125946

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.249172359474368, Training Loss Force: 3.41453921365277, time: 3.1047677993774414
Validation Loss Energy: 5.640428684125032, Validation Loss Force: 3.528749679761623, time: 0.1716902256011963
Test Loss Energy: 10.897871852428, Test Loss Force: 11.248885204423527, time: 9.990279912948608

Epoch 13, Batch 100/199, Loss: 1.253105640411377, Variance: 0.18087667226791382

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.20619385916742, Training Loss Force: 3.451129434070338, time: 3.12127685546875
Validation Loss Energy: 3.2368618814671555, Validation Loss Force: 3.575673305555851, time: 0.17327880859375
Test Loss Energy: 9.855131552476093, Test Loss Force: 11.217304717956289, time: 10.197205543518066

Epoch 14, Batch 100/199, Loss: 0.8310058116912842, Variance: 0.1805604100227356

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.209854068852551, Training Loss Force: 3.430526485723221, time: 3.00213623046875
Validation Loss Energy: 1.8795751718871578, Validation Loss Force: 3.625615378624989, time: 0.17366719245910645
Test Loss Energy: 9.48104826400431, Test Loss Force: 11.200707351530001, time: 9.976138830184937

Epoch 15, Batch 100/199, Loss: 1.5761746168136597, Variance: 0.1753084361553192

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.1885397155224755, Training Loss Force: 3.4460654509528514, time: 2.9975695610046387
Validation Loss Energy: 5.285614863527804, Validation Loss Force: 3.4405278858086015, time: 0.17939138412475586
Test Loss Energy: 11.028190124231214, Test Loss Force: 11.1027521724645, time: 10.119725704193115

Epoch 16, Batch 100/199, Loss: 2.301652193069458, Variance: 0.1755891889333725

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.203985573718775, Training Loss Force: 3.4241080229751635, time: 3.1033928394317627
Validation Loss Energy: 5.913564121958759, Validation Loss Force: 3.4558432923346603, time: 0.19185996055603027
Test Loss Energy: 11.452268926658613, Test Loss Force: 11.079499325525836, time: 10.751179456710815

Epoch 17, Batch 100/199, Loss: 1.1741483211517334, Variance: 0.17579078674316406

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.217080911793972, Training Loss Force: 3.4197661715630883, time: 3.251453161239624
Validation Loss Energy: 3.891357978144767, Validation Loss Force: 3.571917211067768, time: 0.192673921585083
Test Loss Energy: 10.246441398100522, Test Loss Force: 11.347290184490253, time: 10.68314504623413

Epoch 18, Batch 100/199, Loss: 0.9611213207244873, Variance: 0.18342873454093933

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.224445091805581, Training Loss Force: 3.432947914218428, time: 3.1497936248779297
Validation Loss Energy: 1.8703952188416362, Validation Loss Force: 3.5852071077078946, time: 0.20444083213806152
Test Loss Energy: 9.470395095895038, Test Loss Force: 11.307595774242255, time: 10.534930229187012

Epoch 19, Batch 100/199, Loss: 1.7190015316009521, Variance: 0.1871630847454071

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.226106628272097, Training Loss Force: 3.4215486783580955, time: 3.066805124282837
Validation Loss Energy: 5.09916318818889, Validation Loss Force: 3.4549179906987577, time: 0.17901110649108887
Test Loss Energy: 10.820281154067981, Test Loss Force: 11.547028428090178, time: 10.62775993347168

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–„â–â–„â–…â–‚â–â–…â–ˆâ–ƒâ–â–„â–…â–‚â–â–…â–‡â–ƒâ–â–…
wandb:   test_error_force â–‚â–ƒâ–„â–ƒâ–…â–…â–„â–â–ƒâ–„â–„â–…â–…â–…â–…â–„â–ƒâ–†â–†â–ˆ
wandb:          test_loss â–…â–„â–ƒâ–ƒâ–…â–ƒâ–â–ƒâ–ˆâ–‚â–â–†â–…â–‚â–‚â–†â–‡â–„â–‚â–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–…â–â–‡â–‡â–ƒâ–â–‡â–‡â–„â–â–†â–‡â–ƒâ–â–†â–‡â–„â–â–†
wandb:  valid_error_force â–†â–ƒâ–„â–…â–‚â–‚â–†â–…â–‚â–„â–â–„â–„â–†â–ˆâ–â–‚â–†â–†â–‚
wandb:         valid_loss â–ˆâ–„â–â–†â–†â–‚â–â–†â–†â–ƒâ–â–†â–†â–‚â–â–…â–†â–ƒâ–â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 6363
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.82028
wandb:   test_error_force 11.54703
wandb:          test_loss 9.80428
wandb: train_error_energy 4.22611
wandb:  train_error_force 3.42155
wandb:         train_loss 1.43482
wandb: valid_error_energy 5.09916
wandb:  valid_error_force 3.45492
wandb:         valid_loss 1.68844
wandb: 
wandb: ğŸš€ View run al_63_62 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/0enfkf46
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_110153-0enfkf46/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.21233868598938, Uncertainty Bias: -0.15561628341674805
0.00015258789 0.3413589
2.0313153 4.7329345
(48745, 22, 3)
Found uncertainty sample 0 after 2364 steps.
Found uncertainty sample 1 after 1623 steps.
Found uncertainty sample 2 after 1356 steps.
Found uncertainty sample 3 after 1254 steps.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 1376 steps.
Found uncertainty sample 7 after 52 steps.
Found uncertainty sample 8 after 1914 steps.
Found uncertainty sample 9 after 13 steps.
Found uncertainty sample 10 after 3741 steps.
Found uncertainty sample 11 after 798 steps.
Found uncertainty sample 12 after 5 steps.
Found uncertainty sample 13 after 579 steps.
Found uncertainty sample 14 after 2319 steps.
Found uncertainty sample 15 after 3625 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 544 steps.
Found uncertainty sample 18 after 1452 steps.
Found uncertainty sample 19 after 10 steps.
Found uncertainty sample 20 after 147 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 143 steps.
Found uncertainty sample 23 after 415 steps.
Found uncertainty sample 24 after 489 steps.
Found uncertainty sample 25 after 478 steps.
Found uncertainty sample 26 after 3892 steps.
Found uncertainty sample 27 after 2422 steps.
Found uncertainty sample 28 after 3113 steps.
Found uncertainty sample 29 after 539 steps.
Found uncertainty sample 30 after 425 steps.
Found uncertainty sample 31 after 2466 steps.
Found uncertainty sample 32 after 1584 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 3815 steps.
Found uncertainty sample 35 after 2891 steps.
Found uncertainty sample 36 after 1174 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 253 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 877 steps.
Found uncertainty sample 41 after 341 steps.
Found uncertainty sample 42 after 3956 steps.
Found uncertainty sample 43 after 2923 steps.
Found uncertainty sample 44 after 2172 steps.
Found uncertainty sample 45 after 2181 steps.
Found uncertainty sample 46 after 74 steps.
Found uncertainty sample 47 after 7 steps.
Found uncertainty sample 48 after 2029 steps.
Found uncertainty sample 49 after 433 steps.
Found uncertainty sample 50 after 120 steps.
Found uncertainty sample 51 after 103 steps.
Found uncertainty sample 52 after 411 steps.
Found uncertainty sample 53 after 2211 steps.
Found uncertainty sample 54 after 3813 steps.
Found uncertainty sample 55 after 148 steps.
Found uncertainty sample 56 after 1427 steps.
Found uncertainty sample 57 after 825 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 2886 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 828 steps.
Found uncertainty sample 62 after 1439 steps.
Found uncertainty sample 63 after 1457 steps.
Found uncertainty sample 64 after 1107 steps.
Found uncertainty sample 65 after 2969 steps.
Found uncertainty sample 66 after 452 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 99 steps.
Found uncertainty sample 69 after 190 steps.
Found uncertainty sample 70 after 1603 steps.
Found uncertainty sample 71 after 2885 steps.
Found uncertainty sample 72 after 2686 steps.
Found uncertainty sample 73 after 56 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 3648 steps.
Found uncertainty sample 76 after 1335 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 1079 steps.
Found uncertainty sample 79 after 1310 steps.
Found uncertainty sample 80 after 819 steps.
Found uncertainty sample 81 after 3433 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 406 steps.
Found uncertainty sample 84 after 1501 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 849 steps.
Found uncertainty sample 87 after 390 steps.
Found uncertainty sample 88 after 13 steps.
Found uncertainty sample 89 after 1514 steps.
Found uncertainty sample 90 after 1535 steps.
Found uncertainty sample 91 after 746 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 49 steps.
Found uncertainty sample 94 after 20 steps.
Found uncertainty sample 95 after 631 steps.
Found uncertainty sample 96 after 1026 steps.
Found uncertainty sample 97 after 4 steps.
Found uncertainty sample 98 after 1481 steps.
Found uncertainty sample 99 after 218 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_112343-p2utm863
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_63
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/p2utm863
Training model 63. Added 87 samples to the dataset.
Epoch 0, Batch 100/202, Loss: 0.8885518312454224, Variance: 0.1389007568359375
Epoch 0, Batch 200/202, Loss: 1.552093267440796, Variance: 0.14273597300052643

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.243410235746367, Training Loss Force: 3.573614846699095, time: 3.1463022232055664
Validation Loss Energy: 3.2488540066575293, Validation Loss Force: 3.442705144717483, time: 0.1763138771057129
Test Loss Energy: 10.267019687818378, Test Loss Force: 11.866027422204724, time: 9.908739805221558

Epoch 1, Batch 100/202, Loss: 1.3904993534088135, Variance: 0.14086000621318817
Epoch 1, Batch 200/202, Loss: 0.6870076060295105, Variance: 0.13590951263904572

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.661870293876575, Training Loss Force: 3.3705760671697305, time: 3.057227373123169
Validation Loss Energy: 2.3840654945248074, Validation Loss Force: 3.433214707381609, time: 0.17815065383911133
Test Loss Energy: 9.948233784579063, Test Loss Force: 11.538665520566829, time: 9.902671575546265

Epoch 2, Batch 100/202, Loss: 0.7954138517379761, Variance: 0.13871103525161743
Epoch 2, Batch 200/202, Loss: 0.703679084777832, Variance: 0.1320399045944214

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6943223525963957, Training Loss Force: 3.370273170991918, time: 3.2870876789093018
Validation Loss Energy: 3.1222144369501477, Validation Loss Force: 3.5056407974608046, time: 0.17746520042419434
Test Loss Energy: 10.178157952940769, Test Loss Force: 11.368639641579849, time: 10.88971471786499

Epoch 3, Batch 100/202, Loss: 1.0016084909439087, Variance: 0.1401752084493637
Epoch 3, Batch 200/202, Loss: 1.3394098281860352, Variance: 0.1398475170135498

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.692264616312042, Training Loss Force: 3.3771475341204766, time: 3.1166372299194336
Validation Loss Energy: 3.318692625593758, Validation Loss Force: 3.474892303765064, time: 0.18777871131896973
Test Loss Energy: 9.975319068297365, Test Loss Force: 11.524290424859494, time: 9.992623329162598

Epoch 4, Batch 100/202, Loss: 1.501896858215332, Variance: 0.14376303553581238
Epoch 4, Batch 200/202, Loss: 0.49590641260147095, Variance: 0.13304123282432556

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.694021622482305, Training Loss Force: 3.3867828237985504, time: 3.1486728191375732
Validation Loss Energy: 2.110127000416292, Validation Loss Force: 3.483095156764611, time: 0.178253173828125
Test Loss Energy: 9.70878166175346, Test Loss Force: 11.447501978950307, time: 10.089715957641602

Epoch 5, Batch 100/202, Loss: 0.6533241271972656, Variance: 0.1356913447380066
Epoch 5, Batch 200/202, Loss: 0.8752427101135254, Variance: 0.13645759224891663

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.709316925952849, Training Loss Force: 3.3970868682890667, time: 3.1101531982421875
Validation Loss Energy: 2.810813417723304, Validation Loss Force: 3.4796963905884555, time: 0.17493677139282227
Test Loss Energy: 10.021972208786737, Test Loss Force: 11.339560240244861, time: 9.923078775405884

Epoch 6, Batch 100/202, Loss: 0.7872323393821716, Variance: 0.131865993142128
Epoch 6, Batch 200/202, Loss: 1.2515159845352173, Variance: 0.14200928807258606

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.719475497493595, Training Loss Force: 3.3795487011857808, time: 3.0813827514648438
Validation Loss Energy: 3.5423702400834527, Validation Loss Force: 3.5224151878982695, time: 0.1760265827178955
Test Loss Energy: 10.044897547883377, Test Loss Force: 11.680484297020254, time: 10.09714126586914

Epoch 7, Batch 100/202, Loss: 1.320319414138794, Variance: 0.14120076596736908
Epoch 7, Batch 200/202, Loss: 0.7660796642303467, Variance: 0.13526368141174316

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.710736723952699, Training Loss Force: 3.3774632381834695, time: 3.046250343322754
Validation Loss Energy: 2.363593905588358, Validation Loss Force: 3.5079928516492402, time: 0.1839001178741455
Test Loss Energy: 9.895648150475422, Test Loss Force: 11.467772281099323, time: 9.968081712722778

Epoch 8, Batch 100/202, Loss: 0.6108595728874207, Variance: 0.13151773810386658
Epoch 8, Batch 200/202, Loss: 0.9354525804519653, Variance: 0.13666582107543945

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6845108224939644, Training Loss Force: 3.373687846437621, time: 3.273432493209839
Validation Loss Energy: 2.714574755174871, Validation Loss Force: 3.481715777072875, time: 0.17529296875
Test Loss Energy: 9.671881602395166, Test Loss Force: 11.171205102652708, time: 10.093822717666626

Epoch 9, Batch 100/202, Loss: 0.7755923867225647, Variance: 0.13081666827201843
Epoch 9, Batch 200/202, Loss: 1.4691864252090454, Variance: 0.13721296191215515

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6946387246668264, Training Loss Force: 3.3785759269288955, time: 3.1600232124328613
Validation Loss Energy: 3.2730288061599038, Validation Loss Force: 3.4626367053077716, time: 0.17430615425109863
Test Loss Energy: 9.815580778982707, Test Loss Force: 11.418792063837493, time: 9.944899320602417

Epoch 10, Batch 100/202, Loss: 1.4208958148956299, Variance: 0.13937735557556152
Epoch 10, Batch 200/202, Loss: 0.5963117480278015, Variance: 0.1342524290084839

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.680141144985957, Training Loss Force: 3.3701238536924136, time: 3.113654851913452
Validation Loss Energy: 2.19567581827431, Validation Loss Force: 3.469698490269012, time: 0.17987871170043945
Test Loss Energy: 10.086012960340652, Test Loss Force: 11.447528587330341, time: 9.94116473197937

Epoch 11, Batch 100/202, Loss: 0.5591940879821777, Variance: 0.13011008501052856
Epoch 11, Batch 200/202, Loss: 0.829561710357666, Variance: 0.13192914426326752

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6851144421719324, Training Loss Force: 3.3702272184736697, time: 3.265155076980591
Validation Loss Energy: 2.7565257336022397, Validation Loss Force: 3.533221432884718, time: 0.1763160228729248
Test Loss Energy: 10.051551806629966, Test Loss Force: 11.363066930009888, time: 9.971203088760376

Epoch 12, Batch 100/202, Loss: 0.6753175258636475, Variance: 0.1332489252090454
Epoch 12, Batch 200/202, Loss: 1.4248815774917603, Variance: 0.13920870423316956

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.701521749845626, Training Loss Force: 3.37009177169824, time: 3.101598024368286
Validation Loss Energy: 3.3277947836064214, Validation Loss Force: 3.4901831610177845, time: 0.17607736587524414
Test Loss Energy: 9.953227057404195, Test Loss Force: 11.439936879821325, time: 9.95461130142212

Epoch 13, Batch 100/202, Loss: 1.2773715257644653, Variance: 0.14558589458465576
Epoch 13, Batch 200/202, Loss: 0.7492254972457886, Variance: 0.13518287241458893

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.7127753724205634, Training Loss Force: 3.371159897954095, time: 3.1668970584869385
Validation Loss Energy: 2.337602738195364, Validation Loss Force: 3.47373726323785, time: 0.17857766151428223
Test Loss Energy: 9.885182262273366, Test Loss Force: 11.509464054570826, time: 10.138208627700806

Epoch 14, Batch 100/202, Loss: 0.681678831577301, Variance: 0.13231676816940308
Epoch 14, Batch 200/202, Loss: 0.8310534358024597, Variance: 0.13220256567001343

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.686616311389218, Training Loss Force: 3.3693811352445593, time: 3.059915542602539
Validation Loss Energy: 2.899642430437271, Validation Loss Force: 3.43375156269244, time: 0.17702603340148926
Test Loss Energy: 9.99137121474726, Test Loss Force: 11.150449484356587, time: 9.972082614898682

Epoch 15, Batch 100/202, Loss: 0.6094409823417664, Variance: 0.13549508154392242
Epoch 15, Batch 200/202, Loss: 1.3458466529846191, Variance: 0.1388476938009262

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6883750107268476, Training Loss Force: 3.388270442190497, time: 3.024669647216797
Validation Loss Energy: 3.2970446755593725, Validation Loss Force: 3.5070931012978326, time: 0.17638349533081055
Test Loss Energy: 10.02223653921329, Test Loss Force: 11.436203732496248, time: 10.207395315170288

Epoch 16, Batch 100/202, Loss: 1.5567175149917603, Variance: 0.14282551407814026
Epoch 16, Batch 200/202, Loss: 0.6860405206680298, Variance: 0.13985759019851685

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.704321173355132, Training Loss Force: 3.3870090313997188, time: 3.0917022228240967
Validation Loss Energy: 2.353182675666785, Validation Loss Force: 3.3995161258387565, time: 0.17783904075622559
Test Loss Energy: 9.785977473287376, Test Loss Force: 11.200063000648807, time: 9.964512348175049

Epoch 17, Batch 100/202, Loss: 0.8502869009971619, Variance: 0.13981983065605164
Epoch 17, Batch 200/202, Loss: 0.6282312870025635, Variance: 0.1341419816017151

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.69635610675234, Training Loss Force: 3.3646941095595424, time: 3.1128809452056885
Validation Loss Energy: 2.8720919110153447, Validation Loss Force: 3.431544354783431, time: 0.17691469192504883
Test Loss Energy: 9.963780763519468, Test Loss Force: 11.261847938883415, time: 9.995006561279297

Epoch 18, Batch 100/202, Loss: 0.9473944902420044, Variance: 0.13496699929237366
Epoch 18, Batch 200/202, Loss: 1.3101186752319336, Variance: 0.1410789042711258

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.697129960299337, Training Loss Force: 3.375222667381537, time: 3.259159564971924
Validation Loss Energy: 3.5238152201532906, Validation Loss Force: 3.429540443804654, time: 0.1788346767425537
Test Loss Energy: 10.142702673592275, Test Loss Force: 11.407530493673123, time: 9.99640679359436

Epoch 19, Batch 100/202, Loss: 1.4752280712127686, Variance: 0.13818922638893127
Epoch 19, Batch 200/202, Loss: 0.5000984072685242, Variance: 0.13910195231437683

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.706982822535239, Training Loss Force: 3.3696617545868754, time: 3.067023277282715
Validation Loss Energy: 2.3961543137917998, Validation Loss Force: 3.4229693103064656, time: 0.1795511245727539
Test Loss Energy: 9.482424149480874, Test Loss Force: 11.24158763469563, time: 9.943283319473267

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–…â–‡â–…â–ƒâ–†â–†â–…â–ƒâ–„â–†â–†â–…â–…â–†â–†â–„â–…â–‡â–
wandb:   test_error_force â–ˆâ–…â–ƒâ–…â–„â–ƒâ–†â–„â–â–„â–„â–ƒâ–„â–…â–â–„â–â–‚â–„â–‚
wandb:          test_loss â–ˆâ–†â–†â–†â–…â–…â–†â–„â–ƒâ–„â–…â–…â–…â–…â–„â–…â–‚â–ƒâ–„â–
wandb: train_error_energy â–ˆâ–â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–‚â–†â–‡â–â–„â–ˆâ–‚â–„â–‡â–â–„â–‡â–‚â–…â–‡â–‚â–…â–ˆâ–‚
wandb:  valid_error_force â–ƒâ–ƒâ–‡â–…â–…â–…â–‡â–‡â–…â–„â–…â–ˆâ–†â–…â–ƒâ–‡â–â–ƒâ–ƒâ–‚
wandb:         valid_loss â–†â–‚â–†â–‡â–â–„â–ˆâ–‚â–ƒâ–†â–â–„â–‡â–‚â–„â–‡â–â–„â–‡â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 6441
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.48242
wandb:   test_error_force 11.24159
wandb:          test_loss 9.64885
wandb: train_error_energy 2.70698
wandb:  train_error_force 3.36966
wandb:         train_loss 0.97826
wandb: valid_error_energy 2.39615
wandb:  valid_error_force 3.42297
wandb:         valid_loss 0.87189
wandb: 
wandb: ğŸš€ View run al_63_63 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/p2utm863
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_112343-p2utm863/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.704732656478882, Uncertainty Bias: -0.1540541797876358
5.340576e-05 0.019560814
1.9605517 4.9621954
(48745, 22, 3)
Found uncertainty sample 0 after 551 steps.
Found uncertainty sample 1 after 5 steps.
Found uncertainty sample 2 after 445 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 1494 steps.
Found uncertainty sample 5 after 2189 steps.
Found uncertainty sample 6 after 1643 steps.
Found uncertainty sample 7 after 1351 steps.
Found uncertainty sample 8 after 998 steps.
Found uncertainty sample 9 after 405 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 51 steps.
Found uncertainty sample 12 after 791 steps.
Found uncertainty sample 13 after 739 steps.
Did not find any uncertainty samples for sample 14.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1072 steps.
Found uncertainty sample 17 after 3655 steps.
Found uncertainty sample 18 after 2322 steps.
Found uncertainty sample 19 after 732 steps.
Found uncertainty sample 20 after 2337 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 516 steps.
Found uncertainty sample 23 after 255 steps.
Found uncertainty sample 24 after 948 steps.
Found uncertainty sample 25 after 322 steps.
Found uncertainty sample 26 after 1393 steps.
Found uncertainty sample 27 after 868 steps.
Found uncertainty sample 28 after 1182 steps.
Found uncertainty sample 29 after 121 steps.
Found uncertainty sample 30 after 153 steps.
Found uncertainty sample 31 after 1817 steps.
Found uncertainty sample 32 after 306 steps.
Found uncertainty sample 33 after 371 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 433 steps.
Found uncertainty sample 36 after 3805 steps.
Found uncertainty sample 37 after 1558 steps.
Found uncertainty sample 38 after 1094 steps.
Found uncertainty sample 39 after 260 steps.
Found uncertainty sample 40 after 292 steps.
Found uncertainty sample 41 after 43 steps.
Found uncertainty sample 42 after 1404 steps.
Found uncertainty sample 43 after 1836 steps.
Found uncertainty sample 44 after 3132 steps.
Found uncertainty sample 45 after 2047 steps.
Found uncertainty sample 46 after 620 steps.
Found uncertainty sample 47 after 2353 steps.
Found uncertainty sample 48 after 12 steps.
Found uncertainty sample 49 after 2420 steps.
Found uncertainty sample 50 after 3392 steps.
Found uncertainty sample 51 after 1566 steps.
Found uncertainty sample 52 after 376 steps.
Found uncertainty sample 53 after 318 steps.
Found uncertainty sample 54 after 288 steps.
Found uncertainty sample 55 after 1773 steps.
Found uncertainty sample 56 after 1245 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 2244 steps.
Found uncertainty sample 59 after 2619 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 68 steps.
Found uncertainty sample 62 after 1269 steps.
Found uncertainty sample 63 after 657 steps.
Found uncertainty sample 64 after 485 steps.
Found uncertainty sample 65 after 1276 steps.
Found uncertainty sample 66 after 142 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 3454 steps.
Found uncertainty sample 69 after 2749 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 615 steps.
Found uncertainty sample 72 after 391 steps.
Found uncertainty sample 73 after 234 steps.
Found uncertainty sample 74 after 3926 steps.
Found uncertainty sample 75 after 206 steps.
Found uncertainty sample 76 after 391 steps.
Found uncertainty sample 77 after 2833 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 20 steps.
Found uncertainty sample 80 after 979 steps.
Found uncertainty sample 81 after 2965 steps.
Found uncertainty sample 82 after 66 steps.
Found uncertainty sample 83 after 60 steps.
Found uncertainty sample 84 after 16 steps.
Found uncertainty sample 85 after 2728 steps.
Found uncertainty sample 86 after 796 steps.
Found uncertainty sample 87 after 586 steps.
Found uncertainty sample 88 after 187 steps.
Found uncertainty sample 89 after 1910 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 2549 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 58 steps.
Found uncertainty sample 94 after 900 steps.
Found uncertainty sample 95 after 988 steps.
Found uncertainty sample 96 after 763 steps.
Found uncertainty sample 97 after 1910 steps.
Found uncertainty sample 98 after 929 steps.
Found uncertainty sample 99 after 304 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_114253-jbdfhyaw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_64
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jbdfhyaw
Training model 64. Added 92 samples to the dataset.
Epoch 0, Batch 100/204, Loss: 1.2005159854888916, Variance: 0.13957320153713226
Epoch 0, Batch 200/204, Loss: 0.5976810455322266, Variance: 0.1387518048286438

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.992533926018057, Training Loss Force: 3.4744466271034273, time: 3.1016757488250732
Validation Loss Energy: 3.5678587875468013, Validation Loss Force: 3.423780327003221, time: 0.1762404441833496
Test Loss Energy: 9.98026235131633, Test Loss Force: 11.337454895309467, time: 9.801140546798706

Epoch 1, Batch 100/204, Loss: 1.4233413934707642, Variance: 0.14354676008224487
Epoch 1, Batch 200/204, Loss: 0.8155595660209656, Variance: 0.1362743079662323

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.684914191817944, Training Loss Force: 3.364974118323262, time: 3.1276538372039795
Validation Loss Energy: 3.340090017171689, Validation Loss Force: 3.5308245665306646, time: 0.17438697814941406
Test Loss Energy: 9.70984258813505, Test Loss Force: 11.290336201434044, time: 9.810253620147705

Epoch 2, Batch 100/204, Loss: 1.4776506423950195, Variance: 0.14025115966796875
Epoch 2, Batch 200/204, Loss: 0.8835911750793457, Variance: 0.13309845328330994

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.666514285507016, Training Loss Force: 3.3970198737310553, time: 3.4026966094970703
Validation Loss Energy: 3.419413729384291, Validation Loss Force: 3.442989994222411, time: 0.1798534393310547
Test Loss Energy: 9.98264335832277, Test Loss Force: 11.456339677714741, time: 10.813475370407104

Epoch 3, Batch 100/204, Loss: 1.1412397623062134, Variance: 0.1448104828596115
Epoch 3, Batch 200/204, Loss: 0.6557624340057373, Variance: 0.1338152140378952

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6792760305802803, Training Loss Force: 3.3720311180808964, time: 3.104156255722046
Validation Loss Energy: 3.1086169785522078, Validation Loss Force: 3.4790558271229077, time: 0.17466115951538086
Test Loss Energy: 9.747544741394414, Test Loss Force: 11.245686281182772, time: 9.881179332733154

Epoch 4, Batch 100/204, Loss: 1.2317168712615967, Variance: 0.13978925347328186
Epoch 4, Batch 200/204, Loss: 0.3993111252784729, Variance: 0.13361549377441406

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6741726027474706, Training Loss Force: 3.377944381045018, time: 3.0934760570526123
Validation Loss Energy: 3.550568305696363, Validation Loss Force: 3.495240032989091, time: 0.17605280876159668
Test Loss Energy: 9.870140101100345, Test Loss Force: 11.479929130453629, time: 10.035663604736328

Epoch 5, Batch 100/204, Loss: 1.4038065671920776, Variance: 0.13859713077545166
Epoch 5, Batch 200/204, Loss: 0.7750660181045532, Variance: 0.14081671833992004

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6928264855953943, Training Loss Force: 3.371425919714996, time: 3.0544626712799072
Validation Loss Energy: 2.9574232367850786, Validation Loss Force: 3.48601267924913, time: 0.18590164184570312
Test Loss Energy: 9.786392962255608, Test Loss Force: 11.432615499650307, time: 9.875816106796265

Epoch 6, Batch 100/204, Loss: 1.3226196765899658, Variance: 0.1430450677871704
Epoch 6, Batch 200/204, Loss: 0.6972514390945435, Variance: 0.1378030627965927

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6837970923667713, Training Loss Force: 3.379214781654069, time: 3.1816930770874023
Validation Loss Energy: 3.2780134219933563, Validation Loss Force: 3.487891171831463, time: 0.17365717887878418
Test Loss Energy: 9.977647035746406, Test Loss Force: 11.49088369232708, time: 10.038211584091187

Epoch 7, Batch 100/204, Loss: 1.4622628688812256, Variance: 0.14220955967903137
Epoch 7, Batch 200/204, Loss: 0.5644869804382324, Variance: 0.1383129060268402

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.681466020925036, Training Loss Force: 3.3809978736055575, time: 3.1938858032226562
Validation Loss Energy: 3.309378876225177, Validation Loss Force: 3.47626567138986, time: 0.17957472801208496
Test Loss Energy: 9.907136358192906, Test Loss Force: 11.342999925956907, time: 9.863890171051025

Epoch 8, Batch 100/204, Loss: 1.5065479278564453, Variance: 0.1405017375946045
Epoch 8, Batch 200/204, Loss: 0.5703129768371582, Variance: 0.13636299967765808

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6970211531627517, Training Loss Force: 3.386946844405169, time: 3.1358678340911865
Validation Loss Energy: 3.3877158301175996, Validation Loss Force: 3.4841415347486127, time: 0.18344593048095703
Test Loss Energy: 9.825551716808226, Test Loss Force: 11.351147424980619, time: 10.07913088798523

Epoch 9, Batch 100/204, Loss: 1.1548758745193481, Variance: 0.14493682980537415
Epoch 9, Batch 200/204, Loss: 0.6001745462417603, Variance: 0.14097672700881958

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7197278824335975, Training Loss Force: 3.370463363566251, time: 3.23490834236145
Validation Loss Energy: 3.200345755886306, Validation Loss Force: 3.4443845154160706, time: 0.17650699615478516
Test Loss Energy: 9.77610925705236, Test Loss Force: 11.42761357653689, time: 9.852883577346802

Epoch 10, Batch 100/204, Loss: 1.2588344812393188, Variance: 0.148354172706604
Epoch 10, Batch 200/204, Loss: 0.7078800201416016, Variance: 0.13798357546329498

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.714886479426743, Training Loss Force: 3.3879291165969665, time: 3.2690041065216064
Validation Loss Energy: 3.4053222794861098, Validation Loss Force: 3.464506903464419, time: 0.17853307723999023
Test Loss Energy: 9.761988918164109, Test Loss Force: 11.40457023911421, time: 9.838889122009277

Epoch 11, Batch 100/204, Loss: 1.305245041847229, Variance: 0.14634153246879578
Epoch 11, Batch 200/204, Loss: 0.7468098402023315, Variance: 0.1397835910320282

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.689185716806413, Training Loss Force: 3.397774540944191, time: 3.285646677017212
Validation Loss Energy: 3.2832226553277644, Validation Loss Force: 3.4673753274022605, time: 0.1863880157470703
Test Loss Energy: 9.645566194334394, Test Loss Force: 11.248109807503825, time: 9.86404037475586

Epoch 12, Batch 100/204, Loss: 1.3069283962249756, Variance: 0.13875548541545868
Epoch 12, Batch 200/204, Loss: 0.8073829412460327, Variance: 0.1360364705324173

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.668840794414292, Training Loss Force: 3.375187211999514, time: 3.0527336597442627
Validation Loss Energy: 3.3987974611702745, Validation Loss Force: 3.4913937574969074, time: 0.1752767562866211
Test Loss Energy: 9.702501953494847, Test Loss Force: 11.104634806284913, time: 9.986863851547241

Epoch 13, Batch 100/204, Loss: 1.1640236377716064, Variance: 0.14694172143936157
Epoch 13, Batch 200/204, Loss: 0.5871785879135132, Variance: 0.14284619688987732

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6941625015624764, Training Loss Force: 3.3680170755769567, time: 3.0575811862945557
Validation Loss Energy: 3.5687000872474033, Validation Loss Force: 3.477367493991197, time: 0.1829226016998291
Test Loss Energy: 9.87262266389024, Test Loss Force: 11.248070706335476, time: 11.296649694442749

Epoch 14, Batch 100/204, Loss: 1.3432044982910156, Variance: 0.1439148187637329
Epoch 14, Batch 200/204, Loss: 0.6946849822998047, Variance: 0.13979238271713257

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6843032689994173, Training Loss Force: 3.367711345382021, time: 3.1117749214172363
Validation Loss Energy: 3.403313247116977, Validation Loss Force: 3.412765775030757, time: 0.18371248245239258
Test Loss Energy: 9.776658389477202, Test Loss Force: 11.348217294875793, time: 11.177663087844849

Epoch 15, Batch 100/204, Loss: 1.5036869049072266, Variance: 0.14433060586452484
Epoch 15, Batch 200/204, Loss: 0.9637545347213745, Variance: 0.13891656696796417

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.7032719050545055, Training Loss Force: 3.37243654620289, time: 3.150418519973755
Validation Loss Energy: 3.4792877927056782, Validation Loss Force: 3.4638182991306374, time: 0.18648672103881836
Test Loss Energy: 10.08790908954344, Test Loss Force: 11.524149357543614, time: 11.34843897819519

Epoch 16, Batch 100/204, Loss: 1.701444149017334, Variance: 0.14511288702487946
Epoch 16, Batch 200/204, Loss: 0.825602650642395, Variance: 0.14125481247901917

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6853963998634547, Training Loss Force: 3.368017740309848, time: 3.1010024547576904
Validation Loss Energy: 3.2506563463919553, Validation Loss Force: 3.4782256608796027, time: 0.2014925479888916
Test Loss Energy: 9.759915978001384, Test Loss Force: 11.350863387751748, time: 11.136527299880981

Epoch 17, Batch 100/204, Loss: 1.5437756776809692, Variance: 0.13907518982887268
Epoch 17, Batch 200/204, Loss: 0.6825098991394043, Variance: 0.14003777503967285

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6753641866652296, Training Loss Force: 3.375901709262118, time: 3.2010135650634766
Validation Loss Energy: 3.6017077264304413, Validation Loss Force: 3.4476609248965384, time: 0.18318486213684082
Test Loss Energy: 9.732353963139074, Test Loss Force: 11.296948584391519, time: 11.297899961471558

Epoch 18, Batch 100/204, Loss: 1.2914174795150757, Variance: 0.1445024311542511
Epoch 18, Batch 200/204, Loss: 0.8133875131607056, Variance: 0.13380305469036102

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6813116244314323, Training Loss Force: 3.3632426102990656, time: 3.1117420196533203
Validation Loss Energy: 3.656865824242938, Validation Loss Force: 3.438907411099112, time: 0.20250797271728516
Test Loss Energy: 9.954725483191057, Test Loss Force: 11.219683220809193, time: 11.154847145080566

Epoch 19, Batch 100/204, Loss: 1.5762754678726196, Variance: 0.14470741152763367
Epoch 19, Batch 200/204, Loss: 0.7788602113723755, Variance: 0.13698630034923553

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.7018191962652565, Training Loss Force: 3.3758371865110237, time: 3.2620620727539062
Validation Loss Energy: 3.4980882497019397, Validation Loss Force: 3.4119528656462608, time: 0.18602895736694336
Test Loss Energy: 9.798627197881672, Test Loss Force: 11.382825307631768, time: 11.190674066543579

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–‚â–†â–ƒâ–…â–ƒâ–†â–…â–„â–ƒâ–ƒâ–â–‚â–…â–ƒâ–ˆâ–ƒâ–‚â–†â–ƒ
wandb:   test_error_force â–…â–„â–‡â–ƒâ–‡â–†â–‡â–…â–…â–†â–†â–ƒâ–â–ƒâ–…â–ˆâ–…â–„â–ƒâ–†
wandb:          test_loss â–†â–ƒâ–ˆâ–„â–†â–…â–†â–†â–„â–ƒâ–ƒâ–â–â–â–ƒâ–„â–ƒâ–‚â–…â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–‚
wandb:  train_error_force â–ˆâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–‚â–â–â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–…â–†â–ƒâ–‡â–â–„â–…â–…â–ƒâ–…â–„â–…â–‡â–…â–†â–„â–‡â–ˆâ–†
wandb:  valid_error_force â–‚â–ˆâ–ƒâ–…â–†â–…â–…â–…â–…â–ƒâ–„â–„â–†â–…â–â–„â–…â–ƒâ–ƒâ–
wandb:         valid_loss â–‡â–…â–…â–‚â–ˆâ–â–ƒâ–„â–…â–ƒâ–…â–„â–†â–†â–…â–†â–„â–‡â–ˆâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 6523
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.79863
wandb:   test_error_force 11.38283
wandb:          test_loss 10.31275
wandb: train_error_energy 2.70182
wandb:  train_error_force 3.37584
wandb:         train_loss 0.9819
wandb: valid_error_energy 3.49809
wandb:  valid_error_force 3.41195
wandb:         valid_loss 1.3157
wandb: 
wandb: ğŸš€ View run al_63_64 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jbdfhyaw
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_114253-jbdfhyaw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.022249221801758, Uncertainty Bias: -0.17288073897361755
3.0517578e-05 0.0331192
1.9221842 5.192628
(48745, 22, 3)
Found uncertainty sample 0 after 1002 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 5 steps.
Found uncertainty sample 4 after 1123 steps.
Found uncertainty sample 5 after 1241 steps.
Found uncertainty sample 6 after 2135 steps.
Found uncertainty sample 7 after 9 steps.
Found uncertainty sample 8 after 3260 steps.
Found uncertainty sample 9 after 3207 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 3100 steps.
Found uncertainty sample 12 after 3602 steps.
Found uncertainty sample 13 after 38 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 174 steps.
Found uncertainty sample 16 after 46 steps.
Found uncertainty sample 17 after 1049 steps.
Found uncertainty sample 18 after 121 steps.
Found uncertainty sample 19 after 488 steps.
Found uncertainty sample 20 after 2027 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 669 steps.
Found uncertainty sample 23 after 124 steps.
Found uncertainty sample 24 after 49 steps.
Found uncertainty sample 25 after 743 steps.
Found uncertainty sample 26 after 439 steps.
Found uncertainty sample 27 after 2397 steps.
Found uncertainty sample 28 after 1776 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 1321 steps.
Found uncertainty sample 31 after 1282 steps.
Found uncertainty sample 32 after 631 steps.
Found uncertainty sample 33 after 2822 steps.
Found uncertainty sample 34 after 2600 steps.
Found uncertainty sample 35 after 844 steps.
Found uncertainty sample 36 after 1359 steps.
Found uncertainty sample 37 after 477 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 854 steps.
Found uncertainty sample 40 after 220 steps.
Found uncertainty sample 41 after 1226 steps.
Found uncertainty sample 42 after 581 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1751 steps.
Found uncertainty sample 45 after 911 steps.
Found uncertainty sample 46 after 2067 steps.
Found uncertainty sample 47 after 1038 steps.
Found uncertainty sample 48 after 1996 steps.
Found uncertainty sample 49 after 1799 steps.
Found uncertainty sample 50 after 1048 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 2822 steps.
Found uncertainty sample 53 after 179 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2089 steps.
Found uncertainty sample 56 after 2512 steps.
Found uncertainty sample 57 after 277 steps.
Found uncertainty sample 58 after 1029 steps.
Found uncertainty sample 59 after 1999 steps.
Found uncertainty sample 60 after 13 steps.
Found uncertainty sample 61 after 1038 steps.
Found uncertainty sample 62 after 214 steps.
Found uncertainty sample 63 after 1120 steps.
Found uncertainty sample 64 after 3096 steps.
Found uncertainty sample 65 after 2129 steps.
Found uncertainty sample 66 after 2506 steps.
Found uncertainty sample 67 after 2454 steps.
Found uncertainty sample 68 after 92 steps.
Found uncertainty sample 69 after 434 steps.
Found uncertainty sample 70 after 1721 steps.
Found uncertainty sample 71 after 2526 steps.
Found uncertainty sample 72 after 404 steps.
Found uncertainty sample 73 after 1209 steps.
Found uncertainty sample 74 after 73 steps.
Found uncertainty sample 75 after 1916 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 127 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 553 steps.
Found uncertainty sample 80 after 957 steps.
Found uncertainty sample 81 after 170 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 462 steps.
Found uncertainty sample 84 after 202 steps.
Found uncertainty sample 85 after 598 steps.
Found uncertainty sample 86 after 818 steps.
Found uncertainty sample 87 after 284 steps.
Found uncertainty sample 88 after 2687 steps.
Found uncertainty sample 89 after 2615 steps.
Found uncertainty sample 90 after 466 steps.
Found uncertainty sample 91 after 493 steps.
Found uncertainty sample 92 after 1348 steps.
Found uncertainty sample 93 after 599 steps.
Found uncertainty sample 94 after 796 steps.
Found uncertainty sample 95 after 1270 steps.
Found uncertainty sample 96 after 1032 steps.
Found uncertainty sample 97 after 1419 steps.
Found uncertainty sample 98 after 2914 steps.
Found uncertainty sample 99 after 1344 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_120254-sty94x7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_65
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sty94x7k
Training model 65. Added 91 samples to the dataset.
Epoch 0, Batch 100/207, Loss: 1.103581428527832, Variance: 0.12842755019664764
Epoch 0, Batch 200/207, Loss: 1.257339596748352, Variance: 0.12931093573570251

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.964492411242444, Training Loss Force: 3.5864470889844053, time: 3.143697738647461
Validation Loss Energy: 2.8199406260148843, Validation Loss Force: 3.3698151791400326, time: 0.17995667457580566
Test Loss Energy: 9.79036100104769, Test Loss Force: 11.249407430021726, time: 9.592125654220581

Epoch 1, Batch 100/207, Loss: 0.6644335985183716, Variance: 0.1340082287788391
Epoch 1, Batch 200/207, Loss: 1.5386149883270264, Variance: 0.13742420077323914

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6541069556017547, Training Loss Force: 3.3418157248354805, time: 3.158613681793213
Validation Loss Energy: 2.0120826580962436, Validation Loss Force: 3.411087295008753, time: 0.17367291450500488
Test Loss Energy: 9.335214107666243, Test Loss Force: 11.2796316025811, time: 9.519308805465698

Epoch 2, Batch 100/207, Loss: 1.055759310722351, Variance: 0.14041772484779358
Epoch 2, Batch 200/207, Loss: 1.286673665046692, Variance: 0.13595572113990784

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6641744580472158, Training Loss Force: 3.346356508083054, time: 3.428438425064087
Validation Loss Energy: 2.8381566321703717, Validation Loss Force: 3.4089197778750058, time: 0.17636656761169434
Test Loss Energy: 10.043071073320018, Test Loss Force: 11.264522677970124, time: 10.573582887649536

Epoch 3, Batch 100/207, Loss: 0.9472445249557495, Variance: 0.13882513344287872
Epoch 3, Batch 200/207, Loss: 1.5902056694030762, Variance: 0.14371104538440704

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6852073402332617, Training Loss Force: 3.3529184548654283, time: 3.127406120300293
Validation Loss Energy: 2.1170314139586477, Validation Loss Force: 3.438671493139001, time: 0.17510175704956055
Test Loss Energy: 9.298393150647684, Test Loss Force: 11.322696253746983, time: 9.607583045959473

Epoch 4, Batch 100/207, Loss: 1.0331040620803833, Variance: 0.1443580985069275
Epoch 4, Batch 200/207, Loss: 1.201454997062683, Variance: 0.1327865719795227

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.674812984760444, Training Loss Force: 3.369576399289764, time: 3.2071220874786377
Validation Loss Energy: 2.721314950222105, Validation Loss Force: 3.5623952660477443, time: 0.17541837692260742
Test Loss Energy: 9.905670588993038, Test Loss Force: 11.416701743823316, time: 9.783857107162476

Epoch 5, Batch 100/207, Loss: 1.0485162734985352, Variance: 0.14110669493675232
Epoch 5, Batch 200/207, Loss: 1.4482166767120361, Variance: 0.14154228568077087

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.7045087008856314, Training Loss Force: 3.3588533889981895, time: 3.1206552982330322
Validation Loss Energy: 2.1554955605626462, Validation Loss Force: 3.403516154376203, time: 0.17456483840942383
Test Loss Energy: 9.211362157187228, Test Loss Force: 11.14737558692676, time: 9.587354183197021

Epoch 6, Batch 100/207, Loss: 1.0403891801834106, Variance: 0.14107030630111694
Epoch 6, Batch 200/207, Loss: 1.2502684593200684, Variance: 0.13645806908607483

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6676849102801192, Training Loss Force: 3.3676822292397977, time: 3.2626402378082275
Validation Loss Energy: 2.945938407576415, Validation Loss Force: 3.494834274137924, time: 0.1749567985534668
Test Loss Energy: 9.614195217383141, Test Loss Force: 11.294267246836924, time: 9.771475791931152

Epoch 7, Batch 100/207, Loss: 0.7594189643859863, Variance: 0.141867995262146
Epoch 7, Batch 200/207, Loss: 1.3222306966781616, Variance: 0.14455671608448029

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6976932065555492, Training Loss Force: 3.3760202701101045, time: 3.2621970176696777
Validation Loss Energy: 2.1392663755289862, Validation Loss Force: 3.4769457029615234, time: 0.17541122436523438
Test Loss Energy: 9.286196883025516, Test Loss Force: 11.31826416779156, time: 9.58972978591919

Epoch 8, Batch 100/207, Loss: 0.9596877694129944, Variance: 0.14006051421165466
Epoch 8, Batch 200/207, Loss: 1.1373766660690308, Variance: 0.1320040225982666

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6478693457929583, Training Loss Force: 3.3679801530271747, time: 3.1142284870147705
Validation Loss Energy: 2.676857218217169, Validation Loss Force: 3.4566469052823137, time: 0.17534804344177246
Test Loss Energy: 9.562722644552219, Test Loss Force: 11.112965845306833, time: 9.596173286437988

Epoch 9, Batch 100/207, Loss: 0.9341029524803162, Variance: 0.1381882131099701
Epoch 9, Batch 200/207, Loss: 1.514043927192688, Variance: 0.13837458193302155

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7023806857424346, Training Loss Force: 3.365546221134819, time: 3.2805230617523193
Validation Loss Energy: 2.092233975856973, Validation Loss Force: 3.4287526403663615, time: 0.1760556697845459
Test Loss Energy: 9.273617908929275, Test Loss Force: 11.143223490518134, time: 9.578727960586548

Epoch 10, Batch 100/207, Loss: 0.8778072595596313, Variance: 0.14043280482292175
Epoch 10, Batch 200/207, Loss: 1.2321841716766357, Variance: 0.13988330960273743

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.68588874654487, Training Loss Force: 3.369947688795787, time: 3.186547040939331
Validation Loss Energy: 2.5702485195391596, Validation Loss Force: 3.526579100927119, time: 0.17700505256652832
Test Loss Energy: 9.981282633001042, Test Loss Force: 11.26883038630335, time: 9.583185195922852

Epoch 11, Batch 100/207, Loss: 0.8737791180610657, Variance: 0.14120127260684967
Epoch 11, Batch 200/207, Loss: 1.3100452423095703, Variance: 0.14216862618923187

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.712240769189222, Training Loss Force: 3.355562859658414, time: 3.1714847087860107
Validation Loss Energy: 2.3493238193678563, Validation Loss Force: 3.40736554723643, time: 0.18072199821472168
Test Loss Energy: 9.40160537804071, Test Loss Force: 11.324290644106702, time: 9.77437686920166

Epoch 12, Batch 100/207, Loss: 0.8405516743659973, Variance: 0.14354616403579712
Epoch 12, Batch 200/207, Loss: 1.2806658744812012, Variance: 0.14175190031528473

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.7138881567458717, Training Loss Force: 3.3788029449128363, time: 3.2007598876953125
Validation Loss Energy: 2.810435548182665, Validation Loss Force: 3.5784423077200356, time: 0.17504644393920898
Test Loss Energy: 9.876924244741627, Test Loss Force: 11.110750886529676, time: 9.564885377883911

Epoch 13, Batch 100/207, Loss: 0.7476731538772583, Variance: 0.1384943425655365
Epoch 13, Batch 200/207, Loss: 1.371947169303894, Variance: 0.1439826488494873

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6650369696732077, Training Loss Force: 3.3687201267026436, time: 3.081782579421997
Validation Loss Energy: 1.9783941801658886, Validation Loss Force: 3.4766863490738036, time: 0.18187236785888672
Test Loss Energy: 9.33534531225451, Test Loss Force: 11.286609055683646, time: 9.741221904754639

Epoch 14, Batch 100/207, Loss: 0.9513430595397949, Variance: 0.14410068094730377
Epoch 14, Batch 200/207, Loss: 1.2081873416900635, Variance: 0.14143912494182587

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6825125847830398, Training Loss Force: 3.3816870880015455, time: 3.119633674621582
Validation Loss Energy: 2.9892479461303747, Validation Loss Force: 3.476154559486042, time: 0.18641352653503418
Test Loss Energy: 9.986872258099634, Test Loss Force: 11.312122229946503, time: 9.6407949924469

Epoch 15, Batch 100/207, Loss: 0.653654932975769, Variance: 0.13770617544651031
Epoch 15, Batch 200/207, Loss: 1.2899179458618164, Variance: 0.143607035279274

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.69062708788261, Training Loss Force: 3.3679414559264225, time: 3.1900010108947754
Validation Loss Energy: 2.4184085919105467, Validation Loss Force: 3.4136472665121986, time: 0.18034076690673828
Test Loss Energy: 9.419164558596702, Test Loss Force: 11.444233568946345, time: 9.586766719818115

Epoch 16, Batch 100/207, Loss: 0.9339016675949097, Variance: 0.13973170518875122
Epoch 16, Batch 200/207, Loss: 1.3698954582214355, Variance: 0.14086474478244781

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6801819401792004, Training Loss Force: 3.361347680364537, time: 3.435589075088501
Validation Loss Energy: 2.965002783177887, Validation Loss Force: 3.494495705701541, time: 0.17393016815185547
Test Loss Energy: 9.958185915605306, Test Loss Force: 11.13697796372213, time: 9.638547897338867

Epoch 17, Batch 100/207, Loss: 0.7466204166412354, Variance: 0.13598161935806274
Epoch 17, Batch 200/207, Loss: 1.5092750787734985, Variance: 0.14183451235294342

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6929404040938327, Training Loss Force: 3.3691438015455617, time: 3.1370511054992676
Validation Loss Energy: 2.164576064000868, Validation Loss Force: 3.4575243887191283, time: 0.1798267364501953
Test Loss Energy: 9.428897653131772, Test Loss Force: 11.45648420514594, time: 9.562375783920288

Epoch 18, Batch 100/207, Loss: 0.7629905343055725, Variance: 0.14520889520645142
Epoch 18, Batch 200/207, Loss: 1.0794215202331543, Variance: 0.13922323286533356

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.690082150865047, Training Loss Force: 3.3582816822010835, time: 3.1291396617889404
Validation Loss Energy: 2.435162942124878, Validation Loss Force: 3.5039349848011385, time: 0.17662334442138672
Test Loss Energy: 9.551313324665973, Test Loss Force: 11.21704928496912, time: 9.804208517074585

Epoch 19, Batch 100/207, Loss: 0.8050391674041748, Variance: 0.14108416438102722
Epoch 19, Batch 200/207, Loss: 1.3858637809753418, Variance: 0.14665478467941284

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6868073971501976, Training Loss Force: 3.3583981446185813, time: 3.1439056396484375
Validation Loss Energy: 2.1642683352411956, Validation Loss Force: 3.415339663355823, time: 0.17809009552001953
Test Loss Energy: 9.336129149617229, Test Loss Force: 11.195977952135058, time: 10.557764768600464

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–‚â–ˆâ–‚â–‡â–â–„â–‚â–„â–‚â–‡â–ƒâ–‡â–‚â–ˆâ–ƒâ–‡â–ƒâ–„â–‚
wandb:   test_error_force â–„â–„â–„â–…â–‡â–‚â–…â–…â–â–‚â–„â–…â–â–…â–…â–ˆâ–‚â–ˆâ–ƒâ–ƒ
wandb:          test_loss â–ˆâ–†â–‡â–„â–ˆâ–‚â–ƒâ–‚â–‚â–‚â–…â–‚â–‚â–ƒâ–„â–ƒâ–ƒâ–„â–â–‚
wandb: train_error_energy â–ˆâ–â–â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–â–‡â–‚â–†â–‚â–ˆâ–‚â–†â–‚â–…â–„â–‡â–â–ˆâ–„â–ˆâ–‚â–„â–‚
wandb:  valid_error_force â–â–‚â–‚â–ƒâ–‡â–‚â–…â–…â–„â–ƒâ–†â–‚â–ˆâ–…â–…â–‚â–…â–„â–†â–ƒ
wandb:         valid_loss â–†â–â–†â–‚â–‡â–‚â–ˆâ–‚â–…â–‚â–…â–ƒâ–‡â–â–ˆâ–„â–ˆâ–‚â–„â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 6604
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.33613
wandb:   test_error_force 11.19598
wandb:          test_loss 9.59944
wandb: train_error_energy 2.68681
wandb:  train_error_force 3.3584
wandb:         train_loss 0.96026
wandb: valid_error_energy 2.16427
wandb:  valid_error_force 3.41534
wandb:         valid_loss 0.80354
wandb: 
wandb: ğŸš€ View run al_63_65 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sty94x7k
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_120254-sty94x7k/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.0388360023498535, Uncertainty Bias: -0.1922816038131714
3.8146973e-05 0.00166893
1.8693053 5.3244343
(48745, 22, 3)
Found uncertainty sample 0 after 469 steps.
Found uncertainty sample 1 after 2445 steps.
Found uncertainty sample 2 after 1473 steps.
Found uncertainty sample 3 after 600 steps.
Found uncertainty sample 4 after 410 steps.
Found uncertainty sample 5 after 2607 steps.
Found uncertainty sample 6 after 2018 steps.
Found uncertainty sample 7 after 2431 steps.
Found uncertainty sample 8 after 11 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 11 steps.
Found uncertainty sample 11 after 2 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 2109 steps.
Found uncertainty sample 14 after 1046 steps.
Found uncertainty sample 15 after 990 steps.
Found uncertainty sample 16 after 407 steps.
Found uncertainty sample 17 after 378 steps.
Found uncertainty sample 18 after 1757 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 323 steps.
Found uncertainty sample 21 after 773 steps.
Found uncertainty sample 22 after 946 steps.
Found uncertainty sample 23 after 1073 steps.
Found uncertainty sample 24 after 2025 steps.
Found uncertainty sample 25 after 17 steps.
Found uncertainty sample 26 after 2817 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 195 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 803 steps.
Found uncertainty sample 31 after 367 steps.
Found uncertainty sample 32 after 3127 steps.
Found uncertainty sample 33 after 2653 steps.
Found uncertainty sample 34 after 35 steps.
Found uncertainty sample 35 after 3051 steps.
Found uncertainty sample 36 after 102 steps.
Found uncertainty sample 37 after 314 steps.
Found uncertainty sample 38 after 2677 steps.
Found uncertainty sample 39 after 1284 steps.
Found uncertainty sample 40 after 73 steps.
Found uncertainty sample 41 after 905 steps.
Found uncertainty sample 42 after 1116 steps.
Found uncertainty sample 43 after 2251 steps.
Found uncertainty sample 44 after 314 steps.
Found uncertainty sample 45 after 1658 steps.
Found uncertainty sample 46 after 528 steps.
Found uncertainty sample 47 after 23 steps.
Found uncertainty sample 48 after 56 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 2157 steps.
Found uncertainty sample 51 after 205 steps.
Found uncertainty sample 52 after 1453 steps.
Found uncertainty sample 53 after 2392 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 356 steps.
Found uncertainty sample 56 after 1679 steps.
Found uncertainty sample 57 after 1459 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 2393 steps.
Found uncertainty sample 60 after 1313 steps.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 1429 steps.
Found uncertainty sample 64 after 2619 steps.
Found uncertainty sample 65 after 1382 steps.
Found uncertainty sample 66 after 2180 steps.
Found uncertainty sample 67 after 2485 steps.
Found uncertainty sample 68 after 80 steps.
Found uncertainty sample 69 after 465 steps.
Found uncertainty sample 70 after 150 steps.
Found uncertainty sample 71 after 448 steps.
Found uncertainty sample 72 after 4 steps.
Found uncertainty sample 73 after 8 steps.
Found uncertainty sample 74 after 4 steps.
Found uncertainty sample 75 after 2006 steps.
Found uncertainty sample 76 after 162 steps.
Found uncertainty sample 77 after 2129 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 91 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 377 steps.
Found uncertainty sample 82 after 668 steps.
Found uncertainty sample 83 after 1576 steps.
Found uncertainty sample 84 after 980 steps.
Found uncertainty sample 85 after 539 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 1096 steps.
Found uncertainty sample 88 after 564 steps.
Found uncertainty sample 89 after 608 steps.
Found uncertainty sample 90 after 727 steps.
Found uncertainty sample 91 after 2482 steps.
Found uncertainty sample 92 after 2687 steps.
Found uncertainty sample 93 after 2212 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 0 steps.
Found uncertainty sample 96 after 3269 steps.
Found uncertainty sample 97 after 239 steps.
Found uncertainty sample 98 after 508 steps.
Found uncertainty sample 99 after 550 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_122255-tof7hfcl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_66
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/tof7hfcl
Training model 66. Added 88 samples to the dataset.
Epoch 0, Batch 100/209, Loss: 0.8376713395118713, Variance: 0.11561472713947296
Epoch 0, Batch 200/209, Loss: 1.0997138023376465, Variance: 0.12803786993026733

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.825624097752403, Training Loss Force: 3.779856716549795, time: 3.2327511310577393
Validation Loss Energy: 3.747713110095594, Validation Loss Force: 3.4068771781321234, time: 1.1671154499053955
Test Loss Energy: 10.127631539782527, Test Loss Force: 11.148622580776234, time: 9.64799976348877

Epoch 1, Batch 100/209, Loss: 1.2140651941299438, Variance: 0.13348764181137085
Epoch 1, Batch 200/209, Loss: 0.7313718795776367, Variance: 0.14138281345367432

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.626228398610053, Training Loss Force: 3.3223809788539067, time: 3.2537925243377686
Validation Loss Energy: 2.733124542380104, Validation Loss Force: 3.4109862032165026, time: 0.18215727806091309
Test Loss Energy: 9.780362955876722, Test Loss Force: 11.217962652821189, time: 9.812752485275269

Epoch 2, Batch 100/209, Loss: 0.9074757099151611, Variance: 0.13989897072315216
Epoch 2, Batch 200/209, Loss: 1.4587860107421875, Variance: 0.14317451417446136

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6528136403592657, Training Loss Force: 3.3401483050822454, time: 3.135545253753662
Validation Loss Energy: 1.6040759062339052, Validation Loss Force: 3.415536725549969, time: 0.18394923210144043
Test Loss Energy: 9.237393308847407, Test Loss Force: 11.287981401770018, time: 9.710281133651733

Epoch 3, Batch 100/209, Loss: 0.8106787204742432, Variance: 0.14500699937343597
Epoch 3, Batch 200/209, Loss: 0.9461023807525635, Variance: 0.14368006587028503

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.649656450230143, Training Loss Force: 3.3393594479607698, time: 3.141223907470703
Validation Loss Energy: 3.4100911845844455, Validation Loss Force: 3.4529996960065894, time: 0.17935466766357422
Test Loss Energy: 9.858000519680017, Test Loss Force: 11.31403167732899, time: 9.700307369232178

Epoch 4, Batch 100/209, Loss: 1.3750114440917969, Variance: 0.14408108592033386
Epoch 4, Batch 200/209, Loss: 0.6436723470687866, Variance: 0.13803239166736603

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6434471133424378, Training Loss Force: 3.3434522261880146, time: 3.084571361541748
Validation Loss Energy: 2.239392618696172, Validation Loss Force: 3.3980647126783126, time: 0.17943954467773438
Test Loss Energy: 9.409691770122256, Test Loss Force: 11.26195255840779, time: 9.848464488983154

Epoch 5, Batch 100/209, Loss: 0.9842174053192139, Variance: 0.14365722239017487
Epoch 5, Batch 200/209, Loss: 1.349727749824524, Variance: 0.13579061627388

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.694832321926425, Training Loss Force: 3.360808615974843, time: 3.1469168663024902
Validation Loss Energy: 2.1941596083924018, Validation Loss Force: 3.45208936887075, time: 0.17736053466796875
Test Loss Energy: 9.691874796872401, Test Loss Force: 11.127077184164683, time: 9.663503646850586

Epoch 6, Batch 100/209, Loss: 0.7579131126403809, Variance: 0.14034506678581238
Epoch 6, Batch 200/209, Loss: 0.9758732318878174, Variance: 0.14181846380233765

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.698975786682941, Training Loss Force: 3.349443600878534, time: 3.1821765899658203
Validation Loss Energy: 3.8872540744507447, Validation Loss Force: 3.4023532090943087, time: 0.17803239822387695
Test Loss Energy: 10.491249345753108, Test Loss Force: 11.163076713538109, time: 9.982311964035034

Epoch 7, Batch 100/209, Loss: 1.3390581607818604, Variance: 0.1395871341228485
Epoch 7, Batch 200/209, Loss: 0.720953106880188, Variance: 0.14083680510520935

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6464508279636774, Training Loss Force: 3.346045645207415, time: 3.175448417663574
Validation Loss Energy: 2.607630382759376, Validation Loss Force: 3.4196648445136106, time: 0.17621207237243652
Test Loss Energy: 9.987773716066915, Test Loss Force: 11.26972940073906, time: 9.70038104057312

Epoch 8, Batch 100/209, Loss: 0.7046990990638733, Variance: 0.13892799615859985
Epoch 8, Batch 200/209, Loss: 1.452998161315918, Variance: 0.14657682180404663

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.698104014070477, Training Loss Force: 3.3504601718273057, time: 3.1806399822235107
Validation Loss Energy: 1.5893349304370532, Validation Loss Force: 3.4116327230817034, time: 0.18332171440124512
Test Loss Energy: 9.207276721625536, Test Loss Force: 11.1310177985118, time: 9.836973428726196

Epoch 9, Batch 100/209, Loss: 0.6984766125679016, Variance: 0.14163252711296082
Epoch 9, Batch 200/209, Loss: 1.0405449867248535, Variance: 0.14015604555606842

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6682643327744113, Training Loss Force: 3.346248135627871, time: 3.223261833190918
Validation Loss Energy: 3.1499842298135268, Validation Loss Force: 3.400745927531716, time: 0.1752772331237793
Test Loss Energy: 9.713229882478414, Test Loss Force: 11.357338951594215, time: 9.667692184448242

Epoch 10, Batch 100/209, Loss: 1.285162329673767, Variance: 0.1421978771686554
Epoch 10, Batch 200/209, Loss: 0.764737069606781, Variance: 0.14040857553482056

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6800088165806817, Training Loss Force: 3.364074630055675, time: 3.298013210296631
Validation Loss Energy: 2.4397366777168075, Validation Loss Force: 3.434226237644272, time: 0.17880630493164062
Test Loss Energy: 9.422059434696434, Test Loss Force: 11.255965533003105, time: 9.734685182571411

Epoch 11, Batch 100/209, Loss: 0.9388650059700012, Variance: 0.1430422067642212
Epoch 11, Batch 200/209, Loss: 1.3741919994354248, Variance: 0.1384362131357193

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.657731420024304, Training Loss Force: 3.3622820091902272, time: 3.215930938720703
Validation Loss Energy: 2.151685969812254, Validation Loss Force: 3.4524705227812924, time: 0.17738842964172363
Test Loss Energy: 9.73391462903255, Test Loss Force: 11.187938789228589, time: 9.876183032989502

Epoch 12, Batch 100/209, Loss: 0.8163845539093018, Variance: 0.1417449712753296
Epoch 12, Batch 200/209, Loss: 0.7893168926239014, Variance: 0.13877655565738678

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6859707957477292, Training Loss Force: 3.34163137748036, time: 3.196054458618164
Validation Loss Energy: 3.731516212088094, Validation Loss Force: 3.446155124797202, time: 0.17821645736694336
Test Loss Energy: 10.456629474407851, Test Loss Force: 11.12129049570614, time: 9.64111328125

Epoch 13, Batch 100/209, Loss: 1.219604730606079, Variance: 0.14108993113040924
Epoch 13, Batch 200/209, Loss: 0.7438119649887085, Variance: 0.14534330368041992

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.711028282645977, Training Loss Force: 3.363548485237302, time: 3.18841814994812
Validation Loss Energy: 2.8416951232355965, Validation Loss Force: 3.399031208106212, time: 0.17900300025939941
Test Loss Energy: 9.902261428772706, Test Loss Force: 11.128152841684116, time: 9.841150999069214

Epoch 14, Batch 100/209, Loss: 0.7476102113723755, Variance: 0.1410539448261261
Epoch 14, Batch 200/209, Loss: 1.3601033687591553, Variance: 0.14383018016815186

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.7123162606485702, Training Loss Force: 3.3567171726970715, time: 3.3388867378234863
Validation Loss Energy: 1.7271646267384622, Validation Loss Force: 3.390408345430159, time: 0.17555975914001465
Test Loss Energy: 9.287377178722236, Test Loss Force: 11.215870598086125, time: 10.654584169387817

Epoch 15, Batch 100/209, Loss: 0.8369327187538147, Variance: 0.14281649887561798
Epoch 15, Batch 200/209, Loss: 0.8771688938140869, Variance: 0.13879817724227905

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6594141451430153, Training Loss Force: 3.3637926978894486, time: 3.325796365737915
Validation Loss Energy: 3.500769019217051, Validation Loss Force: 3.46694162357328, time: 0.20093846321105957
Test Loss Energy: 9.720421107287505, Test Loss Force: 11.280234539332762, time: 11.048524618148804

Epoch 16, Batch 100/209, Loss: 1.479176640510559, Variance: 0.1433568149805069
Epoch 16, Batch 200/209, Loss: 0.6639524102210999, Variance: 0.14161747694015503

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.7034512340895924, Training Loss Force: 3.376586326263953, time: 3.3672897815704346
Validation Loss Energy: 2.1562617094264978, Validation Loss Force: 3.4569557473877155, time: 0.2009589672088623
Test Loss Energy: 9.441215299920415, Test Loss Force: 11.388564496448, time: 10.840653419494629

Epoch 17, Batch 100/209, Loss: 0.7999382019042969, Variance: 0.14328411221504211
Epoch 17, Batch 200/209, Loss: 1.047211766242981, Variance: 0.13525061309337616

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6849700817187347, Training Loss Force: 3.358988738914371, time: 3.2696404457092285
Validation Loss Energy: 2.2731307796797133, Validation Loss Force: 3.4649242967784604, time: 0.19161415100097656
Test Loss Energy: 9.546983689833814, Test Loss Force: 11.240343126346776, time: 11.033684253692627

Epoch 18, Batch 100/209, Loss: 0.8384989500045776, Variance: 0.1349898874759674
Epoch 18, Batch 200/209, Loss: 0.5391988754272461, Variance: 0.13759498298168182

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.668570870413963, Training Loss Force: 3.3649940693137044, time: 3.2704625129699707
Validation Loss Energy: 3.9018058371408446, Validation Loss Force: 3.455542688987658, time: 0.18617844581604004
Test Loss Energy: 10.275579029165876, Test Loss Force: 11.12761321758155, time: 10.958313941955566

Epoch 19, Batch 100/209, Loss: 1.215011477470398, Variance: 0.13662885129451752
Epoch 19, Batch 200/209, Loss: 0.9361647367477417, Variance: 0.1396937221288681

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6434806932346664, Training Loss Force: 3.3603145866528092, time: 3.2708394527435303
Validation Loss Energy: 2.7673578082096157, Validation Loss Force: 3.4578151946237106, time: 0.18770384788513184
Test Loss Energy: 9.714650525619119, Test Loss Force: 11.112225553168926, time: 11.054886817932129

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–â–…â–‚â–„â–ˆâ–…â–â–„â–‚â–„â–ˆâ–…â–â–„â–‚â–ƒâ–‡â–„
wandb:   test_error_force â–‚â–„â–…â–†â–…â–â–‚â–…â–â–‡â–…â–ƒâ–â–â–„â–…â–ˆâ–„â–â–
wandb:          test_loss â–ˆâ–…â–‚â–…â–ƒâ–‚â–‡â–†â–‚â–†â–ƒâ–ƒâ–‡â–ƒâ–â–…â–ƒâ–ƒâ–†â–‚
wandb: train_error_energy â–ˆâ–â–‚â–‚â–‚â–ƒâ–„â–‚â–„â–‚â–ƒâ–‚â–ƒâ–„â–„â–‚â–„â–ƒâ–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–â–
wandb: valid_error_energy â–ˆâ–„â–â–‡â–ƒâ–ƒâ–ˆâ–„â–â–†â–„â–ƒâ–‡â–…â–â–‡â–ƒâ–ƒâ–ˆâ–…
wandb:  valid_error_force â–ƒâ–ƒâ–ƒâ–‡â–‚â–‡â–‚â–„â–ƒâ–‚â–…â–‡â–†â–‚â–â–ˆâ–‡â–ˆâ–‡â–‡
wandb:         valid_loss â–ˆâ–„â–â–†â–‚â–‚â–ˆâ–ƒâ–â–…â–ƒâ–‚â–‡â–„â–â–‡â–‚â–ƒâ–ˆâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 6683
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.71465
wandb:   test_error_force 11.11223
wandb:          test_loss 9.56977
wandb: train_error_energy 2.64348
wandb:  train_error_force 3.36031
wandb:         train_loss 0.95201
wandb: valid_error_energy 2.76736
wandb:  valid_error_force 3.45782
wandb:         valid_loss 1.00562
wandb: 
wandb: ğŸš€ View run al_63_66 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/tof7hfcl
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_122255-tof7hfcl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.910590648651123, Uncertainty Bias: -0.17836545407772064
2.670288e-05 0.015262604
1.9502537 5.3938365
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 456 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 14 steps.
Found uncertainty sample 4 after 448 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 146 steps.
Found uncertainty sample 7 after 610 steps.
Found uncertainty sample 8 after 1189 steps.
Found uncertainty sample 9 after 1994 steps.
Found uncertainty sample 10 after 3570 steps.
Found uncertainty sample 11 after 531 steps.
Found uncertainty sample 12 after 30 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 2048 steps.
Found uncertainty sample 15 after 692 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 312 steps.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 2168 steps.
Found uncertainty sample 21 after 559 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 1834 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 424 steps.
Found uncertainty sample 26 after 2286 steps.
Found uncertainty sample 27 after 2261 steps.
Found uncertainty sample 28 after 2357 steps.
Found uncertainty sample 29 after 1019 steps.
Found uncertainty sample 30 after 473 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 884 steps.
Found uncertainty sample 33 after 43 steps.
Found uncertainty sample 34 after 2601 steps.
Found uncertainty sample 35 after 3823 steps.
Found uncertainty sample 36 after 983 steps.
Found uncertainty sample 37 after 344 steps.
Found uncertainty sample 38 after 1386 steps.
Found uncertainty sample 39 after 1899 steps.
Found uncertainty sample 40 after 1567 steps.
Found uncertainty sample 41 after 383 steps.
Found uncertainty sample 42 after 42 steps.
Found uncertainty sample 43 after 2374 steps.
Found uncertainty sample 44 after 2214 steps.
Found uncertainty sample 45 after 231 steps.
Found uncertainty sample 46 after 337 steps.
Found uncertainty sample 47 after 829 steps.
Found uncertainty sample 48 after 3646 steps.
Found uncertainty sample 49 after 1372 steps.
Found uncertainty sample 50 after 254 steps.
Found uncertainty sample 51 after 1011 steps.
Found uncertainty sample 52 after 1041 steps.
Found uncertainty sample 53 after 8 steps.
Found uncertainty sample 54 after 41 steps.
Found uncertainty sample 55 after 1587 steps.
Found uncertainty sample 56 after 1911 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 1526 steps.
Found uncertainty sample 59 after 803 steps.
Found uncertainty sample 60 after 182 steps.
Found uncertainty sample 61 after 2275 steps.
Found uncertainty sample 62 after 2130 steps.
Found uncertainty sample 63 after 295 steps.
Found uncertainty sample 64 after 128 steps.
Found uncertainty sample 65 after 221 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 1031 steps.
Did not find any uncertainty samples for sample 68.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 420 steps.
Found uncertainty sample 71 after 22 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 40 steps.
Found uncertainty sample 74 after 2580 steps.
Found uncertainty sample 75 after 612 steps.
Found uncertainty sample 76 after 1712 steps.
Found uncertainty sample 77 after 3 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 1634 steps.
Found uncertainty sample 80 after 1125 steps.
Found uncertainty sample 81 after 1169 steps.
Found uncertainty sample 82 after 141 steps.
Found uncertainty sample 83 after 261 steps.
Found uncertainty sample 84 after 2073 steps.
Found uncertainty sample 85 after 1158 steps.
Found uncertainty sample 86 after 306 steps.
Found uncertainty sample 87 after 2300 steps.
Found uncertainty sample 88 after 2717 steps.
Found uncertainty sample 89 after 1352 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2416 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 804 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 199 steps.
Found uncertainty sample 96 after 1008 steps.
Found uncertainty sample 97 after 277 steps.
Found uncertainty sample 98 after 1212 steps.
Found uncertainty sample 99 after 1603 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_124429-jzxfjrkp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_67
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jzxfjrkp
Training model 67. Added 83 samples to the dataset.
Epoch 0, Batch 100/212, Loss: 1.5117167234420776, Variance: 0.1282356083393097
Epoch 0, Batch 200/212, Loss: 0.5484404563903809, Variance: 0.13526515662670135

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.0714903267333367, Training Loss Force: 3.7272169739864585, time: 3.2061448097229004
Validation Loss Energy: 2.511318796065652, Validation Loss Force: 3.39948489649104, time: 0.18074488639831543
Test Loss Energy: 9.660316013069147, Test Loss Force: 11.159184979148847, time: 9.763668298721313

Epoch 1, Batch 100/212, Loss: 0.7784610390663147, Variance: 0.13643497228622437
Epoch 1, Batch 200/212, Loss: 1.3560086488723755, Variance: 0.1433110386133194

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6400256812659495, Training Loss Force: 3.324990367595098, time: 3.2575058937072754
Validation Loss Energy: 2.3047266964775686, Validation Loss Force: 3.3959981203349567, time: 0.18796849250793457
Test Loss Energy: 9.359448139784499, Test Loss Force: 11.108331030169209, time: 9.72716736793518

Epoch 2, Batch 100/212, Loss: 0.8533272743225098, Variance: 0.14035046100616455
Epoch 2, Batch 200/212, Loss: 0.5495254993438721, Variance: 0.13675197958946228

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6568571254204274, Training Loss Force: 3.338307435122659, time: 3.4641318321228027
Validation Loss Energy: 3.0439992274132615, Validation Loss Force: 3.406182227727683, time: 0.17929983139038086
Test Loss Energy: 9.700602331534148, Test Loss Force: 11.41794081951133, time: 9.74458360671997

Epoch 3, Batch 100/212, Loss: 1.4109545946121216, Variance: 0.14424148201942444
Epoch 3, Batch 200/212, Loss: 0.5696470141410828, Variance: 0.14073476195335388

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6563371068379404, Training Loss Force: 3.3378584910610964, time: 3.1904170513153076
Validation Loss Energy: 2.4862311087322597, Validation Loss Force: 3.4149879017725433, time: 0.18780970573425293
Test Loss Energy: 9.673706244309848, Test Loss Force: 11.243364996139723, time: 10.806993007659912

Epoch 4, Batch 100/212, Loss: 0.7957831621170044, Variance: 0.13689184188842773
Epoch 4, Batch 200/212, Loss: 1.4218637943267822, Variance: 0.13900312781333923

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6358023617894886, Training Loss Force: 3.3490371176339986, time: 3.2132248878479004
Validation Loss Energy: 2.459481697230095, Validation Loss Force: 3.477121722921379, time: 0.18367505073547363
Test Loss Energy: 9.682395666407622, Test Loss Force: 11.482607931212087, time: 9.96037220954895

Epoch 5, Batch 100/212, Loss: 0.45820701122283936, Variance: 0.13499900698661804
Epoch 5, Batch 200/212, Loss: 0.6656793355941772, Variance: 0.1415848731994629

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6556045227220224, Training Loss Force: 3.3492673291720814, time: 3.2803707122802734
Validation Loss Energy: 3.463286064204583, Validation Loss Force: 3.430786444677979, time: 0.17887425422668457
Test Loss Energy: 9.897957124596058, Test Loss Force: 11.490131429682412, time: 9.706602811813354

Epoch 6, Batch 100/212, Loss: 1.4305285215377808, Variance: 0.14700424671173096
Epoch 6, Batch 200/212, Loss: 0.7621150016784668, Variance: 0.13714513182640076

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.698877209007169, Training Loss Force: 3.355036015584654, time: 3.1773931980133057
Validation Loss Energy: 2.4976083330761534, Validation Loss Force: 3.4465082780626437, time: 0.1768953800201416
Test Loss Energy: 9.711126906462937, Test Loss Force: 11.10673151025848, time: 10.178036212921143

Epoch 7, Batch 100/212, Loss: 0.8555513024330139, Variance: 0.1404973864555359
Epoch 7, Batch 200/212, Loss: 1.3473711013793945, Variance: 0.14472733438014984

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.695938953084251, Training Loss Force: 3.348603697393081, time: 3.316826105117798
Validation Loss Energy: 2.248746058840015, Validation Loss Force: 3.403722401672619, time: 0.19028186798095703
Test Loss Energy: 9.647188545410323, Test Loss Force: 11.132458530839534, time: 9.960273504257202

Epoch 8, Batch 100/212, Loss: 0.6034196615219116, Variance: 0.14010563492774963
Epoch 8, Batch 200/212, Loss: 0.7582960724830627, Variance: 0.13950270414352417

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6883086549313893, Training Loss Force: 3.3448845019133264, time: 3.178239583969116
Validation Loss Energy: 3.005612458337473, Validation Loss Force: 3.42777985453509, time: 0.18790721893310547
Test Loss Energy: 9.545343487920384, Test Loss Force: 11.332948878534365, time: 10.158514022827148

Epoch 9, Batch 100/212, Loss: 1.2506318092346191, Variance: 0.14256948232650757
Epoch 9, Batch 200/212, Loss: 0.780295729637146, Variance: 0.14203861355781555

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.679077567807093, Training Loss Force: 3.3739121244232444, time: 3.2243125438690186
Validation Loss Energy: 2.792424286306858, Validation Loss Force: 3.502473662863701, time: 0.18126392364501953
Test Loss Energy: 9.907319175390748, Test Loss Force: 11.161008320022866, time: 10.009807586669922

Epoch 10, Batch 100/212, Loss: 0.6556198596954346, Variance: 0.13562634587287903
Epoch 10, Batch 200/212, Loss: 1.2579492330551147, Variance: 0.14271754026412964

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.680918269129024, Training Loss Force: 3.351619824497504, time: 3.2170071601867676
Validation Loss Energy: 2.220487952539617, Validation Loss Force: 3.434032571154601, time: 0.19603872299194336
Test Loss Energy: 9.8328169890177, Test Loss Force: 11.504667108315592, time: 9.992168664932251

Epoch 11, Batch 100/212, Loss: 0.4972158670425415, Variance: 0.1412268877029419
Epoch 11, Batch 200/212, Loss: 0.6071177124977112, Variance: 0.1379845291376114

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.694058230134317, Training Loss Force: 3.351062188483595, time: 3.4245188236236572
Validation Loss Energy: 3.213410756689046, Validation Loss Force: 3.48757871135244, time: 0.18670225143432617
Test Loss Energy: 9.689371212249839, Test Loss Force: 11.247799768114586, time: 9.97584080696106

Epoch 12, Batch 100/212, Loss: 1.3143583536148071, Variance: 0.14322137832641602
Epoch 12, Batch 200/212, Loss: 0.7192318439483643, Variance: 0.1375723034143448

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.666492604396394, Training Loss Force: 3.3456433426077554, time: 3.2539305686950684
Validation Loss Energy: 2.6386749150846285, Validation Loss Force: 3.462452001251224, time: 0.18160414695739746
Test Loss Energy: 9.636574893496709, Test Loss Force: 11.101686163164189, time: 10.039942026138306

Epoch 13, Batch 100/212, Loss: 0.9826326966285706, Variance: 0.14355196058750153
Epoch 13, Batch 200/212, Loss: 1.321544885635376, Variance: 0.14438265562057495

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6932455276640743, Training Loss Force: 3.357774755751516, time: 3.164757013320923
Validation Loss Energy: 2.1312768395712376, Validation Loss Force: 3.4584124489403756, time: 0.18053531646728516
Test Loss Energy: 9.420386217127438, Test Loss Force: 11.171185731602822, time: 10.231372594833374

Epoch 14, Batch 100/212, Loss: 0.6908421516418457, Variance: 0.1401366889476776
Epoch 14, Batch 200/212, Loss: 0.9247023463249207, Variance: 0.1390228271484375

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.672646130839167, Training Loss Force: 3.3615003645228034, time: 3.1890950202941895
Validation Loss Energy: 3.1137164916649596, Validation Loss Force: 3.5518943004603627, time: 0.18132567405700684
Test Loss Energy: 9.596224007974232, Test Loss Force: 11.348277884199627, time: 10.029204845428467

Epoch 15, Batch 100/212, Loss: 1.6628668308258057, Variance: 0.14182664453983307
Epoch 15, Batch 200/212, Loss: 0.5718482732772827, Variance: 0.14016219973564148

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.686761743490659, Training Loss Force: 3.375025042670136, time: 3.28368878364563
Validation Loss Energy: 2.871922849466299, Validation Loss Force: 3.507971308409372, time: 0.1824655532836914
Test Loss Energy: 9.633916932705015, Test Loss Force: 10.927771548243667, time: 10.17544150352478

Epoch 16, Batch 100/212, Loss: 0.8948964476585388, Variance: 0.14166176319122314
Epoch 16, Batch 200/212, Loss: 1.2085734605789185, Variance: 0.14762315154075623

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.7029571363807725, Training Loss Force: 3.352415722799512, time: 3.2953507900238037
Validation Loss Energy: 2.450573261374389, Validation Loss Force: 3.400727946998182, time: 0.18677663803100586
Test Loss Energy: 9.61589228736431, Test Loss Force: 11.218464756589764, time: 10.026988744735718

Epoch 17, Batch 100/212, Loss: 0.7729842662811279, Variance: 0.1433401107788086
Epoch 17, Batch 200/212, Loss: 0.9999269247055054, Variance: 0.14150261878967285

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.7078426754727123, Training Loss Force: 3.3577433688548757, time: 3.2682323455810547
Validation Loss Energy: 3.665602860523566, Validation Loss Force: 3.4279422916770437, time: 0.18159151077270508
Test Loss Energy: 9.829263352661679, Test Loss Force: 11.24319379038007, time: 10.160886764526367

Epoch 18, Batch 100/212, Loss: 1.1579370498657227, Variance: 0.14630955457687378
Epoch 18, Batch 200/212, Loss: 0.655707061290741, Variance: 0.14286240935325623

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.665566602718105, Training Loss Force: 3.345687311361467, time: 3.3811423778533936
Validation Loss Energy: 2.5402218078799432, Validation Loss Force: 3.419526956644995, time: 0.18665313720703125
Test Loss Energy: 9.381389766231868, Test Loss Force: 11.067855011581562, time: 9.99012804031372

Epoch 19, Batch 100/212, Loss: 0.7795292139053345, Variance: 0.1382347196340561
Epoch 19, Batch 200/212, Loss: 1.3348568677902222, Variance: 0.14618578553199768

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.684968578355241, Training Loss Force: 3.363338013071752, time: 3.2735674381256104
Validation Loss Energy: 2.1923007663621963, Validation Loss Force: 3.4432045916117993, time: 0.18465542793273926
Test Loss Energy: 9.586680226513867, Test Loss Force: 11.165841807334171, time: 9.988609790802002

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–â–…â–…â–…â–ˆâ–…â–…â–ƒâ–ˆâ–‡â–…â–…â–‚â–„â–…â–„â–‡â–â–„
wandb:   test_error_force â–„â–ƒâ–‡â–…â–ˆâ–ˆâ–ƒâ–ƒâ–†â–„â–ˆâ–…â–ƒâ–„â–†â–â–…â–…â–ƒâ–„
wandb:          test_loss â–…â–‚â–‡â–†â–‡â–ˆâ–„â–ƒâ–…â–†â–‡â–…â–ƒâ–‚â–…â–ƒâ–„â–„â–â–‚
wandb: train_error_energy â–ˆâ–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–‚â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–‚â–…â–ƒâ–‚â–‡â–ƒâ–‚â–…â–„â–â–†â–ƒâ–â–…â–„â–‚â–ˆâ–ƒâ–
wandb:  valid_error_force â–â–â–â–‚â–…â–ƒâ–ƒâ–â–‚â–†â–ƒâ–…â–„â–„â–ˆâ–†â–â–‚â–‚â–ƒ
wandb:         valid_loss â–‚â–â–…â–‚â–‚â–‡â–‚â–â–…â–„â–â–†â–ƒâ–â–†â–„â–‚â–ˆâ–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 6757
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.58668
wandb:   test_error_force 11.16584
wandb:          test_loss 9.29414
wandb: train_error_energy 2.68497
wandb:  train_error_force 3.36334
wandb:         train_loss 0.96541
wandb: valid_error_energy 2.1923
wandb:  valid_error_force 3.4432
wandb:         valid_loss 0.80488
wandb: 
wandb: ğŸš€ View run al_63_67 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jzxfjrkp
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_124429-jzxfjrkp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.929786443710327, Uncertainty Bias: -0.19457121193408966
3.0517578e-05 0.014602661
1.9582391 5.4322257
(48745, 22, 3)
Found uncertainty sample 0 after 520 steps.
Found uncertainty sample 1 after 359 steps.
Found uncertainty sample 2 after 2314 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 1647 steps.
Found uncertainty sample 5 after 449 steps.
Found uncertainty sample 6 after 238 steps.
Found uncertainty sample 7 after 2727 steps.
Found uncertainty sample 8 after 1362 steps.
Found uncertainty sample 9 after 1387 steps.
Found uncertainty sample 10 after 3533 steps.
Found uncertainty sample 11 after 206 steps.
Found uncertainty sample 12 after 541 steps.
Found uncertainty sample 13 after 634 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1592 steps.
Found uncertainty sample 16 after 2197 steps.
Found uncertainty sample 17 after 1447 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 1915 steps.
Found uncertainty sample 20 after 439 steps.
Found uncertainty sample 21 after 3 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 196 steps.
Found uncertainty sample 24 after 57 steps.
Found uncertainty sample 25 after 3289 steps.
Found uncertainty sample 26 after 21 steps.
Found uncertainty sample 27 after 48 steps.
Found uncertainty sample 28 after 1582 steps.
Found uncertainty sample 29 after 2641 steps.
Found uncertainty sample 30 after 184 steps.
Found uncertainty sample 31 after 536 steps.
Found uncertainty sample 32 after 1659 steps.
Found uncertainty sample 33 after 1728 steps.
Found uncertainty sample 34 after 3426 steps.
Found uncertainty sample 35 after 3595 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 130 steps.
Found uncertainty sample 38 after 2282 steps.
Found uncertainty sample 39 after 1812 steps.
Found uncertainty sample 40 after 2027 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 985 steps.
Found uncertainty sample 43 after 1781 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 157 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 168 steps.
Found uncertainty sample 49 after 165 steps.
Found uncertainty sample 50 after 104 steps.
Found uncertainty sample 51 after 2734 steps.
Found uncertainty sample 52 after 3319 steps.
Found uncertainty sample 53 after 2373 steps.
Found uncertainty sample 54 after 1276 steps.
Found uncertainty sample 55 after 811 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 2994 steps.
Found uncertainty sample 58 after 759 steps.
Found uncertainty sample 59 after 1038 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 393 steps.
Found uncertainty sample 62 after 2244 steps.
Found uncertainty sample 63 after 737 steps.
Found uncertainty sample 64 after 296 steps.
Found uncertainty sample 65 after 1375 steps.
Found uncertainty sample 66 after 14 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 60 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 2360 steps.
Found uncertainty sample 71 after 2680 steps.
Found uncertainty sample 72 after 14 steps.
Found uncertainty sample 73 after 878 steps.
Found uncertainty sample 74 after 1454 steps.
Found uncertainty sample 75 after 1810 steps.
Found uncertainty sample 76 after 32 steps.
Found uncertainty sample 77 after 302 steps.
Found uncertainty sample 78 after 3686 steps.
Found uncertainty sample 79 after 353 steps.
Found uncertainty sample 80 after 429 steps.
Found uncertainty sample 81 after 306 steps.
Found uncertainty sample 82 after 3397 steps.
Found uncertainty sample 83 after 22 steps.
Found uncertainty sample 84 after 1515 steps.
Found uncertainty sample 85 after 991 steps.
Found uncertainty sample 86 after 489 steps.
Found uncertainty sample 87 after 276 steps.
Found uncertainty sample 88 after 56 steps.
Found uncertainty sample 89 after 3117 steps.
Found uncertainty sample 90 after 197 steps.
Found uncertainty sample 91 after 2019 steps.
Found uncertainty sample 92 after 230 steps.
Found uncertainty sample 93 after 1917 steps.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 1613 steps.
Found uncertainty sample 97 after 1597 steps.
Found uncertainty sample 98 after 459 steps.
Found uncertainty sample 99 after 58 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_130520-bco6i7gl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_68
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/bco6i7gl
Training model 68. Added 88 samples to the dataset.
Epoch 0, Batch 100/214, Loss: 4.563784599304199, Variance: 0.1196158304810524
Epoch 0, Batch 200/214, Loss: 1.2152931690216064, Variance: 0.13113653659820557

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1636628786886725, Training Loss Force: 3.643353181979396, time: 3.3552098274230957
Validation Loss Energy: 3.2096284985763166, Validation Loss Force: 3.3772148645302185, time: 0.18365073204040527
Test Loss Energy: 9.658414544470086, Test Loss Force: 11.363804247008213, time: 9.831367492675781

Epoch 1, Batch 100/214, Loss: 1.2819223403930664, Variance: 0.14090940356254578
Epoch 1, Batch 200/214, Loss: 0.5578601956367493, Variance: 0.1363120973110199

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.608704790522758, Training Loss Force: 3.3135992233963987, time: 3.2689566612243652
Validation Loss Energy: 2.30632974732639, Validation Loss Force: 3.4083048213612397, time: 0.1819000244140625
Test Loss Energy: 9.454037997637853, Test Loss Force: 11.234672496311347, time: 9.748162984848022

Epoch 2, Batch 100/214, Loss: 0.5955088138580322, Variance: 0.13319584727287292
Epoch 2, Batch 200/214, Loss: 0.8151412010192871, Variance: 0.1400195062160492

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6324384970141903, Training Loss Force: 3.3251052425917766, time: 3.443915367126465
Validation Loss Energy: 2.8483528193642647, Validation Loss Force: 3.3721627799455494, time: 0.181884765625
Test Loss Energy: 9.665048718692875, Test Loss Force: 11.114169154034657, time: 9.78189730644226

Epoch 3, Batch 100/214, Loss: 0.8573997020721436, Variance: 0.13622188568115234
Epoch 3, Batch 200/214, Loss: 1.3381870985031128, Variance: 0.1425422728061676

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6369954803946736, Training Loss Force: 3.331027884978517, time: 3.1861610412597656
Validation Loss Energy: 3.439648398349473, Validation Loss Force: 3.419720790136525, time: 0.18206405639648438
Test Loss Energy: 9.748284445965572, Test Loss Force: 11.389747958476656, time: 10.016935110092163

Epoch 4, Batch 100/214, Loss: 1.3544858694076538, Variance: 0.14580833911895752
Epoch 4, Batch 200/214, Loss: 0.7097129821777344, Variance: 0.13875648379325867

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.665602186688633, Training Loss Force: 3.340843917151493, time: 3.416572093963623
Validation Loss Energy: 2.0995573970646784, Validation Loss Force: 3.3787478302653584, time: 0.20458626747131348
Test Loss Energy: 9.568077438805771, Test Loss Force: 11.240773518992292, time: 12.023801326751709

Epoch 5, Batch 100/214, Loss: 0.5703483819961548, Variance: 0.137924462556839
Epoch 5, Batch 200/214, Loss: 0.7001203298568726, Variance: 0.14246788620948792

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6539226348726204, Training Loss Force: 3.344323611596329, time: 3.375308036804199
Validation Loss Energy: 2.8141806205480244, Validation Loss Force: 3.4092187112034136, time: 0.20109272003173828
Test Loss Energy: 9.7101821271058, Test Loss Force: 11.179727753720831, time: 10.837219476699829

Epoch 6, Batch 100/214, Loss: 0.7227623462677002, Variance: 0.14010342955589294
Epoch 6, Batch 200/214, Loss: 1.353065013885498, Variance: 0.1462116241455078

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.676171340539914, Training Loss Force: 3.3452994643309073, time: 3.333362579345703
Validation Loss Energy: 2.9716719467675703, Validation Loss Force: 3.6002059343757313, time: 0.2079908847808838
Test Loss Energy: 9.474854004738992, Test Loss Force: 11.31691493732731, time: 10.984198808670044

Epoch 7, Batch 100/214, Loss: 1.1857819557189941, Variance: 0.1429000198841095
Epoch 7, Batch 200/214, Loss: 0.7604043483734131, Variance: 0.1423206329345703

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6649196906689685, Training Loss Force: 3.3608760607246295, time: 3.5198495388031006
Validation Loss Energy: 2.3806803668338397, Validation Loss Force: 3.453382909606731, time: 0.20283722877502441
Test Loss Energy: 9.67485225128971, Test Loss Force: 11.219422471741892, time: 10.84155797958374

Epoch 8, Batch 100/214, Loss: 0.5574169158935547, Variance: 0.13807973265647888
Epoch 8, Batch 200/214, Loss: 0.6908766031265259, Variance: 0.1323816031217575

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.691971771450796, Training Loss Force: 3.3553036113523036, time: 3.4593427181243896
Validation Loss Energy: 2.4225120502662776, Validation Loss Force: 3.453683679955593, time: 0.20756959915161133
Test Loss Energy: 10.051403940327386, Test Loss Force: 11.193143777276635, time: 10.995178699493408

Epoch 9, Batch 100/214, Loss: 0.77854984998703, Variance: 0.13905933499336243
Epoch 9, Batch 200/214, Loss: 1.2380824089050293, Variance: 0.14726701378822327

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6740708312114405, Training Loss Force: 3.341072106203118, time: 3.4242117404937744
Validation Loss Energy: 3.307638048842314, Validation Loss Force: 3.421847336147422, time: 0.20802927017211914
Test Loss Energy: 9.647029160934121, Test Loss Force: 11.220886925911412, time: 10.903860330581665

Epoch 10, Batch 100/214, Loss: 1.5345370769500732, Variance: 0.14426565170288086
Epoch 10, Batch 200/214, Loss: 0.6077254414558411, Variance: 0.13400337100028992

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6524384716590546, Training Loss Force: 3.35665505282954, time: 3.4548699855804443
Validation Loss Energy: 1.9770508172085062, Validation Loss Force: 3.38702861746598, time: 0.18950223922729492
Test Loss Energy: 9.396405500890754, Test Loss Force: 11.10510152081478, time: 10.981955766677856

Epoch 11, Batch 100/214, Loss: 0.7830855250358582, Variance: 0.1376166045665741
Epoch 11, Batch 200/214, Loss: 0.9296612739562988, Variance: 0.1385408192873001

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.674543260222377, Training Loss Force: 3.357493395920898, time: 3.491398334503174
Validation Loss Energy: 2.857462826464322, Validation Loss Force: 3.3539604122752498, time: 0.20586752891540527
Test Loss Energy: 9.62098809058759, Test Loss Force: 10.895145463425893, time: 10.85110592842102

Epoch 12, Batch 100/214, Loss: 0.7851719856262207, Variance: 0.14124508202075958
Epoch 12, Batch 200/214, Loss: 1.5231702327728271, Variance: 0.15146282315254211

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.699471467023059, Training Loss Force: 3.3443576373147446, time: 3.458972454071045
Validation Loss Energy: 3.3944147200382644, Validation Loss Force: 3.4091667736251265, time: 0.2043595314025879
Test Loss Energy: 9.483640505921162, Test Loss Force: 11.268764728050476, time: 10.957891464233398

Epoch 13, Batch 100/214, Loss: 1.4461102485656738, Variance: 0.14643675088882446
Epoch 13, Batch 200/214, Loss: 0.7721977233886719, Variance: 0.1428888738155365

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6429951220842294, Training Loss Force: 3.338008699010166, time: 3.4692633152008057
Validation Loss Energy: 2.3298245709243868, Validation Loss Force: 3.435148811583137, time: 0.20639634132385254
Test Loss Energy: 9.478764965618069, Test Loss Force: 11.116633511581712, time: 10.851311206817627

Epoch 14, Batch 100/214, Loss: 0.606484055519104, Variance: 0.14052695035934448
Epoch 14, Batch 200/214, Loss: 0.6402069926261902, Variance: 0.13967356085777283

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6763062634752064, Training Loss Force: 3.3558397435086658, time: 3.3465311527252197
Validation Loss Energy: 2.9925741221649576, Validation Loss Force: 3.3824665163369247, time: 0.18975591659545898
Test Loss Energy: 9.685996718467848, Test Loss Force: 11.310694904399806, time: 11.06814694404602

Epoch 15, Batch 100/214, Loss: 0.810435950756073, Variance: 0.13847142457962036
Epoch 15, Batch 200/214, Loss: 1.3869550228118896, Variance: 0.1422404944896698

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6588553167695705, Training Loss Force: 3.3507092393249938, time: 3.448333740234375
Validation Loss Energy: 3.3523844157276597, Validation Loss Force: 3.4410643514484276, time: 0.1905679702758789
Test Loss Energy: 9.85290796672679, Test Loss Force: 11.472967215227401, time: 10.902337074279785

Epoch 16, Batch 100/214, Loss: 1.2272322177886963, Variance: 0.14669455587863922
Epoch 16, Batch 200/214, Loss: 0.6942682266235352, Variance: 0.14000555872917175

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6777929931067557, Training Loss Force: 3.352205041163474, time: 3.304720401763916
Validation Loss Energy: 2.220608360880277, Validation Loss Force: 3.430953390870139, time: 0.19284868240356445
Test Loss Energy: 9.4056495533594, Test Loss Force: 11.145527922654518, time: 11.094852447509766

Epoch 17, Batch 100/214, Loss: 0.7324005365371704, Variance: 0.14014071226119995
Epoch 17, Batch 200/214, Loss: 0.7267158031463623, Variance: 0.14640335738658905

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6870221334024347, Training Loss Force: 3.3515782025454466, time: 3.419080972671509
Validation Loss Energy: 2.981199903854534, Validation Loss Force: 3.418602267349243, time: 0.18900108337402344
Test Loss Energy: 9.618591834824977, Test Loss Force: 11.128738962834934, time: 10.98906397819519

Epoch 18, Batch 100/214, Loss: 0.9760943651199341, Variance: 0.1416294425725937
Epoch 18, Batch 200/214, Loss: 1.3664765357971191, Variance: 0.13873934745788574

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.664345424860601, Training Loss Force: 3.3478125041498075, time: 3.4139537811279297
Validation Loss Energy: 3.4563747479187037, Validation Loss Force: 3.3859896988983675, time: 0.19002199172973633
Test Loss Energy: 9.725675828664166, Test Loss Force: 11.260495192345115, time: 11.146894931793213

Epoch 19, Batch 100/214, Loss: 1.4012866020202637, Variance: 0.14632101356983185
Epoch 19, Batch 200/214, Loss: 0.5252969264984131, Variance: 0.1404251605272293

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.688446004782699, Training Loss Force: 3.347387207121596, time: 3.3846962451934814
Validation Loss Energy: 2.336556415878649, Validation Loss Force: 3.376231668916374, time: 0.1978921890258789
Test Loss Energy: 9.443422914113182, Test Loss Force: 11.052859011704365, time: 11.048276662826538

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–‚â–„â–…â–ƒâ–„â–‚â–„â–ˆâ–„â–â–ƒâ–‚â–‚â–„â–†â–â–ƒâ–…â–‚
wandb:   test_error_force â–‡â–…â–„â–‡â–…â–„â–†â–…â–…â–…â–„â–â–†â–„â–†â–ˆâ–„â–„â–…â–ƒ
wandb:          test_loss â–ˆâ–‚â–ƒâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–†â–„â–ƒâ–â–‚â–â–ƒâ–†â–â–â–…â–
wandb: train_error_energy â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–ƒâ–…â–ˆâ–‚â–…â–†â–ƒâ–ƒâ–‡â–â–…â–ˆâ–ƒâ–†â–ˆâ–‚â–†â–ˆâ–ƒ
wandb:  valid_error_force â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–ˆâ–„â–„â–ƒâ–‚â–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚
wandb:         valid_loss â–‡â–‚â–…â–ˆâ–â–…â–†â–ƒâ–ƒâ–‡â–â–…â–ˆâ–ƒâ–…â–ˆâ–‚â–…â–ˆâ–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 6836
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.44342
wandb:   test_error_force 11.05286
wandb:          test_loss 9.18667
wandb: train_error_energy 2.68845
wandb:  train_error_force 3.34739
wandb:         train_loss 0.96374
wandb: valid_error_energy 2.33656
wandb:  valid_error_force 3.37623
wandb:         valid_loss 0.8236
wandb: 
wandb: ğŸš€ View run al_63_68 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/bco6i7gl
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_130520-bco6i7gl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.9851698875427246, Uncertainty Bias: -0.2002871036529541
4.196167e-05 0.011669159
1.889088 5.5155993
(48745, 22, 3)
Found uncertainty sample 0 after 362 steps.
Found uncertainty sample 1 after 359 steps.
Found uncertainty sample 2 after 1117 steps.
Found uncertainty sample 3 after 21 steps.
Found uncertainty sample 4 after 1915 steps.
Found uncertainty sample 5 after 3308 steps.
Found uncertainty sample 6 after 2499 steps.
Found uncertainty sample 7 after 845 steps.
Found uncertainty sample 8 after 1952 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 751 steps.
Found uncertainty sample 11 after 3468 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 3071 steps.
Found uncertainty sample 14 after 725 steps.
Found uncertainty sample 15 after 633 steps.
Found uncertainty sample 16 after 1336 steps.
Found uncertainty sample 17 after 1606 steps.
Found uncertainty sample 18 after 3242 steps.
Found uncertainty sample 19 after 1334 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 812 steps.
Found uncertainty sample 22 after 1186 steps.
Found uncertainty sample 23 after 3155 steps.
Found uncertainty sample 24 after 609 steps.
Found uncertainty sample 25 after 1309 steps.
Found uncertainty sample 26 after 465 steps.
Found uncertainty sample 27 after 621 steps.
Found uncertainty sample 28 after 58 steps.
Found uncertainty sample 29 after 2315 steps.
Found uncertainty sample 30 after 2146 steps.
Found uncertainty sample 31 after 585 steps.
Found uncertainty sample 32 after 223 steps.
Found uncertainty sample 33 after 2501 steps.
Found uncertainty sample 34 after 452 steps.
Found uncertainty sample 35 after 298 steps.
Found uncertainty sample 36 after 1685 steps.
Found uncertainty sample 37 after 2129 steps.
Found uncertainty sample 38 after 2015 steps.
Found uncertainty sample 39 after 1711 steps.
Found uncertainty sample 40 after 1491 steps.
Found uncertainty sample 41 after 824 steps.
Found uncertainty sample 42 after 355 steps.
Found uncertainty sample 43 after 1494 steps.
Found uncertainty sample 44 after 240 steps.
Found uncertainty sample 45 after 546 steps.
Found uncertainty sample 46 after 797 steps.
Found uncertainty sample 47 after 3961 steps.
Found uncertainty sample 48 after 1446 steps.
Found uncertainty sample 49 after 987 steps.
Found uncertainty sample 50 after 2303 steps.
Found uncertainty sample 51 after 500 steps.
Found uncertainty sample 52 after 716 steps.
Found uncertainty sample 53 after 1105 steps.
Found uncertainty sample 54 after 123 steps.
Found uncertainty sample 55 after 868 steps.
Found uncertainty sample 56 after 552 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 839 steps.
Found uncertainty sample 59 after 2856 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 50 steps.
Found uncertainty sample 62 after 288 steps.
Found uncertainty sample 63 after 1246 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 3961 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 51 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 1761 steps.
Did not find any uncertainty samples for sample 70.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 1676 steps.
Found uncertainty sample 73 after 2475 steps.
Found uncertainty sample 74 after 1800 steps.
Found uncertainty sample 75 after 674 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 3324 steps.
Found uncertainty sample 78 after 309 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 3478 steps.
Found uncertainty sample 81 after 3138 steps.
Found uncertainty sample 82 after 20 steps.
Found uncertainty sample 83 after 498 steps.
Found uncertainty sample 84 after 500 steps.
Found uncertainty sample 85 after 2729 steps.
Found uncertainty sample 86 after 679 steps.
Found uncertainty sample 87 after 1813 steps.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 2038 steps.
Did not find any uncertainty samples for sample 92.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 1214 steps.
Found uncertainty sample 95 after 1131 steps.
Found uncertainty sample 96 after 133 steps.
Found uncertainty sample 97 after 2573 steps.
Found uncertainty sample 98 after 2027 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_132927-wyo36t0b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_69
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/wyo36t0b
Training model 69. Added 82 samples to the dataset.
Epoch 0, Batch 100/216, Loss: 0.621631920337677, Variance: 0.12377573549747467
Epoch 0, Batch 200/216, Loss: 0.4412626028060913, Variance: 0.10945411026477814

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.5493294717995894, Training Loss Force: 3.7566746752218685, time: 3.4099998474121094
Validation Loss Energy: 1.500823082087817, Validation Loss Force: 3.4111049972058547, time: 0.1793363094329834
Test Loss Energy: 9.127089001539773, Test Loss Force: 11.262882928389828, time: 9.624810218811035

Epoch 1, Batch 100/216, Loss: 0.6700034141540527, Variance: 0.10248994827270508
Epoch 1, Batch 200/216, Loss: 0.4838030934333801, Variance: 0.10130402445793152

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.634647336208169, Training Loss Force: 3.317935361847355, time: 3.243377447128296
Validation Loss Energy: 1.51109759315379, Validation Loss Force: 3.39657277373331, time: 0.17980217933654785
Test Loss Energy: 8.924881494676741, Test Loss Force: 11.265757789914181, time: 10.734752893447876

Epoch 2, Batch 100/216, Loss: 0.4803125858306885, Variance: 0.0970640480518341
Epoch 2, Batch 200/216, Loss: 0.47112196683883667, Variance: 0.09859061241149902

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6254902778805265, Training Loss Force: 3.3246924787188044, time: 3.3952841758728027
Validation Loss Energy: 1.286144195792502, Validation Loss Force: 3.42521915935692, time: 0.18166112899780273
Test Loss Energy: 9.004634086608766, Test Loss Force: 11.386912580505365, time: 9.583117485046387

Epoch 3, Batch 100/216, Loss: 0.5834078788757324, Variance: 0.09852606058120728
Epoch 3, Batch 200/216, Loss: 0.6081989407539368, Variance: 0.10265296697616577

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6516579827329654, Training Loss Force: 3.332049625024237, time: 3.241375684738159
Validation Loss Energy: 1.54417395360199, Validation Loss Force: 3.370766644400582, time: 0.18633604049682617
Test Loss Energy: 9.16494132939967, Test Loss Force: 11.438967268774094, time: 9.644254446029663

Epoch 4, Batch 100/216, Loss: 0.5993894338607788, Variance: 0.09882374852895737
Epoch 4, Batch 200/216, Loss: 0.5945174098014832, Variance: 0.09928219020366669

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.653618805685692, Training Loss Force: 3.326014487086567, time: 3.2589075565338135
Validation Loss Energy: 1.3765390457742765, Validation Loss Force: 3.402814857901104, time: 0.1803891658782959
Test Loss Energy: 9.134156902616231, Test Loss Force: 11.42661921307444, time: 9.734260320663452

Epoch 5, Batch 100/216, Loss: 0.5690667629241943, Variance: 0.09956314414739609
Epoch 5, Batch 200/216, Loss: 0.47939735651016235, Variance: 0.09512141346931458

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6519328059410727, Training Loss Force: 3.3442661388611645, time: 3.2811214923858643
Validation Loss Energy: 1.4273241998971837, Validation Loss Force: 3.4323057162606445, time: 0.1802692413330078
Test Loss Energy: 9.092338443224577, Test Loss Force: 11.35323831857186, time: 9.585692405700684

Epoch 6, Batch 100/216, Loss: 0.6561459302902222, Variance: 0.09604287892580032
Epoch 6, Batch 200/216, Loss: 0.6655516028404236, Variance: 0.09770365804433823

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6467681378775585, Training Loss Force: 3.338539821380507, time: 3.2818570137023926
Validation Loss Energy: 1.4199938202786677, Validation Loss Force: 3.4522016497875843, time: 0.18070459365844727
Test Loss Energy: 9.039865577636569, Test Loss Force: 11.355504969233474, time: 9.754939079284668

Epoch 7, Batch 100/216, Loss: 0.4041154980659485, Variance: 0.09333064407110214
Epoch 7, Batch 200/216, Loss: 0.3855358958244324, Variance: 0.09745509177446365

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.649869648733187, Training Loss Force: 3.3501433579849667, time: 3.206794500350952
Validation Loss Energy: 1.5156114585791085, Validation Loss Force: 3.4231386501919436, time: 0.18077540397644043
Test Loss Energy: 8.90506437436939, Test Loss Force: 11.364300642985487, time: 9.606242656707764

Epoch 8, Batch 100/216, Loss: 0.42949771881103516, Variance: 0.09620551764965057
Epoch 8, Batch 200/216, Loss: 0.624826967716217, Variance: 0.09568452835083008

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6615645624152495, Training Loss Force: 3.3417519619478555, time: 3.3184659481048584
Validation Loss Energy: 1.3872187590185514, Validation Loss Force: 3.379883943915168, time: 0.1897735595703125
Test Loss Energy: 9.013728857485077, Test Loss Force: 11.421648524263421, time: 9.752814769744873

Epoch 9, Batch 100/216, Loss: 0.5929566621780396, Variance: 0.09418891370296478
Epoch 9, Batch 200/216, Loss: 0.4535045623779297, Variance: 0.09425238519906998

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6603317848419337, Training Loss Force: 3.3444581631658843, time: 3.3804287910461426
Validation Loss Energy: 1.3994691804182011, Validation Loss Force: 3.38702861746598, time: 0.18140745162963867
Test Loss Energy: 8.745092219497929, Test Loss Force: 11.192704664394409, time: 9.530014753341675

Epoch 10, Batch 100/216, Loss: 0.3721103072166443, Variance: 0.09570489078760147
Epoch 10, Batch 200/216, Loss: 0.4779784679412842, Variance: 0.09378384798765182

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.665595335788845, Training Loss Force: 3.3466305274770582, time: 3.238302230834961
Validation Loss Energy: 1.6962578431136097, Validation Loss Force: 3.444753255521727, time: 0.17998933792114258
Test Loss Energy: 9.025486553159487, Test Loss Force: 11.42983400994394, time: 9.571552276611328

Epoch 11, Batch 100/216, Loss: 0.6146933436393738, Variance: 0.09363530576229095
Epoch 11, Batch 200/216, Loss: 0.6212836503982544, Variance: 0.09565277397632599

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6639639106024917, Training Loss Force: 3.3401095893772657, time: 3.3376998901367188
Validation Loss Energy: 1.524796254678554, Validation Loss Force: 3.4384590667737864, time: 0.1783590316772461
Test Loss Energy: 8.994513846206384, Test Loss Force: 11.706727490329328, time: 9.727290868759155

Epoch 12, Batch 100/216, Loss: 0.6944741010665894, Variance: 0.09682472050189972
Epoch 12, Batch 200/216, Loss: 0.6050257086753845, Variance: 0.09405647218227386

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6649986907197047, Training Loss Force: 3.36727354588398, time: 3.3737449645996094
Validation Loss Energy: 1.4833236530775684, Validation Loss Force: 3.451263934346796, time: 0.17935705184936523
Test Loss Energy: 8.883877123009055, Test Loss Force: 11.333893727839923, time: 9.58495545387268

Epoch 13, Batch 100/216, Loss: 0.5370427966117859, Variance: 0.09047496318817139
Epoch 13, Batch 200/216, Loss: 0.7072435617446899, Variance: 0.09279342740774155

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6601771918907138, Training Loss Force: 3.3415222395505557, time: 3.3537065982818604
Validation Loss Energy: 1.5055624000233874, Validation Loss Force: 3.4631619261557285, time: 0.18092131614685059
Test Loss Energy: 8.696297782915453, Test Loss Force: 11.396871529171214, time: 9.720989227294922

Epoch 14, Batch 100/216, Loss: 0.7057096362113953, Variance: 0.0925324335694313
Epoch 14, Batch 200/216, Loss: 0.7077520489692688, Variance: 0.0960143655538559

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6575853493194441, Training Loss Force: 3.355520852908896, time: 3.409057378768921
Validation Loss Energy: 1.5821235674770358, Validation Loss Force: 3.4472914611161576, time: 0.1824510097503662
Test Loss Energy: 8.898197159981185, Test Loss Force: 11.514659408389436, time: 9.590863227844238

Epoch 15, Batch 100/216, Loss: 0.4968579411506653, Variance: 0.0981283187866211
Epoch 15, Batch 200/216, Loss: 0.6492780447006226, Variance: 0.08883368223905563

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6701766457398013, Training Loss Force: 3.34285874615382, time: 3.2494006156921387
Validation Loss Energy: 1.4445404494210987, Validation Loss Force: 3.396510983045315, time: 0.1795210838317871
Test Loss Energy: 8.821584930470342, Test Loss Force: 11.23487419025689, time: 9.77565312385559

Epoch 16, Batch 100/216, Loss: 0.4593389630317688, Variance: 0.09447304904460907
Epoch 16, Batch 200/216, Loss: 0.8624244928359985, Variance: 0.09557893872261047

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.676181199657928, Training Loss Force: 3.3550361289809785, time: 3.3069875240325928
Validation Loss Energy: 1.5472602314654789, Validation Loss Force: 3.390530368122127, time: 0.17917251586914062
Test Loss Energy: 8.8053456486733, Test Loss Force: 11.445748003595572, time: 9.587085008621216

Epoch 17, Batch 100/216, Loss: 0.6514033675193787, Variance: 0.0862380713224411
Epoch 17, Batch 200/216, Loss: 0.4238715171813965, Variance: 0.09402620047330856

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6533270214800946, Training Loss Force: 3.3491858847809794, time: 3.2952566146850586
Validation Loss Energy: 1.4863842961556324, Validation Loss Force: 3.4437711732536123, time: 0.18091893196105957
Test Loss Energy: 8.935940153793013, Test Loss Force: 11.400074425482378, time: 10.551805257797241

Epoch 18, Batch 100/216, Loss: 0.561373233795166, Variance: 0.09333785623311996
Epoch 18, Batch 200/216, Loss: 0.4990966320037842, Variance: 0.0918702483177185

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6480393926277472, Training Loss Force: 3.3443088562874097, time: 3.458333969116211
Validation Loss Energy: 1.3376356564462921, Validation Loss Force: 3.5557194223836834, time: 0.1847989559173584
Test Loss Energy: 8.984650258225345, Test Loss Force: 11.585269664517643, time: 9.635574340820312

Epoch 19, Batch 100/216, Loss: 0.5495256781578064, Variance: 0.09123106300830841
Epoch 19, Batch 200/216, Loss: 0.40533924102783203, Variance: 0.09095505625009537

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6550711285939192, Training Loss Force: 3.3455787054386144, time: 3.395913600921631
Validation Loss Energy: 1.5956782733989576, Validation Loss Force: 3.419373537603451, time: 0.18273258209228516
Test Loss Energy: 9.037435870206556, Test Loss Force: 11.545137493723589, time: 9.609875440597534

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–„â–†â–ˆâ–ˆâ–‡â–†â–„â–†â–‚â–†â–…â–„â–â–„â–ƒâ–ƒâ–…â–…â–†
wandb:   test_error_force â–‚â–‚â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–â–„â–ˆâ–ƒâ–„â–…â–‚â–„â–„â–†â–†
wandb:          test_loss â–â–‚â–„â–‡â–†â–…â–†â–ƒâ–„â–‚â–ˆâ–‡â–‚â–ƒâ–ƒâ–…â–ƒâ–„â–†â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–‚â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–…â–â–…â–ƒâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–ˆâ–…â–„â–…â–†â–„â–…â–„â–‚â–†
wandb:  valid_error_force â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–„â–ƒâ–â–‚â–„â–„â–„â–„â–„â–‚â–‚â–„â–ˆâ–ƒ
wandb:         valid_loss â–…â–„â–â–„â–‚â–ƒâ–ƒâ–…â–‚â–‚â–ˆâ–…â–„â–…â–†â–ƒâ–„â–„â–ƒâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 6909
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.03744
wandb:   test_error_force 11.54514
wandb:          test_loss 11.91432
wandb: train_error_energy 1.65507
wandb:  train_error_force 3.34558
wandb:         train_loss 0.51992
wandb: valid_error_energy 1.59568
wandb:  valid_error_force 3.41937
wandb:         valid_loss 0.54137
wandb: 
wandb: ğŸš€ View run al_63_69 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/wyo36t0b
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_132927-wyo36t0b/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.7117419242858887, Uncertainty Bias: -0.10575221478939056
2.0980835e-05 0.0025749207
1.9826485 6.068395
(48745, 22, 3)
Found uncertainty sample 0 after 2255 steps.
Found uncertainty sample 1 after 415 steps.
Found uncertainty sample 2 after 1704 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 58 steps.
Found uncertainty sample 5 after 837 steps.
Found uncertainty sample 6 after 1267 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 1583 steps.
Found uncertainty sample 10 after 2556 steps.
Found uncertainty sample 11 after 1870 steps.
Found uncertainty sample 12 after 1460 steps.
Found uncertainty sample 13 after 58 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 116 steps.
Found uncertainty sample 16 after 2509 steps.
Found uncertainty sample 17 after 1159 steps.
Found uncertainty sample 18 after 210 steps.
Found uncertainty sample 19 after 405 steps.
Found uncertainty sample 20 after 2365 steps.
Found uncertainty sample 21 after 2370 steps.
Found uncertainty sample 22 after 1486 steps.
Found uncertainty sample 23 after 985 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 2070 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 52 steps.
Found uncertainty sample 29 after 484 steps.
Found uncertainty sample 30 after 879 steps.
Found uncertainty sample 31 after 1117 steps.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 12 steps.
Found uncertainty sample 35 after 210 steps.
Found uncertainty sample 36 after 1753 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 1008 steps.
Found uncertainty sample 39 after 1198 steps.
Found uncertainty sample 40 after 2349 steps.
Found uncertainty sample 41 after 1334 steps.
Found uncertainty sample 42 after 1934 steps.
Found uncertainty sample 43 after 290 steps.
Found uncertainty sample 44 after 3188 steps.
Found uncertainty sample 45 after 683 steps.
Found uncertainty sample 46 after 59 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 131 steps.
Found uncertainty sample 49 after 334 steps.
Found uncertainty sample 50 after 35 steps.
Found uncertainty sample 51 after 2906 steps.
Found uncertainty sample 52 after 1253 steps.
Found uncertainty sample 53 after 555 steps.
Found uncertainty sample 54 after 821 steps.
Found uncertainty sample 55 after 420 steps.
Found uncertainty sample 56 after 716 steps.
Found uncertainty sample 57 after 2105 steps.
Found uncertainty sample 58 after 3296 steps.
Found uncertainty sample 59 after 2394 steps.
Found uncertainty sample 60 after 207 steps.
Found uncertainty sample 61 after 2973 steps.
Found uncertainty sample 62 after 296 steps.
Found uncertainty sample 63 after 1499 steps.
Found uncertainty sample 64 after 2377 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 41 steps.
Found uncertainty sample 67 after 53 steps.
Found uncertainty sample 68 after 716 steps.
Found uncertainty sample 69 after 2355 steps.
Found uncertainty sample 70 after 567 steps.
Found uncertainty sample 71 after 9 steps.
Found uncertainty sample 72 after 256 steps.
Found uncertainty sample 73 after 100 steps.
Found uncertainty sample 74 after 1470 steps.
Found uncertainty sample 75 after 1417 steps.
Found uncertainty sample 76 after 2136 steps.
Found uncertainty sample 77 after 41 steps.
Did not find any uncertainty samples for sample 78.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 719 steps.
Found uncertainty sample 82 after 1077 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 1679 steps.
Found uncertainty sample 85 after 1183 steps.
Found uncertainty sample 86 after 1167 steps.
Found uncertainty sample 87 after 1604 steps.
Found uncertainty sample 88 after 142 steps.
Found uncertainty sample 89 after 2080 steps.
Found uncertainty sample 90 after 705 steps.
Found uncertainty sample 91 after 1510 steps.
Found uncertainty sample 92 after 2956 steps.
Did not find any uncertainty samples for sample 93.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 1288 steps.
Found uncertainty sample 96 after 1242 steps.
Found uncertainty sample 97 after 3693 steps.
Found uncertainty sample 98 after 329 steps.
Found uncertainty sample 99 after 140 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_135118-nnpb1drk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_70
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/nnpb1drk
Training model 70. Added 83 samples to the dataset.
Epoch 0, Batch 100/219, Loss: 6.748265266418457, Variance: 0.10168105363845825
Epoch 0, Batch 200/219, Loss: 0.5233530402183533, Variance: 0.11387184262275696

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.8416787700694126, Training Loss Force: 3.8552679378544945, time: 3.494171380996704
Validation Loss Energy: 1.3238009719070096, Validation Loss Force: 3.4567564585021104, time: 0.20418429374694824
Test Loss Energy: 8.707715848686162, Test Loss Force: 11.280044354234386, time: 10.820520877838135

Epoch 1, Batch 100/219, Loss: 0.8879242539405823, Variance: 0.12909194827079773
Epoch 1, Batch 200/219, Loss: 1.0078991651535034, Variance: 0.13379612565040588

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6637447731266573, Training Loss Force: 3.331139820937769, time: 3.4315285682678223
Validation Loss Energy: 2.354696992860953, Validation Loss Force: 3.372710601711818, time: 0.21281790733337402
Test Loss Energy: 9.435476465509526, Test Loss Force: 11.043686437197369, time: 10.985101699829102

Epoch 2, Batch 100/219, Loss: 0.6934065818786621, Variance: 0.1311795711517334
Epoch 2, Batch 200/219, Loss: 0.6174294948577881, Variance: 0.1268846094608307

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.7052666737019706, Training Loss Force: 3.319056355389442, time: 3.6134774684906006
Validation Loss Energy: 1.7364925155978097, Validation Loss Force: 3.3685834290920464, time: 0.20247459411621094
Test Loss Energy: 9.090054608567097, Test Loss Force: 11.275225467490532, time: 10.8687744140625

Epoch 3, Batch 100/219, Loss: 0.7979096174240112, Variance: 0.1321074664592743
Epoch 3, Batch 200/219, Loss: 1.0035674571990967, Variance: 0.13460564613342285

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.676617102587793, Training Loss Force: 3.327803370857285, time: 3.490933418273926
Validation Loss Energy: 2.438473865322965, Validation Loss Force: 3.4094631462583562, time: 0.19479942321777344
Test Loss Energy: 9.286836178967354, Test Loss Force: 11.181345823953285, time: 11.06264877319336

Epoch 4, Batch 100/219, Loss: 0.5800533294677734, Variance: 0.12806463241577148
Epoch 4, Batch 200/219, Loss: 0.8426750898361206, Variance: 0.1315043717622757

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.679418184360303, Training Loss Force: 3.3456565112038774, time: 3.393162727355957
Validation Loss Energy: 1.550895889779258, Validation Loss Force: 3.4049248150606286, time: 0.1971602439880371
Test Loss Energy: 8.977409726773017, Test Loss Force: 11.374785706381848, time: 10.952756404876709

Epoch 5, Batch 100/219, Loss: 0.6974502801895142, Variance: 0.13760906457901
Epoch 5, Batch 200/219, Loss: 0.9381805658340454, Variance: 0.1366543024778366

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6905821025925047, Training Loss Force: 3.337594921188151, time: 3.33343505859375
Validation Loss Energy: 2.214199330520324, Validation Loss Force: 3.414696205191378, time: 0.19082307815551758
Test Loss Energy: 9.305866542731142, Test Loss Force: 11.182134068066642, time: 10.960150718688965

Epoch 6, Batch 100/219, Loss: 0.7475131154060364, Variance: 0.13641813397407532
Epoch 6, Batch 200/219, Loss: 0.698158860206604, Variance: 0.13057026267051697

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.672763197881288, Training Loss Force: 3.3380930107157547, time: 3.5866928100585938
Validation Loss Energy: 1.6943819947273842, Validation Loss Force: 3.3388400639210234, time: 0.1695694923400879
Test Loss Energy: 9.061513826409008, Test Loss Force: 11.306559870743119, time: 10.005545616149902

Epoch 7, Batch 100/219, Loss: 0.7845635414123535, Variance: 0.13210324943065643
Epoch 7, Batch 200/219, Loss: 0.9332044124603271, Variance: 0.13608860969543457

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6806193237554377, Training Loss Force: 3.3448790634060015, time: 3.506845712661743
Validation Loss Energy: 2.2662088866094017, Validation Loss Force: 3.3813787218917817, time: 0.2142620086669922
Test Loss Energy: 9.311386335987633, Test Loss Force: 11.214212405438348, time: 10.220514297485352

Epoch 8, Batch 100/219, Loss: 0.5861832499504089, Variance: 0.13333429396152496
Epoch 8, Batch 200/219, Loss: 0.8323651552200317, Variance: 0.1333099901676178

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6883552398101624, Training Loss Force: 3.3432436561375543, time: 3.527890205383301
Validation Loss Energy: 1.7349743295272306, Validation Loss Force: 3.382693082192907, time: 0.2505307197570801
Test Loss Energy: 9.05837962178065, Test Loss Force: 11.328743705262516, time: 8.583657503128052

Epoch 9, Batch 100/219, Loss: 0.7013156414031982, Variance: 0.13892130553722382
Epoch 9, Batch 200/219, Loss: 0.8742926716804504, Variance: 0.14263464510440826

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.690098002076061, Training Loss Force: 3.336790174559686, time: 3.3487799167633057
Validation Loss Energy: 2.034993086520754, Validation Loss Force: 3.3968692577011126, time: 0.16463303565979004
Test Loss Energy: 9.399500602019152, Test Loss Force: 11.30573620941168, time: 8.596895933151245

Epoch 10, Batch 100/219, Loss: 0.747721791267395, Variance: 0.13736936450004578
Epoch 10, Batch 200/219, Loss: 0.6185017824172974, Variance: 0.1345900297164917

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6937631168997047, Training Loss Force: 3.3375549932549973, time: 3.314406156539917
Validation Loss Energy: 1.5251091048285828, Validation Loss Force: 3.4843029698794097, time: 0.16393828392028809
Test Loss Energy: 9.126388323525314, Test Loss Force: 11.354622821694118, time: 8.782069206237793

Epoch 11, Batch 100/219, Loss: 0.8110774755477905, Variance: 0.13745027780532837
Epoch 11, Batch 200/219, Loss: 1.1542214155197144, Variance: 0.14099468290805817

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6790232069153284, Training Loss Force: 3.345315495948774, time: 3.3604254722595215
Validation Loss Energy: 2.6555272397231673, Validation Loss Force: 3.505899316339227, time: 0.1655724048614502
Test Loss Energy: 9.80456982469079, Test Loss Force: 11.316781527032107, time: 8.533155918121338

Epoch 12, Batch 100/219, Loss: 0.6670325994491577, Variance: 0.13238970935344696
Epoch 12, Batch 200/219, Loss: 0.8632187843322754, Variance: 0.13626182079315186

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6928817486996968, Training Loss Force: 3.3533642890975113, time: 3.354637384414673
Validation Loss Energy: 1.8126666562421925, Validation Loss Force: 3.3914576181130567, time: 0.16666030883789062
Test Loss Energy: 9.279105530574505, Test Loss Force: 11.28929957513454, time: 8.549946308135986

Epoch 13, Batch 100/219, Loss: 0.8440066576004028, Variance: 0.140095517039299
Epoch 13, Batch 200/219, Loss: 0.7643807530403137, Variance: 0.14248992502689362

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6921453081194424, Training Loss Force: 3.3381300286987, time: 3.339188575744629
Validation Loss Energy: 2.2154741113807606, Validation Loss Force: 3.395775395521743, time: 0.20111536979675293
Test Loss Energy: 9.772036561430076, Test Loss Force: 11.34736327126563, time: 8.756332397460938

Epoch 14, Batch 100/219, Loss: 0.5785603523254395, Variance: 0.13314393162727356
Epoch 14, Batch 200/219, Loss: 0.8585637807846069, Variance: 0.13948771357536316

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6543103896980074, Training Loss Force: 3.3367730514040392, time: 3.3633971214294434
Validation Loss Energy: 1.7385608892943327, Validation Loss Force: 3.4485498757943684, time: 0.16865801811218262
Test Loss Energy: 9.174386987212527, Test Loss Force: 11.294074427014621, time: 8.582817316055298

Epoch 15, Batch 100/219, Loss: 0.7618687748908997, Variance: 0.14227548241615295
Epoch 15, Batch 200/219, Loss: 0.8600736856460571, Variance: 0.1408863067626953

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.70390393039486, Training Loss Force: 3.337583990064918, time: 3.3966851234436035
Validation Loss Energy: 2.24714434400607, Validation Loss Force: 3.351138581856193, time: 0.16660284996032715
Test Loss Energy: 9.489810950080127, Test Loss Force: 11.177144507171146, time: 9.991859436035156

Epoch 16, Batch 100/219, Loss: 0.6554883122444153, Variance: 0.1371157169342041
Epoch 16, Batch 200/219, Loss: 0.9028456211090088, Variance: 0.13782468438148499

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.678093294299623, Training Loss Force: 3.3437735426158466, time: 3.5007858276367188
Validation Loss Energy: 1.8082340372214945, Validation Loss Force: 3.4514908898737824, time: 0.20467352867126465
Test Loss Energy: 9.028926084985, Test Loss Force: 11.159369559046944, time: 11.456343412399292

Epoch 17, Batch 100/219, Loss: 0.6761400699615479, Variance: 0.13666334748268127
Epoch 17, Batch 200/219, Loss: 0.8355951905250549, Variance: 0.1462840735912323

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6717553403885894, Training Loss Force: 3.3327708968337886, time: 3.38836407661438
Validation Loss Energy: 2.052136662402167, Validation Loss Force: 3.371318418544191, time: 0.20996451377868652
Test Loss Energy: 9.365905167294923, Test Loss Force: 11.113519160647362, time: 10.332032680511475

Epoch 18, Batch 100/219, Loss: 0.4851993918418884, Variance: 0.13284823298454285
Epoch 18, Batch 200/219, Loss: 0.8952866196632385, Variance: 0.14019057154655457

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6931420291045822, Training Loss Force: 3.3295141867096447, time: 3.332493305206299
Validation Loss Energy: 1.706143017177928, Validation Loss Force: 3.3846648174800973, time: 0.18937134742736816
Test Loss Energy: 8.936825761447686, Test Loss Force: 11.076960081197326, time: 9.762438297271729

Epoch 19, Batch 100/219, Loss: 0.7390100955963135, Variance: 0.14164718985557556
Epoch 19, Batch 200/219, Loss: 1.0291731357574463, Variance: 0.14108748733997345

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6927809568499024, Training Loss Force: 3.3352618101451847, time: 3.3642585277557373
Validation Loss Energy: 2.210925648736894, Validation Loss Force: 3.581692052570279, time: 0.18034601211547852
Test Loss Energy: 9.534350682912569, Test Loss Force: 11.339050624015679, time: 9.584911346435547

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–†â–ƒâ–…â–ƒâ–…â–ƒâ–…â–ƒâ–…â–„â–ˆâ–…â–ˆâ–„â–†â–ƒâ–…â–‚â–†
wandb:   test_error_force â–†â–â–†â–„â–ˆâ–„â–‡â–…â–‡â–‡â–ˆâ–‡â–†â–‡â–†â–„â–ƒâ–‚â–‚â–‡
wandb:          test_loss â–‡â–‡â–†â–†â–†â–…â–†â–…â–†â–†â–‡â–ˆâ–‡â–†â–…â–…â–ƒâ–„â–â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–†â–ƒâ–‡â–‚â–†â–ƒâ–†â–ƒâ–…â–‚â–ˆâ–„â–†â–ƒâ–†â–„â–…â–ƒâ–†
wandb:  valid_error_force â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–â–‚â–‚â–ƒâ–…â–†â–ƒâ–ƒâ–„â–â–„â–‚â–‚â–ˆ
wandb:         valid_loss â–â–†â–ƒâ–†â–ƒâ–…â–ƒâ–…â–ƒâ–„â–ƒâ–ˆâ–„â–…â–ƒâ–…â–„â–„â–ƒâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 6983
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.53435
wandb:   test_error_force 11.33905
wandb:          test_loss 9.63953
wandb: train_error_energy 2.69278
wandb:  train_error_force 3.33526
wandb:         train_loss 0.95667
wandb: valid_error_energy 2.21093
wandb:  valid_error_force 3.58169
wandb:         valid_loss 0.85309
wandb: 
wandb: ğŸš€ View run al_63_70 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/nnpb1drk
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_135118-nnpb1drk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.5731430053710938, Uncertainty Bias: -0.11484652757644653
3.0517578e-05 0.005581379
2.2062285 5.7607713
(48745, 22, 3)
Found uncertainty sample 0 after 607 steps.
Found uncertainty sample 1 after 2453 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 3596 steps.
Found uncertainty sample 4 after 1855 steps.
Found uncertainty sample 5 after 1855 steps.
Found uncertainty sample 6 after 1428 steps.
Found uncertainty sample 7 after 75 steps.
Found uncertainty sample 8 after 8 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 326 steps.
Did not find any uncertainty samples for sample 12.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 796 steps.
Found uncertainty sample 15 after 490 steps.
Found uncertainty sample 16 after 1522 steps.
Found uncertainty sample 17 after 3488 steps.
Found uncertainty sample 18 after 654 steps.
Found uncertainty sample 19 after 9 steps.
Found uncertainty sample 20 after 2935 steps.
Found uncertainty sample 21 after 137 steps.
Found uncertainty sample 22 after 2364 steps.
Found uncertainty sample 23 after 979 steps.
Found uncertainty sample 24 after 7 steps.
Found uncertainty sample 25 after 11 steps.
Found uncertainty sample 26 after 1695 steps.
Found uncertainty sample 27 after 1086 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 2193 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 184 steps.
Found uncertainty sample 32 after 131 steps.
Found uncertainty sample 33 after 531 steps.
Found uncertainty sample 34 after 483 steps.
Found uncertainty sample 35 after 2094 steps.
Found uncertainty sample 36 after 3381 steps.
Found uncertainty sample 37 after 1726 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 1576 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 190 steps.
Found uncertainty sample 42 after 1264 steps.
Found uncertainty sample 43 after 535 steps.
Found uncertainty sample 44 after 370 steps.
Found uncertainty sample 45 after 14 steps.
Found uncertainty sample 46 after 83 steps.
Found uncertainty sample 47 after 14 steps.
Found uncertainty sample 48 after 26 steps.
Found uncertainty sample 49 after 1587 steps.
Found uncertainty sample 50 after 1238 steps.
Found uncertainty sample 51 after 136 steps.
Found uncertainty sample 52 after 961 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2846 steps.
Found uncertainty sample 56 after 1131 steps.
Found uncertainty sample 57 after 719 steps.
Found uncertainty sample 58 after 502 steps.
Found uncertainty sample 59 after 1427 steps.
Found uncertainty sample 60 after 57 steps.
Found uncertainty sample 61 after 2796 steps.
Found uncertainty sample 62 after 300 steps.
Found uncertainty sample 63 after 728 steps.
Found uncertainty sample 64 after 2353 steps.
Found uncertainty sample 65 after 1610 steps.
Found uncertainty sample 66 after 690 steps.
Found uncertainty sample 67 after 1268 steps.
Found uncertainty sample 68 after 1285 steps.
Found uncertainty sample 69 after 562 steps.
Found uncertainty sample 70 after 1162 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 782 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 3 steps.
Found uncertainty sample 75 after 1566 steps.
Found uncertainty sample 76 after 1412 steps.
Found uncertainty sample 77 after 1368 steps.
Found uncertainty sample 78 after 2279 steps.
Found uncertainty sample 79 after 214 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 2943 steps.
Found uncertainty sample 82 after 2977 steps.
Found uncertainty sample 83 after 118 steps.
Found uncertainty sample 84 after 69 steps.
Found uncertainty sample 85 after 816 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 61 steps.
Found uncertainty sample 88 after 999 steps.
Found uncertainty sample 89 after 1553 steps.
Found uncertainty sample 90 after 696 steps.
Found uncertainty sample 91 after 291 steps.
Found uncertainty sample 92 after 2152 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 1466 steps.
Found uncertainty sample 95 after 324 steps.
Found uncertainty sample 96 after 963 steps.
Found uncertainty sample 97 after 1262 steps.
Found uncertainty sample 98 after 536 steps.
Found uncertainty sample 99 after 2943 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_141117-o4ae1v2g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_71
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/o4ae1v2g
Training model 71. Added 88 samples to the dataset.
Epoch 0, Batch 100/221, Loss: 2.408642530441284, Variance: 0.17598682641983032
Epoch 0, Batch 200/221, Loss: 1.688644289970398, Variance: 0.20668861269950867

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.339794095936058, Training Loss Force: 3.829294686692239, time: 3.37827730178833
Validation Loss Energy: 5.847343987978429, Validation Loss Force: 3.4729861445413492, time: 0.1873760223388672
Test Loss Energy: 10.886098935976786, Test Loss Force: 10.607717745902917, time: 9.685941696166992

Epoch 1, Batch 100/221, Loss: 1.3758816719055176, Variance: 0.1848096251487732
Epoch 1, Batch 200/221, Loss: 1.1521879434585571, Variance: 0.19021829962730408

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.115241183762183, Training Loss Force: 3.365935450379443, time: 3.364650011062622
Validation Loss Energy: 4.946739923065233, Validation Loss Force: 3.488277057461356, time: 0.1843278408050537
Test Loss Energy: 10.074106568309041, Test Loss Force: 10.885120870496856, time: 9.679718017578125

Epoch 2, Batch 100/221, Loss: 1.9749832153320312, Variance: 0.19057179987430573
Epoch 2, Batch 200/221, Loss: 1.8176186084747314, Variance: 0.18178832530975342

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.148931604380082, Training Loss Force: 3.3755411958242285, time: 3.5708138942718506
Validation Loss Energy: 1.9262550028002376, Validation Loss Force: 3.6994184547026023, time: 0.18583202362060547
Test Loss Energy: 9.059575460016918, Test Loss Force: 11.004805746278308, time: 9.676794528961182

Epoch 3, Batch 100/221, Loss: 1.631165862083435, Variance: 0.1816118210554123
Epoch 3, Batch 200/221, Loss: 1.7054553031921387, Variance: 0.19121207296848297

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.135822542290371, Training Loss Force: 3.3750756037080683, time: 3.291555881500244
Validation Loss Energy: 3.9172423756822887, Validation Loss Force: 3.3638904541724726, time: 0.18993639945983887
Test Loss Energy: 10.049660555163863, Test Loss Force: 10.85270541436983, time: 9.824832916259766

Epoch 4, Batch 100/221, Loss: 1.0312188863754272, Variance: 0.19246646761894226
Epoch 4, Batch 200/221, Loss: 0.8925269842147827, Variance: 0.18460530042648315

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.143584242623853, Training Loss Force: 3.3911351634669495, time: 3.4869191646575928
Validation Loss Energy: 5.959268077506167, Validation Loss Force: 3.472952298831168, time: 0.18951916694641113
Test Loss Energy: 10.512382997968661, Test Loss Force: 11.071859111117218, time: 9.864098072052002

Epoch 5, Batch 100/221, Loss: 1.3911062479019165, Variance: 0.19363060593605042
Epoch 5, Batch 200/221, Loss: 1.1459296941757202, Variance: 0.18242888152599335

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.175437170245632, Training Loss Force: 3.3753281921313185, time: 3.3891654014587402
Validation Loss Energy: 5.694210964952127, Validation Loss Force: 3.649708458550808, time: 0.19356560707092285
Test Loss Energy: 10.871943417695094, Test Loss Force: 11.002770760129794, time: 9.73500657081604

Epoch 6, Batch 100/221, Loss: 1.6746022701263428, Variance: 0.18682458996772766
Epoch 6, Batch 200/221, Loss: 1.6523913145065308, Variance: 0.19745928049087524

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.160323599698395, Training Loss Force: 3.446486007873504, time: 3.408904790878296
Validation Loss Energy: 1.7633735523988607, Validation Loss Force: 3.4786411615059034, time: 0.19094133377075195
Test Loss Energy: 9.323016683563486, Test Loss Force: 11.226370290029307, time: 9.89624810218811

Epoch 7, Batch 100/221, Loss: 1.755075216293335, Variance: 0.1976850926876068
Epoch 7, Batch 200/221, Loss: 1.7508981227874756, Variance: 0.18794944882392883

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.140167021860619, Training Loss Force: 3.391365024524024, time: 3.4677679538726807
Validation Loss Energy: 3.1561698673528276, Validation Loss Force: 3.4349612684949795, time: 0.18738222122192383
Test Loss Energy: 9.598788559642113, Test Loss Force: 11.223357975190915, time: 9.670380353927612

Epoch 8, Batch 100/221, Loss: 0.7999426126480103, Variance: 0.18742631375789642
Epoch 8, Batch 200/221, Loss: 0.8776116371154785, Variance: 0.19334325194358826

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.151174341619829, Training Loss Force: 3.3969512311809478, time: 3.4411816596984863
Validation Loss Energy: 6.034905891678939, Validation Loss Force: 3.4450981143614827, time: 0.19175004959106445
Test Loss Energy: 11.346424703328438, Test Loss Force: 10.914381932549086, time: 9.855630874633789

Epoch 9, Batch 100/221, Loss: 1.0271244049072266, Variance: 0.1859714686870575
Epoch 9, Batch 200/221, Loss: 1.2806730270385742, Variance: 0.19850997626781464

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.175258401178243, Training Loss Force: 3.3838676687386564, time: 3.358017683029175
Validation Loss Energy: 5.120262091776704, Validation Loss Force: 3.527952301550056, time: 0.18194198608398438
Test Loss Energy: 10.393595621073754, Test Loss Force: 11.231023744089997, time: 9.700820446014404

Epoch 10, Batch 100/221, Loss: 2.1017560958862305, Variance: 0.19048531353473663
Epoch 10, Batch 200/221, Loss: 1.7222625017166138, Variance: 0.1878269910812378

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.146259532311736, Training Loss Force: 3.4024196801164672, time: 3.32110595703125
Validation Loss Energy: 2.1953071338359402, Validation Loss Force: 3.4778115519354635, time: 0.19289302825927734
Test Loss Energy: 9.731229201826833, Test Loss Force: 11.148171753864803, time: 9.681558609008789

Epoch 11, Batch 100/221, Loss: 1.7486025094985962, Variance: 0.1874404400587082
Epoch 11, Batch 200/221, Loss: 1.9141035079956055, Variance: 0.19669264554977417

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.112564688033723, Training Loss Force: 3.3862599766724224, time: 3.5253214836120605
Validation Loss Energy: 3.612440657600593, Validation Loss Force: 3.39779828904521, time: 0.18578243255615234
Test Loss Energy: 9.988469973341052, Test Loss Force: 11.028207350196137, time: 9.692119598388672

Epoch 12, Batch 100/221, Loss: 1.0274509191513062, Variance: 0.19175860285758972
Epoch 12, Batch 200/221, Loss: 0.8919330835342407, Variance: 0.19475138187408447

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.153481778855981, Training Loss Force: 3.3729230701423583, time: 3.342336893081665
Validation Loss Energy: 5.546607373492077, Validation Loss Force: 3.4266817059746604, time: 0.19421124458312988
Test Loss Energy: 10.535280441634283, Test Loss Force: 11.071246591431631, time: 9.667331457138062

Epoch 13, Batch 100/221, Loss: 1.134716510772705, Variance: 0.1948065459728241
Epoch 13, Batch 200/221, Loss: 0.972175121307373, Variance: 0.1916833072900772

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.180413436238001, Training Loss Force: 3.39541814101305, time: 3.4110445976257324
Validation Loss Energy: 4.929680349119121, Validation Loss Force: 3.4546387079224776, time: 0.19492483139038086
Test Loss Energy: 10.790587489000073, Test Loss Force: 10.936049510402363, time: 9.872518301010132

Epoch 14, Batch 100/221, Loss: 1.694044828414917, Variance: 0.18833014369010925
Epoch 14, Batch 200/221, Loss: 1.782644510269165, Variance: 0.19577521085739136

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.124124688733967, Training Loss Force: 3.4012335719537448, time: 3.4127016067504883
Validation Loss Energy: 2.17832143037899, Validation Loss Force: 3.4673029599298344, time: 0.18493390083312988
Test Loss Energy: 9.264361750720255, Test Loss Force: 10.898284743978133, time: 9.72511076927185

Epoch 15, Batch 100/221, Loss: 0.372043251991272, Variance: 0.14581617712974548
Epoch 15, Batch 200/221, Loss: 0.8845215439796448, Variance: 0.13825185596942902

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 3.02774048153915, Training Loss Force: 3.4677952285149636, time: 3.375540256500244
Validation Loss Energy: 3.6278118984149934, Validation Loss Force: 3.3499954541282864, time: 0.19167494773864746
Test Loss Energy: 10.071804355869277, Test Loss Force: 11.289325670559474, time: 9.859811067581177

Epoch 16, Batch 100/221, Loss: 1.1711777448654175, Variance: 0.13582970201969147
Epoch 16, Batch 200/221, Loss: 0.7076401114463806, Variance: 0.14151935279369354

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.616264157231133, Training Loss Force: 3.3305980916804816, time: 3.4565536975860596
Validation Loss Energy: 2.6741684882800905, Validation Loss Force: 3.3832076149218597, time: 0.1853325366973877
Test Loss Energy: 9.866953418833582, Test Loss Force: 11.134228053397047, time: 9.674322605133057

Epoch 17, Batch 100/221, Loss: 0.7716354727745056, Variance: 0.13892045617103577
Epoch 17, Batch 200/221, Loss: 1.1658604145050049, Variance: 0.14195126295089722

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6356217875940153, Training Loss Force: 3.319973728595477, time: 3.3946075439453125
Validation Loss Energy: 1.7528007760119175, Validation Loss Force: 3.384671219218043, time: 0.18932485580444336
Test Loss Energy: 9.414008301283209, Test Loss Force: 11.483931877653106, time: 9.76592755317688

Epoch 18, Batch 100/221, Loss: 0.8487263917922974, Variance: 0.1466338336467743
Epoch 18, Batch 200/221, Loss: 0.9729902148246765, Variance: 0.1434839367866516

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6519839209691973, Training Loss Force: 3.3091186248598934, time: 3.5503768920898438
Validation Loss Energy: 3.1617288582482015, Validation Loss Force: 3.4528057511804864, time: 0.1838829517364502
Test Loss Energy: 9.586444728827196, Test Loss Force: 11.254394482100459, time: 9.750479936599731

Epoch 19, Batch 100/221, Loss: 1.3835961818695068, Variance: 0.1452486515045166
Epoch 19, Batch 200/221, Loss: 0.5902265310287476, Variance: 0.13951954245567322

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6490006647392055, Training Loss Force: 3.3229442065383954, time: 3.475127696990967
Validation Loss Energy: 2.1583161106341135, Validation Loss Force: 3.35661128845752, time: 0.19125795364379883
Test Loss Energy: 9.372082144636575, Test Loss Force: 11.333891666693065, time: 9.71342420578003

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–„â–â–„â–…â–‡â–‚â–ƒâ–ˆâ–…â–ƒâ–„â–†â–†â–‚â–„â–ƒâ–‚â–ƒâ–‚
wandb:   test_error_force â–â–ƒâ–„â–ƒâ–…â–„â–†â–†â–ƒâ–†â–…â–„â–…â–„â–ƒâ–†â–…â–ˆâ–†â–‡
wandb:          test_loss â–‚â–‚â–â–‚â–„â–„â–ƒâ–‚â–…â–„â–ƒâ–ƒâ–ƒâ–„â–‚â–ˆâ–‡â–ˆâ–‡â–‡
wandb: train_error_energy â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–‚â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–â–â–
wandb: valid_error_energy â–ˆâ–†â–â–…â–ˆâ–‡â–â–ƒâ–ˆâ–‡â–‚â–„â–‡â–†â–‚â–„â–ƒâ–â–ƒâ–‚
wandb:  valid_error_force â–ƒâ–„â–ˆâ–â–ƒâ–‡â–„â–ƒâ–ƒâ–…â–„â–‚â–ƒâ–ƒâ–ƒâ–â–‚â–‚â–ƒâ–
wandb:         valid_loss â–‡â–†â–ƒâ–„â–ˆâ–ˆâ–‚â–ƒâ–ˆâ–†â–‚â–„â–‡â–†â–ƒâ–„â–‚â–â–„â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7062
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.37208
wandb:   test_error_force 11.33389
wandb:          test_loss 9.58437
wandb: train_error_energy 2.649
wandb:  train_error_force 3.32294
wandb:         train_loss 0.94773
wandb: valid_error_energy 2.15832
wandb:  valid_error_force 3.35661
wandb:         valid_loss 0.77544
wandb: 
wandb: ğŸš€ View run al_63_71 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/o4ae1v2g
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_141117-o4ae1v2g/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.024418354034424, Uncertainty Bias: -0.18756408989429474
2.2888184e-05 0.022741556
1.8778341 6.0114746
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 399 steps.
Found uncertainty sample 3 after 196 steps.
Found uncertainty sample 4 after 596 steps.
Found uncertainty sample 5 after 123 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 409 steps.
Found uncertainty sample 8 after 56 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 1458 steps.
Found uncertainty sample 11 after 2916 steps.
Found uncertainty sample 12 after 289 steps.
Found uncertainty sample 13 after 3626 steps.
Found uncertainty sample 14 after 1394 steps.
Found uncertainty sample 15 after 3981 steps.
Found uncertainty sample 16 after 990 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 1701 steps.
Found uncertainty sample 19 after 8 steps.
Found uncertainty sample 20 after 1397 steps.
Found uncertainty sample 21 after 3478 steps.
Found uncertainty sample 22 after 734 steps.
Found uncertainty sample 23 after 3884 steps.
Found uncertainty sample 24 after 555 steps.
Found uncertainty sample 25 after 10 steps.
Found uncertainty sample 26 after 859 steps.
Found uncertainty sample 27 after 1272 steps.
Found uncertainty sample 28 after 1349 steps.
Found uncertainty sample 29 after 59 steps.
Found uncertainty sample 30 after 459 steps.
Found uncertainty sample 31 after 508 steps.
Found uncertainty sample 32 after 183 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 1391 steps.
Found uncertainty sample 35 after 639 steps.
Found uncertainty sample 36 after 1527 steps.
Found uncertainty sample 37 after 536 steps.
Found uncertainty sample 38 after 9 steps.
Found uncertainty sample 39 after 1375 steps.
Found uncertainty sample 40 after 2319 steps.
Found uncertainty sample 41 after 1741 steps.
Found uncertainty sample 42 after 172 steps.
Found uncertainty sample 43 after 1998 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 3357 steps.
Found uncertainty sample 46 after 616 steps.
Found uncertainty sample 47 after 19 steps.
Found uncertainty sample 48 after 2578 steps.
Found uncertainty sample 49 after 582 steps.
Found uncertainty sample 50 after 1367 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 1039 steps.
Found uncertainty sample 55 after 617 steps.
Found uncertainty sample 56 after 477 steps.
Found uncertainty sample 57 after 673 steps.
Found uncertainty sample 58 after 2438 steps.
Found uncertainty sample 59 after 1384 steps.
Found uncertainty sample 60 after 937 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 2978 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 678 steps.
Found uncertainty sample 65 after 1451 steps.
Found uncertainty sample 66 after 454 steps.
Found uncertainty sample 67 after 1184 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 3247 steps.
Found uncertainty sample 70 after 164 steps.
Found uncertainty sample 71 after 3046 steps.
Found uncertainty sample 72 after 897 steps.
Found uncertainty sample 73 after 2073 steps.
Found uncertainty sample 74 after 681 steps.
Found uncertainty sample 75 after 1372 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 68 steps.
Found uncertainty sample 78 after 597 steps.
Found uncertainty sample 79 after 969 steps.
Found uncertainty sample 80 after 748 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 1357 steps.
Found uncertainty sample 83 after 66 steps.
Found uncertainty sample 84 after 1924 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1972 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 786 steps.
Found uncertainty sample 90 after 1956 steps.
Found uncertainty sample 91 after 2990 steps.
Found uncertainty sample 92 after 813 steps.
Found uncertainty sample 93 after 1206 steps.
Found uncertainty sample 94 after 7 steps.
Found uncertainty sample 95 after 172 steps.
Found uncertainty sample 96 after 630 steps.
Found uncertainty sample 97 after 777 steps.
Found uncertainty sample 98 after 1874 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_143256-5z6gssje
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_72
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5z6gssje
Training model 72. Added 85 samples to the dataset.
Epoch 0, Batch 100/224, Loss: 0.7393949031829834, Variance: 0.14150461554527283
Epoch 0, Batch 200/224, Loss: 0.8063681125640869, Variance: 0.1416572630405426

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9245291191041156, Training Loss Force: 3.4026496622462927, time: 3.8649544715881348
Validation Loss Energy: 3.8828827451124637, Validation Loss Force: 3.412109123719416, time: 0.18257665634155273
Test Loss Energy: 10.43808915530455, Test Loss Force: 11.205625308556682, time: 8.791999340057373

Epoch 1, Batch 100/224, Loss: 1.3057605028152466, Variance: 0.14026586711406708
Epoch 1, Batch 200/224, Loss: 0.6211697459220886, Variance: 0.143511563539505

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.67941598094525, Training Loss Force: 3.317987857865723, time: 3.510206937789917
Validation Loss Energy: 2.307833042397834, Validation Loss Force: 3.390497079084811, time: 0.17297697067260742
Test Loss Energy: 9.275913993887203, Test Loss Force: 11.135758049238735, time: 8.965672492980957

Epoch 2, Batch 100/224, Loss: 1.0599842071533203, Variance: 0.1449035406112671
Epoch 2, Batch 200/224, Loss: 1.2958929538726807, Variance: 0.13965287804603577

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6705034649756625, Training Loss Force: 3.335417282281045, time: 3.345254421234131
Validation Loss Energy: 1.9685008508441006, Validation Loss Force: 3.3886654583576954, time: 0.16965603828430176
Test Loss Energy: 9.33800554870156, Test Loss Force: 11.294425115763966, time: 8.822835445404053

Epoch 3, Batch 100/224, Loss: 0.6555142402648926, Variance: 0.13912862539291382
Epoch 3, Batch 200/224, Loss: 0.9400576949119568, Variance: 0.14564402401447296

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.659610275640199, Training Loss Force: 3.340021453961858, time: 3.4438717365264893
Validation Loss Energy: 3.7657493665830315, Validation Loss Force: 3.36596762330026, time: 0.17398476600646973
Test Loss Energy: 10.443737485782787, Test Loss Force: 11.143013957686009, time: 8.84740686416626

Epoch 4, Batch 100/224, Loss: 1.1196784973144531, Variance: 0.13359931111335754
Epoch 4, Batch 200/224, Loss: 0.778073787689209, Variance: 0.14181530475616455

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.654245792705175, Training Loss Force: 3.335276495737297, time: 3.5322539806365967
Validation Loss Energy: 2.558725780908402, Validation Loss Force: 3.434154983517575, time: 0.21854925155639648
Test Loss Energy: 9.301809496902651, Test Loss Force: 11.313274882026528, time: 8.96260666847229

Epoch 5, Batch 100/224, Loss: 0.9723175764083862, Variance: 0.14607524871826172
Epoch 5, Batch 200/224, Loss: 1.3494813442230225, Variance: 0.13465899229049683

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.693952704024442, Training Loss Force: 3.3308319867806415, time: 3.376920461654663
Validation Loss Energy: 1.5376438190602957, Validation Loss Force: 3.503394566783972, time: 0.17527222633361816
Test Loss Energy: 9.232546489366717, Test Loss Force: 11.160954692898716, time: 8.830845594406128

Epoch 6, Batch 100/224, Loss: 0.7883613109588623, Variance: 0.14439895749092102
Epoch 6, Batch 200/224, Loss: 1.0045242309570312, Variance: 0.14620420336723328

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6546882372921914, Training Loss Force: 3.32786964815427, time: 3.489265203475952
Validation Loss Energy: 4.009385206293239, Validation Loss Force: 3.4516664088280606, time: 0.17949199676513672
Test Loss Energy: 10.098972034166412, Test Loss Force: 11.165847515125469, time: 8.970088720321655

Epoch 7, Batch 100/224, Loss: 0.8956433534622192, Variance: 0.13600733876228333
Epoch 7, Batch 200/224, Loss: 0.74189692735672, Variance: 0.14199094474315643

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.639364313174272, Training Loss Force: 3.346701826557298, time: 3.4507195949554443
Validation Loss Energy: 1.9508708542134088, Validation Loss Force: 3.5055459960719264, time: 0.1747264862060547
Test Loss Energy: 9.295660764734127, Test Loss Force: 11.236976630001125, time: 9.793516397476196

Epoch 8, Batch 100/224, Loss: 2.9163262844085693, Variance: 0.1446993052959442
Epoch 8, Batch 200/224, Loss: 1.840234637260437, Variance: 0.17593006789684296

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 3.7032771867441956, Training Loss Force: 4.013583391483447, time: 3.3471808433532715
Validation Loss Energy: 4.8339818262514225, Validation Loss Force: 3.4313485729368742, time: 0.1723473072052002
Test Loss Energy: 10.049944914155477, Test Loss Force: 10.893012367619553, time: 8.789170026779175

Epoch 9, Batch 100/224, Loss: 1.9520328044891357, Variance: 0.19583088159561157
Epoch 9, Batch 200/224, Loss: 1.6048719882965088, Variance: 0.18908700346946716

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.166194061521104, Training Loss Force: 3.3990060216412554, time: 3.56014347076416
Validation Loss Energy: 5.109388600708186, Validation Loss Force: 3.5241983780861457, time: 0.1812736988067627
Test Loss Energy: 10.23395727872883, Test Loss Force: 11.084695001107193, time: 8.82766604423523

Epoch 10, Batch 100/224, Loss: 1.7872321605682373, Variance: 0.20055840909481049
Epoch 10, Batch 200/224, Loss: 1.9329333305358887, Variance: 0.1938590407371521

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.140773767121275, Training Loss Force: 3.409010973813026, time: 3.427835702896118
Validation Loss Energy: 4.341247404673961, Validation Loss Force: 3.622062915070857, time: 0.17221903800964355
Test Loss Energy: 9.932628860365211, Test Loss Force: 11.104634148769739, time: 8.79994535446167

Epoch 11, Batch 100/224, Loss: 1.8974956274032593, Variance: 0.19793297350406647
Epoch 11, Batch 200/224, Loss: 1.73036527633667, Variance: 0.18632178008556366

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.082524829709975, Training Loss Force: 3.4511613643892773, time: 3.4399497509002686
Validation Loss Energy: 4.958051404343792, Validation Loss Force: 3.425011743047488, time: 0.17089605331420898
Test Loss Energy: 10.319178132914267, Test Loss Force: 11.055708477288011, time: 8.958943843841553

Epoch 12, Batch 100/224, Loss: 1.9165854454040527, Variance: 0.20235398411750793
Epoch 12, Batch 200/224, Loss: 2.1977548599243164, Variance: 0.18439677357673645

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.096351448457069, Training Loss Force: 3.376590819337875, time: 3.381803512573242
Validation Loss Energy: 4.85438667011138, Validation Loss Force: 3.5461109147331804, time: 0.17042231559753418
Test Loss Energy: 10.237963509359767, Test Loss Force: 11.050686772760624, time: 8.79575800895691

Epoch 13, Batch 100/224, Loss: 1.7403881549835205, Variance: 0.20182907581329346
Epoch 13, Batch 200/224, Loss: 1.5767595767974854, Variance: 0.1862604320049286

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.128684498042728, Training Loss Force: 3.38849111139861, time: 3.42881178855896
Validation Loss Energy: 5.0448488394376, Validation Loss Force: 3.485546410057557, time: 0.17248249053955078
Test Loss Energy: 10.430308274617673, Test Loss Force: 11.175573293055516, time: 8.776535511016846

Epoch 14, Batch 100/224, Loss: 1.8605830669403076, Variance: 0.20480182766914368
Epoch 14, Batch 200/224, Loss: 1.6687276363372803, Variance: 0.1957443356513977

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.134941498061122, Training Loss Force: 3.393157732828328, time: 3.6196365356445312
Validation Loss Energy: 5.114759269173391, Validation Loss Force: 3.4722590184453215, time: 0.17327404022216797
Test Loss Energy: 10.388362173344083, Test Loss Force: 11.160848268705035, time: 8.784590721130371

Epoch 15, Batch 100/224, Loss: 1.7065198421478271, Variance: 0.19776542484760284
Epoch 15, Batch 200/224, Loss: 1.711766004562378, Variance: 0.18563400208950043

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.1065731506523155, Training Loss Force: 3.3696845608225963, time: 3.378805160522461
Validation Loss Energy: 4.839565923093, Validation Loss Force: 3.452062759907812, time: 0.17071056365966797
Test Loss Energy: 10.188275553955997, Test Loss Force: 11.022885142581156, time: 8.78037428855896

Epoch 16, Batch 100/224, Loss: 1.876401662826538, Variance: 0.20005440711975098
Epoch 16, Batch 200/224, Loss: 2.0734400749206543, Variance: 0.18707574903964996

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.16810445245265, Training Loss Force: 3.3797425997044974, time: 3.386268377304077
Validation Loss Energy: 4.247015380801342, Validation Loss Force: 3.487066794985952, time: 0.18044495582580566
Test Loss Energy: 10.043797669556742, Test Loss Force: 11.089636167020565, time: 8.925434589385986

Epoch 17, Batch 100/224, Loss: 1.7516664266586304, Variance: 0.20053929090499878
Epoch 17, Batch 200/224, Loss: 2.0110185146331787, Variance: 0.1894097924232483

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.106855940964673, Training Loss Force: 3.4144722517216266, time: 3.419794797897339
Validation Loss Energy: 5.1245641710106025, Validation Loss Force: 3.4330474828530164, time: 0.17579984664916992
Test Loss Energy: 10.54783188403131, Test Loss Force: 11.206330612493506, time: 8.729934453964233

Epoch 18, Batch 100/224, Loss: 1.821077823638916, Variance: 0.20262345671653748
Epoch 18, Batch 200/224, Loss: 2.110363245010376, Variance: 0.1896655559539795

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.147332223389026, Training Loss Force: 3.4050398962407846, time: 3.4555463790893555
Validation Loss Energy: 5.14404716460642, Validation Loss Force: 3.464987200812185, time: 0.1677255630493164
Test Loss Energy: 10.257780405832165, Test Loss Force: 10.9536282532006, time: 8.940059661865234

Epoch 19, Batch 100/224, Loss: 1.780840277671814, Variance: 0.2016802430152893
Epoch 19, Batch 200/224, Loss: 1.778393268585205, Variance: 0.19385218620300293

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.141973631245782, Training Loss Force: 3.3828094218513605, time: 3.435941219329834
Validation Loss Energy: 4.887129166008905, Validation Loss Force: 3.380813420597557, time: 0.172499418258667
Test Loss Energy: 10.300914422517328, Test Loss Force: 11.101546434194795, time: 8.71047854423523

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–â–‚â–‡â–â–â–†â–â–…â–†â–…â–‡â–†â–‡â–‡â–†â–…â–ˆâ–†â–‡
wandb:   test_error_force â–†â–…â–ˆâ–…â–ˆâ–…â–†â–‡â–â–„â–…â–„â–„â–†â–…â–ƒâ–„â–†â–‚â–„
wandb:          test_loss â–‡â–…â–†â–ˆâ–†â–†â–†â–†â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–ƒâ–â–‚
wandb: train_error_energy â–‚â–â–â–â–â–â–â–â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  train_error_force â–‚â–â–â–â–â–â–â–â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–„â–â–â–â–â–â–â–â–ˆâ–†â–†â–†â–…â–†â–†â–…â–†â–†â–†â–†
wandb: valid_error_energy â–†â–‚â–‚â–…â–ƒâ–â–†â–‚â–‡â–ˆâ–†â–ˆâ–‡â–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–ˆ
wandb:  valid_error_force â–‚â–‚â–‚â–â–ƒâ–…â–ƒâ–…â–ƒâ–…â–ˆâ–ƒâ–†â–„â–„â–ƒâ–„â–ƒâ–„â–
wandb:         valid_loss â–†â–‚â–â–†â–ƒâ–â–‡â–‚â–‡â–ˆâ–†â–‡â–‡â–ˆâ–ˆâ–‡â–†â–ˆâ–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 7138
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.30091
wandb:   test_error_force 11.10155
wandb:          test_loss 8.26192
wandb: train_error_energy 4.14197
wandb:  train_error_force 3.38281
wandb:         train_loss 1.40238
wandb: valid_error_energy 4.88713
wandb:  valid_error_force 3.38081
wandb:         valid_loss 1.56678
wandb: 
wandb: ğŸš€ View run al_63_72 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5z6gssje
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_143256-5z6gssje/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.5385642051696777, Uncertainty Bias: -0.2608634829521179
0.00010681152 0.3061447
1.8878906 5.646936
(48745, 22, 3)
Found uncertainty sample 0 after 498 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 270 steps.
Found uncertainty sample 3 after 265 steps.
Found uncertainty sample 4 after 1608 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 125 steps.
Found uncertainty sample 7 after 2645 steps.
Found uncertainty sample 8 after 476 steps.
Found uncertainty sample 9 after 1582 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 341 steps.
Found uncertainty sample 12 after 2966 steps.
Found uncertainty sample 13 after 1689 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1303 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 260 steps.
Found uncertainty sample 18 after 481 steps.
Found uncertainty sample 19 after 795 steps.
Found uncertainty sample 20 after 221 steps.
Found uncertainty sample 21 after 150 steps.
Found uncertainty sample 22 after 793 steps.
Found uncertainty sample 23 after 2840 steps.
Found uncertainty sample 24 after 1637 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 1255 steps.
Found uncertainty sample 27 after 2229 steps.
Found uncertainty sample 28 after 1086 steps.
Found uncertainty sample 29 after 861 steps.
Found uncertainty sample 30 after 1589 steps.
Found uncertainty sample 31 after 1452 steps.
Found uncertainty sample 32 after 753 steps.
Found uncertainty sample 33 after 457 steps.
Did not find any uncertainty samples for sample 34.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 224 steps.
Found uncertainty sample 37 after 883 steps.
Found uncertainty sample 38 after 1497 steps.
Found uncertainty sample 39 after 2786 steps.
Found uncertainty sample 40 after 2913 steps.
Found uncertainty sample 41 after 248 steps.
Found uncertainty sample 42 after 69 steps.
Found uncertainty sample 43 after 419 steps.
Found uncertainty sample 44 after 447 steps.
Found uncertainty sample 45 after 369 steps.
Found uncertainty sample 46 after 1012 steps.
Found uncertainty sample 47 after 1422 steps.
Found uncertainty sample 48 after 104 steps.
Found uncertainty sample 49 after 949 steps.
Found uncertainty sample 50 after 1600 steps.
Found uncertainty sample 51 after 3028 steps.
Found uncertainty sample 52 after 168 steps.
Found uncertainty sample 53 after 2551 steps.
Found uncertainty sample 54 after 1168 steps.
Found uncertainty sample 55 after 1945 steps.
Found uncertainty sample 56 after 453 steps.
Found uncertainty sample 57 after 60 steps.
Found uncertainty sample 58 after 917 steps.
Found uncertainty sample 59 after 1413 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 1603 steps.
Found uncertainty sample 62 after 3722 steps.
Found uncertainty sample 63 after 508 steps.
Found uncertainty sample 64 after 2639 steps.
Found uncertainty sample 65 after 1993 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 1001 steps.
Found uncertainty sample 68 after 313 steps.
Did not find any uncertainty samples for sample 69.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 439 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 84 steps.
Found uncertainty sample 74 after 1330 steps.
Found uncertainty sample 75 after 45 steps.
Found uncertainty sample 76 after 115 steps.
Found uncertainty sample 77 after 3476 steps.
Found uncertainty sample 78 after 640 steps.
Found uncertainty sample 79 after 2400 steps.
Found uncertainty sample 80 after 331 steps.
Found uncertainty sample 81 after 274 steps.
Found uncertainty sample 82 after 1339 steps.
Found uncertainty sample 83 after 89 steps.
Found uncertainty sample 84 after 2223 steps.
Found uncertainty sample 85 after 801 steps.
Found uncertainty sample 86 after 184 steps.
Found uncertainty sample 87 after 722 steps.
Found uncertainty sample 88 after 1208 steps.
Found uncertainty sample 89 after 2651 steps.
Found uncertainty sample 90 after 554 steps.
Found uncertainty sample 91 after 3332 steps.
Found uncertainty sample 92 after 266 steps.
Found uncertainty sample 93 after 912 steps.
Found uncertainty sample 94 after 333 steps.
Found uncertainty sample 95 after 711 steps.
Found uncertainty sample 96 after 2935 steps.
Found uncertainty sample 97 after 1701 steps.
Found uncertainty sample 98 after 131 steps.
Found uncertainty sample 99 after 163 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_145317-jsa4nimr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_73
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jsa4nimr
Training model 73. Added 88 samples to the dataset.
Epoch 0, Batch 100/226, Loss: 1.0657907724380493, Variance: 0.15365910530090332
Epoch 0, Batch 200/226, Loss: 0.7234798669815063, Variance: 0.15224380791187286

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9458316364893653, Training Loss Force: 3.4384200238080975, time: 3.548959493637085
Validation Loss Energy: 2.1113790133565575, Validation Loss Force: 3.406601959067757, time: 0.23030829429626465
Test Loss Energy: 9.55521676842121, Test Loss Force: 11.343312213022132, time: 8.259482622146606

Epoch 1, Batch 100/226, Loss: 0.7361926436424255, Variance: 0.14682455360889435
Epoch 1, Batch 200/226, Loss: 0.9278615117073059, Variance: 0.14517450332641602

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6298414152786207, Training Loss Force: 3.3043004895865917, time: 3.4374825954437256
Validation Loss Energy: 2.4442596447434664, Validation Loss Force: 3.4340280064371096, time: 0.16725420951843262
Test Loss Energy: 9.750778055943043, Test Loss Force: 11.324724562152731, time: 8.232213497161865

Epoch 2, Batch 100/226, Loss: 0.8489492535591125, Variance: 0.14486628770828247
Epoch 2, Batch 200/226, Loss: 1.297788381576538, Variance: 0.14566326141357422

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6438496415982486, Training Loss Force: 3.313094284331043, time: 3.723372220993042
Validation Loss Energy: 3.5140205054295053, Validation Loss Force: 3.3587916647344453, time: 0.16488313674926758
Test Loss Energy: 9.820020638539523, Test Loss Force: 11.345630681475358, time: 8.271466255187988

Epoch 3, Batch 100/226, Loss: 1.2294044494628906, Variance: 0.14690396189689636
Epoch 3, Batch 200/226, Loss: 0.5711609125137329, Variance: 0.14266976714134216

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6276442496495664, Training Loss Force: 3.3138194990193575, time: 3.412318468093872
Validation Loss Energy: 2.263043477698179, Validation Loss Force: 3.3800602422114565, time: 0.16612696647644043
Test Loss Energy: 9.70543070328413, Test Loss Force: 11.1363266739574, time: 8.266530752182007

Epoch 4, Batch 100/226, Loss: 0.5951936841011047, Variance: 0.1404552459716797
Epoch 4, Batch 200/226, Loss: 1.11637282371521, Variance: 0.1465904861688614

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6388191815375395, Training Loss Force: 3.3143465184625613, time: 3.4142794609069824
Validation Loss Energy: 2.626659465301219, Validation Loss Force: 3.3657290889776847, time: 0.16474485397338867
Test Loss Energy: 9.65987777687382, Test Loss Force: 11.142948360055104, time: 8.455045461654663

Epoch 5, Batch 100/226, Loss: 0.8919130563735962, Variance: 0.14243070781230927
Epoch 5, Batch 200/226, Loss: 1.6351795196533203, Variance: 0.14615783095359802

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6410478494318026, Training Loss Force: 3.3244905385121895, time: 3.435774326324463
Validation Loss Energy: 3.405322836158975, Validation Loss Force: 3.389742620351121, time: 0.17128562927246094
Test Loss Energy: 9.528603557281961, Test Loss Force: 11.15989454993946, time: 8.274436950683594

Epoch 6, Batch 100/226, Loss: 1.6205214262008667, Variance: 0.1445104479789734
Epoch 6, Batch 200/226, Loss: 0.7038546800613403, Variance: 0.1427391767501831

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6559513017810774, Training Loss Force: 3.322336322904125, time: 3.3893187046051025
Validation Loss Energy: 2.272563530030462, Validation Loss Force: 3.3802167786210435, time: 0.1636362075805664
Test Loss Energy: 9.39199882486572, Test Loss Force: 11.06127378365161, time: 8.209372520446777

Epoch 7, Batch 100/226, Loss: 0.45460039377212524, Variance: 0.1389501690864563
Epoch 7, Batch 200/226, Loss: 0.6139109134674072, Variance: 0.1416221708059311

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6433167214226776, Training Loss Force: 3.3244159019820496, time: 3.4430577754974365
Validation Loss Energy: 2.9316878049061827, Validation Loss Force: 3.359217185472313, time: 0.17053866386413574
Test Loss Energy: 10.045597181334724, Test Loss Force: 11.022058529426275, time: 8.433468103408813

Epoch 8, Batch 100/226, Loss: 0.9065014719963074, Variance: 0.14213663339614868
Epoch 8, Batch 200/226, Loss: 1.1690502166748047, Variance: 0.14407439529895782

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.643609965849986, Training Loss Force: 3.3204844289394826, time: 3.424830436706543
Validation Loss Energy: 3.463839786703219, Validation Loss Force: 3.4719992748865973, time: 0.17471551895141602
Test Loss Energy: 9.51034667851432, Test Loss Force: 11.164403080194559, time: 8.229397535324097

Epoch 9, Batch 100/226, Loss: 1.2367427349090576, Variance: 0.14750584959983826
Epoch 9, Batch 200/226, Loss: 0.6609192490577698, Variance: 0.14064337313175201

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6445082511172333, Training Loss Force: 3.335550885448825, time: 3.5000498294830322
Validation Loss Energy: 2.3071821248170012, Validation Loss Force: 3.3415484444102295, time: 0.1689748764038086
Test Loss Energy: 9.371144140949996, Test Loss Force: 11.04583168537636, time: 8.475237846374512

Epoch 10, Batch 100/226, Loss: 0.5903162956237793, Variance: 0.14120498299598694
Epoch 10, Batch 200/226, Loss: 0.756665825843811, Variance: 0.13778676092624664

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.668417423077965, Training Loss Force: 3.3315383293087653, time: 3.436046838760376
Validation Loss Energy: 2.732994948937174, Validation Loss Force: 3.361125460052914, time: 0.17458891868591309
Test Loss Energy: 9.703213203047609, Test Loss Force: 10.991221128370631, time: 8.27365255355835

Epoch 11, Batch 100/226, Loss: 1.2406350374221802, Variance: 0.1438184231519699
Epoch 11, Batch 200/226, Loss: 1.4513816833496094, Variance: 0.1448076069355011

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.651927725488818, Training Loss Force: 3.330876751688886, time: 3.4791793823242188
Validation Loss Energy: 3.4094804031171657, Validation Loss Force: 3.352641097585628, time: 0.17828154563903809
Test Loss Energy: 9.598279218543391, Test Loss Force: 11.201231456409161, time: 9.334914922714233

Epoch 12, Batch 100/226, Loss: 1.1514923572540283, Variance: 0.14805276691913605
Epoch 12, Batch 200/226, Loss: 0.5931541919708252, Variance: 0.13908717036247253

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6756480641611855, Training Loss Force: 3.3199509816488977, time: 3.419867992401123
Validation Loss Energy: 2.163347932326503, Validation Loss Force: 3.370453460246835, time: 0.1669602394104004
Test Loss Energy: 9.324892159328634, Test Loss Force: 11.008847333505356, time: 8.435322999954224

Epoch 13, Batch 100/226, Loss: 0.6896322965621948, Variance: 0.1402803212404251
Epoch 13, Batch 200/226, Loss: 0.8474741578102112, Variance: 0.13806146383285522

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6134703008328195, Training Loss Force: 3.310577620498588, time: 3.559788465499878
Validation Loss Energy: 2.661862677930391, Validation Loss Force: 3.3904812695754503, time: 0.17716264724731445
Test Loss Energy: 9.636444761450925, Test Loss Force: 11.142925034587712, time: 8.250216245651245

Epoch 14, Batch 100/226, Loss: 1.115483283996582, Variance: 0.1428355723619461
Epoch 14, Batch 200/226, Loss: 1.3570704460144043, Variance: 0.1449892222881317

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6554278785431054, Training Loss Force: 3.3245718206325465, time: 3.504147529602051
Validation Loss Energy: 3.015857855412616, Validation Loss Force: 3.3968227755169007, time: 0.17294573783874512
Test Loss Energy: 9.631582045025343, Test Loss Force: 11.09374534774018, time: 8.45255994796753

Epoch 15, Batch 100/226, Loss: 1.3789085149765015, Variance: 0.14638856053352356
Epoch 15, Batch 200/226, Loss: 0.5482694506645203, Variance: 0.1376855969429016

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.663376294603486, Training Loss Force: 3.3231288358933804, time: 3.43501877784729
Validation Loss Energy: 2.159178842240011, Validation Loss Force: 3.3812052069598173, time: 0.17724108695983887
Test Loss Energy: 9.402010514642365, Test Loss Force: 11.014650511138774, time: 8.277846574783325

Epoch 16, Batch 100/226, Loss: 0.747646689414978, Variance: 0.14171800017356873
Epoch 16, Batch 200/226, Loss: 0.7650148868560791, Variance: 0.13942328095436096

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.641441991062175, Training Loss Force: 3.3150599297156185, time: 3.438941717147827
Validation Loss Energy: 2.593221573660962, Validation Loss Force: 3.3459204417559487, time: 0.16761374473571777
Test Loss Energy: 9.362893948315152, Test Loss Force: 10.840812219273989, time: 8.28439474105835

Epoch 17, Batch 100/226, Loss: 0.7847368717193604, Variance: 0.1397741138935089
Epoch 17, Batch 200/226, Loss: 1.3235833644866943, Variance: 0.14915713667869568

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6581695382170105, Training Loss Force: 3.31569426386402, time: 3.4059431552886963
Validation Loss Energy: 3.2451240757940925, Validation Loss Force: 3.4515756154838084, time: 0.16575837135314941
Test Loss Energy: 9.689576702063125, Test Loss Force: 11.200718533951191, time: 8.423757553100586

Epoch 18, Batch 100/226, Loss: 1.3013545274734497, Variance: 0.14518535137176514
Epoch 18, Batch 200/226, Loss: 0.6144194602966309, Variance: 0.13784249126911163

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.63477822128771, Training Loss Force: 3.3282642199217527, time: 3.4561736583709717
Validation Loss Energy: 2.3467016118381245, Validation Loss Force: 3.37827816536931, time: 0.1676464080810547
Test Loss Energy: 9.759000847449062, Test Loss Force: 11.200384105878534, time: 8.278319120407104

Epoch 19, Batch 100/226, Loss: 0.7714108228683472, Variance: 0.14835625886917114
Epoch 19, Batch 200/226, Loss: 1.0044879913330078, Variance: 0.14094562828540802

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.671661802192949, Training Loss Force: 3.3161043110667694, time: 3.431729793548584
Validation Loss Energy: 2.6275999641063317, Validation Loss Force: 3.5242302197740134, time: 0.1658036708831787
Test Loss Energy: 9.413340727525648, Test Loss Force: 11.151498794636762, time: 8.24943733215332

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–…â–†â–…â–„â–ƒâ–‚â–ˆâ–ƒâ–â–…â–„â–â–„â–„â–‚â–â–…â–…â–‚
wandb:   test_error_force â–ˆâ–ˆâ–ˆâ–…â–…â–…â–„â–„â–…â–„â–ƒâ–†â–ƒâ–…â–…â–ƒâ–â–†â–†â–…
wandb:          test_loss â–†â–ˆâ–ˆâ–…â–…â–…â–„â–‡â–…â–‚â–„â–…â–â–†â–†â–ƒâ–â–†â–…â–‚
wandb: train_error_energy â–ˆâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚
wandb: valid_error_energy â–â–ƒâ–ˆâ–‚â–„â–‡â–‚â–…â–ˆâ–‚â–„â–‡â–â–„â–†â–â–ƒâ–‡â–‚â–„
wandb:  valid_error_force â–ƒâ–…â–‚â–‚â–‚â–ƒâ–‚â–‚â–†â–â–‚â–â–‚â–ƒâ–ƒâ–ƒâ–â–…â–‚â–ˆ
wandb:         valid_loss â–â–ƒâ–ˆâ–‚â–ƒâ–‡â–‚â–„â–ˆâ–â–„â–‡â–â–ƒâ–…â–â–ƒâ–‡â–‚â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 7217
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.41334
wandb:   test_error_force 11.1515
wandb:          test_loss 9.03468
wandb: train_error_energy 2.67166
wandb:  train_error_force 3.3161
wandb:         train_loss 0.94716
wandb: valid_error_energy 2.6276
wandb:  valid_error_force 3.52423
wandb:         valid_loss 0.97685
wandb: 
wandb: ğŸš€ View run al_63_73 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jsa4nimr
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_145317-jsa4nimr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.773820638656616, Uncertainty Bias: -0.16274245083332062
3.2424927e-05 0.00715065
2.1819584 6.2679048
(48745, 22, 3)
Found uncertainty sample 0 after 1312 steps.
Found uncertainty sample 1 after 1504 steps.
Found uncertainty sample 2 after 2808 steps.
Found uncertainty sample 3 after 1048 steps.
Found uncertainty sample 4 after 1525 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 1672 steps.
Found uncertainty sample 7 after 1701 steps.
Found uncertainty sample 8 after 41 steps.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 2705 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 2589 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 285 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 30 steps.
Found uncertainty sample 18 after 2872 steps.
Found uncertainty sample 19 after 145 steps.
Found uncertainty sample 20 after 485 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 959 steps.
Found uncertainty sample 23 after 711 steps.
Found uncertainty sample 24 after 1976 steps.
Found uncertainty sample 25 after 1030 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 2249 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1191 steps.
Found uncertainty sample 30 after 36 steps.
Found uncertainty sample 31 after 1640 steps.
Found uncertainty sample 32 after 72 steps.
Found uncertainty sample 33 after 520 steps.
Found uncertainty sample 34 after 79 steps.
Found uncertainty sample 35 after 2549 steps.
Found uncertainty sample 36 after 1825 steps.
Found uncertainty sample 37 after 888 steps.
Found uncertainty sample 38 after 1353 steps.
Found uncertainty sample 39 after 1082 steps.
Found uncertainty sample 40 after 2576 steps.
Found uncertainty sample 41 after 20 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 431 steps.
Found uncertainty sample 44 after 1430 steps.
Found uncertainty sample 45 after 2440 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 355 steps.
Found uncertainty sample 48 after 1616 steps.
Found uncertainty sample 49 after 181 steps.
Found uncertainty sample 50 after 518 steps.
Found uncertainty sample 51 after 477 steps.
Found uncertainty sample 52 after 1078 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 670 steps.
Found uncertainty sample 55 after 306 steps.
Found uncertainty sample 56 after 448 steps.
Found uncertainty sample 57 after 1031 steps.
Found uncertainty sample 58 after 596 steps.
Found uncertainty sample 59 after 1438 steps.
Found uncertainty sample 60 after 2011 steps.
Found uncertainty sample 61 after 425 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 980 steps.
Found uncertainty sample 64 after 455 steps.
Found uncertainty sample 65 after 104 steps.
Found uncertainty sample 66 after 7 steps.
Found uncertainty sample 67 after 866 steps.
Found uncertainty sample 68 after 2913 steps.
Did not find any uncertainty samples for sample 69.
Did not find any uncertainty samples for sample 70.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 291 steps.
Found uncertainty sample 73 after 1823 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 743 steps.
Found uncertainty sample 76 after 407 steps.
Found uncertainty sample 77 after 1820 steps.
Found uncertainty sample 78 after 741 steps.
Found uncertainty sample 79 after 2050 steps.
Found uncertainty sample 80 after 962 steps.
Found uncertainty sample 81 after 225 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1646 steps.
Found uncertainty sample 84 after 2529 steps.
Found uncertainty sample 85 after 609 steps.
Found uncertainty sample 86 after 3628 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 1946 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 45 steps.
Found uncertainty sample 91 after 3091 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 3680 steps.
Found uncertainty sample 94 after 1242 steps.
Found uncertainty sample 95 after 170 steps.
Found uncertainty sample 96 after 472 steps.
Found uncertainty sample 97 after 2611 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 724 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_151555-p9ec7wou
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_74
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/p9ec7wou
Training model 74. Added 80 samples to the dataset.
Epoch 0, Batch 100/228, Loss: 0.7609988451004028, Variance: 0.1847563534975052
Epoch 0, Batch 200/228, Loss: 0.7893279790878296, Variance: 0.18393069505691528

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.471960080393273, Training Loss Force: 3.609363869056211, time: 3.5749895572662354
Validation Loss Energy: 3.272907451475373, Validation Loss Force: 3.4606717057622456, time: 0.1755974292755127
Test Loss Energy: 9.596427940269304, Test Loss Force: 10.674156100892386, time: 8.366849660873413

Epoch 1, Batch 100/228, Loss: 0.9445775747299194, Variance: 0.19102808833122253
Epoch 1, Batch 200/228, Loss: 0.8315561413764954, Variance: 0.19089199602603912

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.161074586628657, Training Loss Force: 3.393900620928175, time: 3.5235800743103027
Validation Loss Energy: 2.999235993340112, Validation Loss Force: 3.4551752848968773, time: 0.1659541130065918
Test Loss Energy: 9.411645872577864, Test Loss Force: 10.920251976211475, time: 8.34121036529541

Epoch 2, Batch 100/228, Loss: 0.9321385025978088, Variance: 0.18702220916748047
Epoch 2, Batch 200/228, Loss: 0.8614609241485596, Variance: 0.1989738792181015

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.10891059727914, Training Loss Force: 3.399239430126369, time: 3.6866986751556396
Validation Loss Energy: 3.562528978869874, Validation Loss Force: 3.363643792426071, time: 0.16931486129760742
Test Loss Energy: 9.853780470710253, Test Loss Force: 10.637789249034675, time: 8.387527227401733

Epoch 3, Batch 100/228, Loss: 0.9854781627655029, Variance: 0.19875884056091309
Epoch 3, Batch 200/228, Loss: 0.9555636048316956, Variance: 0.19705410301685333

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.167922475412995, Training Loss Force: 3.387607901015302, time: 3.4795403480529785
Validation Loss Energy: 3.2451773493872555, Validation Loss Force: 3.4552328448711, time: 0.1678304672241211
Test Loss Energy: 9.440467714493623, Test Loss Force: 10.838917120644629, time: 8.343578100204468

Epoch 4, Batch 100/228, Loss: 0.9185789823532104, Variance: 0.19297504425048828
Epoch 4, Batch 200/228, Loss: 1.0242153406143188, Variance: 0.20163238048553467

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.117507385691192, Training Loss Force: 3.39309844458716, time: 3.417707681655884
Validation Loss Energy: 3.752368843600512, Validation Loss Force: 3.3879917171894047, time: 0.1750469207763672
Test Loss Energy: 10.267682612337437, Test Loss Force: 10.774458907576964, time: 8.499659061431885

Epoch 5, Batch 100/228, Loss: 0.9172847270965576, Variance: 0.19507688283920288
Epoch 5, Batch 200/228, Loss: 0.8940311670303345, Variance: 0.19819247722625732

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.143136453499834, Training Loss Force: 3.378424223632991, time: 3.47346568107605
Validation Loss Energy: 3.381657113991478, Validation Loss Force: 3.4984285994914903, time: 0.1676936149597168
Test Loss Energy: 9.664096702005592, Test Loss Force: 10.967646294852115, time: 8.36157751083374

Epoch 6, Batch 100/228, Loss: 1.017532229423523, Variance: 0.19059252738952637
Epoch 6, Batch 200/228, Loss: 0.986731231212616, Variance: 0.1967078447341919

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.115002988207107, Training Loss Force: 3.400451624354276, time: 3.4313509464263916
Validation Loss Energy: 3.614442397555197, Validation Loss Force: 3.4005837687261935, time: 0.16910147666931152
Test Loss Energy: 10.254880045776085, Test Loss Force: 10.919187659662994, time: 8.366604566574097

Epoch 7, Batch 100/228, Loss: 0.8580858707427979, Variance: 0.19934767484664917
Epoch 7, Batch 200/228, Loss: 0.8717169761657715, Variance: 0.19985909759998322

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.128676723971885, Training Loss Force: 3.397130913479959, time: 3.4782493114471436
Validation Loss Energy: 3.372795772660135, Validation Loss Force: 3.394729240206887, time: 0.16858530044555664
Test Loss Energy: 9.586842665404532, Test Loss Force: 10.95333457241584, time: 8.546570539474487

Epoch 8, Batch 100/228, Loss: 0.9086872935295105, Variance: 0.19392360746860504
Epoch 8, Batch 200/228, Loss: 0.9301550388336182, Variance: 0.20251423120498657

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.115630675438644, Training Loss Force: 3.3822523076419233, time: 3.3822851181030273
Validation Loss Energy: 3.7455216560286595, Validation Loss Force: 3.487623634852631, time: 0.17574191093444824
Test Loss Energy: 10.275317277504513, Test Loss Force: 11.052302539358356, time: 8.393791198730469

Epoch 9, Batch 100/228, Loss: 0.7972264885902405, Variance: 0.19918397068977356
Epoch 9, Batch 200/228, Loss: 0.9617523550987244, Variance: 0.19759489595890045

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.1261100347942286, Training Loss Force: 3.3785478255179813, time: 3.475534200668335
Validation Loss Energy: 3.1958559665623913, Validation Loss Force: 3.3455908914199752, time: 0.17056012153625488
Test Loss Energy: 9.643093764739346, Test Loss Force: 11.029924458068827, time: 8.56631350517273

Epoch 10, Batch 100/228, Loss: 0.9521088600158691, Variance: 0.19572198390960693
Epoch 10, Batch 200/228, Loss: 1.0121010541915894, Variance: 0.19957402348518372

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.128528957601466, Training Loss Force: 3.4171981245045946, time: 3.4594030380249023
Validation Loss Energy: 3.3980377697116557, Validation Loss Force: 3.529828233437211, time: 0.1664721965789795
Test Loss Energy: 10.15460091430101, Test Loss Force: 11.0628401433467, time: 8.309478044509888

Epoch 11, Batch 100/228, Loss: 0.9525187015533447, Variance: 0.2023492008447647
Epoch 11, Batch 200/228, Loss: 0.9057168364524841, Variance: 0.19298747181892395

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.11140790450884, Training Loss Force: 3.403376488084277, time: 3.5181572437286377
Validation Loss Energy: 3.251085652505304, Validation Loss Force: 3.3701721734482417, time: 0.16694021224975586
Test Loss Energy: 9.760927432195137, Test Loss Force: 11.201023233944158, time: 8.353813886642456

Epoch 12, Batch 100/228, Loss: 0.8474788665771484, Variance: 0.19106608629226685
Epoch 12, Batch 200/228, Loss: 0.8951960206031799, Variance: 0.19931650161743164

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.075367556971052, Training Loss Force: 3.3605804558326366, time: 3.492685317993164
Validation Loss Energy: 3.7873083602936193, Validation Loss Force: 3.397995406906643, time: 0.17017626762390137
Test Loss Energy: 10.168668922442547, Test Loss Force: 10.877939214632002, time: 8.51684832572937

Epoch 13, Batch 100/228, Loss: 1.019881010055542, Variance: 0.2052568942308426
Epoch 13, Batch 200/228, Loss: 1.122298240661621, Variance: 0.19606515765190125

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.121146912881381, Training Loss Force: 3.381563639827903, time: 3.4065401554107666
Validation Loss Energy: 3.879724183277478, Validation Loss Force: 3.350799568081519, time: 0.16632652282714844
Test Loss Energy: 9.980505701879267, Test Loss Force: 11.032613713784885, time: 8.356680870056152

Epoch 14, Batch 100/228, Loss: 1.0382126569747925, Variance: 0.19605346024036407
Epoch 14, Batch 200/228, Loss: 1.0118376016616821, Variance: 0.20352929830551147

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.0950539754341975, Training Loss Force: 3.3981541543817846, time: 3.4654269218444824
Validation Loss Energy: 3.7358871517553696, Validation Loss Force: 3.3332287457763567, time: 0.16944479942321777
Test Loss Energy: 10.1050411836827, Test Loss Force: 10.911754744400199, time: 8.552776336669922

Epoch 15, Batch 100/228, Loss: 0.9701740741729736, Variance: 0.19921579957008362
Epoch 15, Batch 200/228, Loss: 0.9194269180297852, Variance: 0.1976604461669922

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.089235986628752, Training Loss Force: 3.3695666432616527, time: 3.513763189315796
Validation Loss Energy: 3.2527812223842574, Validation Loss Force: 3.3896468169510863, time: 0.16837692260742188
Test Loss Energy: 9.685388820041393, Test Loss Force: 11.165537811488663, time: 8.339320182800293

Epoch 16, Batch 100/228, Loss: 0.8198426365852356, Variance: 0.19661357998847961
Epoch 16, Batch 200/228, Loss: 0.9185622930526733, Variance: 0.20143096148967743

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.09270068631096, Training Loss Force: 3.3958035797712265, time: 3.3996541500091553
Validation Loss Energy: 3.524245528277796, Validation Loss Force: 3.4381073608577934, time: 0.16863560676574707
Test Loss Energy: 9.873391397056862, Test Loss Force: 10.817576374465292, time: 8.343492984771729

Epoch 17, Batch 100/228, Loss: 1.1135650873184204, Variance: 0.2056736946105957
Epoch 17, Batch 200/228, Loss: 1.110292911529541, Variance: 0.1998048573732376

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.107105266648839, Training Loss Force: 3.401541039987061, time: 3.4965431690216064
Validation Loss Energy: 3.4894490208436606, Validation Loss Force: 3.400334212280895, time: 0.16572952270507812
Test Loss Energy: 9.879722316887426, Test Loss Force: 11.061434543780127, time: 8.479148149490356

Epoch 18, Batch 100/228, Loss: 0.8648512363433838, Variance: 0.20076988637447357
Epoch 18, Batch 200/228, Loss: 1.041014552116394, Variance: 0.20558735728263855

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.090260138227851, Training Loss Force: 3.366702495938679, time: 3.4403293132781982
Validation Loss Energy: 3.749344272924088, Validation Loss Force: 3.415980950496095, time: 0.1667940616607666
Test Loss Energy: 10.550505065599806, Test Loss Force: 10.997228023790623, time: 8.385833978652954

Epoch 19, Batch 100/228, Loss: 1.0681486129760742, Variance: 0.2056685984134674
Epoch 19, Batch 200/228, Loss: 0.8250827789306641, Variance: 0.19714491069316864

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.068239123130296, Training Loss Force: 3.3873155598655567, time: 3.450277805328369
Validation Loss Energy: 3.3407102064103844, Validation Loss Force: 3.451251576209197, time: 0.17077183723449707
Test Loss Energy: 9.693870980299168, Test Loss Force: 11.096665479883777, time: 8.553776025772095

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–„â–â–†â–ƒâ–†â–‚â–†â–‚â–†â–ƒâ–†â–„â–…â–ƒâ–„â–„â–ˆâ–ƒ
wandb:   test_error_force â–â–…â–â–ƒâ–ƒâ–…â–„â–…â–†â–†â–†â–ˆâ–„â–†â–„â–ˆâ–ƒâ–†â–…â–‡
wandb:          test_loss â–‚â–‚â–‚â–â–†â–„â–‡â–ƒâ–†â–„â–‡â–†â–†â–†â–…â–…â–ƒâ–†â–ˆâ–…
wandb: train_error_energy â–ˆâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–‚â–‚â–â–‚
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–â–…â–ƒâ–‡â–„â–†â–„â–‡â–ƒâ–„â–ƒâ–‡â–ˆâ–‡â–ƒâ–…â–…â–‡â–„
wandb:  valid_error_force â–†â–…â–‚â–…â–ƒâ–‡â–ƒâ–ƒâ–†â–â–ˆâ–‚â–ƒâ–‚â–â–ƒâ–…â–ƒâ–„â–…
wandb:         valid_loss â–ƒâ–â–…â–ƒâ–‡â–…â–†â–ƒâ–ˆâ–â–…â–‚â–‡â–‡â–†â–‚â–…â–„â–‡â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 7289
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.69387
wandb:   test_error_force 11.09667
wandb:          test_loss 8.0676
wandb: train_error_energy 4.06824
wandb:  train_error_force 3.38732
wandb:         train_loss 1.392
wandb: valid_error_energy 3.34071
wandb:  valid_error_force 3.45125
wandb:         valid_loss 1.17444
wandb: 
wandb: ğŸš€ View run al_63_74 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/p9ec7wou
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_151555-p9ec7wou/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.531905174255371, Uncertainty Bias: -0.25754231214523315
3.6239624e-05 0.046596527
1.973391 6.0086513
(48745, 22, 3)
Found uncertainty sample 0 after 2551 steps.
Found uncertainty sample 1 after 2874 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 2717 steps.
Found uncertainty sample 5 after 3014 steps.
Found uncertainty sample 6 after 701 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1714 steps.
Found uncertainty sample 9 after 79 steps.
Found uncertainty sample 10 after 2421 steps.
Found uncertainty sample 11 after 2591 steps.
Found uncertainty sample 12 after 2031 steps.
Found uncertainty sample 13 after 528 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1094 steps.
Found uncertainty sample 16 after 126 steps.
Found uncertainty sample 17 after 1469 steps.
Found uncertainty sample 18 after 2388 steps.
Did not find any uncertainty samples for sample 19.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 625 steps.
Found uncertainty sample 22 after 3376 steps.
Found uncertainty sample 23 after 908 steps.
Found uncertainty sample 24 after 2037 steps.
Found uncertainty sample 25 after 310 steps.
Found uncertainty sample 26 after 232 steps.
Found uncertainty sample 27 after 679 steps.
Found uncertainty sample 28 after 623 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 12 steps.
Found uncertainty sample 31 after 2059 steps.
Found uncertainty sample 32 after 3685 steps.
Found uncertainty sample 33 after 1482 steps.
Found uncertainty sample 34 after 2833 steps.
Found uncertainty sample 35 after 494 steps.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 1055 steps.
Found uncertainty sample 39 after 2466 steps.
Found uncertainty sample 40 after 2979 steps.
Found uncertainty sample 41 after 30 steps.
Found uncertainty sample 42 after 38 steps.
Found uncertainty sample 43 after 40 steps.
Found uncertainty sample 44 after 16 steps.
Found uncertainty sample 45 after 2207 steps.
Found uncertainty sample 46 after 2832 steps.
Found uncertainty sample 47 after 196 steps.
Found uncertainty sample 48 after 115 steps.
Found uncertainty sample 49 after 1065 steps.
Found uncertainty sample 50 after 1538 steps.
Found uncertainty sample 51 after 1841 steps.
Found uncertainty sample 52 after 821 steps.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 1110 steps.
Found uncertainty sample 57 after 1605 steps.
Found uncertainty sample 58 after 59 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 1243 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 780 steps.
Found uncertainty sample 63 after 1816 steps.
Found uncertainty sample 64 after 290 steps.
Found uncertainty sample 65 after 2679 steps.
Found uncertainty sample 66 after 2209 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 112 steps.
Found uncertainty sample 69 after 2366 steps.
Found uncertainty sample 70 after 3102 steps.
Found uncertainty sample 71 after 2340 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 1528 steps.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 12 steps.
Found uncertainty sample 77 after 3 steps.
Found uncertainty sample 78 after 7 steps.
Found uncertainty sample 79 after 1854 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 685 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 33 steps.
Found uncertainty sample 84 after 3240 steps.
Found uncertainty sample 85 after 848 steps.
Found uncertainty sample 86 after 1339 steps.
Found uncertainty sample 87 after 68 steps.
Found uncertainty sample 88 after 17 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 3937 steps.
Found uncertainty sample 91 after 759 steps.
Found uncertainty sample 92 after 1870 steps.
Found uncertainty sample 93 after 717 steps.
Found uncertainty sample 94 after 2159 steps.
Found uncertainty sample 95 after 3881 steps.
Found uncertainty sample 96 after 1120 steps.
Found uncertainty sample 97 after 20 steps.
Found uncertainty sample 98 after 46 steps.
Found uncertainty sample 99 after 1952 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_153946-s3eq5jb4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_75
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/s3eq5jb4
Training model 75. Added 80 samples to the dataset.
Epoch 0, Batch 100/231, Loss: 0.586731493473053, Variance: 0.1355450451374054
Epoch 0, Batch 200/231, Loss: 0.735246479511261, Variance: 0.14609698951244354

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7094016352093706, Training Loss Force: 3.4682507550052186, time: 3.4952392578125
Validation Loss Energy: 1.5372174076458438, Validation Loss Force: 3.378749889954958, time: 0.1765592098236084
Test Loss Energy: 9.233832966769253, Test Loss Force: 11.090568472242298, time: 8.397219896316528

Epoch 1, Batch 100/231, Loss: 0.5831617116928101, Variance: 0.1501525342464447
Epoch 1, Batch 200/231, Loss: 0.9429806470870972, Variance: 0.14530125260353088

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5761658549756694, Training Loss Force: 3.305980407664168, time: 3.4178597927093506
Validation Loss Energy: 2.7904017267871093, Validation Loss Force: 3.5625705066655895, time: 0.17320966720581055
Test Loss Energy: 9.76873379276083, Test Loss Force: 11.123733211211178, time: 8.393423318862915

Epoch 2, Batch 100/231, Loss: 0.7882015109062195, Variance: 0.12987542152404785
Epoch 2, Batch 200/231, Loss: 0.6985197067260742, Variance: 0.17507830262184143

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.50235509925498, Training Loss Force: 3.858309883303492, time: 3.7411794662475586
Validation Loss Energy: 1.7135018149461927, Validation Loss Force: 3.4062081130158965, time: 0.17568612098693848
Test Loss Energy: 8.979679672005343, Test Loss Force: 10.703014600444922, time: 9.533847570419312

Epoch 3, Batch 100/231, Loss: 1.586403727531433, Variance: 0.199324369430542
Epoch 3, Batch 200/231, Loss: 1.6784348487854004, Variance: 0.18940086662769318

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.080390342134103, Training Loss Force: 3.3470281720781867, time: 3.5387542247772217
Validation Loss Energy: 4.817757038933395, Validation Loss Force: 3.3949308671185254, time: 0.17473077774047852
Test Loss Energy: 10.370876727752181, Test Loss Force: 10.784791469421927, time: 8.46043062210083

Epoch 4, Batch 100/231, Loss: 1.8558026552200317, Variance: 0.19629362225532532
Epoch 4, Batch 200/231, Loss: 1.9278842210769653, Variance: 0.1887829601764679

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.060932594496149, Training Loss Force: 3.3858639122784875, time: 3.5086967945098877
Validation Loss Energy: 5.7798961676620255, Validation Loss Force: 3.4936682671552735, time: 0.17188429832458496
Test Loss Energy: 10.718112058177786, Test Loss Force: 10.958864969853233, time: 8.660880327224731

Epoch 5, Batch 100/231, Loss: 1.1560825109481812, Variance: 0.19445928931236267
Epoch 5, Batch 200/231, Loss: 1.0074974298477173, Variance: 0.19143129885196686

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.086398702741399, Training Loss Force: 3.365119187101728, time: 3.4770853519439697
Validation Loss Energy: 3.345387872826176, Validation Loss Force: 3.534996272979622, time: 0.17379164695739746
Test Loss Energy: 9.554254800404976, Test Loss Force: 11.051584257657488, time: 8.43851113319397

Epoch 6, Batch 100/231, Loss: 0.9806621074676514, Variance: 0.195114403963089
Epoch 6, Batch 200/231, Loss: 0.9537148475646973, Variance: 0.20174261927604675

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.114325638811887, Training Loss Force: 3.373041152854028, time: 3.479351043701172
Validation Loss Energy: 2.5708670944265473, Validation Loss Force: 3.6754914302923596, time: 0.1727001667022705
Test Loss Energy: 9.440415836080044, Test Loss Force: 10.938358008873722, time: 8.507680654525757

Epoch 7, Batch 100/231, Loss: 1.6487846374511719, Variance: 0.19167393445968628
Epoch 7, Batch 200/231, Loss: 1.6010863780975342, Variance: 0.19947272539138794

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.06754166247876, Training Loss Force: 3.364959565133976, time: 3.6969611644744873
Validation Loss Energy: 5.253272615420085, Validation Loss Force: 3.3691119899771924, time: 0.1778254508972168
Test Loss Energy: 10.91460685403423, Test Loss Force: 10.820652608172447, time: 8.463175058364868

Epoch 8, Batch 100/231, Loss: 1.7585718631744385, Variance: 0.1886051893234253
Epoch 8, Batch 200/231, Loss: 1.7707171440124512, Variance: 0.20343026518821716

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.065686902234263, Training Loss Force: 3.3900917756932603, time: 3.4975383281707764
Validation Loss Energy: 9.663879677697384, Validation Loss Force: 3.5872844438375413, time: 0.1713275909423828
Test Loss Energy: 13.275750424985592, Test Loss Force: 10.920014986291784, time: 8.440825700759888

Epoch 9, Batch 100/231, Loss: 0.43826156854629517, Variance: 0.11971645802259445
Epoch 9, Batch 200/231, Loss: 0.9111509323120117, Variance: 0.10564544051885605

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.010928965151134, Training Loss Force: 3.5212770147998436, time: 3.4836602210998535
Validation Loss Energy: 1.617062332661776, Validation Loss Force: 3.348010859697919, time: 0.1740248203277588
Test Loss Energy: 9.040750338915325, Test Loss Force: 11.098258378010474, time: 8.580575227737427

Epoch 10, Batch 100/231, Loss: 0.5714170336723328, Variance: 0.09974786639213562
Epoch 10, Batch 200/231, Loss: 0.6201789379119873, Variance: 0.10284429788589478

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.649082265855353, Training Loss Force: 3.3175643747674255, time: 3.5869157314300537
Validation Loss Energy: 1.8272294410550156, Validation Loss Force: 3.393387992606391, time: 0.1817476749420166
Test Loss Energy: 9.325734100514563, Test Loss Force: 11.132753960334865, time: 8.439516544342041

Epoch 11, Batch 100/231, Loss: 0.657209575176239, Variance: 0.10003259778022766
Epoch 11, Batch 200/231, Loss: 0.4333401918411255, Variance: 0.09915651381015778

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6168599265286194, Training Loss Force: 3.309695346435715, time: 3.600316047668457
Validation Loss Energy: 2.1041102017568907, Validation Loss Force: 3.4508943035645556, time: 0.17107343673706055
Test Loss Energy: 9.19106428136994, Test Loss Force: 11.01535886482683, time: 8.44213056564331

Epoch 12, Batch 100/231, Loss: 0.4914247989654541, Variance: 0.09681446850299835
Epoch 12, Batch 200/231, Loss: 0.28161513805389404, Variance: 0.09628468751907349

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6271003345135802, Training Loss Force: 3.3310597714128196, time: 3.74114990234375
Validation Loss Energy: 1.5544342138426235, Validation Loss Force: 3.374067825823534, time: 0.1717057228088379
Test Loss Energy: 9.026254465597946, Test Loss Force: 11.245489320413842, time: 8.364940166473389

Epoch 13, Batch 100/231, Loss: 0.3951641321182251, Variance: 0.09327040612697601
Epoch 13, Batch 200/231, Loss: 0.5453058481216431, Variance: 0.09394830465316772

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6188905963623357, Training Loss Force: 3.331528632627481, time: 3.5009572505950928
Validation Loss Energy: 1.4160255777618023, Validation Loss Force: 3.3739472504810144, time: 0.17014575004577637
Test Loss Energy: 8.97470546245704, Test Loss Force: 11.307724297474318, time: 8.436256647109985

Epoch 14, Batch 100/231, Loss: 0.5898516178131104, Variance: 0.13074758648872375
Epoch 14, Batch 200/231, Loss: 1.4097013473510742, Variance: 0.1329784095287323

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5460375606399483, Training Loss Force: 3.8712794019377674, time: 3.462777853012085
Validation Loss Energy: 2.120197546544595, Validation Loss Force: 3.3357320479821633, time: 0.17445683479309082
Test Loss Energy: 9.250564093232857, Test Loss Force: 11.200349700580974, time: 8.571455955505371

Epoch 15, Batch 100/231, Loss: 1.0510188341140747, Variance: 0.1389545351266861
Epoch 15, Batch 200/231, Loss: 1.321956992149353, Variance: 0.1342088282108307

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.640991478365632, Training Loss Force: 3.2901388638282887, time: 3.5959293842315674
Validation Loss Energy: 2.634997812476063, Validation Loss Force: 3.4449307228310313, time: 0.16929221153259277
Test Loss Energy: 9.50991058274073, Test Loss Force: 11.123739809679066, time: 8.389631032943726

Epoch 16, Batch 100/231, Loss: 0.8215922713279724, Variance: 0.10888677835464478
Epoch 16, Batch 200/231, Loss: 0.5250102877616882, Variance: 0.1001797467470169

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.1853343370528604, Training Loss Force: 4.029007245685427, time: 3.529531955718994
Validation Loss Energy: 1.9534609973861272, Validation Loss Force: 3.3804002579972883, time: 0.16963768005371094
Test Loss Energy: 9.33964937860652, Test Loss Force: 11.05939773897262, time: 8.398108005523682

Epoch 17, Batch 100/231, Loss: 0.49601972103118896, Variance: 0.09759991616010666
Epoch 17, Batch 200/231, Loss: 0.7751783132553101, Variance: 0.09738306701183319

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6330849871224176, Training Loss Force: 3.3055189745379856, time: 3.745522975921631
Validation Loss Energy: 1.885714410743176, Validation Loss Force: 3.42592028883016, time: 0.17198395729064941
Test Loss Energy: 9.395591776898584, Test Loss Force: 11.26904918963311, time: 8.37375807762146

Epoch 18, Batch 100/231, Loss: 0.40660303831100464, Variance: 0.0924958735704422
Epoch 18, Batch 200/231, Loss: 0.318942129611969, Variance: 0.09344880282878876

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6122351413841627, Training Loss Force: 3.304250558544954, time: 3.5056848526000977
Validation Loss Energy: 1.7345608607568859, Validation Loss Force: 3.3815269082083965, time: 0.1723639965057373
Test Loss Energy: 8.961140812492461, Test Loss Force: 11.460368557644982, time: 8.42088794708252

Epoch 19, Batch 100/231, Loss: 0.45561808347702026, Variance: 0.09230081737041473
Epoch 19, Batch 200/231, Loss: 0.4352409839630127, Variance: 0.0928293764591217

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.646774959971796, Training Loss Force: 3.353126749355205, time: 3.5524299144744873
Validation Loss Energy: 1.32532569888375, Validation Loss Force: 3.3821394710288435, time: 0.17319822311401367
Test Loss Energy: 9.03844763217337, Test Loss Force: 11.271116599207206, time: 8.579517602920532

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–ƒâ–„â–‚â–‚â–„â–ˆâ–â–‚â–â–â–â–â–‚â–‚â–‚â–â–
wandb:   test_error_force â–…â–…â–â–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–…â–…â–„â–†â–‡â–†â–…â–„â–†â–ˆâ–†
wandb:          test_loss â–„â–„â–â–‚â–‚â–â–â–‚â–„â–‡â–ˆâ–‡â–ˆâ–ˆâ–…â–„â–‡â–ˆâ–ˆâ–‡
wandb: train_error_energy â–„â–„â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚â–â–â–â–â–„â–„â–ƒâ–â–â–
wandb:  train_error_force â–ƒâ–â–†â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–â–â–â–‡â–â–ˆâ–â–â–‚
wandb:         train_loss â–…â–„â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ƒâ–â–â–â–â–…â–„â–…â–â–â–
wandb: valid_error_energy â–â–‚â–â–„â–…â–ƒâ–‚â–„â–ˆâ–â–â–‚â–â–â–‚â–‚â–‚â–â–â–
wandb:  valid_error_force â–‚â–†â–‚â–‚â–„â–…â–ˆâ–‚â–†â–â–‚â–ƒâ–‚â–‚â–â–ƒâ–‚â–ƒâ–‚â–‚
wandb:         valid_loss â–â–‚â–‚â–ƒâ–„â–ƒâ–‚â–„â–ˆâ–â–â–‚â–â–â–‚â–‚â–‚â–‚â–â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 7361
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.03845
wandb:   test_error_force 11.27112
wandb:          test_loss 11.5536
wandb: train_error_energy 1.64677
wandb:  train_error_force 3.35313
wandb:         train_loss 0.52715
wandb: valid_error_energy 1.32533
wandb:  valid_error_force 3.38214
wandb:         valid_loss 0.40078
wandb: 
wandb: ğŸš€ View run al_63_75 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/s3eq5jb4
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_153946-s3eq5jb4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.687981605529785, Uncertainty Bias: -0.11833395063877106
6.866455e-05 0.0018482208
1.980905 6.5972786
(48745, 22, 3)
Found uncertainty sample 0 after 1259 steps.
Found uncertainty sample 1 after 1741 steps.
Found uncertainty sample 2 after 1163 steps.
Found uncertainty sample 3 after 13 steps.
Found uncertainty sample 4 after 1785 steps.
Found uncertainty sample 5 after 19 steps.
Found uncertainty sample 6 after 586 steps.
Found uncertainty sample 7 after 3256 steps.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 3391 steps.
Found uncertainty sample 11 after 1608 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 3542 steps.
Found uncertainty sample 14 after 1766 steps.
Found uncertainty sample 15 after 3759 steps.
Found uncertainty sample 16 after 3680 steps.
Found uncertainty sample 17 after 1432 steps.
Found uncertainty sample 18 after 2241 steps.
Found uncertainty sample 19 after 2498 steps.
Found uncertainty sample 20 after 3890 steps.
Found uncertainty sample 21 after 1756 steps.
Found uncertainty sample 22 after 1937 steps.
Found uncertainty sample 23 after 68 steps.
Found uncertainty sample 24 after 7 steps.
Found uncertainty sample 25 after 3887 steps.
Found uncertainty sample 26 after 2576 steps.
Found uncertainty sample 27 after 401 steps.
Found uncertainty sample 28 after 1415 steps.
Did not find any uncertainty samples for sample 29.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1467 steps.
Found uncertainty sample 32 after 2293 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 745 steps.
Found uncertainty sample 35 after 476 steps.
Found uncertainty sample 36 after 433 steps.
Found uncertainty sample 37 after 1597 steps.
Found uncertainty sample 38 after 1678 steps.
Found uncertainty sample 39 after 63 steps.
Found uncertainty sample 40 after 1964 steps.
Found uncertainty sample 41 after 2915 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 78 steps.
Found uncertainty sample 44 after 786 steps.
Found uncertainty sample 45 after 1204 steps.
Found uncertainty sample 46 after 124 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 776 steps.
Found uncertainty sample 49 after 576 steps.
Found uncertainty sample 50 after 783 steps.
Found uncertainty sample 51 after 1754 steps.
Found uncertainty sample 52 after 573 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 39 steps.
Found uncertainty sample 55 after 2884 steps.
Found uncertainty sample 56 after 116 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 1242 steps.
Found uncertainty sample 59 after 3020 steps.
Found uncertainty sample 60 after 442 steps.
Found uncertainty sample 61 after 142 steps.
Found uncertainty sample 62 after 2385 steps.
Found uncertainty sample 63 after 395 steps.
Found uncertainty sample 64 after 3099 steps.
Found uncertainty sample 65 after 1994 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1572 steps.
Found uncertainty sample 68 after 661 steps.
Found uncertainty sample 69 after 2883 steps.
Found uncertainty sample 70 after 1180 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 739 steps.
Found uncertainty sample 73 after 565 steps.
Found uncertainty sample 74 after 2880 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 1409 steps.
Found uncertainty sample 77 after 1259 steps.
Found uncertainty sample 78 after 75 steps.
Found uncertainty sample 79 after 2378 steps.
Found uncertainty sample 80 after 731 steps.
Found uncertainty sample 81 after 660 steps.
Found uncertainty sample 82 after 181 steps.
Found uncertainty sample 83 after 2128 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1241 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 640 steps.
Found uncertainty sample 88 after 1861 steps.
Found uncertainty sample 89 after 210 steps.
Found uncertainty sample 90 after 1399 steps.
Found uncertainty sample 91 after 2161 steps.
Found uncertainty sample 92 after 3686 steps.
Found uncertainty sample 93 after 3180 steps.
Found uncertainty sample 94 after 1685 steps.
Found uncertainty sample 95 after 2646 steps.
Found uncertainty sample 96 after 2182 steps.
Found uncertainty sample 97 after 462 steps.
Found uncertainty sample 98 after 1855 steps.
Found uncertainty sample 99 after 84 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_160217-f5vu4q0k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_76
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/f5vu4q0k
Training model 76. Added 89 samples to the dataset.
Epoch 0, Batch 100/233, Loss: 0.7779000401496887, Variance: 0.16067498922348022
Epoch 0, Batch 200/233, Loss: 0.9282233119010925, Variance: 0.16959840059280396

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.063671370543733, Training Loss Force: 3.74564250254015, time: 3.57822847366333
Validation Loss Energy: 5.739211842671269, Validation Loss Force: 3.5018803609243765, time: 0.18044257164001465
Test Loss Energy: 10.560163916878913, Test Loss Force: 10.79636064010719, time: 8.576826095581055

Epoch 1, Batch 100/233, Loss: 1.0920770168304443, Variance: 0.17273011803627014
Epoch 1, Batch 200/233, Loss: 1.1449589729309082, Variance: 0.17185957729816437

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.123305588743545, Training Loss Force: 3.360004970036289, time: 3.526310682296753
Validation Loss Energy: 4.998466299667976, Validation Loss Force: 3.3904681877631266, time: 0.17709827423095703
Test Loss Energy: 10.078540430954407, Test Loss Force: 10.760320246865167, time: 8.640324354171753

Epoch 2, Batch 100/233, Loss: 2.002718448638916, Variance: 0.18614667654037476
Epoch 2, Batch 200/233, Loss: 1.8694798946380615, Variance: 0.17699085175991058

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.153980122356852, Training Loss Force: 3.3700831036396317, time: 3.7423012256622314
Validation Loss Energy: 1.6719592952333857, Validation Loss Force: 3.453541283036772, time: 0.17903947830200195
Test Loss Energy: 9.083832276668096, Test Loss Force: 10.865795720763227, time: 8.636784553527832

Epoch 3, Batch 100/233, Loss: 1.6431050300598145, Variance: 0.18190228939056396
Epoch 3, Batch 200/233, Loss: 1.6916791200637817, Variance: 0.18456250429153442

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.11445794873214, Training Loss Force: 3.384754103746053, time: 3.5024571418762207
Validation Loss Energy: 3.1359372024099583, Validation Loss Force: 3.3582238584123294, time: 0.17364716529846191
Test Loss Energy: 9.742736440178549, Test Loss Force: 10.770980405153491, time: 8.630937337875366

Epoch 4, Batch 100/233, Loss: 0.9160486459732056, Variance: 0.18555018305778503
Epoch 4, Batch 200/233, Loss: 1.0041743516921997, Variance: 0.1797618865966797

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.093723443679466, Training Loss Force: 3.394962659350906, time: 3.5466082096099854
Validation Loss Energy: 5.32581677581391, Validation Loss Force: 3.370315182707213, time: 0.18497109413146973
Test Loss Energy: 10.492909063276501, Test Loss Force: 10.683263949792149, time: 8.818699598312378

Epoch 5, Batch 100/233, Loss: 1.175913691520691, Variance: 0.17900756001472473
Epoch 5, Batch 200/233, Loss: 1.1125874519348145, Variance: 0.18622809648513794

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.0974712482812565, Training Loss Force: 3.4186228453709258, time: 3.4926340579986572
Validation Loss Energy: 5.576318340302367, Validation Loss Force: 3.3659121230156375, time: 0.18466424942016602
Test Loss Energy: 10.693783455791898, Test Loss Force: 10.68159685970499, time: 8.641151189804077

Epoch 6, Batch 100/233, Loss: 1.9079484939575195, Variance: 0.17743268609046936
Epoch 6, Batch 200/233, Loss: 2.210575580596924, Variance: 0.2023431956768036

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.105311823597602, Training Loss Force: 3.378909058734341, time: 3.539680242538452
Validation Loss Energy: 2.0546132443104623, Validation Loss Force: 3.449838239472707, time: 0.18158292770385742
Test Loss Energy: 9.175827477052392, Test Loss Force: 10.862186988366178, time: 8.633383989334106

Epoch 7, Batch 100/233, Loss: 1.529283046722412, Variance: 0.17600715160369873
Epoch 7, Batch 200/233, Loss: 1.827155590057373, Variance: 0.19257193803787231

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.080299813192655, Training Loss Force: 3.3700415199377147, time: 3.866774797439575
Validation Loss Energy: 3.553507816759042, Validation Loss Force: 3.450642520427798, time: 0.18048763275146484
Test Loss Energy: 9.450512769667945, Test Loss Force: 10.741131351995936, time: 8.651113510131836

Epoch 8, Batch 100/233, Loss: 0.8519704937934875, Variance: 0.1776883900165558
Epoch 8, Batch 200/233, Loss: 0.9408649802207947, Variance: 0.1889733076095581

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.100028087842819, Training Loss Force: 3.3838582407869984, time: 3.5652692317962646
Validation Loss Energy: 5.674248230681409, Validation Loss Force: 3.4165700773889336, time: 0.18175148963928223
Test Loss Energy: 10.703644159566306, Test Loss Force: 11.158440247617706, time: 9.691882133483887

Epoch 9, Batch 100/233, Loss: 1.1096644401550293, Variance: 0.1894732117652893
Epoch 9, Batch 200/233, Loss: 1.1811326742172241, Variance: 0.1861102283000946

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.112676668194996, Training Loss Force: 3.3843314557268656, time: 3.5793604850769043
Validation Loss Energy: 5.009085836575565, Validation Loss Force: 3.5219923391901524, time: 0.18622374534606934
Test Loss Energy: 10.326034272135528, Test Loss Force: 11.135500653032501, time: 8.800174951553345

Epoch 10, Batch 100/233, Loss: 2.0455737113952637, Variance: 0.19020462036132812
Epoch 10, Batch 200/233, Loss: 1.7394367456436157, Variance: 0.18248209357261658

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.079309476397726, Training Loss Force: 3.386132689521939, time: 3.5090386867523193
Validation Loss Energy: 2.4440801734118485, Validation Loss Force: 3.3768202391363475, time: 0.1780993938446045
Test Loss Energy: 9.219279912947849, Test Loss Force: 10.996808972116057, time: 8.585821390151978

Epoch 11, Batch 100/233, Loss: 1.6609845161437988, Variance: 0.19091585278511047
Epoch 11, Batch 200/233, Loss: 1.5113364458084106, Variance: 0.1798090636730194

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.069672559795716, Training Loss Force: 3.410613753471545, time: 3.609194278717041
Validation Loss Energy: 3.7782099876564375, Validation Loss Force: 3.4019638164253676, time: 0.17830848693847656
Test Loss Energy: 9.843628054033383, Test Loss Force: 10.888448759976342, time: 8.797626972198486

Epoch 12, Batch 100/233, Loss: 0.8885456323623657, Variance: 0.18772047758102417
Epoch 12, Batch 200/233, Loss: 0.878791868686676, Variance: 0.18396121263504028

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.08748373671983, Training Loss Force: 3.3788821788413475, time: 3.582138776779175
Validation Loss Energy: 5.829751010093247, Validation Loss Force: 3.4463376021622896, time: 0.177154541015625
Test Loss Energy: 10.800012553997254, Test Loss Force: 10.724470052711204, time: 8.675756216049194

Epoch 13, Batch 100/233, Loss: 1.1298365592956543, Variance: 0.1821240484714508
Epoch 13, Batch 200/233, Loss: 1.0998762845993042, Variance: 0.19120439887046814

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.1153438059145495, Training Loss Force: 3.336860292714602, time: 3.6004412174224854
Validation Loss Energy: 5.288313947580168, Validation Loss Force: 3.3138587011048117, time: 0.18088746070861816
Test Loss Energy: 10.871632697474544, Test Loss Force: 10.821656517262992, time: 8.622948408126831

Epoch 14, Batch 100/233, Loss: 1.720190405845642, Variance: 0.18267808854579926
Epoch 14, Batch 200/233, Loss: 1.521209955215454, Variance: 0.18745480477809906

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.120954848111938, Training Loss Force: 3.407678637735344, time: 3.507781744003296
Validation Loss Energy: 2.2707358060134, Validation Loss Force: 3.5288649110446406, time: 0.17923617362976074
Test Loss Energy: 9.611391759208786, Test Loss Force: 11.035467073164067, time: 8.788320302963257

Epoch 15, Batch 100/233, Loss: 1.6557233333587646, Variance: 0.18195435404777527
Epoch 15, Batch 200/233, Loss: 1.5641993284225464, Variance: 0.1900622695684433

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.090292804039696, Training Loss Force: 3.3591191568143106, time: 3.623241424560547
Validation Loss Energy: 3.278436994376198, Validation Loss Force: 3.3467939171481365, time: 0.17725324630737305
Test Loss Energy: 9.656533659360239, Test Loss Force: 11.001233195868759, time: 8.574136734008789

Epoch 16, Batch 100/233, Loss: 0.8454375267028809, Variance: 0.18231020867824554
Epoch 16, Batch 200/233, Loss: 0.9520531892776489, Variance: 0.18833720684051514

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.086242900002715, Training Loss Force: 3.3571086465145585, time: 3.4742743968963623
Validation Loss Energy: 5.206526122931442, Validation Loss Force: 3.4588968656673407, time: 0.17814159393310547
Test Loss Energy: 10.311256931028014, Test Loss Force: 11.145415440927419, time: 8.761668682098389

Epoch 17, Batch 100/233, Loss: 1.2819573879241943, Variance: 0.18840241432189941
Epoch 17, Batch 200/233, Loss: 1.158247470855713, Variance: 0.18163850903511047

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.087642639825296, Training Loss Force: 3.37117577171987, time: 3.549525260925293
Validation Loss Energy: 4.870952029888105, Validation Loss Force: 3.3845087820760886, time: 0.17521905899047852
Test Loss Energy: 10.41191661456574, Test Loss Force: 11.016082336700668, time: 8.602264404296875

Epoch 18, Batch 100/233, Loss: 1.9256823062896729, Variance: 0.18984636664390564
Epoch 18, Batch 200/233, Loss: 0.7516409754753113, Variance: 0.14178094267845154

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 3.3660373858781605, Training Loss Force: 3.4565164436208473, time: 3.531269073486328
Validation Loss Energy: 2.335341032012889, Validation Loss Force: 3.3546786872727266, time: 0.1769120693206787
Test Loss Energy: 9.600125297318046, Test Loss Force: 11.332712224367103, time: 8.593683958053589

Epoch 19, Batch 100/233, Loss: 0.8076703548431396, Variance: 0.14197489619255066
Epoch 19, Batch 200/233, Loss: 1.4781169891357422, Variance: 0.13645398616790771

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6194214054573983, Training Loss Force: 3.2955929348459247, time: 3.5454084873199463
Validation Loss Energy: 2.282460004553936, Validation Loss Force: 3.3549692148408754, time: 0.21950054168701172
Test Loss Energy: 9.424585225611624, Test Loss Force: 11.253298003267172, time: 8.73507285118103

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–…â–â–„â–‡â–‡â–â–‚â–‡â–†â–‚â–„â–ˆâ–ˆâ–ƒâ–ƒâ–†â–†â–ƒâ–‚
wandb:   test_error_force â–‚â–‚â–ƒâ–‚â–â–â–ƒâ–‚â–†â–†â–„â–ƒâ–â–ƒâ–…â–„â–†â–…â–ˆâ–‡
wandb:          test_loss â–ƒâ–‚â–â–â–‚â–ƒâ–â–‚â–„â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ˆâ–†
wandb: train_error_energy â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–ƒâ–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–„â–
wandb:         train_loss â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–
wandb: valid_error_energy â–ˆâ–‡â–â–ƒâ–‡â–ˆâ–‚â–„â–ˆâ–‡â–‚â–…â–ˆâ–‡â–‚â–„â–‡â–†â–‚â–‚
wandb:  valid_error_force â–‡â–ƒâ–†â–‚â–ƒâ–ƒâ–…â–…â–„â–ˆâ–ƒâ–„â–…â–â–ˆâ–‚â–†â–ƒâ–‚â–‚
wandb:         valid_loss â–ˆâ–†â–â–ƒâ–†â–‡â–‚â–ƒâ–‡â–†â–‚â–„â–ˆâ–†â–‚â–ƒâ–†â–…â–â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 7441
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.42459
wandb:   test_error_force 11.2533
wandb:          test_loss 9.82637
wandb: train_error_energy 2.61942
wandb:  train_error_force 3.29559
wandb:         train_loss 0.92215
wandb: valid_error_energy 2.28246
wandb:  valid_error_force 3.35497
wandb:         valid_loss 0.80492
wandb: 
wandb: ğŸš€ View run al_63_76 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/f5vu4q0k
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_160217-f5vu4q0k/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.698342800140381, Uncertainty Bias: -0.15497569739818573
7.534027e-05 0.022382736
1.9818434 5.501443
(48745, 22, 3)
Found uncertainty sample 0 after 1507 steps.
Found uncertainty sample 1 after 1894 steps.
Found uncertainty sample 2 after 700 steps.
Found uncertainty sample 3 after 309 steps.
Found uncertainty sample 4 after 824 steps.
Found uncertainty sample 5 after 3208 steps.
Found uncertainty sample 6 after 675 steps.
Found uncertainty sample 7 after 844 steps.
Found uncertainty sample 8 after 3512 steps.
Found uncertainty sample 9 after 739 steps.
Found uncertainty sample 10 after 1137 steps.
Found uncertainty sample 11 after 3191 steps.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 3884 steps.
Found uncertainty sample 15 after 851 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 2504 steps.
Found uncertainty sample 18 after 289 steps.
Found uncertainty sample 19 after 648 steps.
Found uncertainty sample 20 after 1114 steps.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 2429 steps.
Found uncertainty sample 24 after 383 steps.
Found uncertainty sample 25 after 33 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 18 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 2433 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 42 steps.
Found uncertainty sample 32 after 387 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 783 steps.
Did not find any uncertainty samples for sample 35.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 887 steps.
Found uncertainty sample 39 after 741 steps.
Found uncertainty sample 40 after 3646 steps.
Found uncertainty sample 41 after 649 steps.
Found uncertainty sample 42 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 264 steps.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 1479 steps.
Found uncertainty sample 47 after 610 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 2127 steps.
Found uncertainty sample 50 after 3752 steps.
Found uncertainty sample 51 after 1103 steps.
Found uncertainty sample 52 after 156 steps.
Found uncertainty sample 53 after 3168 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 922 steps.
Found uncertainty sample 56 after 194 steps.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 661 steps.
Found uncertainty sample 60 after 8 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 349 steps.
Found uncertainty sample 63 after 29 steps.
Found uncertainty sample 64 after 848 steps.
Found uncertainty sample 65 after 1823 steps.
Found uncertainty sample 66 after 1360 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 55 steps.
Found uncertainty sample 69 after 1402 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1037 steps.
Found uncertainty sample 72 after 847 steps.
Found uncertainty sample 73 after 2466 steps.
Found uncertainty sample 74 after 2609 steps.
Found uncertainty sample 75 after 131 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 46 steps.
Found uncertainty sample 78 after 2356 steps.
Found uncertainty sample 79 after 1824 steps.
Found uncertainty sample 80 after 2038 steps.
Found uncertainty sample 81 after 1652 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 1006 steps.
Found uncertainty sample 84 after 3156 steps.
Found uncertainty sample 85 after 1437 steps.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 1756 steps.
Found uncertainty sample 89 after 2295 steps.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 2948 steps.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 1125 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 3 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_162804-b64lc7ny
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_77
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/b64lc7ny
Training model 77. Added 70 samples to the dataset.
Epoch 0, Batch 100/235, Loss: 0.6000803112983704, Variance: 0.11990642547607422
Epoch 0, Batch 200/235, Loss: 0.7043540477752686, Variance: 0.16644179821014404

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.6477304603385647, Training Loss Force: 3.7213895592570685, time: 3.5630502700805664
Validation Loss Energy: 2.196071111675619, Validation Loss Force: 3.4105044028520006, time: 0.1773688793182373
Test Loss Energy: 9.552578313913022, Test Loss Force: 10.967976819802777, time: 8.449262380599976

Epoch 1, Batch 100/235, Loss: 1.7021130323410034, Variance: 0.1760132610797882
Epoch 1, Batch 200/235, Loss: 1.6685762405395508, Variance: 0.18509183824062347

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.059169652110858, Training Loss Force: 3.3129482223644477, time: 3.613138198852539
Validation Loss Energy: 4.824002351803804, Validation Loss Force: 3.40029157113945, time: 0.1744527816772461
Test Loss Energy: 10.04966508315843, Test Loss Force: 10.8065114572968, time: 8.481101751327515

Epoch 2, Batch 100/235, Loss: 1.7119858264923096, Variance: 0.1882191300392151
Epoch 2, Batch 200/235, Loss: 2.014259099960327, Variance: 0.18135105073451996

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.04947212796851, Training Loss Force: 3.3439079452234215, time: 3.780486822128296
Validation Loss Energy: 5.506595509322022, Validation Loss Force: 3.403840972992825, time: 0.17626166343688965
Test Loss Energy: 10.866680806136342, Test Loss Force: 10.851157796188824, time: 8.465813875198364

Epoch 3, Batch 100/235, Loss: 0.9225839972496033, Variance: 0.1781827211380005
Epoch 3, Batch 200/235, Loss: 1.2701826095581055, Variance: 0.1879502534866333

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.073972996351073, Training Loss Force: 3.336632787079332, time: 3.551961898803711
Validation Loss Energy: 2.9567746015629908, Validation Loss Force: 3.469164418322504, time: 0.1787114143371582
Test Loss Energy: 9.603758236532617, Test Loss Force: 10.932236314102228, time: 8.441530466079712

Epoch 4, Batch 100/235, Loss: 0.7992773056030273, Variance: 0.18071117997169495
Epoch 4, Batch 200/235, Loss: 0.8177535533905029, Variance: 0.18646864593029022

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.078233069761239, Training Loss Force: 3.3669923872166536, time: 3.645331859588623
Validation Loss Energy: 1.8281050874713767, Validation Loss Force: 3.869139494091227, time: 0.1744844913482666
Test Loss Energy: 9.071357348519161, Test Loss Force: 11.04514723073276, time: 8.639780044555664

Epoch 5, Batch 100/235, Loss: 1.661486029624939, Variance: 0.1965298354625702
Epoch 5, Batch 200/235, Loss: 1.4140263795852661, Variance: 0.17932510375976562

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.054121208166724, Training Loss Force: 3.366704959250392, time: 3.5455245971679688
Validation Loss Energy: 5.094318798250084, Validation Loss Force: 3.744967321189872, time: 0.18095922470092773
Test Loss Energy: 10.426124039240385, Test Loss Force: 10.889683093161294, time: 8.40418553352356

Epoch 6, Batch 100/235, Loss: 1.7014720439910889, Variance: 0.17915482819080353
Epoch 6, Batch 200/235, Loss: 1.875441312789917, Variance: 0.19698381423950195

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.080741353360097, Training Loss Force: 3.3570471218563114, time: 3.6807827949523926
Validation Loss Energy: 5.594456634925926, Validation Loss Force: 3.3916533999596137, time: 0.17728281021118164
Test Loss Energy: 10.755460668792155, Test Loss Force: 10.985077633007363, time: 8.409609079360962

Epoch 7, Batch 100/235, Loss: 1.044445514678955, Variance: 0.18959841132164001
Epoch 7, Batch 200/235, Loss: 1.2562220096588135, Variance: 0.184629887342453

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.028348646444627, Training Loss Force: 3.444550855427013, time: 3.7751123905181885
Validation Loss Energy: 3.4517077139546304, Validation Loss Force: 3.3599436992281886, time: 0.17932391166687012
Test Loss Energy: 9.896667047181362, Test Loss Force: 10.779137356540735, time: 8.415388822555542

Epoch 8, Batch 100/235, Loss: 0.9741532802581787, Variance: 0.19331146776676178
Epoch 8, Batch 200/235, Loss: 1.0017582178115845, Variance: 0.1919100284576416

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.121918391635446, Training Loss Force: 3.3672696500995216, time: 3.598419427871704
Validation Loss Energy: 2.2774930909175843, Validation Loss Force: 3.5589652148565865, time: 0.17577123641967773
Test Loss Energy: 9.439631994054096, Test Loss Force: 10.968096072537202, time: 8.451675176620483

Epoch 9, Batch 100/235, Loss: 1.6720973253250122, Variance: 0.1841176450252533
Epoch 9, Batch 200/235, Loss: 1.5307531356811523, Variance: 0.18881645798683167

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.061898949997502, Training Loss Force: 3.3694089936642575, time: 3.578596830368042
Validation Loss Energy: 4.799901200121445, Validation Loss Force: 3.5636354218559894, time: 0.176408052444458
Test Loss Energy: 10.302638021603656, Test Loss Force: 11.104955785607482, time: 8.689911365509033

Epoch 10, Batch 100/235, Loss: 1.668230414390564, Variance: 0.19499421119689941
Epoch 10, Batch 200/235, Loss: 1.7489426136016846, Variance: 0.18540890514850616

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.085196422405228, Training Loss Force: 3.381187355763894, time: 3.556195020675659
Validation Loss Energy: 5.873859318540937, Validation Loss Force: 3.372335905206508, time: 0.17162632942199707
Test Loss Energy: 10.991111663752172, Test Loss Force: 10.845285598115007, time: 8.507508039474487

Epoch 11, Batch 100/235, Loss: 1.1156104803085327, Variance: 0.18386626243591309
Epoch 11, Batch 200/235, Loss: 1.203670859336853, Variance: 0.19080117344856262

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 3.8518000605609948, Training Loss Force: 3.4842868931670736, time: 3.599102258682251
Validation Loss Energy: 1.321611160025026, Validation Loss Force: 4.808377435164905, time: 0.17710614204406738
Test Loss Energy: 9.44915584487719, Test Loss Force: 12.138928423819097, time: 8.523632764816284

Epoch 12, Batch 100/235, Loss: 0.6361738443374634, Variance: 0.13859981298446655
Epoch 12, Batch 200/235, Loss: 0.9432329535484314, Variance: 0.13999126851558685

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6055720917112857, Training Loss Force: 3.3870453280747355, time: 3.7839879989624023
Validation Loss Energy: 2.639239158700409, Validation Loss Force: 3.3529976465555444, time: 0.17622613906860352
Test Loss Energy: 9.632351500992334, Test Loss Force: 10.952611725414577, time: 9.602171421051025

Epoch 13, Batch 100/235, Loss: 0.7472476959228516, Variance: 0.13927443325519562
Epoch 13, Batch 200/235, Loss: 1.3973989486694336, Variance: 0.14504799246788025

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.628027191579515, Training Loss Force: 3.2856517158502054, time: 3.506531000137329
Validation Loss Energy: 3.776413938325384, Validation Loss Force: 3.342272007799922, time: 0.17467713356018066
Test Loss Energy: 10.165792186300463, Test Loss Force: 11.06449613028754, time: 8.5742666721344

Epoch 14, Batch 100/235, Loss: 1.0835397243499756, Variance: 0.1306292712688446
Epoch 14, Batch 200/235, Loss: 0.7756998538970947, Variance: 0.14015120267868042

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.609323557049954, Training Loss Force: 3.2899039915763857, time: 3.6018662452697754
Validation Loss Energy: 1.8165911442718832, Validation Loss Force: 3.363247051675314, time: 0.17706513404846191
Test Loss Energy: 9.40943532521457, Test Loss Force: 10.976456093522303, time: 8.689994812011719

Epoch 15, Batch 100/235, Loss: 0.7600164413452148, Variance: 0.13654440641403198
Epoch 15, Batch 200/235, Loss: 0.7720970511436462, Variance: 0.1374160200357437

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.606745732455529, Training Loss Force: 3.297891618975295, time: 3.5322859287261963
Validation Loss Energy: 2.149545840983456, Validation Loss Force: 3.425875587999115, time: 0.17938518524169922
Test Loss Energy: 9.252163207442734, Test Loss Force: 11.009025408200133, time: 8.53771185874939

Epoch 16, Batch 100/235, Loss: 1.069186806678772, Variance: 0.13669294118881226
Epoch 16, Batch 200/235, Loss: 1.5616545677185059, Variance: 0.13661393523216248

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6115888081726086, Training Loss Force: 3.30356981674547, time: 3.608280658721924
Validation Loss Energy: 3.7150361902615456, Validation Loss Force: 3.3445429547519527, time: 0.17367792129516602
Test Loss Energy: 9.97800804421973, Test Loss Force: 11.138612858881885, time: 8.520222663879395

Epoch 17, Batch 100/235, Loss: 1.305164098739624, Variance: 0.14122271537780762
Epoch 17, Batch 200/235, Loss: 0.5485692620277405, Variance: 0.13489648699760437

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6461413199680415, Training Loss Force: 3.2956813671316847, time: 3.8029446601867676
Validation Loss Energy: 1.6622846270128988, Validation Loss Force: 3.3338848404148327, time: 0.1779625415802002
Test Loss Energy: 9.061149159633175, Test Loss Force: 11.06278507062181, time: 8.50173282623291

Epoch 18, Batch 100/235, Loss: 0.7063993215560913, Variance: 0.13577091693878174
Epoch 18, Batch 200/235, Loss: 0.869173526763916, Variance: 0.13583943247795105

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6232620462733967, Training Loss Force: 3.294609242672362, time: 3.4934537410736084
Validation Loss Energy: 2.6338978825624664, Validation Loss Force: 3.3780999187179948, time: 0.17986321449279785
Test Loss Energy: 9.693755686645474, Test Loss Force: 11.06760638224432, time: 8.479230880737305

Epoch 19, Batch 100/235, Loss: 0.7143544554710388, Variance: 0.13194936513900757
Epoch 19, Batch 200/235, Loss: 1.3865032196044922, Variance: 0.142344668507576

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.607767839739506, Training Loss Force: 3.3022051050283987, time: 3.507094144821167
Validation Loss Energy: 3.9570635237288574, Validation Loss Force: 3.354901801756946, time: 0.17453789710998535
Test Loss Energy: 10.33220760283274, Test Loss Force: 11.079736441352685, time: 8.705938339233398

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–…â–ˆâ–ƒâ–â–†â–‡â–„â–‚â–†â–ˆâ–‚â–ƒâ–…â–‚â–‚â–„â–â–ƒâ–†
wandb:   test_error_force â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–â–ˆâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒ
wandb:          test_loss â–‚â–‚â–ƒâ–â–â–‚â–‚â–‚â–â–‚â–‚â–ˆâ–…â–†â–…â–„â–†â–„â–…â–†
wandb: train_error_energy â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–„â–‚â–‚â–ƒâ–„â–ƒâ–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–†â–‡â–„â–‚â–‡â–ˆâ–„â–‚â–†â–ˆâ–â–ƒâ–…â–‚â–‚â–…â–‚â–ƒâ–…
wandb:  valid_error_force â–â–â–â–‚â–„â–ƒâ–â–â–‚â–‚â–â–ˆâ–â–â–â–â–â–â–â–
wandb:         valid_loss â–‚â–†â–‡â–„â–ƒâ–‡â–ˆâ–„â–ƒâ–†â–ˆâ–ƒâ–ƒâ–…â–â–‚â–…â–â–ƒâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 7504
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.33221
wandb:   test_error_force 11.07974
wandb:          test_loss 10.28719
wandb: train_error_energy 2.60777
wandb:  train_error_force 3.30221
wandb:         train_loss 0.92179
wandb: valid_error_energy 3.95706
wandb:  valid_error_force 3.3549
wandb:         valid_loss 1.53704
wandb: 
wandb: ğŸš€ View run al_63_77 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/b64lc7ny
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_162804-b64lc7ny/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.7232229709625244, Uncertainty Bias: -0.14917582273483276
1.5258789e-05 0.02336502
2.0490153 5.8883877
(48745, 22, 3)
Found uncertainty sample 0 after 2174 steps.
Found uncertainty sample 1 after 1998 steps.
Found uncertainty sample 2 after 3466 steps.
Found uncertainty sample 3 after 700 steps.
Found uncertainty sample 4 after 1075 steps.
Found uncertainty sample 5 after 3092 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 399 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 44 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 3239 steps.
Found uncertainty sample 12 after 357 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 144 steps.
Found uncertainty sample 15 after 1596 steps.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 972 steps.
Found uncertainty sample 20 after 1111 steps.
Found uncertainty sample 21 after 1091 steps.
Found uncertainty sample 22 after 657 steps.
Found uncertainty sample 23 after 1538 steps.
Found uncertainty sample 24 after 3 steps.
Found uncertainty sample 25 after 2081 steps.
Found uncertainty sample 26 after 1219 steps.
Found uncertainty sample 27 after 3351 steps.
Found uncertainty sample 28 after 2081 steps.
Found uncertainty sample 29 after 2872 steps.
Found uncertainty sample 30 after 1166 steps.
Found uncertainty sample 31 after 3051 steps.
Found uncertainty sample 32 after 665 steps.
Found uncertainty sample 33 after 485 steps.
Found uncertainty sample 34 after 2020 steps.
Found uncertainty sample 35 after 2422 steps.
Found uncertainty sample 36 after 663 steps.
Found uncertainty sample 37 after 987 steps.
Found uncertainty sample 38 after 1660 steps.
Found uncertainty sample 39 after 53 steps.
Found uncertainty sample 40 after 276 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 319 steps.
Found uncertainty sample 43 after 2826 steps.
Found uncertainty sample 44 after 2914 steps.
Found uncertainty sample 45 after 1907 steps.
Found uncertainty sample 46 after 1357 steps.
Found uncertainty sample 47 after 48 steps.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 1143 steps.
Found uncertainty sample 51 after 2834 steps.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 53 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 158 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 44 steps.
Found uncertainty sample 59 after 496 steps.
Found uncertainty sample 60 after 497 steps.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 2846 steps.
Found uncertainty sample 64 after 203 steps.
Found uncertainty sample 65 after 2419 steps.
Found uncertainty sample 66 after 294 steps.
Found uncertainty sample 67 after 1746 steps.
Found uncertainty sample 68 after 545 steps.
Found uncertainty sample 69 after 1678 steps.
Found uncertainty sample 70 after 40 steps.
Found uncertainty sample 71 after 3291 steps.
Found uncertainty sample 72 after 1086 steps.
Found uncertainty sample 73 after 1438 steps.
Found uncertainty sample 74 after 2342 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 269 steps.
Found uncertainty sample 77 after 177 steps.
Found uncertainty sample 78 after 2756 steps.
Found uncertainty sample 79 after 2732 steps.
Found uncertainty sample 80 after 2626 steps.
Found uncertainty sample 81 after 1351 steps.
Found uncertainty sample 82 after 1534 steps.
Found uncertainty sample 83 after 3559 steps.
Found uncertainty sample 84 after 320 steps.
Found uncertainty sample 85 after 42 steps.
Found uncertainty sample 86 after 2430 steps.
Found uncertainty sample 87 after 2562 steps.
Found uncertainty sample 88 after 1157 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 29 steps.
Found uncertainty sample 91 after 2332 steps.
Found uncertainty sample 92 after 240 steps.
Found uncertainty sample 93 after 1487 steps.
Found uncertainty sample 94 after 1327 steps.
Found uncertainty sample 95 after 531 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 976 steps.
Found uncertainty sample 98 after 325 steps.
Found uncertainty sample 99 after 2856 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_165121-l8lfcun0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_78
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/l8lfcun0
Training model 78. Added 83 samples to the dataset.
Epoch 0, Batch 100/237, Loss: 0.9965789318084717, Variance: 0.145470529794693
Epoch 0, Batch 200/237, Loss: 0.5277783274650574, Variance: 0.13207513093948364

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.0360929450533685, Training Loss Force: 3.471124944754934, time: 3.588162422180176
Validation Loss Energy: 3.7051284152789155, Validation Loss Force: 3.338109597787807, time: 0.1813504695892334
Test Loss Energy: 10.211491276935066, Test Loss Force: 10.94521179782798, time: 8.497061252593994

Epoch 1, Batch 100/237, Loss: 1.039976716041565, Variance: 0.13184203207492828
Epoch 1, Batch 200/237, Loss: 0.8389791250228882, Variance: 0.14094644784927368

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.598108197923743, Training Loss Force: 3.2991926567639274, time: 3.6586763858795166
Validation Loss Energy: 2.9707513768510188, Validation Loss Force: 3.360299580190668, time: 0.18282437324523926
Test Loss Energy: 9.46527171900349, Test Loss Force: 11.01374211428795, time: 8.490727663040161

Epoch 2, Batch 100/237, Loss: 1.199027419090271, Variance: 0.14197009801864624
Epoch 2, Batch 200/237, Loss: 0.590362548828125, Variance: 0.1362435221672058

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.614365238914535, Training Loss Force: 3.293858975335019, time: 3.768721342086792
Validation Loss Energy: 3.7778094615301994, Validation Loss Force: 3.33549490534175, time: 0.19166994094848633
Test Loss Energy: 10.091817821399442, Test Loss Force: 10.866322353112068, time: 8.492759227752686

Epoch 3, Batch 100/237, Loss: 1.4079763889312744, Variance: 0.13719889521598816
Epoch 3, Batch 200/237, Loss: 0.7459886074066162, Variance: 0.1393071860074997

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.638495585622382, Training Loss Force: 3.2909245436186274, time: 3.5525572299957275
Validation Loss Energy: 3.3516268396259266, Validation Loss Force: 3.305987847801843, time: 0.178436279296875
Test Loss Energy: 9.585912729102716, Test Loss Force: 11.021717731383916, time: 8.491026401519775

Epoch 4, Batch 100/237, Loss: 1.3427923917770386, Variance: 0.13633982837200165
Epoch 4, Batch 200/237, Loss: 0.6582974195480347, Variance: 0.1376526951789856

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.62809169344777, Training Loss Force: 3.299090425319043, time: 3.590235948562622
Validation Loss Energy: 3.6834954954118797, Validation Loss Force: 3.306081535844992, time: 0.18173766136169434
Test Loss Energy: 9.990944063045335, Test Loss Force: 10.979187271659764, time: 8.677634716033936

Epoch 5, Batch 100/237, Loss: 1.0576635599136353, Variance: 0.13447123765945435
Epoch 5, Batch 200/237, Loss: 0.6479864120483398, Variance: 0.13681522011756897

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6379166801358975, Training Loss Force: 3.298381316163425, time: 3.498230218887329
Validation Loss Energy: 3.202148986297316, Validation Loss Force: 3.3711163462942615, time: 0.1766819953918457
Test Loss Energy: 9.534411864466382, Test Loss Force: 11.118654209599061, time: 8.49666166305542

Epoch 6, Batch 100/237, Loss: 1.2119511365890503, Variance: 0.1380411684513092
Epoch 6, Batch 200/237, Loss: 0.6273040771484375, Variance: 0.13860216736793518

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6317380872094907, Training Loss Force: 3.2951086763212722, time: 3.5475714206695557
Validation Loss Energy: 3.4388524118200676, Validation Loss Force: 3.3580783441254654, time: 0.1796095371246338
Test Loss Energy: 9.78356453249431, Test Loss Force: 10.826059290166034, time: 8.495949268341064

Epoch 7, Batch 100/237, Loss: 1.4515283107757568, Variance: 0.13696874678134918
Epoch 7, Batch 200/237, Loss: 0.8253796100616455, Variance: 0.1442144811153412

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6352299692536487, Training Loss Force: 3.304503233215466, time: 3.821202516555786
Validation Loss Energy: 3.205938703826434, Validation Loss Force: 3.3212018285319322, time: 0.1844789981842041
Test Loss Energy: 9.620618399183382, Test Loss Force: 11.07656816484758, time: 8.53738808631897

Epoch 8, Batch 100/237, Loss: 1.474037766456604, Variance: 0.13913089036941528
Epoch 8, Batch 200/237, Loss: 0.49232494831085205, Variance: 0.13657502830028534

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6377415212573587, Training Loss Force: 3.2962155223667278, time: 3.5738093852996826
Validation Loss Energy: 3.828600126714468, Validation Loss Force: 3.317314080911319, time: 0.18093538284301758
Test Loss Energy: 9.995069112730475, Test Loss Force: 11.064051076452605, time: 8.507664442062378

Epoch 9, Batch 100/237, Loss: 1.4809932708740234, Variance: 0.1372489035129547
Epoch 9, Batch 200/237, Loss: 0.725796103477478, Variance: 0.13997973501682281

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.635280693660904, Training Loss Force: 3.3025572852702516, time: 3.652254581451416
Validation Loss Energy: 3.1368976301036318, Validation Loss Force: 3.339558561587646, time: 0.17587709426879883
Test Loss Energy: 9.396126886996269, Test Loss Force: 11.018596357627914, time: 8.692190170288086

Epoch 10, Batch 100/237, Loss: 1.2945934534072876, Variance: 0.14119800925254822
Epoch 10, Batch 200/237, Loss: 0.5392374992370605, Variance: 0.13502003252506256

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.610165100810053, Training Loss Force: 3.299142397425141, time: 3.6082348823547363
Validation Loss Energy: 3.527480465629835, Validation Loss Force: 3.347810958472162, time: 0.1764976978302002
Test Loss Energy: 9.98407689528941, Test Loss Force: 10.829378305522283, time: 8.51647663116455

Epoch 11, Batch 100/237, Loss: 1.0398989915847778, Variance: 0.13604551553726196
Epoch 11, Batch 200/237, Loss: 0.5648924112319946, Variance: 0.1367100179195404

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.643188029236462, Training Loss Force: 3.311642821547914, time: 3.6285922527313232
Validation Loss Energy: 3.184780124907511, Validation Loss Force: 3.3303375539183433, time: 0.17932534217834473
Test Loss Energy: 9.446963712602003, Test Loss Force: 10.893409007819454, time: 8.479612588882446

Epoch 12, Batch 100/237, Loss: 1.3172342777252197, Variance: 0.142050638794899
Epoch 12, Batch 200/237, Loss: 0.5453770756721497, Variance: 0.1379907727241516

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.639109022677598, Training Loss Force: 3.3010681426081416, time: 3.8526651859283447
Validation Loss Energy: 3.8493344652431127, Validation Loss Force: 3.3246908313794745, time: 0.18422436714172363
Test Loss Energy: 10.186075256368257, Test Loss Force: 10.803569501245288, time: 8.519569635391235

Epoch 13, Batch 100/237, Loss: 0.9785355925559998, Variance: 0.13139265775680542
Epoch 13, Batch 200/237, Loss: 0.8420036435127258, Variance: 0.14126090705394745

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.662687878510813, Training Loss Force: 3.2891143731176866, time: 3.734018325805664
Validation Loss Energy: 3.3635135309757027, Validation Loss Force: 3.3421222627992853, time: 0.1855790615081787
Test Loss Energy: 9.413820550389945, Test Loss Force: 10.949451520957469, time: 9.584690570831299

Epoch 14, Batch 100/237, Loss: 1.2136223316192627, Variance: 0.14318819344043732
Epoch 14, Batch 200/237, Loss: 0.357252299785614, Variance: 0.1329423189163208

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6145878615536158, Training Loss Force: 3.302166372203229, time: 3.5616002082824707
Validation Loss Energy: 3.627074306869108, Validation Loss Force: 3.320161963620449, time: 0.1795048713684082
Test Loss Energy: 9.948116774857395, Test Loss Force: 10.755882490424266, time: 8.677055358886719

Epoch 15, Batch 100/237, Loss: 1.4371428489685059, Variance: 0.13854941725730896
Epoch 15, Batch 200/237, Loss: 0.7911413908004761, Variance: 0.14565397799015045

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6302819357529708, Training Loss Force: 3.312482147995884, time: 3.6997718811035156
Validation Loss Energy: 3.107097038962105, Validation Loss Force: 3.3351728144221657, time: 0.1796717643737793
Test Loss Energy: 9.406535333553515, Test Loss Force: 10.999111828081313, time: 8.51606273651123

Epoch 16, Batch 100/237, Loss: 1.2467087507247925, Variance: 0.14210394024848938
Epoch 16, Batch 200/237, Loss: 0.6573581695556641, Variance: 0.139174222946167

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6261625626602663, Training Loss Force: 3.308714268484881, time: 3.5989770889282227
Validation Loss Energy: 3.985312667596132, Validation Loss Force: 3.432673176018712, time: 0.17752647399902344
Test Loss Energy: 10.186876837314228, Test Loss Force: 10.972252445126072, time: 8.683695316314697

Epoch 17, Batch 100/237, Loss: 1.1022077798843384, Variance: 0.13713307678699493
Epoch 17, Batch 200/237, Loss: 0.6058582067489624, Variance: 0.13983657956123352

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6327631718692053, Training Loss Force: 3.2941661168870273, time: 3.641155958175659
Validation Loss Energy: 3.06775780261461, Validation Loss Force: 3.3272037640258425, time: 0.18218541145324707
Test Loss Energy: 9.433563595317546, Test Loss Force: 10.956781560743226, time: 8.472781896591187

Epoch 18, Batch 100/237, Loss: 1.3268287181854248, Variance: 0.14214500784873962
Epoch 18, Batch 200/237, Loss: 0.6001597046852112, Variance: 0.13251835107803345

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6244870867926684, Training Loss Force: 3.2969682944051253, time: 3.5114011764526367
Validation Loss Energy: 3.618622287095267, Validation Loss Force: 3.325166953680754, time: 0.1783123016357422
Test Loss Energy: 10.01484545310733, Test Loss Force: 10.871418258926363, time: 8.523823022842407

Epoch 19, Batch 100/237, Loss: 1.3041948080062866, Variance: 0.13594800233840942
Epoch 19, Batch 200/237, Loss: 0.7254433631896973, Variance: 0.14201784133911133

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.625058059338505, Training Loss Force: 3.285709477198175, time: 3.6635141372680664
Validation Loss Energy: 3.102231829458156, Validation Loss Force: 3.345147000477568, time: 0.17965149879455566
Test Loss Energy: 9.278966855494682, Test Loss Force: 11.093643199726746, time: 8.668023586273193

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‚â–‡â–ƒâ–†â–ƒâ–…â–„â–†â–‚â–†â–‚â–ˆâ–‚â–†â–‚â–ˆâ–‚â–‡â–
wandb:   test_error_force â–…â–†â–ƒâ–†â–…â–ˆâ–‚â–‡â–‡â–†â–‚â–„â–‚â–…â–â–†â–…â–…â–ƒâ–ˆ
wandb:          test_loss â–ˆâ–…â–†â–„â–…â–…â–ƒâ–…â–†â–‚â–ƒâ–ƒâ–…â–â–ƒâ–„â–…â–ƒâ–…â–ƒ
wandb: train_error_energy â–ˆâ–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–
wandb:  train_error_force â–ˆâ–‚â–â–â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–â–‡â–„â–†â–ƒâ–„â–ƒâ–‡â–‚â–…â–‚â–‡â–„â–†â–‚â–ˆâ–‚â–…â–‚
wandb:  valid_error_force â–ƒâ–„â–ƒâ–â–â–…â–„â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ˆâ–‚â–‚â–ƒ
wandb:         valid_loss â–†â–â–†â–ƒâ–…â–‚â–„â–‚â–‡â–‚â–„â–‚â–†â–ƒâ–…â–‚â–ˆâ–â–…â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7578
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.27897
wandb:   test_error_force 11.09364
wandb:          test_loss 9.51004
wandb: train_error_energy 2.62506
wandb:  train_error_force 3.28571
wandb:         train_loss 0.92094
wandb: valid_error_energy 3.10223
wandb:  valid_error_force 3.34515
wandb:         valid_loss 1.12586
wandb: 
wandb: ğŸš€ View run al_63_78 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/l8lfcun0
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_165121-l8lfcun0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.7697858810424805, Uncertainty Bias: -0.14298270642757416
3.4332275e-05 0.05500412
1.9690331 5.6399593
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 3783 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 540 steps.
Found uncertainty sample 4 after 106 steps.
Found uncertainty sample 5 after 2199 steps.
Found uncertainty sample 6 after 438 steps.
Found uncertainty sample 7 after 52 steps.
Found uncertainty sample 8 after 2315 steps.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 735 steps.
Found uncertainty sample 13 after 746 steps.
Found uncertainty sample 14 after 768 steps.
Found uncertainty sample 15 after 220 steps.
Found uncertainty sample 16 after 3625 steps.
Found uncertainty sample 17 after 945 steps.
Found uncertainty sample 18 after 24 steps.
Found uncertainty sample 19 after 860 steps.
Found uncertainty sample 20 after 1536 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 1111 steps.
Found uncertainty sample 23 after 2161 steps.
Found uncertainty sample 24 after 64 steps.
Found uncertainty sample 25 after 1884 steps.
Found uncertainty sample 26 after 39 steps.
Found uncertainty sample 27 after 608 steps.
Found uncertainty sample 28 after 1585 steps.
Found uncertainty sample 29 after 206 steps.
Found uncertainty sample 30 after 8 steps.
Found uncertainty sample 31 after 3190 steps.
Found uncertainty sample 32 after 1872 steps.
Found uncertainty sample 33 after 565 steps.
Found uncertainty sample 34 after 2553 steps.
Found uncertainty sample 35 after 1045 steps.
Found uncertainty sample 36 after 206 steps.
Found uncertainty sample 37 after 88 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 3986 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 355 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 1060 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 0 steps.
Found uncertainty sample 48 after 2180 steps.
Found uncertainty sample 49 after 3264 steps.
Found uncertainty sample 50 after 2502 steps.
Found uncertainty sample 51 after 2832 steps.
Found uncertainty sample 52 after 1453 steps.
Found uncertainty sample 53 after 1909 steps.
Found uncertainty sample 54 after 63 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 418 steps.
Found uncertainty sample 57 after 1353 steps.
Found uncertainty sample 58 after 183 steps.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 11 steps.
Found uncertainty sample 62 after 1897 steps.
Found uncertainty sample 63 after 65 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 69 steps.
Found uncertainty sample 66 after 1625 steps.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 635 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 500 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 56 steps.
Found uncertainty sample 75 after 3501 steps.
Found uncertainty sample 76 after 718 steps.
Found uncertainty sample 77 after 2646 steps.
Found uncertainty sample 78 after 190 steps.
Found uncertainty sample 79 after 1655 steps.
Found uncertainty sample 80 after 2682 steps.
Found uncertainty sample 81 after 2255 steps.
Found uncertainty sample 82 after 630 steps.
Found uncertainty sample 83 after 3414 steps.
Found uncertainty sample 84 after 2925 steps.
Found uncertainty sample 85 after 1492 steps.
Found uncertainty sample 86 after 976 steps.
Found uncertainty sample 87 after 1859 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 25 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 10 steps.
Found uncertainty sample 92 after 879 steps.
Found uncertainty sample 93 after 3109 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 979 steps.
Found uncertainty sample 96 after 1381 steps.
Found uncertainty sample 97 after 3121 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 3042 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_171619-8qz0n205
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_79
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8qz0n205
Training model 79. Added 75 samples to the dataset.
Epoch 0, Batch 100/239, Loss: 0.5004291534423828, Variance: 0.11329659819602966
Epoch 0, Batch 200/239, Loss: 0.6402493119239807, Variance: 0.10729935765266418

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.9418959031707999, Training Loss Force: 3.364901675120674, time: 3.6412546634674072
Validation Loss Energy: 1.7954845029312476, Validation Loss Force: 3.3456821301025195, time: 0.1843733787536621
Test Loss Energy: 9.280456523739009, Test Loss Force: 11.036683714061278, time: 8.459805965423584

Epoch 1, Batch 100/239, Loss: 0.6168724298477173, Variance: 0.10045112669467926
Epoch 1, Batch 200/239, Loss: 0.19399970769882202, Variance: 0.09460858255624771

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6208426110920724, Training Loss Force: 3.3059837116525825, time: 3.639230489730835
Validation Loss Energy: 1.37700308827439, Validation Loss Force: 3.323343404710179, time: 0.18251919746398926
Test Loss Energy: 9.017554318075346, Test Loss Force: 11.096863238064726, time: 8.561821937561035

Epoch 2, Batch 100/239, Loss: 0.35388606786727905, Variance: 0.09644514322280884
Epoch 2, Batch 200/239, Loss: 0.6070858240127563, Variance: 0.09522010385990143

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6393951374073017, Training Loss Force: 3.2991102952391778, time: 3.880106210708618
Validation Loss Energy: 1.481577509468846, Validation Loss Force: 3.3450778617077583, time: 0.17617535591125488
Test Loss Energy: 8.73081094327849, Test Loss Force: 11.021104969210464, time: 8.581750631332397

Epoch 3, Batch 100/239, Loss: 0.39455747604370117, Variance: 0.09418685734272003
Epoch 3, Batch 200/239, Loss: 0.35305219888687134, Variance: 0.09640820324420929

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.643663744530715, Training Loss Force: 3.2932946513676087, time: 3.5814125537872314
Validation Loss Energy: 1.7565032629031183, Validation Loss Force: 3.3525699547935037, time: 0.18159079551696777
Test Loss Energy: 9.463332058565575, Test Loss Force: 11.125783152332488, time: 8.560459613800049

Epoch 4, Batch 100/239, Loss: 0.41984570026397705, Variance: 0.09508495777845383
Epoch 4, Batch 200/239, Loss: 0.37873196601867676, Variance: 0.0939934253692627

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.637679953078435, Training Loss Force: 3.3284850661109107, time: 3.5273945331573486
Validation Loss Energy: 1.8893452538366744, Validation Loss Force: 3.3645742711196167, time: 0.18013978004455566
Test Loss Energy: 8.899107195956715, Test Loss Force: 11.043634880546529, time: 8.73963713645935

Epoch 5, Batch 100/239, Loss: 0.5061473250389099, Variance: 0.09429936856031418
Epoch 5, Batch 200/239, Loss: 0.6946775913238525, Variance: 0.0958678126335144

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6250614872977514, Training Loss Force: 3.3228898963705915, time: 3.601372718811035
Validation Loss Energy: 1.2304108879087874, Validation Loss Force: 3.3512035455795175, time: 0.17907094955444336
Test Loss Energy: 8.69500241739999, Test Loss Force: 11.139160811509996, time: 8.53069257736206

Epoch 6, Batch 100/239, Loss: 0.5048403143882751, Variance: 0.0925799086689949
Epoch 6, Batch 200/239, Loss: 0.3967772126197815, Variance: 0.09522794187068939

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.628194362756082, Training Loss Force: 3.310153856622507, time: 3.6290791034698486
Validation Loss Energy: 1.5331141162919728, Validation Loss Force: 3.3438264611076436, time: 0.18462610244750977
Test Loss Energy: 8.847242200815788, Test Loss Force: 11.124621290376789, time: 8.510714530944824

Epoch 7, Batch 100/239, Loss: 0.5353209972381592, Variance: 0.09530062228441238
Epoch 7, Batch 200/239, Loss: 0.7522284984588623, Variance: 0.09215644001960754

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6362288790458954, Training Loss Force: 3.314984472751047, time: 3.896343946456909
Validation Loss Energy: 1.758859185967964, Validation Loss Force: 3.3031753694869153, time: 0.17916512489318848
Test Loss Energy: 9.102860243532595, Test Loss Force: 11.09391652085724, time: 8.537389755249023

Epoch 8, Batch 100/239, Loss: 0.6432663798332214, Variance: 0.095537930727005
Epoch 8, Batch 200/239, Loss: 0.48845547437667847, Variance: 0.09010195732116699

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6366574086122436, Training Loss Force: 3.3081671736207676, time: 3.741055965423584
Validation Loss Energy: 1.6517052262188643, Validation Loss Force: 3.3640716511899713, time: 0.18426775932312012
Test Loss Energy: 9.039549212578292, Test Loss Force: 11.172856962636606, time: 8.502474308013916

Epoch 9, Batch 100/239, Loss: 0.35942769050598145, Variance: 0.090814508497715
Epoch 9, Batch 200/239, Loss: 0.6139456033706665, Variance: 0.09203420579433441

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.635240136967227, Training Loss Force: 3.3192952821993047, time: 3.680046796798706
Validation Loss Energy: 1.6215282685534333, Validation Loss Force: 3.329265012309696, time: 0.18591737747192383
Test Loss Energy: 8.990105852819385, Test Loss Force: 11.053882655576597, time: 8.734827995300293

Epoch 10, Batch 100/239, Loss: 0.46322888135910034, Variance: 0.08986097574234009
Epoch 10, Batch 200/239, Loss: 0.136601984500885, Variance: 0.08901743590831757

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6272918320682779, Training Loss Force: 3.3090917070947006, time: 3.6057426929473877
Validation Loss Energy: 1.503914175171587, Validation Loss Force: 3.3756502241090702, time: 0.17760229110717773
Test Loss Energy: 9.083139498162181, Test Loss Force: 11.235790290760677, time: 8.542959213256836

Epoch 11, Batch 100/239, Loss: 0.5969123244285583, Variance: 0.09621020406484604
Epoch 11, Batch 200/239, Loss: 0.552632212638855, Variance: 0.09159708023071289

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6175570085901396, Training Loss Force: 3.304215140998474, time: 3.63637375831604
Validation Loss Energy: 1.7215384175946615, Validation Loss Force: 3.3000976365519015, time: 0.1793839931488037
Test Loss Energy: 9.069468573227224, Test Loss Force: 11.000559410691256, time: 8.522093296051025

Epoch 12, Batch 100/239, Loss: 0.5808353424072266, Variance: 0.09289637207984924
Epoch 12, Batch 200/239, Loss: 0.3186802864074707, Variance: 0.09498028457164764

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6205025911137254, Training Loss Force: 3.305473598999343, time: 3.8789615631103516
Validation Loss Energy: 2.0946893485291187, Validation Loss Force: 3.369540071410239, time: 0.182387113571167
Test Loss Energy: 9.224145212801844, Test Loss Force: 11.20853802150423, time: 8.546047925949097

Epoch 13, Batch 100/239, Loss: 0.21485871076583862, Variance: 0.08738131821155548
Epoch 13, Batch 200/239, Loss: 0.3896021246910095, Variance: 0.08568036556243896

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.622095992189831, Training Loss Force: 3.3050791867249996, time: 3.7133729457855225
Validation Loss Energy: 1.2407889401276169, Validation Loss Force: 3.2981143781364093, time: 0.19035720825195312
Test Loss Energy: 8.849627910687857, Test Loss Force: 11.126225142426504, time: 8.524371862411499

Epoch 14, Batch 100/239, Loss: 0.637109637260437, Variance: 0.08813701570034027
Epoch 14, Batch 200/239, Loss: 0.7078331112861633, Variance: 0.09165813773870468

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6397753049121604, Training Loss Force: 3.307603317579734, time: 3.7081639766693115
Validation Loss Energy: 1.5659337392149182, Validation Loss Force: 3.296009932038246, time: 0.17898058891296387
Test Loss Energy: 9.047805075698612, Test Loss Force: 11.320233794133804, time: 8.763846158981323

Epoch 15, Batch 100/239, Loss: 0.4216800332069397, Variance: 0.09232843667268753
Epoch 15, Batch 200/239, Loss: 0.7711309194564819, Variance: 0.09121888130903244

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6133168474501183, Training Loss Force: 3.3025974226348764, time: 3.639122486114502
Validation Loss Energy: 1.6590237487053596, Validation Loss Force: 3.3342097146987415, time: 0.1863241195678711
Test Loss Energy: 9.175567828506942, Test Loss Force: 11.20721456003894, time: 8.59958815574646

Epoch 16, Batch 100/239, Loss: 0.5262700319290161, Variance: 0.0902242511510849
Epoch 16, Batch 200/239, Loss: 0.46229010820388794, Variance: 0.09089586138725281

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6341063846352009, Training Loss Force: 3.3252812260423554, time: 3.7194390296936035
Validation Loss Energy: 2.008359574309045, Validation Loss Force: 3.383502373203781, time: 0.18723559379577637
Test Loss Energy: 9.23247842022487, Test Loss Force: 11.148848840607869, time: 8.835431575775146

Epoch 17, Batch 100/239, Loss: 0.20154136419296265, Variance: 0.09139268100261688
Epoch 17, Batch 200/239, Loss: 0.3954838514328003, Variance: 0.09136760979890823

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.624205112243801, Training Loss Force: 3.321185216854644, time: 3.710994243621826
Validation Loss Energy: 1.5375062651953986, Validation Loss Force: 3.3965785631311043, time: 0.18220138549804688
Test Loss Energy: 9.018503895894284, Test Loss Force: 11.178613647884996, time: 8.604721546173096

Epoch 18, Batch 100/239, Loss: 0.43531882762908936, Variance: 0.08816178143024445
Epoch 18, Batch 200/239, Loss: 0.46259504556655884, Variance: 0.09358109533786774

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6115608051963137, Training Loss Force: 3.300629864956265, time: 3.604979991912842
Validation Loss Energy: 1.4616206480968519, Validation Loss Force: 3.34657709306729, time: 0.18427252769470215
Test Loss Energy: 8.971516663084683, Test Loss Force: 11.095062798304335, time: 8.62303113937378

Epoch 19, Batch 100/239, Loss: 0.6226586103439331, Variance: 0.09063098579645157
Epoch 19, Batch 200/239, Loss: 0.3584306836128235, Variance: 0.09080609679222107

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6254204306597395, Training Loss Force: 3.2939470775551976, time: 3.6607048511505127
Validation Loss Energy: 1.6180573575739974, Validation Loss Force: 3.341115018917681, time: 0.1779484748840332
Test Loss Energy: 9.022113602906026, Test Loss Force: 11.216302473638203, time: 9.873116970062256

wandb: - 0.039 MB of 0.061 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–â–ˆâ–ƒâ–â–‚â–…â–„â–„â–…â–„â–†â–‚â–„â–…â–†â–„â–„â–„
wandb:   test_error_force â–‚â–ƒâ–â–„â–‚â–„â–„â–ƒâ–…â–‚â–†â–â–†â–„â–ˆâ–†â–„â–…â–ƒâ–†
wandb:          test_loss â–ƒâ–„â–â–‡â–‚â–ƒâ–…â–„â–„â–…â–‡â–…â–†â–‡â–†â–†â–ˆâ–ˆâ–‡â–…
wandb: train_error_energy â–ˆâ–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–â–„â–„â–ƒâ–ƒâ–‚â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–„â–„â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–‚â–â–â–
wandb: valid_error_energy â–†â–‚â–ƒâ–…â–†â–â–ƒâ–…â–„â–„â–ƒâ–…â–ˆâ–â–„â–„â–‡â–ƒâ–ƒâ–„
wandb:  valid_error_force â–„â–ƒâ–„â–…â–†â–…â–„â–â–†â–ƒâ–‡â–â–†â–â–â–„â–‡â–ˆâ–…â–„
wandb:         valid_loss â–…â–‚â–ƒâ–…â–†â–â–„â–„â–„â–„â–ƒâ–„â–ˆâ–â–ƒâ–„â–ˆâ–„â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7645
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.02211
wandb:   test_error_force 11.2163
wandb:          test_loss 11.69953
wandb: train_error_energy 1.62542
wandb:  train_error_force 3.29395
wandb:         train_loss 0.48455
wandb: valid_error_energy 1.61806
wandb:  valid_error_force 3.34112
wandb:         valid_loss 0.47222
wandb: 
wandb: ğŸš€ View run al_63_79 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8qz0n205
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_171619-8qz0n205/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.959045886993408, Uncertainty Bias: -0.04955069720745087
4.9591064e-05 0.0034804344
2.0678995 5.8819203
(48745, 22, 3)
Found uncertainty sample 0 after 1353 steps.
Found uncertainty sample 1 after 554 steps.
Found uncertainty sample 2 after 2756 steps.
Found uncertainty sample 3 after 1242 steps.
Found uncertainty sample 4 after 1900 steps.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 150 steps.
Found uncertainty sample 9 after 1284 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 126 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 1948 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 2300 steps.
Found uncertainty sample 16 after 1911 steps.
Found uncertainty sample 17 after 1341 steps.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 2080 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 932 steps.
Found uncertainty sample 23 after 1009 steps.
Found uncertainty sample 24 after 25 steps.
Did not find any uncertainty samples for sample 25.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 295 steps.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 869 steps.
Found uncertainty sample 31 after 7 steps.
Found uncertainty sample 32 after 916 steps.
Found uncertainty sample 33 after 1394 steps.
Found uncertainty sample 34 after 3716 steps.
Found uncertainty sample 35 after 112 steps.
Found uncertainty sample 36 after 2131 steps.
Found uncertainty sample 37 after 950 steps.
Found uncertainty sample 38 after 1269 steps.
Found uncertainty sample 39 after 3099 steps.
Found uncertainty sample 40 after 2051 steps.
Found uncertainty sample 41 after 809 steps.
Found uncertainty sample 42 after 1518 steps.
Found uncertainty sample 43 after 551 steps.
Found uncertainty sample 44 after 799 steps.
Found uncertainty sample 45 after 29 steps.
Found uncertainty sample 46 after 1693 steps.
Found uncertainty sample 47 after 167 steps.
Found uncertainty sample 48 after 237 steps.
Found uncertainty sample 49 after 1215 steps.
Found uncertainty sample 50 after 2013 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 2397 steps.
Found uncertainty sample 54 after 1404 steps.
Found uncertainty sample 55 after 5 steps.
Found uncertainty sample 56 after 2053 steps.
Found uncertainty sample 57 after 576 steps.
Found uncertainty sample 58 after 2465 steps.
Found uncertainty sample 59 after 953 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 6 steps.
Found uncertainty sample 62 after 2019 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 1718 steps.
Found uncertainty sample 65 after 1510 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 2121 steps.
Found uncertainty sample 68 after 125 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 73 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 3872 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 481 steps.
Found uncertainty sample 75 after 304 steps.
Found uncertainty sample 76 after 1198 steps.
Found uncertainty sample 77 after 468 steps.
Found uncertainty sample 78 after 1450 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 1080 steps.
Found uncertainty sample 81 after 1733 steps.
Found uncertainty sample 82 after 3067 steps.
Found uncertainty sample 83 after 2653 steps.
Did not find any uncertainty samples for sample 84.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 993 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 1239 steps.
Found uncertainty sample 89 after 3613 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 1226 steps.
Found uncertainty sample 92 after 1200 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 193 steps.
Found uncertainty sample 95 after 337 steps.
Found uncertainty sample 96 after 2451 steps.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_174140-uyfbl3wr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_80
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/uyfbl3wr
Training model 80. Added 72 samples to the dataset.
Epoch 0, Batch 100/241, Loss: 2.2651476860046387, Variance: 0.15837253630161285
Epoch 0, Batch 200/241, Loss: 1.8923181295394897, Variance: 0.1573764681816101

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.198995337214227, Training Loss Force: 3.6183408778748403, time: 3.748546838760376
Validation Loss Energy: 1.7598866092409182, Validation Loss Force: 3.3548778091564726, time: 0.18818020820617676
Test Loss Energy: 8.897613515004533, Test Loss Force: 10.758284314081335, time: 8.608520984649658

Epoch 1, Batch 100/241, Loss: 1.7949299812316895, Variance: 0.1820772886276245
Epoch 1, Batch 200/241, Loss: 1.462339162826538, Variance: 0.1690659075975418

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.128648306871697, Training Loss Force: 3.3784318809950693, time: 3.6021077632904053
Validation Loss Energy: 3.9638611674157405, Validation Loss Force: 3.3228933346989726, time: 0.18670415878295898
Test Loss Energy: 10.024684402922245, Test Loss Force: 10.518198188077886, time: 8.62855339050293

Epoch 2, Batch 100/241, Loss: 0.80897057056427, Variance: 0.18073691427707672
Epoch 2, Batch 200/241, Loss: 0.8558332920074463, Variance: 0.1769408881664276

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.116734469057839, Training Loss Force: 3.4026843834829434, time: 3.819545030593872
Validation Loss Energy: 5.973704942916968, Validation Loss Force: 3.534097190635652, time: 0.18677878379821777
Test Loss Energy: 10.906182279981335, Test Loss Force: 10.680499140453005, time: 9.764454126358032

Epoch 3, Batch 100/241, Loss: 1.194179654121399, Variance: 0.17402145266532898
Epoch 3, Batch 200/241, Loss: 1.1282037496566772, Variance: 0.17933222651481628

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.115376959680152, Training Loss Force: 3.3634110574336273, time: 3.6735522747039795
Validation Loss Energy: 4.900572036352284, Validation Loss Force: 3.5262907443831426, time: 0.18794012069702148
Test Loss Energy: 10.98422612287719, Test Loss Force: 10.659706645348384, time: 8.619446039199829

Epoch 4, Batch 100/241, Loss: 1.820107102394104, Variance: 0.17598798871040344
Epoch 4, Batch 200/241, Loss: 1.8159558773040771, Variance: 0.18656915426254272

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.144936334998984, Training Loss Force: 3.339086587538639, time: 3.6929659843444824
Validation Loss Energy: 2.1516846894646653, Validation Loss Force: 3.3287661777555315, time: 0.19135236740112305
Test Loss Energy: 9.174155378658163, Test Loss Force: 10.58686179825235, time: 8.82324504852295

Epoch 5, Batch 100/241, Loss: 1.5306282043457031, Variance: 0.17054560780525208
Epoch 5, Batch 200/241, Loss: 1.66884446144104, Variance: 0.18629610538482666

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.081062622416848, Training Loss Force: 3.378647822021824, time: 3.6925737857818604
Validation Loss Energy: 3.5084475420452246, Validation Loss Force: 3.3780446411025182, time: 0.19170212745666504
Test Loss Energy: 9.29162101948221, Test Loss Force: 10.744580975047244, time: 8.64190149307251

Epoch 6, Batch 100/241, Loss: 0.9013698101043701, Variance: 0.17712610960006714
Epoch 6, Batch 200/241, Loss: 0.9345653653144836, Variance: 0.18401965498924255

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.1144210752224035, Training Loss Force: 3.334159024668231, time: 3.6340439319610596
Validation Loss Energy: 5.254002858884155, Validation Loss Force: 3.377002271163144, time: 0.18479633331298828
Test Loss Energy: 10.091626265311977, Test Loss Force: 10.811729199273934, time: 8.836123704910278

Epoch 7, Batch 100/241, Loss: 1.0239489078521729, Variance: 0.18471036851406097
Epoch 7, Batch 200/241, Loss: 1.2802062034606934, Variance: 0.17101331055164337

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.0547562502169034, Training Loss Force: 3.368051742061443, time: 3.718061923980713
Validation Loss Energy: 5.536104960555248, Validation Loss Force: 3.423944601165629, time: 0.18848705291748047
Test Loss Energy: 10.161883981389245, Test Loss Force: 10.866881059144376, time: 8.626364946365356

Epoch 8, Batch 100/241, Loss: 1.882293462753296, Variance: 0.1872144490480423
Epoch 8, Batch 200/241, Loss: 1.466984748840332, Variance: 0.17513920366764069

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.080673238294337, Training Loss Force: 3.3592407405879707, time: 3.5953400135040283
Validation Loss Energy: 2.0749881391702925, Validation Loss Force: 3.3892693370814513, time: 0.19072651863098145
Test Loss Energy: 9.155445136182955, Test Loss Force: 10.742441635178366, time: 8.638418197631836

Epoch 9, Batch 100/241, Loss: 1.7021856307983398, Variance: 0.18808794021606445
Epoch 9, Batch 200/241, Loss: 1.5374547243118286, Variance: 0.1728784441947937

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.074574624199776, Training Loss Force: 3.355864920190122, time: 3.5778517723083496
Validation Loss Energy: 3.558186373851417, Validation Loss Force: 3.333407382098677, time: 0.19025707244873047
Test Loss Energy: 9.725158159039902, Test Loss Force: 10.530939447178264, time: 8.803325414657593

Epoch 10, Batch 100/241, Loss: 0.9307343363761902, Variance: 0.17969956994056702
Epoch 10, Batch 200/241, Loss: 1.0227811336517334, Variance: 0.17984189093112946

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.07982570639848, Training Loss Force: 3.3489122211668496, time: 3.618535041809082
Validation Loss Energy: 6.065090030424326, Validation Loss Force: 3.372290425033452, time: 0.1862931251525879
Test Loss Energy: 10.919634909878406, Test Loss Force: 10.619627957118407, time: 8.623974561691284

Epoch 11, Batch 100/241, Loss: 1.0284531116485596, Variance: 0.17595136165618896
Epoch 11, Batch 200/241, Loss: 1.1797590255737305, Variance: 0.18567150831222534

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.088293052692901, Training Loss Force: 3.369265480161991, time: 3.6634199619293213
Validation Loss Energy: 5.433705766413716, Validation Loss Force: 3.347070917565671, time: 0.18367958068847656
Test Loss Energy: 10.652700648626105, Test Loss Force: 10.530796733742827, time: 8.793827056884766

Epoch 12, Batch 100/241, Loss: 1.8321338891983032, Variance: 0.17552056908607483
Epoch 12, Batch 200/241, Loss: 1.6758630275726318, Variance: 0.19029715657234192

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.093269343148733, Training Loss Force: 3.36567103284856, time: 3.651663064956665
Validation Loss Energy: 1.8589535317799963, Validation Loss Force: 3.3552004567489218, time: 0.18551230430603027
Test Loss Energy: 9.14311208208838, Test Loss Force: 10.726647113432886, time: 8.646408796310425

Epoch 13, Batch 100/241, Loss: 1.749404788017273, Variance: 0.17958354949951172
Epoch 13, Batch 200/241, Loss: 1.6495423316955566, Variance: 0.18944913148880005

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.061218884016015, Training Loss Force: 3.3608233703200674, time: 3.680673599243164
Validation Loss Energy: 3.316135771458356, Validation Loss Force: 3.344598288034716, time: 0.18139314651489258
Test Loss Energy: 9.501140780578696, Test Loss Force: 10.759360680411449, time: 8.607479572296143

Epoch 14, Batch 100/241, Loss: 0.8308825492858887, Variance: 0.17911624908447266
Epoch 14, Batch 200/241, Loss: 0.891304075717926, Variance: 0.1860995590686798

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.070069192137374, Training Loss Force: 3.328484623938751, time: 3.8969810009002686
Validation Loss Energy: 5.630528034555072, Validation Loss Force: 3.3843534146795173, time: 0.1904289722442627
Test Loss Energy: 10.217295988770553, Test Loss Force: 10.756244245014098, time: 8.627964735031128

Epoch 15, Batch 100/241, Loss: 1.1047708988189697, Variance: 0.18773221969604492
Epoch 15, Batch 200/241, Loss: 1.127663493156433, Variance: 0.18093645572662354

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.07311957557285, Training Loss Force: 3.3503777250803117, time: 3.657658100128174
Validation Loss Energy: 4.77024489858648, Validation Loss Force: 3.362531727044021, time: 0.18654584884643555
Test Loss Energy: 10.228943566974555, Test Loss Force: 10.785800801846854, time: 8.631606817245483

Epoch 16, Batch 100/241, Loss: 1.8240976333618164, Variance: 0.18716156482696533
Epoch 16, Batch 200/241, Loss: 1.750436544418335, Variance: 0.1727651059627533

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.079475097197326, Training Loss Force: 3.3425662669241416, time: 3.6658432483673096
Validation Loss Energy: 1.7970247889145599, Validation Loss Force: 3.4429197977741577, time: 0.18439769744873047
Test Loss Energy: 8.918314186482924, Test Loss Force: 10.701443097209141, time: 8.818458080291748

Epoch 17, Batch 100/241, Loss: 1.688185214996338, Variance: 0.18975520133972168
Epoch 17, Batch 200/241, Loss: 1.4745523929595947, Variance: 0.17622175812721252

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.032004231025046, Training Loss Force: 3.355755264413167, time: 3.652113676071167
Validation Loss Energy: 3.5862233696899093, Validation Loss Force: 3.2830075569327977, time: 0.18610930442810059
Test Loss Energy: 9.694105997673336, Test Loss Force: 10.620904926198673, time: 8.650873184204102

Epoch 18, Batch 100/241, Loss: 1.114103078842163, Variance: 0.1920194774866104
Epoch 18, Batch 200/241, Loss: 0.8695120811462402, Variance: 0.1833307445049286

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.089144541816775, Training Loss Force: 3.347960480572668, time: 3.721543312072754
Validation Loss Energy: 6.194664435142322, Validation Loss Force: 3.3444420856288475, time: 0.19184613227844238
Test Loss Energy: 10.924527918634288, Test Loss Force: 10.50479415368175, time: 8.610164880752563

Epoch 19, Batch 100/241, Loss: 1.115179419517517, Variance: 0.17709484696388245
Epoch 19, Batch 200/241, Loss: 1.3150280714035034, Variance: 0.18642482161521912

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.119130392301805, Training Loss Force: 3.344105359756548, time: 3.91031551361084
Validation Loss Energy: 5.201088987726178, Validation Loss Force: 3.3084766765131643, time: 0.19185352325439453
Test Loss Energy: 10.599437246948085, Test Loss Force: 10.45102092126605, time: 8.617048978805542

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–…â–ˆâ–ˆâ–‚â–‚â–…â–…â–‚â–„â–ˆâ–‡â–‚â–ƒâ–…â–…â–â–„â–ˆâ–‡
wandb:   test_error_force â–†â–‚â–…â–…â–ƒâ–†â–‡â–ˆâ–†â–‚â–„â–‚â–†â–†â–†â–‡â–…â–„â–‚â–
wandb:          test_loss â–‚â–ƒâ–†â–ˆâ–â–„â–†â–†â–„â–„â–‡â–†â–„â–…â–†â–‡â–ƒâ–„â–†â–†
wandb: train_error_energy â–ˆâ–…â–…â–„â–†â–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–ƒâ–â–ƒâ–…
wandb:  train_error_force â–ˆâ–‚â–ƒâ–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–„â–ˆâ–†â–‚â–„â–‡â–‡â–â–„â–ˆâ–‡â–â–ƒâ–‡â–†â–â–„â–ˆâ–†
wandb:  valid_error_force â–ƒâ–‚â–ˆâ–ˆâ–‚â–„â–„â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–…â–â–ƒâ–‚
wandb:         valid_loss â–â–„â–ˆâ–†â–‚â–ƒâ–†â–‡â–â–ƒâ–ˆâ–†â–â–ƒâ–‡â–…â–â–ƒâ–ˆâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 7709
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.59944
wandb:   test_error_force 10.45102
wandb:          test_loss 8.45222
wandb: train_error_energy 4.11913
wandb:  train_error_force 3.34411
wandb:         train_loss 1.38463
wandb: valid_error_energy 5.20109
wandb:  valid_error_force 3.30848
wandb:         valid_loss 1.63906
wandb: 
wandb: ğŸš€ View run al_63_80 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/uyfbl3wr
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_174140-uyfbl3wr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.410633087158203, Uncertainty Bias: -0.23484566807746887
0.0001296997 0.010444641
1.8087223 5.848363
(48745, 22, 3)
Found uncertainty sample 0 after 3437 steps.
Found uncertainty sample 1 after 1470 steps.
Found uncertainty sample 2 after 2863 steps.
Found uncertainty sample 3 after 779 steps.
Found uncertainty sample 4 after 3418 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 1632 steps.
Found uncertainty sample 7 after 33 steps.
Found uncertainty sample 8 after 174 steps.
Found uncertainty sample 9 after 1628 steps.
Found uncertainty sample 10 after 602 steps.
Found uncertainty sample 11 after 471 steps.
Found uncertainty sample 12 after 1117 steps.
Found uncertainty sample 13 after 201 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 1881 steps.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 172 steps.
Found uncertainty sample 19 after 3286 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 463 steps.
Found uncertainty sample 22 after 446 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 31 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 16 steps.
Found uncertainty sample 27 after 3037 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1072 steps.
Found uncertainty sample 30 after 1214 steps.
Found uncertainty sample 31 after 73 steps.
Found uncertainty sample 32 after 1126 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 11 steps.
Found uncertainty sample 35 after 92 steps.
Found uncertainty sample 36 after 3225 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 201 steps.
Found uncertainty sample 39 after 1661 steps.
Found uncertainty sample 40 after 745 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 1239 steps.
Found uncertainty sample 43 after 967 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 2467 steps.
Found uncertainty sample 46 after 389 steps.
Found uncertainty sample 47 after 662 steps.
Found uncertainty sample 48 after 1338 steps.
Found uncertainty sample 49 after 1074 steps.
Found uncertainty sample 50 after 2485 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1226 steps.
Found uncertainty sample 53 after 2754 steps.
Found uncertainty sample 54 after 3876 steps.
Found uncertainty sample 55 after 787 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 83 steps.
Found uncertainty sample 59 after 1248 steps.
Found uncertainty sample 60 after 3699 steps.
Found uncertainty sample 61 after 342 steps.
Found uncertainty sample 62 after 3749 steps.
Found uncertainty sample 63 after 1349 steps.
Found uncertainty sample 64 after 768 steps.
Found uncertainty sample 65 after 234 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 2766 steps.
Did not find any uncertainty samples for sample 68.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 2776 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 464 steps.
Found uncertainty sample 73 after 899 steps.
Found uncertainty sample 74 after 3353 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 32 steps.
Found uncertainty sample 77 after 1505 steps.
Found uncertainty sample 78 after 12 steps.
Found uncertainty sample 79 after 236 steps.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 2292 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 1177 steps.
Found uncertainty sample 85 after 2099 steps.
Found uncertainty sample 86 after 831 steps.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 633 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 2580 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 3 steps.
Found uncertainty sample 95 after 625 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 903 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 809 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_180703-jmpapcml
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_81
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jmpapcml
Training model 81. Added 72 samples to the dataset.
Epoch 0, Batch 100/243, Loss: 1.0324879884719849, Variance: 0.1815882921218872
Epoch 0, Batch 200/243, Loss: 0.8615386486053467, Variance: 0.18844574689865112

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.299609241575931, Training Loss Force: 3.402007108163906, time: 3.661076784133911
Validation Loss Energy: 2.0087616034520175, Validation Loss Force: 3.470146611925191, time: 0.19810748100280762
Test Loss Energy: 8.882607703695228, Test Loss Force: 10.656048678588139, time: 8.961193323135376

Epoch 1, Batch 100/243, Loss: 1.7288202047348022, Variance: 0.19026999175548553
Epoch 1, Batch 200/243, Loss: 1.4883784055709839, Variance: 0.18221931159496307

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.049904071614562, Training Loss Force: 3.317031835627431, time: 3.7104156017303467
Validation Loss Energy: 5.158803782910772, Validation Loss Force: 3.343267895555084, time: 0.18867993354797363
Test Loss Energy: 10.54050256238269, Test Loss Force: 10.653785138299726, time: 8.964186429977417

Epoch 2, Batch 100/243, Loss: 1.8532629013061523, Variance: 0.1849774420261383
Epoch 2, Batch 200/243, Loss: 1.6337741613388062, Variance: 0.1911616325378418

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.036573575883162, Training Loss Force: 3.3410572637643305, time: 3.88966965675354
Validation Loss Energy: 5.334253264414821, Validation Loss Force: 3.4617347282649042, time: 0.18772220611572266
Test Loss Energy: 10.432324729097367, Test Loss Force: 10.716843646121246, time: 8.933554410934448

Epoch 3, Batch 100/243, Loss: 1.1437636613845825, Variance: 0.18684954941272736
Epoch 3, Batch 200/243, Loss: 0.9900685548782349, Variance: 0.17788666486740112

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.040327719954466, Training Loss Force: 3.36004059466919, time: 3.694196939468384
Validation Loss Energy: 3.547720200318087, Validation Loss Force: 3.4569599781014886, time: 0.19109869003295898
Test Loss Energy: 9.708927993853967, Test Loss Force: 10.544869503040346, time: 8.987145900726318

Epoch 4, Batch 100/243, Loss: 1.0922354459762573, Variance: 0.18280446529388428
Epoch 4, Batch 200/243, Loss: 1.0324958562850952, Variance: 0.1851576566696167

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.0585846720336205, Training Loss Force: 3.3854563597899237, time: 3.72156023979187
Validation Loss Energy: 2.5431221291729393, Validation Loss Force: 3.39939042910588, time: 0.18802165985107422
Test Loss Energy: 9.389232122255933, Test Loss Force: 10.69503000197147, time: 9.186934471130371

Epoch 5, Batch 100/243, Loss: 1.388702630996704, Variance: 0.1804945170879364
Epoch 5, Batch 200/243, Loss: 1.7459511756896973, Variance: 0.18385104835033417

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.047534813714686, Training Loss Force: 3.3261669078968312, time: 3.6724345684051514
Validation Loss Energy: 5.177476149480839, Validation Loss Force: 3.320116205110961, time: 0.1881570816040039
Test Loss Energy: 10.292539516507855, Test Loss Force: 10.837709698949547, time: 9.035508155822754

Epoch 6, Batch 100/243, Loss: 2.032679319381714, Variance: 0.1871204823255539
Epoch 6, Batch 200/243, Loss: 1.759685754776001, Variance: 0.1797102987766266

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.057002999484561, Training Loss Force: 3.340014422600231, time: 3.7727160453796387
Validation Loss Energy: 5.690782750781423, Validation Loss Force: 3.30738810272601, time: 0.19091320037841797
Test Loss Energy: 11.24530462393504, Test Loss Force: 10.619467262275085, time: 9.186041593551636

Epoch 7, Batch 100/243, Loss: 1.1879832744598389, Variance: 0.11889360845088959
Epoch 7, Batch 200/243, Loss: 1.1500929594039917, Variance: 0.13788680732250214

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 3.0176333852778887, Training Loss Force: 3.463641492498099, time: 3.687716007232666
Validation Loss Energy: 2.5610221121430685, Validation Loss Force: 3.301974514782927, time: 0.19124364852905273
Test Loss Energy: 9.771865518883406, Test Loss Force: 10.88854553128768, time: 8.990296125411987

Epoch 8, Batch 100/243, Loss: 0.6075643301010132, Variance: 0.13097266852855682
Epoch 8, Batch 200/243, Loss: 1.2155277729034424, Variance: 0.1426280438899994

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5738640266388573, Training Loss Force: 3.269020696015327, time: 3.652418375015259
Validation Loss Energy: 2.2664326134337727, Validation Loss Force: 3.3027471767192966, time: 0.1935875415802002
Test Loss Energy: 9.234174716110164, Test Loss Force: 10.962939209004048, time: 9.008309125900269

Epoch 9, Batch 100/243, Loss: 0.9191121459007263, Variance: 0.13486620783805847
Epoch 9, Batch 200/243, Loss: 1.5301928520202637, Variance: 0.1367667019367218

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5959063629039574, Training Loss Force: 3.270106638778674, time: 4.022641658782959
Validation Loss Energy: 2.7028671454857074, Validation Loss Force: 3.3365741270247753, time: 0.18724322319030762
Test Loss Energy: 9.44921193418582, Test Loss Force: 10.876910483177815, time: 10.15101933479309

Epoch 10, Batch 100/243, Loss: 0.7731485366821289, Variance: 0.13312707841396332
Epoch 10, Batch 200/243, Loss: 1.3308298587799072, Variance: 0.13538168370723724

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5807561031192945, Training Loss Force: 3.2666379767117957, time: 3.6397528648376465
Validation Loss Energy: 1.9712365639709697, Validation Loss Force: 3.307297198047185, time: 0.18688726425170898
Test Loss Energy: 9.115082196215615, Test Loss Force: 10.862505328301589, time: 9.00244140625

Epoch 11, Batch 100/243, Loss: 1.0281922817230225, Variance: 0.13850483298301697
Epoch 11, Batch 200/243, Loss: 1.351718544960022, Variance: 0.1336139291524887

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.583055741174045, Training Loss Force: 3.2725535930909695, time: 3.6078131198883057
Validation Loss Energy: 2.525124338780461, Validation Loss Force: 3.3869114378279357, time: 0.19574213027954102
Test Loss Energy: 9.521899551217071, Test Loss Force: 10.882000420060034, time: 9.149678468704224

Epoch 12, Batch 100/243, Loss: 0.6534519195556641, Variance: 0.13447144627571106
Epoch 12, Batch 200/243, Loss: 1.2539881467819214, Variance: 0.14035920798778534

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6257863974485476, Training Loss Force: 3.286013269456863, time: 3.6667089462280273
Validation Loss Energy: 2.219508542301253, Validation Loss Force: 3.3213245748986244, time: 0.19089579582214355
Test Loss Energy: 9.083293384692354, Test Loss Force: 10.857545029162969, time: 8.960846185684204

Epoch 13, Batch 100/243, Loss: 0.9339232444763184, Variance: 0.1403820514678955
Epoch 13, Batch 200/243, Loss: 0.8875434398651123, Variance: 0.13584409654140472

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5982719770019456, Training Loss Force: 3.2664501053470056, time: 3.6461336612701416
Validation Loss Energy: 2.9087865059166553, Validation Loss Force: 3.2913505244924206, time: 0.19202589988708496
Test Loss Energy: 9.582299412752358, Test Loss Force: 10.763313852832072, time: 9.160844802856445

Epoch 14, Batch 100/243, Loss: 0.8466490507125854, Variance: 0.13353273272514343
Epoch 14, Batch 200/243, Loss: 1.3443570137023926, Variance: 0.13546954095363617

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.608739382317167, Training Loss Force: 3.281653151006095, time: 3.6590499877929688
Validation Loss Energy: 2.27315282392516, Validation Loss Force: 3.3320960278310223, time: 0.19048118591308594
Test Loss Energy: 9.264180747518177, Test Loss Force: 11.026764486773123, time: 9.013647079467773

Epoch 15, Batch 100/243, Loss: 0.9453275799751282, Variance: 0.13693435490131378
Epoch 15, Batch 200/243, Loss: 1.1940289735794067, Variance: 0.13279221951961517

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5934316748288198, Training Loss Force: 3.2727946173219262, time: 3.7828354835510254
Validation Loss Energy: 2.8673444934895915, Validation Loss Force: 3.2720751698751815, time: 0.19038772583007812
Test Loss Energy: 9.638384580438595, Test Loss Force: 10.549953284461354, time: 8.972490072250366

Epoch 16, Batch 100/243, Loss: 1.1411432027816772, Variance: 0.1393154263496399
Epoch 16, Batch 200/243, Loss: 1.3613355159759521, Variance: 0.14399753510951996

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.617997301520909, Training Loss Force: 3.275392399283401, time: 3.9164552688598633
Validation Loss Energy: 2.432899844260241, Validation Loss Force: 3.2967514759614716, time: 0.19271516799926758
Test Loss Energy: 9.27461086409681, Test Loss Force: 10.901355633625778, time: 9.072738409042358

Epoch 17, Batch 100/243, Loss: 0.7822392582893372, Variance: 0.1380167454481125
Epoch 17, Batch 200/243, Loss: 1.1940959692001343, Variance: 0.13315266370773315

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6035162459061474, Training Loss Force: 3.278946967656579, time: 3.731820583343506
Validation Loss Energy: 2.7293716765946376, Validation Loss Force: 3.3054572272270972, time: 0.19442272186279297
Test Loss Energy: 9.569872282692339, Test Loss Force: 10.64889128549954, time: 8.953926801681519

Epoch 18, Batch 100/243, Loss: 0.8951107859611511, Variance: 0.13371211290359497
Epoch 18, Batch 200/243, Loss: 1.3315629959106445, Variance: 0.14374694228172302

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6248894086985333, Training Loss Force: 3.2839610745522756, time: 3.689479112625122
Validation Loss Energy: 2.2532012787790245, Validation Loss Force: 3.2902975221013278, time: 0.19398999214172363
Test Loss Energy: 9.085608397693493, Test Loss Force: 10.803718290867051, time: 9.204839706420898

Epoch 19, Batch 100/243, Loss: 0.9871963262557983, Variance: 0.13726943731307983
Epoch 19, Batch 200/243, Loss: 0.9342786073684692, Variance: 0.13077130913734436

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.609526205736317, Training Loss Force: 3.272162164557111, time: 3.674417734146118
Validation Loss Energy: 2.7263267316913615, Validation Loss Force: 3.2649096210919386, time: 0.19814538955688477
Test Loss Energy: 9.366692795862157, Test Loss Force: 10.547366218727788, time: 8.945018768310547

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–†â–†â–ƒâ–ƒâ–…â–ˆâ–„â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚
wandb:   test_error_force â–ƒâ–ƒâ–ƒâ–â–ƒâ–…â–‚â–†â–‡â–†â–†â–†â–†â–„â–ˆâ–â–†â–ƒâ–…â–
wandb:          test_loss â–â–ƒâ–ƒâ–â–‚â–ƒâ–„â–ˆâ–ˆâ–‡â–‡â–ˆâ–†â–†â–‡â–†â–‡â–†â–†â–…
wandb: train_error_energy â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–†â–ƒâ–„â–„â–…â–ƒâ–„â–ˆâ–â–â–â–â–‚â–â–‚â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–†â–†â–†â–†â–†â–†â–„â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–‡â–‡â–„â–‚â–‡â–ˆâ–‚â–‚â–‚â–â–‚â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚
wandb:  valid_error_force â–ˆâ–„â–ˆâ–ˆâ–†â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–…â–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–
wandb:         valid_loss â–‚â–‡â–ˆâ–„â–ƒâ–‡â–ˆâ–‚â–‚â–ƒâ–â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7773
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.36669
wandb:   test_error_force 10.54737
wandb:          test_loss 9.23281
wandb: train_error_energy 2.60953
wandb:  train_error_force 3.27216
wandb:         train_loss 0.9132
wandb: valid_error_energy 2.72633
wandb:  valid_error_force 3.26491
wandb:         valid_loss 0.91949
wandb: 
wandb: ğŸš€ View run al_63_81 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/jmpapcml
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_180703-jmpapcml/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.4424631595611572, Uncertainty Bias: -0.12369123101234436
1.5258789e-05 0.00019836426
1.9798129 5.664446
(48745, 22, 3)
Found uncertainty sample 0 after 2635 steps.
Found uncertainty sample 1 after 282 steps.
Found uncertainty sample 2 after 1844 steps.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 887 steps.
Found uncertainty sample 6 after 2404 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1013 steps.
Found uncertainty sample 9 after 833 steps.
Found uncertainty sample 10 after 2250 steps.
Found uncertainty sample 11 after 2 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 204 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 470 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 1612 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 443 steps.
Found uncertainty sample 20 after 1094 steps.
Found uncertainty sample 21 after 408 steps.
Found uncertainty sample 22 after 1311 steps.
Found uncertainty sample 23 after 354 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 538 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 2775 steps.
Found uncertainty sample 28 after 922 steps.
Found uncertainty sample 29 after 2583 steps.
Found uncertainty sample 30 after 644 steps.
Did not find any uncertainty samples for sample 31.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 36 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 944 steps.
Found uncertainty sample 36 after 3805 steps.
Found uncertainty sample 37 after 654 steps.
Found uncertainty sample 38 after 1254 steps.
Found uncertainty sample 39 after 2221 steps.
Found uncertainty sample 40 after 2296 steps.
Found uncertainty sample 41 after 756 steps.
Found uncertainty sample 42 after 204 steps.
Found uncertainty sample 43 after 2236 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 1318 steps.
Found uncertainty sample 46 after 931 steps.
Found uncertainty sample 47 after 3695 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 2802 steps.
Found uncertainty sample 50 after 815 steps.
Found uncertainty sample 51 after 527 steps.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 1146 steps.
Found uncertainty sample 54 after 3166 steps.
Found uncertainty sample 55 after 2210 steps.
Found uncertainty sample 56 after 1406 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 352 steps.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 53 steps.
Found uncertainty sample 62 after 2296 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 872 steps.
Found uncertainty sample 65 after 3636 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 868 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 389 steps.
Found uncertainty sample 70 after 3822 steps.
Found uncertainty sample 71 after 2632 steps.
Found uncertainty sample 72 after 3423 steps.
Found uncertainty sample 73 after 3204 steps.
Found uncertainty sample 74 after 839 steps.
Found uncertainty sample 75 after 2179 steps.
Found uncertainty sample 76 after 2197 steps.
Found uncertainty sample 77 after 3646 steps.
Found uncertainty sample 78 after 79 steps.
Found uncertainty sample 79 after 1168 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 407 steps.
Found uncertainty sample 82 after 930 steps.
Found uncertainty sample 83 after 1300 steps.
Found uncertainty sample 84 after 423 steps.
Found uncertainty sample 85 after 329 steps.
Found uncertainty sample 86 after 356 steps.
Found uncertainty sample 87 after 3201 steps.
Found uncertainty sample 88 after 1970 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1422 steps.
Found uncertainty sample 91 after 2907 steps.
Found uncertainty sample 92 after 1793 steps.
Found uncertainty sample 93 after 1539 steps.
Found uncertainty sample 94 after 1262 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 706 steps.
Found uncertainty sample 97 after 518 steps.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_183242-ev7d9a7i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_82
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ev7d9a7i
Training model 82. Added 76 samples to the dataset.
Epoch 0, Batch 100/246, Loss: 4.434012413024902, Variance: 0.1050887256860733
Epoch 0, Batch 200/246, Loss: 1.7295781373977661, Variance: 0.15354016423225403

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.469063288739859, Training Loss Force: 3.6048346475228863, time: 3.7256734371185303
Validation Loss Energy: 3.0247500920879555, Validation Loss Force: 3.323899131231128, time: 0.20380687713623047
Test Loss Energy: 9.130255459385886, Test Loss Force: 10.711833343186777, time: 8.86116075515747

Epoch 1, Batch 100/246, Loss: 0.7355734705924988, Variance: 0.17095696926116943
Epoch 1, Batch 200/246, Loss: 0.8705703020095825, Variance: 0.18029870092868805

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.020034055874087, Training Loss Force: 3.3067601340971735, time: 3.779754638671875
Validation Loss Energy: 5.144242890785689, Validation Loss Force: 3.2922527798717196, time: 0.18825507164001465
Test Loss Energy: 10.77419248336096, Test Loss Force: 10.403374681049065, time: 8.853232383728027

Epoch 2, Batch 100/246, Loss: 1.8382929563522339, Variance: 0.1786792278289795
Epoch 2, Batch 200/246, Loss: 1.820149302482605, Variance: 0.18428704142570496

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.025315818251825, Training Loss Force: 3.3083536169614125, time: 3.8640902042388916
Validation Loss Energy: 1.1414464923540963, Validation Loss Force: 3.4277395514196765, time: 0.19564056396484375
Test Loss Energy: 8.685282703998203, Test Loss Force: 10.616763392001891, time: 8.87106704711914

Epoch 3, Batch 100/246, Loss: 0.8575202226638794, Variance: 0.16461125016212463
Epoch 3, Batch 200/246, Loss: 0.7817026972770691, Variance: 0.17376551032066345

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.8208224794574113, Training Loss Force: 3.7666913344614117, time: 3.6533656120300293
Validation Loss Energy: 5.394098603416384, Validation Loss Force: 3.3018305591800847, time: 0.19316720962524414
Test Loss Energy: 10.896640629616256, Test Loss Force: 10.461951943163612, time: 8.906447649002075

Epoch 4, Batch 100/246, Loss: 1.6905274391174316, Variance: 0.17728763818740845
Epoch 4, Batch 200/246, Loss: 1.7813386917114258, Variance: 0.19369804859161377

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.068219572610163, Training Loss Force: 3.317391524968725, time: 3.787374973297119
Validation Loss Energy: 3.1126887065546436, Validation Loss Force: 3.371810684358551, time: 0.19280290603637695
Test Loss Energy: 9.443153785224826, Test Loss Force: 10.541619340248275, time: 9.069363594055176

Epoch 5, Batch 100/246, Loss: 0.9773980379104614, Variance: 0.18863792717456818
Epoch 5, Batch 200/246, Loss: 0.8090319633483887, Variance: 0.17911052703857422

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.046341901335069, Training Loss Force: 3.324328676306488, time: 3.7341907024383545
Validation Loss Energy: 4.215947022877476, Validation Loss Force: 3.4532842671750847, time: 0.19858026504516602
Test Loss Energy: 9.745013750979796, Test Loss Force: 10.59838582888877, time: 8.88037633895874

Epoch 6, Batch 100/246, Loss: 1.9351906776428223, Variance: 0.18843139708042145
Epoch 6, Batch 200/246, Loss: 1.9243638515472412, Variance: 0.1772046685218811

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.00931706455621, Training Loss Force: 3.328216859761221, time: 3.758605480194092
Validation Loss Energy: 3.937377344537382, Validation Loss Force: 3.4349294268071118, time: 0.19238972663879395
Test Loss Energy: 9.585538854782826, Test Loss Force: 10.584752774029914, time: 9.06843900680542

Epoch 7, Batch 100/246, Loss: 0.8216721415519714, Variance: 0.1859428733587265
Epoch 7, Batch 200/246, Loss: 0.9237188696861267, Variance: 0.18787835538387299

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.056730901453573, Training Loss Force: 3.3228175402940807, time: 3.7064478397369385
Validation Loss Energy: 5.404486898415926, Validation Loss Force: 3.3748908109868836, time: 0.19461417198181152
Test Loss Energy: 10.532941254458224, Test Loss Force: 10.53201043214305, time: 8.910265684127808

Epoch 8, Batch 100/246, Loss: 1.6350867748260498, Variance: 0.1812974512577057
Epoch 8, Batch 200/246, Loss: 1.7643440961837769, Variance: 0.19066080451011658

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.013149958628398, Training Loss Force: 3.374335589092149, time: 3.795304775238037
Validation Loss Energy: 2.7731891234704587, Validation Loss Force: 3.7173510030353243, time: 0.19260597229003906
Test Loss Energy: 9.1888392173773, Test Loss Force: 10.441868860299845, time: 8.867611646652222

Epoch 9, Batch 100/246, Loss: 0.9716816544532776, Variance: 0.10772866010665894
Epoch 9, Batch 200/246, Loss: 1.1960978507995605, Variance: 0.13389119505882263

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5650553256245314, Training Loss Force: 3.8119620138675283, time: 3.868201971054077
Validation Loss Energy: 2.0087273124035443, Validation Loss Force: 3.3344558197722782, time: 0.1948397159576416
Test Loss Energy: 8.839769764694111, Test Loss Force: 10.860783025592697, time: 8.910024881362915

Epoch 10, Batch 100/246, Loss: 0.979256272315979, Variance: 0.13362836837768555
Epoch 10, Batch 200/246, Loss: 1.137995719909668, Variance: 0.13344717025756836

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5902594200426505, Training Loss Force: 3.2494429371308358, time: 3.7736754417419434
Validation Loss Energy: 1.9425932957154026, Validation Loss Force: 3.4012624086156955, time: 0.19512271881103516
Test Loss Energy: 9.002849000527178, Test Loss Force: 10.794599996336148, time: 8.876981258392334

Epoch 11, Batch 100/246, Loss: 0.9777886271476746, Variance: 0.13514229655265808
Epoch 11, Batch 200/246, Loss: 1.2955117225646973, Variance: 0.13302502036094666

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.577962643181067, Training Loss Force: 3.2729154720904226, time: 3.8756511211395264
Validation Loss Energy: 1.9627596052527854, Validation Loss Force: 3.409897740763921, time: 0.193190336227417
Test Loss Energy: 8.907517740687608, Test Loss Force: 10.763636254439236, time: 9.049705743789673

Epoch 12, Batch 100/246, Loss: 0.8006590008735657, Variance: 0.13762156665325165
Epoch 12, Batch 200/246, Loss: 1.219848394393921, Variance: 0.1322668194770813

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5907575793481055, Training Loss Force: 3.286096001973534, time: 3.665001153945923
Validation Loss Energy: 1.7017388163069789, Validation Loss Force: 3.5779701934633827, time: 0.19635963439941406
Test Loss Energy: 8.868522747050845, Test Loss Force: 10.77977367734299, time: 8.8815279006958

Epoch 13, Batch 100/246, Loss: 4.174976348876953, Variance: 0.12713027000427246
Epoch 13, Batch 200/246, Loss: 3.7398157119750977, Variance: 0.11079923063516617

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 3.314397525136751, Training Loss Force: 4.582873105458, time: 3.8004255294799805
Validation Loss Energy: 1.9758463719645396, Validation Loss Force: 3.9964960027812095, time: 0.19057202339172363
Test Loss Energy: 8.878799511038936, Test Loss Force: 11.090483568846699, time: 9.031211853027344

Epoch 14, Batch 100/246, Loss: 0.5510123372077942, Variance: 0.09705564379692078
Epoch 14, Batch 200/246, Loss: 1.2015984058380127, Variance: 0.11910694092512131

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.77699294269995, Training Loss Force: 4.635778520619386, time: 3.766554355621338
Validation Loss Energy: 5.582269618565579, Validation Loss Force: 4.007589602300478, time: 0.1922914981842041
Test Loss Energy: 10.204688652920478, Test Loss Force: 11.467345306600073, time: 8.900883674621582

Epoch 15, Batch 100/246, Loss: 2.1354141235351562, Variance: 0.17766129970550537
Epoch 15, Batch 200/246, Loss: 1.6503140926361084, Variance: 0.17465327680110931

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.0412896090163795, Training Loss Force: 3.360980007769451, time: 3.8835608959198
Validation Loss Energy: 2.4729325279954413, Validation Loss Force: 3.473197401893548, time: 0.19598150253295898
Test Loss Energy: 9.030296924848669, Test Loss Force: 10.546607110464146, time: 8.883499145507812

Epoch 16, Batch 100/246, Loss: 4.731555938720703, Variance: 0.11583663523197174
Epoch 16, Batch 200/246, Loss: 2.52276611328125, Variance: 0.11374308913946152

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.8686108303467956, Training Loss Force: 3.9764503912354083, time: 3.7084925174713135
Validation Loss Energy: 2.17256064557012, Validation Loss Force: 3.878524330584646, time: 0.24064350128173828
Test Loss Energy: 9.465127252194257, Test Loss Force: 10.973142706682475, time: 10.141663074493408

Epoch 17, Batch 100/246, Loss: 0.834139883518219, Variance: 0.1269148290157318
Epoch 17, Batch 200/246, Loss: 0.38661110401153564, Variance: 0.10058455169200897

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.3853839461451996, Training Loss Force: 3.8051205650045583, time: 3.7735393047332764
Validation Loss Energy: 1.6882491576093113, Validation Loss Force: 3.2839800644276376, time: 0.1941831111907959
Test Loss Energy: 8.936310647272466, Test Loss Force: 10.787994794750084, time: 8.884504556655884

Epoch 18, Batch 100/246, Loss: 0.6338576078414917, Variance: 0.09782154858112335
Epoch 18, Batch 200/246, Loss: 0.28311240673065186, Variance: 0.0979292243719101

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6051734252561598, Training Loss Force: 3.255547158321891, time: 3.785078763961792
Validation Loss Energy: 1.7218448103394581, Validation Loss Force: 3.29870584305528, time: 0.1950685977935791
Test Loss Energy: 8.961433579284495, Test Loss Force: 10.757657599529145, time: 9.06641173362732

Epoch 19, Batch 100/246, Loss: 0.49861252307891846, Variance: 0.09607282280921936
Epoch 19, Batch 200/246, Loss: 0.2875034809112549, Variance: 0.0945623591542244

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.609323791673473, Training Loss Force: 3.249638818092318, time: 3.677535057067871
Validation Loss Energy: 2.0464727386717745, Validation Loss Force: 3.2675764737854296, time: 0.19850873947143555
Test Loss Energy: 8.946166818389077, Test Loss Force: 10.693865920319157, time: 8.848689794540405

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ˆâ–â–ˆâ–ƒâ–„â–„â–‡â–ƒâ–â–‚â–‚â–‚â–‚â–†â–‚â–ƒâ–‚â–‚â–‚
wandb:   test_error_force â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–â–„â–„â–ƒâ–ƒâ–†â–ˆâ–‚â–…â–„â–ƒâ–ƒ
wandb:          test_loss â–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–â–„â–„â–„â–„â–ƒâ–ˆâ–â–†â–‡â–‡â–‡
wandb: train_error_energy â–†â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–†â–„â–ˆâ–…â–ƒâ–â–
wandb:  train_error_force â–ƒâ–â–â–„â–â–â–â–â–‚â–„â–â–â–â–ˆâ–ˆâ–‚â–…â–„â–â–
wandb:         train_loss â–†â–…â–…â–†â–…â–…â–…â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ˆâ–†â–…â–†â–„â–â–
wandb: valid_error_energy â–„â–‡â–â–ˆâ–„â–†â–…â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚
wandb:  valid_error_force â–‚â–â–ƒâ–â–‚â–ƒâ–ƒâ–‚â–…â–‚â–‚â–‚â–„â–ˆâ–ˆâ–ƒâ–‡â–â–â–
wandb:         valid_loss â–‚â–„â–‚â–„â–‚â–ƒâ–ƒâ–„â–ƒâ–‚â–‚â–‚â–â–‚â–ˆâ–‚â–‚â–â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7841
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.94617
wandb:   test_error_force 10.69387
wandb:          test_loss 11.04033
wandb: train_error_energy 1.60932
wandb:  train_error_force 3.24964
wandb:         train_loss 0.45868
wandb: valid_error_energy 2.04647
wandb:  valid_error_force 3.26758
wandb:         valid_loss 0.70305
wandb: 
wandb: ğŸš€ View run al_63_82 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ev7d9a7i
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_183242-ev7d9a7i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.425363540649414, Uncertainty Bias: -0.10840494930744171
0.000118255615 0.006795883
1.8827673 6.6312222
(48745, 22, 3)
Found uncertainty sample 0 after 2415 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 1020 steps.
Found uncertainty sample 4 after 1827 steps.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 678 steps.
Found uncertainty sample 9 after 2273 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 1954 steps.
Found uncertainty sample 12 after 298 steps.
Found uncertainty sample 13 after 1627 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 1718 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 1015 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 376 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 981 steps.
Found uncertainty sample 25 after 1803 steps.
Found uncertainty sample 26 after 3553 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 1424 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 2059 steps.
Found uncertainty sample 31 after 1649 steps.
Found uncertainty sample 32 after 2157 steps.
Found uncertainty sample 33 after 13 steps.
Found uncertainty sample 34 after 1000 steps.
Found uncertainty sample 35 after 835 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 1204 steps.
Found uncertainty sample 38 after 2010 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 3002 steps.
Found uncertainty sample 41 after 505 steps.
Found uncertainty sample 42 after 2774 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 41 steps.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 3361 steps.
Found uncertainty sample 47 after 2854 steps.
Found uncertainty sample 48 after 559 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 186 steps.
Found uncertainty sample 51 after 1210 steps.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 1076 steps.
Found uncertainty sample 54 after 405 steps.
Found uncertainty sample 55 after 155 steps.
Found uncertainty sample 56 after 1001 steps.
Found uncertainty sample 57 after 38 steps.
Found uncertainty sample 58 after 3372 steps.
Found uncertainty sample 59 after 1625 steps.
Found uncertainty sample 60 after 79 steps.
Found uncertainty sample 61 after 936 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 1055 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 1406 steps.
Found uncertainty sample 67 after 736 steps.
Did not find any uncertainty samples for sample 68.
Did not find any uncertainty samples for sample 69.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 1396 steps.
Found uncertainty sample 72 after 37 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 2552 steps.
Found uncertainty sample 75 after 1900 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 949 steps.
Found uncertainty sample 78 after 3450 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 437 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 947 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 2131 steps.
Found uncertainty sample 85 after 1057 steps.
Found uncertainty sample 86 after 853 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 2637 steps.
Found uncertainty sample 89 after 3618 steps.
Found uncertainty sample 90 after 1642 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 1289 steps.
Found uncertainty sample 93 after 39 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 714 steps.
Found uncertainty sample 96 after 1763 steps.
Found uncertainty sample 97 after 480 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 1384 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_185959-j3ygbsui
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_83
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j3ygbsui
Training model 83. Added 67 samples to the dataset.
Epoch 0, Batch 100/247, Loss: 0.8950904607772827, Variance: 0.11718728393316269
Epoch 0, Batch 200/247, Loss: 1.4698728322982788, Variance: 0.135489821434021

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1485625159392283, Training Loss Force: 3.723281918565739, time: 3.8032751083374023
Validation Loss Energy: 3.9839605092074857, Validation Loss Force: 3.2887184638529807, time: 0.1929793357849121
Test Loss Energy: 9.899321888273398, Test Loss Force: 10.668283720952678, time: 8.799761295318604

Epoch 1, Batch 100/247, Loss: 1.25653076171875, Variance: 0.12962619960308075
Epoch 1, Batch 200/247, Loss: 0.6786283850669861, Variance: 0.13732899725437164

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.627150513421859, Training Loss Force: 3.260195645406822, time: 3.701887845993042
Validation Loss Energy: 2.070698529408567, Validation Loss Force: 3.2533937852047052, time: 0.18808770179748535
Test Loss Energy: 9.343086923896825, Test Loss Force: 10.706202677016911, time: 8.792204141616821

Epoch 2, Batch 100/247, Loss: 0.5713741779327393, Variance: 0.13502223789691925
Epoch 2, Batch 200/247, Loss: 0.6050320863723755, Variance: 0.12950584292411804

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.610011198228911, Training Loss Force: 3.26565965686815, time: 4.000196933746338
Validation Loss Energy: 1.9532278627903408, Validation Loss Force: 3.3544935935451736, time: 0.19634747505187988
Test Loss Energy: 9.143028936729808, Test Loss Force: 10.791826676605567, time: 8.783531188964844

Epoch 3, Batch 100/247, Loss: 0.7782372832298279, Variance: 0.13847680389881134
Epoch 3, Batch 200/247, Loss: 1.0239686965942383, Variance: 0.1332375705242157

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5920675065224534, Training Loss Force: 3.265819784831897, time: 3.742626428604126
Validation Loss Energy: 3.1809374121216623, Validation Loss Force: 3.2832610100881503, time: 0.1939525604248047
Test Loss Energy: 9.375514229286674, Test Loss Force: 10.569365310136106, time: 8.814518213272095

Epoch 4, Batch 100/247, Loss: 1.6066339015960693, Variance: 0.13589359819889069
Epoch 4, Batch 200/247, Loss: 0.7117356061935425, Variance: 0.1370410919189453

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.602917753456322, Training Loss Force: 3.2795122767535534, time: 3.7914419174194336
Validation Loss Energy: 1.8163482957346058, Validation Loss Force: 3.29601366174644, time: 0.20325541496276855
Test Loss Energy: 8.820584392893773, Test Loss Force: 10.633033409135912, time: 9.025497198104858

Epoch 5, Batch 100/247, Loss: 0.7546364068984985, Variance: 0.13209927082061768
Epoch 5, Batch 200/247, Loss: 0.9100953340530396, Variance: 0.14166167378425598

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5918751424669875, Training Loss Force: 3.27509518800429, time: 3.8482182025909424
Validation Loss Energy: 2.5877794840672004, Validation Loss Force: 3.3142850568519773, time: 0.19047904014587402
Test Loss Energy: 9.405213662767851, Test Loss Force: 10.581651764591406, time: 8.849372148513794

Epoch 6, Batch 100/247, Loss: 0.9235424995422363, Variance: 0.13844871520996094
Epoch 6, Batch 200/247, Loss: 1.1654284000396729, Variance: 0.13925671577453613

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.604236975292009, Training Loss Force: 3.279455637655981, time: 3.796595335006714
Validation Loss Energy: 3.738871975989245, Validation Loss Force: 3.271794718085885, time: 0.20023894309997559
Test Loss Energy: 9.759953941342056, Test Loss Force: 10.623518516374167, time: 8.976237058639526

Epoch 7, Batch 100/247, Loss: 1.3383545875549316, Variance: 0.1361784040927887
Epoch 7, Batch 200/247, Loss: 0.6253478527069092, Variance: 0.13741450011730194

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.636771091182462, Training Loss Force: 3.274597023936672, time: 3.777224063873291
Validation Loss Energy: 2.1996117737650165, Validation Loss Force: 3.404555239945675, time: 0.2005460262298584
Test Loss Energy: 9.027241988104645, Test Loss Force: 10.696623305799024, time: 8.774880647659302

Epoch 8, Batch 100/247, Loss: 0.5195358395576477, Variance: 0.13833464682102203
Epoch 8, Batch 200/247, Loss: 0.9225969910621643, Variance: 0.13969862461090088

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.605299482590551, Training Loss Force: 3.277670534271364, time: 3.77420973777771
Validation Loss Energy: 2.1635177175502736, Validation Loss Force: 3.3496322807512784, time: 0.19265985488891602
Test Loss Energy: 8.895177887606337, Test Loss Force: 10.744887670901875, time: 8.817876100540161

Epoch 9, Batch 100/247, Loss: 1.2879719734191895, Variance: 0.14223970472812653
Epoch 9, Batch 200/247, Loss: 1.1763852834701538, Variance: 0.13529756665229797

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.645832134734646, Training Loss Force: 3.2695981692226823, time: 3.929982900619507
Validation Loss Energy: 3.2250809579616946, Validation Loss Force: 3.286012755393526, time: 0.20069098472595215
Test Loss Energy: 9.306894947759353, Test Loss Force: 10.653671392837786, time: 8.810577630996704

Epoch 10, Batch 100/247, Loss: 1.1356475353240967, Variance: 0.13960057497024536
Epoch 10, Batch 200/247, Loss: 0.4562481641769409, Variance: 0.1335441619157791

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6236261625235207, Training Loss Force: 3.2705208317333834, time: 3.892495632171631
Validation Loss Energy: 1.472270802012005, Validation Loss Force: 3.350523347005995, time: 0.18976831436157227
Test Loss Energy: 8.749261411301086, Test Loss Force: 10.734348765872866, time: 8.816442966461182

Epoch 11, Batch 100/247, Loss: 0.7141525745391846, Variance: 0.13822638988494873
Epoch 11, Batch 200/247, Loss: 0.787471354007721, Variance: 0.14170563220977783

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5872311232072094, Training Loss Force: 3.2741543757044584, time: 3.8584139347076416
Validation Loss Energy: 2.5948888645583823, Validation Loss Force: 3.3371136543653583, time: 0.1894848346710205
Test Loss Energy: 9.214884783075805, Test Loss Force: 10.641007370785871, time: 9.005366086959839

Epoch 12, Batch 100/247, Loss: 0.8580634593963623, Variance: 0.13821759819984436
Epoch 12, Batch 200/247, Loss: 1.0973230600357056, Variance: 0.1399049162864685

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6326815744394514, Training Loss Force: 3.275489571121508, time: 3.7558436393737793
Validation Loss Energy: 3.8753092107865945, Validation Loss Force: 3.300812515844903, time: 0.1921846866607666
Test Loss Energy: 10.222630432696299, Test Loss Force: 10.59985707271853, time: 8.84293818473816

Epoch 13, Batch 100/247, Loss: 1.206531047821045, Variance: 0.1370205283164978
Epoch 13, Batch 200/247, Loss: 0.6122384071350098, Variance: 0.1409725844860077

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.601530915024318, Training Loss Force: 3.2823391733464264, time: 3.6879916191101074
Validation Loss Energy: 2.1943779354899835, Validation Loss Force: 3.3843616534379164, time: 0.19701004028320312
Test Loss Energy: 9.338800428588625, Test Loss Force: 10.683700190125812, time: 8.991456270217896

Epoch 14, Batch 100/247, Loss: 0.5520543456077576, Variance: 0.13887912034988403
Epoch 14, Batch 200/247, Loss: 0.7728582620620728, Variance: 0.13253267109394073

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5936364806531222, Training Loss Force: 3.2915452939606338, time: 3.8400614261627197
Validation Loss Energy: 2.2404468457675675, Validation Loss Force: 3.3089868115264847, time: 0.19904303550720215
Test Loss Energy: 8.886099897356656, Test Loss Force: 10.741218503067522, time: 8.834933280944824

Epoch 15, Batch 100/247, Loss: 0.8411307334899902, Variance: 0.14462123811244965
Epoch 15, Batch 200/247, Loss: 1.4633969068527222, Variance: 0.1366504579782486

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6171060340075414, Training Loss Force: 3.2775374962178816, time: 3.719674587249756
Validation Loss Energy: 3.246773775828985, Validation Loss Force: 3.304716685315028, time: 0.18877935409545898
Test Loss Energy: 9.561449811755358, Test Loss Force: 10.817390260365125, time: 8.827344179153442

Epoch 16, Batch 100/247, Loss: 1.3252553939819336, Variance: 0.1428312063217163
Epoch 16, Batch 200/247, Loss: 0.5148504972457886, Variance: 0.13761962950229645

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.626678638286626, Training Loss Force: 3.270486424392119, time: 3.707531690597534
Validation Loss Energy: 1.4648946360503576, Validation Loss Force: 3.297462736880852, time: 0.19188332557678223
Test Loss Energy: 8.829982607023052, Test Loss Force: 10.707162042951817, time: 8.994163274765015

Epoch 17, Batch 100/247, Loss: 0.742294430732727, Variance: 0.1351294219493866
Epoch 17, Batch 200/247, Loss: 0.9524732828140259, Variance: 0.13558943569660187

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5953096039947012, Training Loss Force: 3.2805997108262637, time: 3.6677663326263428
Validation Loss Energy: 2.7075557226896487, Validation Loss Force: 3.280330684127741, time: 0.19182443618774414
Test Loss Energy: 9.124066059204342, Test Loss Force: 10.589582516768985, time: 8.794021368026733

Epoch 18, Batch 100/247, Loss: 0.6443110704421997, Variance: 0.1335582137107849
Epoch 18, Batch 200/247, Loss: 1.5086033344268799, Variance: 0.1422739028930664

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.59475705727029, Training Loss Force: 3.2722215101563976, time: 3.7989718914031982
Validation Loss Energy: 3.7728531803455656, Validation Loss Force: 3.316655926583243, time: 0.19643187522888184
Test Loss Energy: 10.29661136342261, Test Loss Force: 10.773089830413552, time: 8.9933021068573

Epoch 19, Batch 100/247, Loss: 1.0914638042449951, Variance: 0.13307946920394897
Epoch 19, Batch 200/247, Loss: 0.5885729789733887, Variance: 0.1353306770324707

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.590490839939063, Training Loss Force: 3.2629419285569137, time: 3.8403098583221436
Validation Loss Energy: 2.2580597525413095, Validation Loss Force: 3.278334956240076, time: 0.19431805610656738
Test Loss Energy: 9.233749522964075, Test Loss Force: 10.72160128602609, time: 8.836169958114624

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.039 MB of 0.061 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–ƒâ–„â–â–„â–†â–‚â–‚â–„â–â–ƒâ–ˆâ–„â–‚â–…â–â–ƒâ–ˆâ–ƒ
wandb:   test_error_force â–„â–…â–‡â–â–ƒâ–â–ƒâ–…â–†â–ƒâ–†â–ƒâ–‚â–„â–†â–ˆâ–…â–‚â–‡â–…
wandb:          test_loss â–‡â–…â–†â–…â–‚â–„â–„â–â–‚â–„â–â–‚â–†â–„â–‚â–‡â–â–â–ˆâ–ƒ
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–ƒâ–‚â–†â–‚â–„â–‡â–ƒâ–ƒâ–†â–â–„â–ˆâ–ƒâ–ƒâ–†â–â–„â–‡â–ƒ
wandb:  valid_error_force â–ƒâ–â–†â–‚â–ƒâ–„â–‚â–ˆâ–…â–ƒâ–…â–…â–ƒâ–‡â–„â–ƒâ–ƒâ–‚â–„â–‚
wandb:         valid_loss â–ˆâ–‚â–‚â–…â–‚â–ƒâ–‡â–ƒâ–ƒâ–…â–â–„â–‡â–ƒâ–ƒâ–…â–â–„â–‡â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7901
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.23375
wandb:   test_error_force 10.7216
wandb:          test_loss 9.24516
wandb: train_error_energy 2.59049
wandb:  train_error_force 3.26294
wandb:         train_loss 0.89841
wandb: valid_error_energy 2.25806
wandb:  valid_error_force 3.27833
wandb:         valid_loss 0.75461
wandb: 
wandb: ğŸš€ View run al_63_83 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j3ygbsui
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_185959-j3ygbsui/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.6013264656066895, Uncertainty Bias: -0.1409781128168106
3.8146973e-05 0.0033340454
1.964977 5.8452296
(48745, 22, 3)
Found uncertainty sample 0 after 2184 steps.
Found uncertainty sample 1 after 3016 steps.
Found uncertainty sample 2 after 987 steps.
Found uncertainty sample 3 after 2494 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 2608 steps.
Found uncertainty sample 6 after 2927 steps.
Found uncertainty sample 7 after 1 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 3073 steps.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 3195 steps.
Found uncertainty sample 14 after 3389 steps.
Found uncertainty sample 15 after 1559 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 1922 steps.
Found uncertainty sample 18 after 572 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 1714 steps.
Found uncertainty sample 21 after 1086 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 286 steps.
Found uncertainty sample 24 after 865 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 1307 steps.
Found uncertainty sample 28 after 958 steps.
Found uncertainty sample 29 after 3225 steps.
Found uncertainty sample 30 after 3327 steps.
Found uncertainty sample 31 after 2989 steps.
Found uncertainty sample 32 after 1124 steps.
Found uncertainty sample 33 after 2682 steps.
Found uncertainty sample 34 after 2566 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 813 steps.
Found uncertainty sample 37 after 1090 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 2416 steps.
Found uncertainty sample 41 after 553 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 480 steps.
Found uncertainty sample 46 after 3273 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 1841 steps.
Found uncertainty sample 49 after 48 steps.
Found uncertainty sample 50 after 3077 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 3734 steps.
Found uncertainty sample 54 after 3392 steps.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 138 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 1396 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 51 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 75 steps.
Found uncertainty sample 67 after 2236 steps.
Found uncertainty sample 68 after 1877 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 34 steps.
Found uncertainty sample 71 after 3242 steps.
Found uncertainty sample 72 after 1711 steps.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 2379 steps.
Found uncertainty sample 76 after 3110 steps.
Did not find any uncertainty samples for sample 77.
Did not find any uncertainty samples for sample 78.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 2375 steps.
Found uncertainty sample 81 after 206 steps.
Found uncertainty sample 82 after 2375 steps.
Found uncertainty sample 83 after 321 steps.
Found uncertainty sample 84 after 3078 steps.
Found uncertainty sample 85 after 1337 steps.
Found uncertainty sample 86 after 2012 steps.
Found uncertainty sample 87 after 2368 steps.
Found uncertainty sample 88 after 407 steps.
Found uncertainty sample 89 after 1038 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 76 steps.
Found uncertainty sample 92 after 1988 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 186 steps.
Found uncertainty sample 95 after 352 steps.
Found uncertainty sample 96 after 2227 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 2355 steps.
Found uncertainty sample 99 after 543 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_192944-8b7lkwgi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_84
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8b7lkwgi
Training model 84. Added 65 samples to the dataset.
Epoch 0, Batch 100/249, Loss: 0.4807899594306946, Variance: 0.12323600798845291
Epoch 0, Batch 200/249, Loss: 0.2732258439064026, Variance: 0.10598906129598618

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.422753778415578, Training Loss Force: 3.472505719762445, time: 3.822437286376953
Validation Loss Energy: 1.6417520267304653, Validation Loss Force: 3.2791704108755972, time: 0.19751596450805664
Test Loss Energy: 8.66286194717592, Test Loss Force: 10.674791936718906, time: 8.714133739471436

Epoch 1, Batch 100/249, Loss: 0.5675858855247498, Variance: 0.09973889589309692
Epoch 1, Batch 200/249, Loss: 0.7633607983589172, Variance: 0.09625045955181122

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5972415328689866, Training Loss Force: 3.245172346485501, time: 3.7393105030059814
Validation Loss Energy: 1.413844811813872, Validation Loss Force: 3.2991903154495326, time: 0.19054126739501953
Test Loss Energy: 8.745437407969627, Test Loss Force: 10.648009184592462, time: 8.740182161331177

Epoch 2, Batch 100/249, Loss: 0.40649062395095825, Variance: 0.0964604988694191
Epoch 2, Batch 200/249, Loss: 0.988379955291748, Variance: 0.09642917662858963

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6012899271122871, Training Loss Force: 3.277370911290599, time: 4.150731325149536
Validation Loss Energy: 1.7440808017545963, Validation Loss Force: 3.314129355451687, time: 0.1983811855316162
Test Loss Energy: 8.577034936279762, Test Loss Force: 10.97155511997268, time: 8.76001262664795

Epoch 3, Batch 100/249, Loss: 0.42822837829589844, Variance: 0.0940643921494484
Epoch 3, Batch 200/249, Loss: 0.24548953771591187, Variance: 0.09277515113353729

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6225250116621621, Training Loss Force: 3.252077060049939, time: 3.727541446685791
Validation Loss Energy: 1.9081249469350783, Validation Loss Force: 3.3058115495055542, time: 0.19150757789611816
Test Loss Energy: 9.237643784823014, Test Loss Force: 10.806144223413845, time: 8.698839664459229

Epoch 4, Batch 100/249, Loss: 0.0965585708618164, Variance: 0.08742091059684753
Epoch 4, Batch 200/249, Loss: 0.34808027744293213, Variance: 0.09272408485412598

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6149985825917, Training Loss Force: 3.2554714991564966, time: 3.781268358230591
Validation Loss Energy: 1.8588602612414957, Validation Loss Force: 3.301647803478564, time: 0.1971418857574463
Test Loss Energy: 8.850791367467592, Test Loss Force: 10.855721091395742, time: 8.929293155670166

Epoch 5, Batch 100/249, Loss: 0.19358611106872559, Variance: 0.09533603489398956
Epoch 5, Batch 200/249, Loss: 0.5961164832115173, Variance: 0.09035909920930862

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6026153439648994, Training Loss Force: 3.25657912971139, time: 3.8541746139526367
Validation Loss Energy: 1.3868701148033151, Validation Loss Force: 3.2829689238359796, time: 0.19821786880493164
Test Loss Energy: 8.669438075865854, Test Loss Force: 10.831736164264006, time: 8.751147985458374

Epoch 6, Batch 100/249, Loss: 0.35785549879074097, Variance: 0.09294809401035309
Epoch 6, Batch 200/249, Loss: 0.6176943182945251, Variance: 0.09328241646289825

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6061587136306286, Training Loss Force: 3.2818398229228856, time: 3.677734375
Validation Loss Energy: 1.3871935695714184, Validation Loss Force: 3.3363632036762954, time: 0.1922285556793213
Test Loss Energy: 8.696531240439791, Test Loss Force: 11.071842043702052, time: 8.900449275970459

Epoch 7, Batch 100/249, Loss: 0.5643047094345093, Variance: 0.09219332039356232
Epoch 7, Batch 200/249, Loss: 0.3064148426055908, Variance: 0.09381113946437836

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.622842326820406, Training Loss Force: 3.297336050209242, time: 3.8356478214263916
Validation Loss Energy: 1.7257411585558324, Validation Loss Force: 3.3118223917653022, time: 0.19191861152648926
Test Loss Energy: 9.053331625972747, Test Loss Force: 10.841867036826653, time: 9.883892297744751

Epoch 8, Batch 100/249, Loss: 0.385131299495697, Variance: 0.08834430575370789
Epoch 8, Batch 200/249, Loss: 0.45379215478897095, Variance: 0.09728255867958069

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6135332313711126, Training Loss Force: 3.2768158453994953, time: 3.8018717765808105
Validation Loss Energy: 1.9204221010217297, Validation Loss Force: 3.307534006683879, time: 0.19159507751464844
Test Loss Energy: 9.05313197452115, Test Loss Force: 11.009032552265714, time: 8.720664024353027

Epoch 9, Batch 100/249, Loss: 0.5156647562980652, Variance: 0.09620414674282074
Epoch 9, Batch 200/249, Loss: 0.532685399055481, Variance: 0.09480969607830048

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6192838471751927, Training Loss Force: 3.27669237938222, time: 4.001104116439819
Validation Loss Energy: 1.3745950049621105, Validation Loss Force: 3.3013817138491808, time: 0.18982434272766113
Test Loss Energy: 8.65927271639915, Test Loss Force: 10.951965481262745, time: 8.74166464805603

Epoch 10, Batch 100/249, Loss: 0.3624679446220398, Variance: 0.09019317477941513
Epoch 10, Batch 200/249, Loss: 0.41832828521728516, Variance: 0.09073518216609955

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.5989759070943228, Training Loss Force: 3.2792297971392306, time: 3.885010242462158
Validation Loss Energy: 1.3985061085284203, Validation Loss Force: 3.3968414797251585, time: 0.1971275806427002
Test Loss Energy: 8.679588170255087, Test Loss Force: 11.027151548702324, time: 8.744526624679565

Epoch 11, Batch 100/249, Loss: 0.36983388662338257, Variance: 0.08980462700128555
Epoch 11, Batch 200/249, Loss: 0.5977939963340759, Variance: 0.09346219897270203

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6169002902210032, Training Loss Force: 3.26894386779619, time: 3.797628402709961
Validation Loss Energy: 2.19584671684381, Validation Loss Force: 3.2771584281402806, time: 0.19122314453125
Test Loss Energy: 9.11817621547507, Test Loss Force: 10.837955479054369, time: 8.865556240081787

Epoch 12, Batch 100/249, Loss: 0.4611615538597107, Variance: 0.09545505791902542
Epoch 12, Batch 200/249, Loss: 0.4869194030761719, Variance: 0.09180847555398941

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6109167092821937, Training Loss Force: 3.2870434488401066, time: 3.788902997970581
Validation Loss Energy: 2.014371362912663, Validation Loss Force: 3.373493005423322, time: 0.19196462631225586
Test Loss Energy: 9.30085588539133, Test Loss Force: 11.071419252118478, time: 8.711346626281738

Epoch 13, Batch 100/249, Loss: 0.5959069728851318, Variance: 0.09219177067279816
Epoch 13, Batch 200/249, Loss: 0.6929569840431213, Variance: 0.09194981306791306

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.589447021995688, Training Loss Force: 3.2814697083494493, time: 3.7353789806365967
Validation Loss Energy: 1.4745910423462156, Validation Loss Force: 3.26820117207433, time: 0.1964554786682129
Test Loss Energy: 8.609390651790484, Test Loss Force: 10.800347634921856, time: 8.938567399978638

Epoch 14, Batch 100/249, Loss: 0.2836988568305969, Variance: 0.09276224672794342
Epoch 14, Batch 200/249, Loss: 0.33109283447265625, Variance: 0.09006122499704361

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6199838279796863, Training Loss Force: 3.276431378055809, time: 3.8123652935028076
Validation Loss Energy: 1.5316908986121323, Validation Loss Force: 3.2491923524247666, time: 0.1962268352508545
Test Loss Energy: 8.733996513365328, Test Loss Force: 10.943874733726071, time: 8.740443468093872

Epoch 15, Batch 100/249, Loss: 0.3811389207839966, Variance: 0.09082627296447754
Epoch 15, Batch 200/249, Loss: 0.5643742084503174, Variance: 0.08901655673980713

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5877870984065352, Training Loss Force: 3.2601568425158645, time: 3.8561935424804688
Validation Loss Energy: 1.8395520907588758, Validation Loss Force: 3.2735417802048348, time: 0.19577932357788086
Test Loss Energy: 9.039633742915639, Test Loss Force: 11.128816586931336, time: 8.728975772857666

Epoch 16, Batch 100/249, Loss: 0.33314573764801025, Variance: 0.0898895114660263
Epoch 16, Batch 200/249, Loss: 0.32541149854660034, Variance: 0.09529861807823181

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6089320274385368, Training Loss Force: 3.2728454560033042, time: 3.830500364303589
Validation Loss Energy: 1.722347597270963, Validation Loss Force: 3.2821117589587305, time: 0.1986551284790039
Test Loss Energy: 9.260911516786052, Test Loss Force: 11.131498101688607, time: 8.868504762649536

Epoch 17, Batch 100/249, Loss: 0.6203960180282593, Variance: 0.09213480353355408
Epoch 17, Batch 200/249, Loss: 0.4500349164009094, Variance: 0.09092968702316284

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6230445484106863, Training Loss Force: 3.264685352573451, time: 3.8803579807281494
Validation Loss Energy: 1.3307755262303327, Validation Loss Force: 3.253718436819468, time: 0.19385886192321777
Test Loss Energy: 8.574416584948287, Test Loss Force: 10.930473888978016, time: 8.733104228973389

Epoch 18, Batch 100/249, Loss: 0.7501423358917236, Variance: 0.09271343797445297
Epoch 18, Batch 200/249, Loss: 0.36685335636138916, Variance: 0.09642226994037628

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6103859402613505, Training Loss Force: 3.2832979123571295, time: 3.8456108570098877
Validation Loss Energy: 1.4378070736164585, Validation Loss Force: 3.331852650454523, time: 0.1916522979736328
Test Loss Energy: 8.620376608847703, Test Loss Force: 10.88723820459192, time: 8.8995361328125

Epoch 19, Batch 100/249, Loss: 0.3009987473487854, Variance: 0.08688311278820038
Epoch 19, Batch 200/249, Loss: 0.5411284565925598, Variance: 0.09332606196403503

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5981798018527527, Training Loss Force: 3.266794188562527, time: 3.7617790699005127
Validation Loss Energy: 2.0096310707995793, Validation Loss Force: 3.2558718144624486, time: 0.19103336334228516
Test Loss Energy: 8.8781479228276, Test Loss Force: 10.760800783203377, time: 8.738826990127563

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ƒâ–â–‡â–„â–‚â–‚â–†â–†â–‚â–‚â–†â–ˆâ–â–ƒâ–…â–ˆâ–â–â–„
wandb:   test_error_force â–â–â–†â–ƒâ–„â–„â–‡â–„â–†â–…â–†â–„â–‡â–ƒâ–…â–ˆâ–ˆâ–…â–„â–ƒ
wandb:          test_loss â–â–ƒâ–ƒâ–…â–ƒâ–„â–„â–†â–…â–…â–…â–†â–ˆâ–†â–…â–‡â–‡â–„â–†â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–‚â–â–â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–‚â–„â–†â–…â–â–â–„â–†â–â–‚â–ˆâ–‡â–‚â–ƒâ–…â–„â–â–‚â–†
wandb:  valid_error_force â–‚â–ƒâ–„â–„â–ƒâ–ƒâ–…â–„â–„â–ƒâ–ˆâ–‚â–‡â–‚â–â–‚â–ƒâ–â–…â–
wandb:         valid_loss â–ƒâ–‚â–…â–…â–…â–‚â–‚â–„â–…â–‚â–‚â–ˆâ–†â–‚â–ƒâ–…â–ƒâ–â–‚â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 7959
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.87815
wandb:   test_error_force 10.7608
wandb:          test_loss 11.34196
wandb: train_error_energy 1.59818
wandb:  train_error_force 3.26679
wandb:         train_loss 0.45654
wandb: valid_error_energy 2.00963
wandb:  valid_error_force 3.25587
wandb:         valid_loss 0.66873
wandb: 
wandb: ğŸš€ View run al_63_84 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8b7lkwgi
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_192944-8b7lkwgi/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.16877818107605, Uncertainty Bias: -0.07281798124313354
1.5258789e-05 0.0010757446
1.9516294 5.833802
(48745, 22, 3)
Found uncertainty sample 0 after 343 steps.
Found uncertainty sample 1 after 1529 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 105 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 3048 steps.
Found uncertainty sample 6 after 2943 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1117 steps.
Found uncertainty sample 9 after 788 steps.
Found uncertainty sample 10 after 1090 steps.
Found uncertainty sample 11 after 374 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 55 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1292 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 1228 steps.
Found uncertainty sample 18 after 686 steps.
Found uncertainty sample 19 after 2820 steps.
Found uncertainty sample 20 after 2773 steps.
Found uncertainty sample 21 after 2073 steps.
Found uncertainty sample 22 after 1462 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 2856 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 1854 steps.
Found uncertainty sample 28 after 579 steps.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 1654 steps.
Found uncertainty sample 32 after 1469 steps.
Found uncertainty sample 33 after 985 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 41 steps.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 672 steps.
Found uncertainty sample 39 after 623 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 1484 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1349 steps.
Found uncertainty sample 46 after 1548 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 271 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 524 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 2042 steps.
Found uncertainty sample 54 after 1459 steps.
Found uncertainty sample 55 after 2037 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 36 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 116 steps.
Found uncertainty sample 61 after 887 steps.
Found uncertainty sample 62 after 1443 steps.
Found uncertainty sample 63 after 2548 steps.
Found uncertainty sample 64 after 3869 steps.
Found uncertainty sample 65 after 1290 steps.
Found uncertainty sample 66 after 1455 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 366 steps.
Found uncertainty sample 69 after 1049 steps.
Found uncertainty sample 70 after 2356 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 3 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 2866 steps.
Found uncertainty sample 75 after 759 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 1599 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 722 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 2338 steps.
Found uncertainty sample 82 after 2449 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 8 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 1972 steps.
Found uncertainty sample 87 after 540 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 1128 steps.
Found uncertainty sample 90 after 2048 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 730 steps.
Found uncertainty sample 93 after 379 steps.
Found uncertainty sample 94 after 589 steps.
Found uncertainty sample 95 after 2646 steps.
Found uncertainty sample 96 after 1163 steps.
Found uncertainty sample 97 after 2675 steps.
Found uncertainty sample 98 after 1032 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_195614-snelli11
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_85
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/snelli11
Training model 85. Added 68 samples to the dataset.
Epoch 0, Batch 100/251, Loss: 1.5577760934829712, Variance: 0.12639489769935608
Epoch 0, Batch 200/251, Loss: 0.5841412544250488, Variance: 0.12767526507377625

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.893918554591812, Training Loss Force: 3.3727566749299895, time: 3.7952327728271484
Validation Loss Energy: 2.20566850813574, Validation Loss Force: 3.159293703934506, time: 0.1939992904663086
Test Loss Energy: 8.99499885691204, Test Loss Force: 10.933671198741816, time: 8.471203804016113

Epoch 1, Batch 100/251, Loss: 0.7457753419876099, Variance: 0.13266712427139282
Epoch 1, Batch 200/251, Loss: 1.1475237607955933, Variance: 0.12549099326133728

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.616032129207191, Training Loss Force: 3.26569187066494, time: 3.890089273452759
Validation Loss Energy: 2.3822185096266226, Validation Loss Force: 2.995076812030613, time: 0.19022393226623535
Test Loss Energy: 9.314233395345537, Test Loss Force: 10.694826657243148, time: 8.506466388702393

Epoch 2, Batch 100/251, Loss: 0.716164767742157, Variance: 0.13133151829242706
Epoch 2, Batch 200/251, Loss: 0.9349566698074341, Variance: 0.12784594297409058

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6261205766928475, Training Loss Force: 3.269734080367927, time: 4.093786954879761
Validation Loss Energy: 3.8071101715725755, Validation Loss Force: 3.2998213152752625, time: 0.1898179054260254
Test Loss Energy: 10.061270277281444, Test Loss Force: 10.68732430184942, time: 8.462523460388184

Epoch 3, Batch 100/251, Loss: 1.1245744228363037, Variance: 0.12439679354429245
Epoch 3, Batch 200/251, Loss: 0.7743008136749268, Variance: 0.1348573863506317

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6182433788716932, Training Loss Force: 3.2604316633951123, time: 3.8346199989318848
Validation Loss Energy: 2.4751030734295734, Validation Loss Force: 3.3342641350380062, time: 0.1937999725341797
Test Loss Energy: 9.723651875713545, Test Loss Force: 10.663115987434614, time: 8.489407539367676

Epoch 4, Batch 100/251, Loss: 0.8380963802337646, Variance: 0.1270582675933838
Epoch 4, Batch 200/251, Loss: 1.2265796661376953, Variance: 0.13207198679447174

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.637612801888242, Training Loss Force: 3.2714833824852567, time: 3.891342878341675
Validation Loss Energy: 1.671766769923088, Validation Loss Force: 3.2186829051906356, time: 0.20173907279968262
Test Loss Energy: 8.617271833852435, Test Loss Force: 10.639312683714305, time: 8.702736616134644

Epoch 5, Batch 100/251, Loss: 0.7113604545593262, Variance: 0.1344430148601532
Epoch 5, Batch 200/251, Loss: 0.8887379169464111, Variance: 0.13699737191200256

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.610458888595388, Training Loss Force: 3.2622224369755255, time: 3.9337754249572754
Validation Loss Energy: 3.119533233230203, Validation Loss Force: 3.297413103928224, time: 0.19758319854736328
Test Loss Energy: 9.057588281283778, Test Loss Force: 10.690921348196635, time: 8.463573217391968

Epoch 6, Batch 100/251, Loss: 1.2155542373657227, Variance: 0.13267363607883453
Epoch 6, Batch 200/251, Loss: 0.45993828773498535, Variance: 0.12892910838127136

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.607148347971282, Training Loss Force: 3.2610758168461835, time: 3.766732931137085
Validation Loss Energy: 2.0637990479877932, Validation Loss Force: 3.225527175796677, time: 0.19132399559020996
Test Loss Energy: 8.805290914032456, Test Loss Force: 10.675843317472399, time: 8.539802074432373

Epoch 7, Batch 100/251, Loss: 0.9608492851257324, Variance: 0.13407191634178162
Epoch 7, Batch 200/251, Loss: 1.1654317378997803, Variance: 0.13206186890602112

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6285725893674226, Training Loss Force: 3.2512545293099286, time: 4.012977361679077
Validation Loss Energy: 2.416073794580921, Validation Loss Force: 3.246857588495512, time: 0.20832490921020508
Test Loss Energy: 9.205256671796906, Test Loss Force: 10.614628771319936, time: 8.499115943908691

Epoch 8, Batch 100/251, Loss: 0.7812788486480713, Variance: 0.13033360242843628
Epoch 8, Batch 200/251, Loss: 0.6658324599266052, Variance: 0.12656758725643158

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6060002281519346, Training Loss Force: 3.2718167173095827, time: 3.811833381652832
Validation Loss Energy: 4.04791692184957, Validation Loss Force: 3.2795963658183, time: 0.19526457786560059
Test Loss Energy: 9.74716339658297, Test Loss Force: 10.503709738619785, time: 8.596760272979736

Epoch 9, Batch 100/251, Loss: 1.2872170209884644, Variance: 0.1296864002943039
Epoch 9, Batch 200/251, Loss: 0.8202173113822937, Variance: 0.13441163301467896

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6023128142320138, Training Loss Force: 3.259484429735464, time: 3.8133039474487305
Validation Loss Energy: 2.3034096641466926, Validation Loss Force: 3.0979500910507385, time: 0.20441460609436035
Test Loss Energy: 9.274132873881191, Test Loss Force: 10.652751925483253, time: 8.735510349273682

Epoch 10, Batch 100/251, Loss: 0.6652034521102905, Variance: 0.12721478939056396
Epoch 10, Batch 200/251, Loss: 1.4145654439926147, Variance: 0.1337573230266571

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6160563127619816, Training Loss Force: 3.258923605998398, time: 3.9583687782287598
Validation Loss Energy: 1.8335280219518597, Validation Loss Force: 3.322657416738862, time: 0.19423317909240723
Test Loss Energy: 8.770602968882129, Test Loss Force: 10.739943147464208, time: 8.538456201553345

Epoch 11, Batch 100/251, Loss: 0.6366547346115112, Variance: 0.12958505749702454
Epoch 11, Batch 200/251, Loss: 0.8780094981193542, Variance: 0.14274448156356812

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5957462789404016, Training Loss Force: 3.2652071571936343, time: 3.8451149463653564
Validation Loss Energy: 3.476550209157984, Validation Loss Force: 3.234972132959496, time: 0.19090867042541504
Test Loss Energy: 9.454659237560147, Test Loss Force: 10.777746646661765, time: 8.76155686378479

Epoch 12, Batch 100/251, Loss: 1.3923311233520508, Variance: 0.13379406929016113
Epoch 12, Batch 200/251, Loss: 0.5746557712554932, Variance: 0.13218602538108826

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5864352271065036, Training Loss Force: 3.2643187498835298, time: 3.8386683464050293
Validation Loss Energy: 2.1144544973331976, Validation Loss Force: 3.3670905104028015, time: 0.19080185890197754
Test Loss Energy: 8.966559241157855, Test Loss Force: 10.908242428909144, time: 8.551857471466064

Epoch 13, Batch 100/251, Loss: 0.8381731510162354, Variance: 0.13761678338050842
Epoch 13, Batch 200/251, Loss: 1.3063087463378906, Variance: 0.13289567828178406

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6157263503550197, Training Loss Force: 3.2574377055123955, time: 3.763430595397949
Validation Loss Energy: 1.8423636672637624, Validation Loss Force: 3.1790977641718356, time: 0.2005171775817871
Test Loss Energy: 8.982016522904082, Test Loss Force: 10.49526175516139, time: 9.715865850448608

Epoch 14, Batch 100/251, Loss: 0.5449667572975159, Variance: 0.13015274703502655
Epoch 14, Batch 200/251, Loss: 0.7528707981109619, Variance: 0.13357874751091003

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5872127008255323, Training Loss Force: 3.2511311457634986, time: 4.04724907875061
Validation Loss Energy: 3.378310853199932, Validation Loss Force: 3.307682872141389, time: 0.19267678260803223
Test Loss Energy: 10.019423106393843, Test Loss Force: 10.479650670823101, time: 8.53960919380188

Epoch 15, Batch 100/251, Loss: 1.0498111248016357, Variance: 0.12687347829341888
Epoch 15, Batch 200/251, Loss: 0.7368140816688538, Variance: 0.13530606031417847

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5993680693354952, Training Loss Force: 3.2540618558503636, time: 3.789700508117676
Validation Loss Energy: 2.387304673724076, Validation Loss Force: 3.4648863984898233, time: 0.19277119636535645
Test Loss Energy: 9.35624244553642, Test Loss Force: 10.515124761634102, time: 8.457414150238037

Epoch 16, Batch 100/251, Loss: 0.9275559782981873, Variance: 0.13536065816879272
Epoch 16, Batch 200/251, Loss: 1.233056664466858, Variance: 0.13663311302661896

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.607432792280463, Training Loss Force: 3.2634576057933162, time: 3.8513314723968506
Validation Loss Energy: 1.6750410083793832, Validation Loss Force: 3.1868158330403267, time: 0.20041680335998535
Test Loss Energy: 8.838385347841575, Test Loss Force: 10.601297404008637, time: 8.655672788619995

Epoch 17, Batch 100/251, Loss: 0.7081378698348999, Variance: 0.13905951380729675
Epoch 17, Batch 200/251, Loss: 0.8147954940795898, Variance: 0.13886435329914093

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6006570735203005, Training Loss Force: 3.253825753811178, time: 3.8842275142669678
Validation Loss Energy: 3.1325899284737995, Validation Loss Force: 3.289568826187822, time: 0.19458580017089844
Test Loss Energy: 9.061573569356927, Test Loss Force: 10.61424540266746, time: 8.575343132019043

Epoch 18, Batch 100/251, Loss: 1.2728686332702637, Variance: 0.13567277789115906
Epoch 18, Batch 200/251, Loss: 0.8198202848434448, Variance: 0.13838693499565125

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.627555010443786, Training Loss Force: 3.2438197857272697, time: 3.8742446899414062
Validation Loss Energy: 2.0714520083980146, Validation Loss Force: 2.9177498519837473, time: 0.18929624557495117
Test Loss Energy: 8.777938688479821, Test Loss Force: 10.684288409729314, time: 8.721971988677979

Epoch 19, Batch 100/251, Loss: 0.8074489235877991, Variance: 0.13630029559135437
Epoch 19, Batch 200/251, Loss: 1.1214498281478882, Variance: 0.12939339876174927

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5974500691698474, Training Loss Force: 3.2565641371935414, time: 3.7868564128875732
Validation Loss Energy: 2.0748319033640525, Validation Loss Force: 3.460660984242869, time: 0.1975879669189453
Test Loss Energy: 9.335271693871123, Test Loss Force: 10.604435644660992, time: 8.519185304641724

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–„â–ˆâ–†â–â–ƒâ–‚â–„â–†â–„â–‚â–…â–ƒâ–ƒâ–ˆâ–…â–‚â–ƒâ–‚â–„
wandb:   test_error_force â–ˆâ–„â–„â–„â–ƒâ–„â–„â–ƒâ–â–„â–…â–†â–ˆâ–â–â–‚â–ƒâ–ƒâ–„â–ƒ
wandb:          test_loss â–ˆâ–†â–ˆâ–‡â–ƒâ–…â–…â–„â–„â–„â–ƒâ–ˆâ–†â–â–†â–ƒâ–‚â–ƒâ–ƒâ–„
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–‚â–â–â–â–â–‚â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–ƒâ–‡â–ƒâ–â–…â–‚â–ƒâ–ˆâ–ƒâ–â–†â–‚â–‚â–†â–ƒâ–â–…â–‚â–‚
wandb:  valid_error_force â–„â–‚â–†â–†â–…â–†â–…â–…â–†â–ƒâ–†â–…â–‡â–„â–†â–ˆâ–„â–†â–â–ˆ
wandb:         valid_loss â–‚â–‚â–‡â–ƒâ–â–…â–‚â–ƒâ–ˆâ–‚â–‚â–†â–‚â–â–…â–ƒâ–â–…â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 8020
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.33527
wandb:   test_error_force 10.60444
wandb:          test_loss 9.62494
wandb: train_error_energy 2.59745
wandb:  train_error_force 3.25656
wandb:         train_loss 0.89768
wandb: valid_error_energy 2.07483
wandb:  valid_error_force 3.46066
wandb:         valid_loss 0.76036
wandb: 
wandb: ğŸš€ View run al_63_85 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/snelli11
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_195614-snelli11/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.4765586853027344, Uncertainty Bias: -0.11482903361320496
5.8174133e-05 0.010178566
1.9400334 5.6881337
(48745, 22, 3)
Found uncertainty sample 0 after 2691 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 1168 steps.
Found uncertainty sample 4 after 1251 steps.
Found uncertainty sample 5 after 1311 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 1244 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 468 steps.
Found uncertainty sample 10 after 1497 steps.
Found uncertainty sample 11 after 730 steps.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 233 steps.
Found uncertainty sample 16 after 975 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 891 steps.
Found uncertainty sample 19 after 41 steps.
Found uncertainty sample 20 after 3036 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 551 steps.
Found uncertainty sample 23 after 418 steps.
Found uncertainty sample 24 after 3211 steps.
Found uncertainty sample 25 after 1486 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 1039 steps.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 2448 steps.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 0 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 2350 steps.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 1401 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 331 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 19 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 1900 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 1456 steps.
Did not find any uncertainty samples for sample 50.
Found uncertainty sample 51 after 2323 steps.
Found uncertainty sample 52 after 1875 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 158 steps.
Found uncertainty sample 55 after 3708 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 2276 steps.
Found uncertainty sample 59 after 2721 steps.
Did not find any uncertainty samples for sample 60.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 1755 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 59 steps.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 3351 steps.
Found uncertainty sample 69 after 2552 steps.
Found uncertainty sample 70 after 1109 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 3852 steps.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 1337 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 1203 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 575 steps.
Found uncertainty sample 80 after 2166 steps.
Found uncertainty sample 81 after 939 steps.
Found uncertainty sample 82 after 656 steps.
Found uncertainty sample 83 after 3213 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 2611 steps.
Found uncertainty sample 86 after 66 steps.
Found uncertainty sample 87 after 1519 steps.
Found uncertainty sample 88 after 1125 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 1137 steps.
Found uncertainty sample 93 after 1707 steps.
Found uncertainty sample 94 after 1362 steps.
Found uncertainty sample 95 after 692 steps.
Found uncertainty sample 96 after 1627 steps.
Found uncertainty sample 97 after 2480 steps.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_202638-f1kjn5xs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_86
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/f1kjn5xs
Training model 86. Added 56 samples to the dataset.
Epoch 0, Batch 100/253, Loss: 0.21105718612670898, Variance: 0.10116520524024963
Epoch 0, Batch 200/253, Loss: 0.3623178005218506, Variance: 0.09566245973110199

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8727033764042915, Training Loss Force: 3.3373759678604706, time: 3.897178888320923
Validation Loss Energy: 1.8959142608445194, Validation Loss Force: 3.3372354766550956, time: 0.19905352592468262
Test Loss Energy: 9.112760374901324, Test Loss Force: 10.554036658876187, time: 8.484935998916626

Epoch 1, Batch 100/253, Loss: 0.4124128222465515, Variance: 0.09494122862815857
Epoch 1, Batch 200/253, Loss: 0.18307822942733765, Variance: 0.09004040062427521

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6145152443946618, Training Loss Force: 3.2453322993891933, time: 3.7940595149993896
Validation Loss Energy: 1.4677380598098406, Validation Loss Force: 3.2374726629344353, time: 0.20046639442443848
Test Loss Energy: 8.737122543354674, Test Loss Force: 10.85220642097445, time: 8.4951012134552

Epoch 2, Batch 100/253, Loss: 0.7795012593269348, Variance: 0.09765353798866272
Epoch 2, Batch 200/253, Loss: 0.5883917808532715, Variance: 0.09362249076366425

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6065656696275705, Training Loss Force: 3.248548586220001, time: 4.0996482372283936
Validation Loss Energy: 1.3818908652962791, Validation Loss Force: 3.335836669080378, time: 0.19605016708374023
Test Loss Energy: 8.590310811175632, Test Loss Force: 10.986854252998173, time: 8.524659872055054

Epoch 3, Batch 100/253, Loss: 0.62953782081604, Variance: 0.089933842420578
Epoch 3, Batch 200/253, Loss: 0.2801111936569214, Variance: 0.09144384413957596

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6225583448236471, Training Loss Force: 3.2591562582945603, time: 3.885366201400757
Validation Loss Energy: 1.745168629600112, Validation Loss Force: 3.3701904211847498, time: 0.20338654518127441
Test Loss Energy: 8.811654299257658, Test Loss Force: 10.782515837382826, time: 8.492618083953857

Epoch 4, Batch 100/253, Loss: 0.36798954010009766, Variance: 0.09274692833423615
Epoch 4, Batch 200/253, Loss: 0.2865754961967468, Variance: 0.08819900453090668

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5984969290224542, Training Loss Force: 3.253243006288735, time: 3.7934274673461914
Validation Loss Energy: 1.6178262381339816, Validation Loss Force: 3.2400513052457667, time: 0.19540071487426758
Test Loss Energy: 8.979582639553227, Test Loss Force: 10.763114164074649, time: 8.699059247970581

Epoch 5, Batch 100/253, Loss: 0.5060597658157349, Variance: 0.09422076493501663
Epoch 5, Batch 200/253, Loss: 0.6829273700714111, Variance: 0.09355700016021729

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.611160423470651, Training Loss Force: 3.252222888522024, time: 3.7750494480133057
Validation Loss Energy: 1.4996372853848219, Validation Loss Force: 3.2920925582877674, time: 0.19951534271240234
Test Loss Energy: 8.955420411822594, Test Loss Force: 11.1634343085284, time: 8.544296026229858

Epoch 6, Batch 100/253, Loss: 0.271625816822052, Variance: 0.09115047752857208
Epoch 6, Batch 200/253, Loss: 0.3171883225440979, Variance: 0.08840824663639069

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.744601572080009, Training Loss Force: 3.8296794467503674, time: 3.803238868713379
Validation Loss Energy: 1.7602866176613916, Validation Loss Force: 3.2893568006270697, time: 0.1957845687866211
Test Loss Energy: 8.955508374295885, Test Loss Force: 10.778183521194459, time: 8.579453706741333

Epoch 7, Batch 100/253, Loss: 0.7268402576446533, Variance: 0.09152556210756302
Epoch 7, Batch 200/253, Loss: 0.6266708374023438, Variance: 0.09128975868225098

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5945302863349955, Training Loss Force: 3.235981813792081, time: 4.166932582855225
Validation Loss Energy: 1.3768218021892327, Validation Loss Force: 3.3009725592935384, time: 0.1966102123260498
Test Loss Energy: 8.699534352397356, Test Loss Force: 10.763458757984754, time: 8.554896354675293

Epoch 8, Batch 100/253, Loss: 0.4773330092430115, Variance: 0.09229610860347748
Epoch 8, Batch 200/253, Loss: 0.5245603919029236, Variance: 0.09107249975204468

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6087680494446142, Training Loss Force: 3.2532706876720465, time: 3.837170362472534
Validation Loss Energy: 1.4219045998871607, Validation Loss Force: 3.32716700134985, time: 0.21265029907226562
Test Loss Energy: 8.664857932415492, Test Loss Force: 10.860513565615145, time: 8.497330665588379

Epoch 9, Batch 100/253, Loss: 0.344413697719574, Variance: 0.08898891508579254
Epoch 9, Batch 200/253, Loss: 0.18130594491958618, Variance: 0.09153816848993301

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.5991789626361717, Training Loss Force: 3.2385277168740623, time: 3.791098117828369
Validation Loss Energy: 1.653570781723819, Validation Loss Force: 3.2896970836158763, time: 0.19924426078796387
Test Loss Energy: 8.984633451950957, Test Loss Force: 10.938906479120163, time: 8.72333312034607

Epoch 10, Batch 100/253, Loss: 0.40719717741012573, Variance: 0.09194637835025787
Epoch 10, Batch 200/253, Loss: 0.27112919092178345, Variance: 0.08584600687026978

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6020102124881699, Training Loss Force: 3.236505350319572, time: 3.8570449352264404
Validation Loss Energy: 1.8186615386909057, Validation Loss Force: 3.3865540204483495, time: 0.1951429843902588
Test Loss Energy: 9.224325875588264, Test Loss Force: 10.850200272229051, time: 8.555944204330444

Epoch 11, Batch 100/253, Loss: 0.6767908930778503, Variance: 0.09159737080335617
Epoch 11, Batch 200/253, Loss: 0.485088050365448, Variance: 0.088908851146698

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6047460985576758, Training Loss Force: 3.2546853417209642, time: 3.8587019443511963
Validation Loss Energy: 1.6602648174571026, Validation Loss Force: 3.2643278979482027, time: 0.1982114315032959
Test Loss Energy: 8.82953068658461, Test Loss Force: 10.864775121978997, time: 8.675533056259155

Epoch 12, Batch 100/253, Loss: 0.5600451231002808, Variance: 0.09022031724452972
Epoch 12, Batch 200/253, Loss: 0.4190285801887512, Variance: 0.0891951471567154

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6050443533172636, Training Loss Force: 3.237839993975655, time: 3.912001371383667
Validation Loss Energy: 1.26479301362908, Validation Loss Force: 3.3824749888979277, time: 0.1948528289794922
Test Loss Energy: 8.804877488542697, Test Loss Force: 10.85588069038044, time: 8.473038911819458

Epoch 13, Batch 100/253, Loss: 0.5009196400642395, Variance: 0.09120279550552368
Epoch 13, Batch 200/253, Loss: 0.43729013204574585, Variance: 0.08893891423940659

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6141763379940413, Training Loss Force: 3.2602281840425964, time: 3.9033608436584473
Validation Loss Energy: 1.6175811016712298, Validation Loss Force: 3.3029096917954517, time: 0.19768071174621582
Test Loss Energy: 9.110369826930011, Test Loss Force: 10.916127807876464, time: 8.52357530593872

Epoch 14, Batch 100/253, Loss: 0.07811933755874634, Variance: 0.0842701867222786
Epoch 14, Batch 200/253, Loss: 0.3900979161262512, Variance: 0.08926302194595337

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.5880148179230011, Training Loss Force: 3.2508970865032536, time: 4.031900882720947
Validation Loss Energy: 1.6855513038039789, Validation Loss Force: 3.372384692016381, time: 0.19773411750793457
Test Loss Energy: 8.990366098258045, Test Loss Force: 11.03242460122899, time: 8.499998331069946

Epoch 15, Batch 100/253, Loss: 0.5311541557312012, Variance: 0.09160836040973663
Epoch 15, Batch 200/253, Loss: 0.7957115173339844, Variance: 0.09124083817005157

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6097908986488525, Training Loss Force: 3.2503582720559834, time: 3.852879524230957
Validation Loss Energy: 1.3575511124906012, Validation Loss Force: 3.427202273037467, time: 0.1954500675201416
Test Loss Energy: 8.654757387157359, Test Loss Force: 11.066705474537963, time: 8.54747462272644

Epoch 16, Batch 100/253, Loss: 0.5215649604797363, Variance: 0.09348566830158234
Epoch 16, Batch 200/253, Loss: 0.3515542149543762, Variance: 0.09374643862247467

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.5871515712296822, Training Loss Force: 3.27947964705258, time: 3.8208963871002197
Validation Loss Energy: 1.3934821526236105, Validation Loss Force: 3.327850595627848, time: 0.19487333297729492
Test Loss Energy: 8.720488288461812, Test Loss Force: 11.10157468869438, time: 8.696407794952393

Epoch 17, Batch 100/253, Loss: 0.2874729037284851, Variance: 0.08960923552513123
Epoch 17, Batch 200/253, Loss: 0.6941078901290894, Variance: 0.08691062033176422

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.5795469743379278, Training Loss Force: 3.2510232668594328, time: 3.858454942703247
Validation Loss Energy: 1.9706450545182697, Validation Loss Force: 3.1031978238804774, time: 0.19442057609558105
Test Loss Energy: 9.174383825543813, Test Loss Force: 10.945944115845691, time: 8.532454490661621

Epoch 18, Batch 100/253, Loss: 0.18608492612838745, Variance: 0.08926372230052948
Epoch 18, Batch 200/253, Loss: 0.24884343147277832, Variance: 0.09136751294136047

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6159679933499522, Training Loss Force: 3.253962068329231, time: 3.8434386253356934
Validation Loss Energy: 1.505983934983543, Validation Loss Force: 3.356718937856119, time: 0.20395636558532715
Test Loss Energy: 9.071154502756256, Test Loss Force: 10.971005968894929, time: 8.522355794906616

Epoch 19, Batch 100/253, Loss: 0.4771655201911926, Variance: 0.09250333905220032
Epoch 19, Batch 200/253, Loss: 0.5918247103691101, Variance: 0.08906665444374084

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6058170737812938, Training Loss Force: 3.2555324341197918, time: 4.085998773574829
Validation Loss Energy: 1.3463864481134098, Validation Loss Force: 3.28982044232272, time: 0.1957845687866211
Test Loss Energy: 8.675911774417044, Test Loss Force: 10.90661529336109, time: 8.519201755523682

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–ƒâ–â–ƒâ–…â–…â–…â–‚â–‚â–…â–ˆâ–„â–ƒâ–‡â–…â–‚â–‚â–‡â–†â–‚
wandb:   test_error_force â–â–„â–†â–„â–ƒâ–ˆâ–„â–ƒâ–…â–…â–„â–…â–„â–…â–†â–‡â–‡â–†â–†â–…
wandb:          test_loss â–â–‚â–â–‚â–„â–ˆâ–ƒâ–ƒâ–„â–‡â–†â–†â–„â–‡â–†â–†â–‡â–‡â–†â–„
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–â–‚â–…â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚
wandb:  train_error_force â–‚â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–‚â–â–â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–ƒâ–‚â–†â–…â–ƒâ–†â–‚â–ƒâ–…â–†â–…â–â–„â–…â–‚â–‚â–ˆâ–ƒâ–‚
wandb:  valid_error_force â–†â–„â–†â–‡â–„â–…â–…â–…â–†â–…â–‡â–„â–‡â–…â–‡â–ˆâ–†â–â–†â–…
wandb:         valid_loss â–ˆâ–‚â–‚â–†â–„â–„â–…â–‚â–‚â–…â–‡â–…â–‚â–„â–…â–ƒâ–‚â–ˆâ–ƒâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 8070
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.67591
wandb:   test_error_force 10.90662
wandb:          test_loss 11.75299
wandb: train_error_energy 1.60582
wandb:  train_error_force 3.25553
wandb:         train_loss 0.45204
wandb: valid_error_energy 1.34639
wandb:  valid_error_force 3.28982
wandb:         valid_loss 0.32511
wandb: 
wandb: ğŸš€ View run al_63_86 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/f1kjn5xs
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_202638-f1kjn5xs/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2071757316589355, Uncertainty Bias: -0.06144058704376221
5.722046e-05 0.00035858154
1.9617051 5.8631177
(48745, 22, 3)
Found uncertainty sample 0 after 3164 steps.
Found uncertainty sample 1 after 445 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 1256 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 2112 steps.
Found uncertainty sample 6 after 2010 steps.
Found uncertainty sample 7 after 1890 steps.
Found uncertainty sample 8 after 693 steps.
Found uncertainty sample 9 after 1871 steps.
Found uncertainty sample 10 after 34 steps.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 284 steps.
Found uncertainty sample 14 after 3233 steps.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 632 steps.
Found uncertainty sample 18 after 3734 steps.
Found uncertainty sample 19 after 3215 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 3865 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 52 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 975 steps.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 2795 steps.
Found uncertainty sample 29 after 1823 steps.
Found uncertainty sample 30 after 2222 steps.
Found uncertainty sample 31 after 1416 steps.
Found uncertainty sample 32 after 3247 steps.
Found uncertainty sample 33 after 2318 steps.
Found uncertainty sample 34 after 1142 steps.
Found uncertainty sample 35 after 2955 steps.
Found uncertainty sample 36 after 2084 steps.
Found uncertainty sample 37 after 1760 steps.
Found uncertainty sample 38 after 1020 steps.
Found uncertainty sample 39 after 3524 steps.
Found uncertainty sample 40 after 2040 steps.
Found uncertainty sample 41 after 1905 steps.
Found uncertainty sample 42 after 15 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1007 steps.
Found uncertainty sample 45 after 374 steps.
Found uncertainty sample 46 after 2923 steps.
Found uncertainty sample 47 after 1755 steps.
Found uncertainty sample 48 after 1587 steps.
Found uncertainty sample 49 after 1752 steps.
Found uncertainty sample 50 after 1175 steps.
Found uncertainty sample 51 after 3987 steps.
Found uncertainty sample 52 after 3973 steps.
Found uncertainty sample 53 after 2730 steps.
Found uncertainty sample 54 after 3003 steps.
Found uncertainty sample 55 after 1748 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 1130 steps.
Found uncertainty sample 60 after 167 steps.
Found uncertainty sample 61 after 149 steps.
Found uncertainty sample 62 after 438 steps.
Found uncertainty sample 63 after 3833 steps.
Found uncertainty sample 64 after 1227 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 1469 steps.
Found uncertainty sample 67 after 825 steps.
Found uncertainty sample 68 after 2835 steps.
Found uncertainty sample 69 after 3795 steps.
Found uncertainty sample 70 after 1362 steps.
Did not find any uncertainty samples for sample 71.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 578 steps.
Found uncertainty sample 76 after 15 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 1 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 2694 steps.
Found uncertainty sample 81 after 21 steps.
Found uncertainty sample 82 after 734 steps.
Found uncertainty sample 83 after 679 steps.
Found uncertainty sample 84 after 874 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 972 steps.
Found uncertainty sample 87 after 3501 steps.
Found uncertainty sample 88 after 348 steps.
Found uncertainty sample 89 after 1474 steps.
Found uncertainty sample 90 after 677 steps.
Found uncertainty sample 91 after 1887 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 3306 steps.
Found uncertainty sample 94 after 2330 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 332 steps.
Found uncertainty sample 97 after 1623 steps.
Found uncertainty sample 98 after 463 steps.
Found uncertainty sample 99 after 3730 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_205425-ikgcgvr0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_87
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ikgcgvr0
Training model 87. Added 75 samples to the dataset.
Epoch 0, Batch 100/255, Loss: 0.6888547539710999, Variance: 0.09626221656799316
Epoch 0, Batch 200/255, Loss: 0.21533727645874023, Variance: 0.09408138692378998

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.962701049815098, Training Loss Force: 3.782328086726815, time: 3.918133020401001
Validation Loss Energy: 1.6144881604335488, Validation Loss Force: 3.2855030657184967, time: 0.1969444751739502
Test Loss Energy: 8.845732951676304, Test Loss Force: 10.734798315059773, time: 8.55909252166748

Epoch 1, Batch 100/255, Loss: 0.7322956919670105, Variance: 0.08772565424442291
Epoch 1, Batch 200/255, Loss: 0.7331544160842896, Variance: 0.08996336907148361

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5546499533937481, Training Loss Force: 3.2191759461992477, time: 3.8651111125946045
Validation Loss Energy: 1.613847630368373, Validation Loss Force: 3.308400668400202, time: 0.2032461166381836
Test Loss Energy: 8.837220112040287, Test Loss Force: 10.959433300788518, time: 8.536969423294067

Epoch 2, Batch 100/255, Loss: 0.43645018339157104, Variance: 0.08967437595129013
Epoch 2, Batch 200/255, Loss: 0.7945680618286133, Variance: 0.08561588823795319

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5753218373747184, Training Loss Force: 3.234021469307871, time: 4.094319581985474
Validation Loss Energy: 1.573945230350455, Validation Loss Force: 3.264618347582151, time: 0.19421696662902832
Test Loss Energy: 8.903932818733681, Test Loss Force: 10.970810784815933, time: 8.574331045150757

Epoch 3, Batch 100/255, Loss: 0.24209272861480713, Variance: 0.0820566862821579
Epoch 3, Batch 200/255, Loss: 0.28812509775161743, Variance: 0.08587763458490372

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.589755377156334, Training Loss Force: 3.2399699751219133, time: 3.901559829711914
Validation Loss Energy: 1.5531499361431131, Validation Loss Force: 3.3701645024961637, time: 0.1940603256225586
Test Loss Energy: 8.612187819270014, Test Loss Force: 10.94124323209111, time: 8.611351013183594

Epoch 4, Batch 100/255, Loss: 0.5214245319366455, Variance: 0.0922645851969719
Epoch 4, Batch 200/255, Loss: 0.37448811531066895, Variance: 0.0909988060593605

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6083096945871715, Training Loss Force: 3.2529921941960227, time: 3.9633681774139404
Validation Loss Energy: 1.9934122176142899, Validation Loss Force: 3.3205284325008173, time: 0.19481992721557617
Test Loss Energy: 9.0362047453226, Test Loss Force: 10.899815192152953, time: 9.943419694900513

Epoch 5, Batch 100/255, Loss: 0.534271240234375, Variance: 0.09085065126419067
Epoch 5, Batch 200/255, Loss: 0.28146660327911377, Variance: 0.08868859708309174

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5812672271305361, Training Loss Force: 3.235436322159282, time: 3.902529716491699
Validation Loss Energy: 1.6840221457112343, Validation Loss Force: 3.3384309872995823, time: 0.19109416007995605
Test Loss Energy: 9.151439814881517, Test Loss Force: 11.0680735957396, time: 8.568477392196655

Epoch 6, Batch 100/255, Loss: 0.35425400733947754, Variance: 0.09167227894067764
Epoch 6, Batch 200/255, Loss: 0.21731668710708618, Variance: 0.08884069323539734

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6008516714088743, Training Loss Force: 3.2521731227703676, time: 3.9367716312408447
Validation Loss Energy: 1.4316104584882634, Validation Loss Force: 3.329332547861656, time: 0.19667959213256836
Test Loss Energy: 8.866662762529735, Test Loss Force: 11.059057971503691, time: 8.727196455001831

Epoch 7, Batch 100/255, Loss: 0.3702089190483093, Variance: 0.08709032833576202
Epoch 7, Batch 200/255, Loss: 0.16052097082138062, Variance: 0.08564956486225128

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5967165927246036, Training Loss Force: 3.238863493714456, time: 3.8835339546203613
Validation Loss Energy: 1.6534221946027419, Validation Loss Force: 3.221703946561097, time: 0.19313478469848633
Test Loss Energy: 8.695557670311826, Test Loss Force: 11.036489938277196, time: 8.60569167137146

Epoch 8, Batch 100/255, Loss: 0.32515251636505127, Variance: 0.09228421002626419
Epoch 8, Batch 200/255, Loss: 0.43299418687820435, Variance: 0.09168139100074768

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.604410303989864, Training Loss Force: 3.248308914553026, time: 3.81729793548584
Validation Loss Energy: 1.9061498048099552, Validation Loss Force: 3.4128274209838074, time: 0.19629645347595215
Test Loss Energy: 8.909320941079086, Test Loss Force: 11.07985626300095, time: 8.594249248504639

Epoch 9, Batch 100/255, Loss: 0.5358687043190002, Variance: 0.08879663050174713
Epoch 9, Batch 200/255, Loss: 0.574002206325531, Variance: 0.09077805280685425

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.5998295759318428, Training Loss Force: 3.247525127891483, time: 4.037571430206299
Validation Loss Energy: 1.7778004589321104, Validation Loss Force: 3.292554062359617, time: 0.19781875610351562
Test Loss Energy: 9.295380994359046, Test Loss Force: 11.168320840060328, time: 8.528363227844238

Epoch 10, Batch 100/255, Loss: 0.4290645718574524, Variance: 0.08937502652406693
Epoch 10, Batch 200/255, Loss: 0.1619744896888733, Variance: 0.09011225402355194

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.597901957951553, Training Loss Force: 3.247055560576914, time: 3.7971138954162598
Validation Loss Energy: 1.4320660395608316, Validation Loss Force: 3.283918863812879, time: 0.19071626663208008
Test Loss Energy: 8.523031263448356, Test Loss Force: 10.866315661379618, time: 8.539090633392334

Epoch 11, Batch 100/255, Loss: 0.36838406324386597, Variance: 0.08852751553058624
Epoch 11, Batch 200/255, Loss: 0.31274616718292236, Variance: 0.08692236244678497

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6078774710714048, Training Loss Force: 3.2486436666148895, time: 3.904621124267578
Validation Loss Energy: 1.389580254378774, Validation Loss Force: 3.2807576299481425, time: 0.19873595237731934
Test Loss Energy: 8.78059191376329, Test Loss Force: 11.147506488405192, time: 8.743652105331421

Epoch 12, Batch 100/255, Loss: 0.7104752659797668, Variance: 0.08933660387992859
Epoch 12, Batch 200/255, Loss: 0.35005688667297363, Variance: 0.09031270444393158

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6198544628370695, Training Loss Force: 3.251412213398841, time: 3.8617000579833984
Validation Loss Energy: 1.640884118066925, Validation Loss Force: 3.2929226243299565, time: 0.21019339561462402
Test Loss Energy: 8.99569460123108, Test Loss Force: 10.892560692000593, time: 8.593631029129028

Epoch 13, Batch 100/255, Loss: 0.5766912698745728, Variance: 0.0885239988565445
Epoch 13, Batch 200/255, Loss: 0.298700749874115, Variance: 0.09012618660926819

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.5997727306819185, Training Loss Force: 3.240473954861471, time: 3.8501222133636475
Validation Loss Energy: 1.9182437900011329, Validation Loss Force: 3.239505320499952, time: 0.2011110782623291
Test Loss Energy: 9.116491857453939, Test Loss Force: 11.071061871636687, time: 8.573026895523071

Epoch 14, Batch 100/255, Loss: 0.1773495078086853, Variance: 0.08777789771556854
Epoch 14, Batch 200/255, Loss: 0.4182361960411072, Variance: 0.092399463057518

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6185479263001805, Training Loss Force: 3.245975940054153, time: 4.030093431472778
Validation Loss Energy: 1.3501904164678673, Validation Loss Force: 3.2901621503940612, time: 0.1988811492919922
Test Loss Energy: 8.767379048406681, Test Loss Force: 10.988062369514255, time: 8.567583084106445

Epoch 15, Batch 100/255, Loss: 0.09495991468429565, Variance: 0.085723876953125
Epoch 15, Batch 200/255, Loss: 0.34387725591659546, Variance: 0.08997820317745209

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5822470010618377, Training Loss Force: 3.2614765750007675, time: 3.9250283241271973
Validation Loss Energy: 1.3198093491298217, Validation Loss Force: 3.282480510197844, time: 0.19719219207763672
Test Loss Energy: 8.655768291090185, Test Loss Force: 10.980807668843084, time: 8.584945678710938

Epoch 16, Batch 100/255, Loss: 0.5364278554916382, Variance: 0.08562065660953522
Epoch 16, Batch 200/255, Loss: 0.49884915351867676, Variance: 0.09022630006074905

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.590832119789535, Training Loss Force: 3.2462251094480834, time: 3.8524231910705566
Validation Loss Energy: 1.76184701623531, Validation Loss Force: 3.2312230525495096, time: 0.1952207088470459
Test Loss Energy: 9.134547858340971, Test Loss Force: 11.081198223492454, time: 8.670293807983398

Epoch 17, Batch 100/255, Loss: 0.5211269855499268, Variance: 0.08901970833539963
Epoch 17, Batch 200/255, Loss: 0.392997682094574, Variance: 0.08505697548389435

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6036406965502492, Training Loss Force: 3.2406543125036107, time: 3.7699930667877197
Validation Loss Energy: 1.939345076148809, Validation Loss Force: 3.2753297577793488, time: 0.19504523277282715
Test Loss Energy: 9.27753661543037, Test Loss Force: 11.057405491310442, time: 8.46877646446228

Epoch 18, Batch 100/255, Loss: 0.3486524820327759, Variance: 0.08543768525123596
Epoch 18, Batch 200/255, Loss: 0.20485424995422363, Variance: 0.08925211429595947

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.5867195099502103, Training Loss Force: 3.2497818264195084, time: 3.8972551822662354
Validation Loss Energy: 1.3425018067273828, Validation Loss Force: 3.3355612718806937, time: 0.19536495208740234
Test Loss Energy: 8.759261051296097, Test Loss Force: 11.236358621695963, time: 8.698892831802368

Epoch 19, Batch 100/255, Loss: 0.2825484275817871, Variance: 0.08744402229785919
Epoch 19, Batch 200/255, Loss: 0.17638903856277466, Variance: 0.0875203013420105

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.595300535713381, Training Loss Force: 3.263975649330742, time: 3.8838160037994385
Validation Loss Energy: 1.3521183082002246, Validation Loss Force: 3.3417334379366666, time: 0.20226430892944336
Test Loss Energy: 8.958684172588057, Test Loss Force: 11.189208180563046, time: 8.60393738746643

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–„â–„â–‚â–†â–‡â–„â–ƒâ–…â–ˆâ–â–ƒâ–…â–†â–ƒâ–‚â–‡â–ˆâ–ƒâ–…
wandb:   test_error_force â–â–„â–„â–„â–ƒâ–†â–†â–…â–†â–‡â–ƒâ–‡â–ƒâ–†â–…â–„â–†â–†â–ˆâ–‡
wandb:          test_loss â–â–„â–…â–„â–„â–‡â–†â–†â–…â–ˆâ–„â–‡â–…â–†â–„â–„â–†â–ˆâ–‡â–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–„â–„â–ƒâ–ˆâ–…â–‚â–„â–‡â–†â–‚â–‚â–„â–‡â–â–â–†â–‡â–â–
wandb:  valid_error_force â–ƒâ–„â–ƒâ–†â–…â–…â–…â–â–ˆâ–„â–ƒâ–ƒâ–„â–‚â–„â–ƒâ–â–ƒâ–…â–…
wandb:         valid_loss â–ƒâ–ƒâ–„â–…â–ˆâ–„â–‚â–„â–ˆâ–…â–‚â–‚â–ƒâ–†â–â–â–„â–‡â–â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 8137
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.95868
wandb:   test_error_force 11.18921
wandb:          test_loss 12.71878
wandb: train_error_energy 1.5953
wandb:  train_error_force 3.26398
wandb:         train_loss 0.44781
wandb: valid_error_energy 1.35212
wandb:  valid_error_force 3.34173
wandb:         valid_loss 0.43821
wandb: 
wandb: ğŸš€ View run al_63_87 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ikgcgvr0
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_205425-ikgcgvr0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.791480302810669, Uncertainty Bias: -0.032959744334220886
7.6293945e-06 0.004119873
2.0667975 5.8961086
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 12 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 3979 steps.
Found uncertainty sample 4 after 392 steps.
Found uncertainty sample 5 after 173 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 1897 steps.
Found uncertainty sample 8 after 3064 steps.
Found uncertainty sample 9 after 2122 steps.
Found uncertainty sample 10 after 3898 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 1151 steps.
Found uncertainty sample 13 after 1235 steps.
Found uncertainty sample 14 after 751 steps.
Found uncertainty sample 15 after 546 steps.
Found uncertainty sample 16 after 1274 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 2242 steps.
Found uncertainty sample 19 after 1348 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 3863 steps.
Found uncertainty sample 22 after 3612 steps.
Found uncertainty sample 23 after 1799 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 2033 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 424 steps.
Found uncertainty sample 28 after 137 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1047 steps.
Did not find any uncertainty samples for sample 31.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 1502 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 713 steps.
Found uncertainty sample 36 after 1980 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 853 steps.
Found uncertainty sample 39 after 408 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 1087 steps.
Found uncertainty sample 42 after 10 steps.
Found uncertainty sample 43 after 2705 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 313 steps.
Found uncertainty sample 47 after 86 steps.
Found uncertainty sample 48 after 372 steps.
Found uncertainty sample 49 after 763 steps.
Found uncertainty sample 50 after 1584 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 37 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 3825 steps.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 803 steps.
Found uncertainty sample 59 after 2023 steps.
Found uncertainty sample 60 after 2465 steps.
Found uncertainty sample 61 after 1690 steps.
Found uncertainty sample 62 after 3010 steps.
Found uncertainty sample 63 after 2148 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 103 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 254 steps.
Found uncertainty sample 68 after 2557 steps.
Found uncertainty sample 69 after 1285 steps.
Did not find any uncertainty samples for sample 70.
Did not find any uncertainty samples for sample 71.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 595 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 619 steps.
Found uncertainty sample 77 after 1201 steps.
Found uncertainty sample 78 after 792 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 244 steps.
Found uncertainty sample 81 after 924 steps.
Found uncertainty sample 82 after 2121 steps.
Found uncertainty sample 83 after 2150 steps.
Found uncertainty sample 84 after 2555 steps.
Found uncertainty sample 85 after 447 steps.
Found uncertainty sample 86 after 2686 steps.
Found uncertainty sample 87 after 1051 steps.
Found uncertainty sample 88 after 691 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 259 steps.
Found uncertainty sample 91 after 2229 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 652 steps.
Found uncertainty sample 94 after 1180 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 505 steps.
Found uncertainty sample 97 after 2829 steps.
Found uncertainty sample 98 after 3398 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_212028-tp4sauwg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_88
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/tp4sauwg
Training model 88. Added 72 samples to the dataset.
Epoch 0, Batch 100/257, Loss: 2.0330348014831543, Variance: 0.09144531190395355
Epoch 0, Batch 200/257, Loss: 1.1909641027450562, Variance: 0.1364869475364685

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.150361685275135, Training Loss Force: 3.943663119077499, time: 3.904536485671997
Validation Loss Energy: 5.64404561039691, Validation Loss Force: 3.342908106749094, time: 0.2040703296661377
Test Loss Energy: 10.826598718413019, Test Loss Force: 10.720977089617122, time: 8.541303157806396

Epoch 1, Batch 100/257, Loss: 1.9078288078308105, Variance: 0.15881845355033875
Epoch 1, Batch 200/257, Loss: 1.9296987056732178, Variance: 0.1758735626935959

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.115794858898982, Training Loss Force: 3.282875698892669, time: 3.9317514896392822
Validation Loss Energy: 2.320347037265519, Validation Loss Force: 3.251173495483372, time: 0.20109772682189941
Test Loss Energy: 9.282273406041988, Test Loss Force: 10.581803538679207, time: 8.553684711456299

Epoch 2, Batch 100/257, Loss: 1.5308914184570312, Variance: 0.1620485782623291
Epoch 2, Batch 200/257, Loss: 1.6629369258880615, Variance: 0.17901736497879028

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.118321960061083, Training Loss Force: 3.2760333558909704, time: 4.055887222290039
Validation Loss Energy: 3.0467120056180996, Validation Loss Force: 3.2745700551872723, time: 0.19910836219787598
Test Loss Energy: 8.934579701270131, Test Loss Force: 10.479939576462197, time: 8.607383966445923

Epoch 3, Batch 100/257, Loss: 0.9635050892829895, Variance: 0.177961528301239
Epoch 3, Batch 200/257, Loss: 0.9132792949676514, Variance: 0.17747756838798523

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.147014541734721, Training Loss Force: 3.2914854726084597, time: 4.001109838485718
Validation Loss Energy: 5.297418421351949, Validation Loss Force: 3.3687510878254727, time: 0.19899249076843262
Test Loss Energy: 10.062537677417467, Test Loss Force: 10.656395319655413, time: 8.571325778961182

Epoch 4, Batch 100/257, Loss: 1.2509630918502808, Variance: 0.18364056944847107
Epoch 4, Batch 200/257, Loss: 1.168166160583496, Variance: 0.17209094762802124

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.047497425828328, Training Loss Force: 3.3445233249938306, time: 3.857940673828125
Validation Loss Energy: 5.343567581589725, Validation Loss Force: 3.470900246783041, time: 0.1983194351196289
Test Loss Energy: 9.762619904233317, Test Loss Force: 10.663020092810179, time: 8.781083345413208

Epoch 5, Batch 100/257, Loss: 1.6004042625427246, Variance: 0.18071827292442322
Epoch 5, Batch 200/257, Loss: 1.836202621459961, Variance: 0.17430396378040314

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.1178499105690936, Training Loss Force: 3.3022807535221177, time: 3.924464464187622
Validation Loss Energy: 1.8676261944105366, Validation Loss Force: 3.4294774618367283, time: 0.19903349876403809
Test Loss Energy: 8.854213827214714, Test Loss Force: 10.63825562314888, time: 8.580039024353027

Epoch 6, Batch 100/257, Loss: 1.5991042852401733, Variance: 0.1863284707069397
Epoch 6, Batch 200/257, Loss: 1.6802953481674194, Variance: 0.17485052347183228

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.073440066509806, Training Loss Force: 3.3071111248353153, time: 3.9605188369750977
Validation Loss Energy: 3.184236478187728, Validation Loss Force: 3.4379465937344333, time: 0.19739508628845215
Test Loss Energy: 9.513306149650898, Test Loss Force: 10.469779162348985, time: 8.792255878448486

Epoch 7, Batch 100/257, Loss: 0.8754464387893677, Variance: 0.18298117816448212
Epoch 7, Batch 200/257, Loss: 0.8795120120048523, Variance: 0.1794622838497162

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.101887980712165, Training Loss Force: 3.2818786427743034, time: 3.896261215209961
Validation Loss Energy: 6.182692962515233, Validation Loss Force: 3.259517665456383, time: 0.1988997459411621
Test Loss Energy: 11.145581988122117, Test Loss Force: 10.345426059399944, time: 8.579303979873657

Epoch 8, Batch 100/257, Loss: 1.1218782663345337, Variance: 0.18214239180088043
Epoch 8, Batch 200/257, Loss: 1.0893669128417969, Variance: 0.18294179439544678

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.056646683563857, Training Loss Force: 3.3273306789408945, time: 3.8598694801330566
Validation Loss Energy: 5.233183271472992, Validation Loss Force: 3.4429422650909816, time: 0.20070457458496094
Test Loss Energy: 10.78072865051694, Test Loss Force: 10.667884236183582, time: 8.54155445098877

Epoch 9, Batch 100/257, Loss: 1.6885321140289307, Variance: 0.17288750410079956
Epoch 9, Batch 200/257, Loss: 1.7853829860687256, Variance: 0.19293121993541718

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.1058748968960925, Training Loss Force: 3.343055226593134, time: 3.941453218460083
Validation Loss Energy: 2.382043001805802, Validation Loss Force: 3.3509364873393492, time: 0.27361464500427246
Test Loss Energy: 9.445333210203204, Test Loss Force: 10.529502734553226, time: 8.709791898727417

Epoch 10, Batch 100/257, Loss: 1.215843915939331, Variance: 0.17704536020755768
Epoch 10, Batch 200/257, Loss: 1.684938669204712, Variance: 0.18435558676719666

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.067886836637072, Training Loss Force: 3.3053489619360374, time: 3.9172186851501465
Validation Loss Energy: 2.834384995375903, Validation Loss Force: 3.308590293444874, time: 0.19505882263183594
Test Loss Energy: 8.720166430120733, Test Loss Force: 10.538133213446718, time: 8.635257482528687

Epoch 11, Batch 100/257, Loss: 0.9892975091934204, Variance: 0.1781005561351776
Epoch 11, Batch 200/257, Loss: 0.9232186675071716, Variance: 0.18005773425102234

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.037505281333023, Training Loss Force: 3.338328389501457, time: 3.8794519901275635
Validation Loss Energy: 5.231972218522119, Validation Loss Force: 3.329015556065513, time: 0.20913314819335938
Test Loss Energy: 10.059386361073276, Test Loss Force: 10.617049826130046, time: 8.849942207336426

Epoch 12, Batch 100/257, Loss: 1.1269713640213013, Variance: 0.18054001033306122
Epoch 12, Batch 200/257, Loss: 0.9881907105445862, Variance: 0.17880040407180786

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.0208771275575215, Training Loss Force: 3.293342976024539, time: 3.880087375640869
Validation Loss Energy: 4.782972800569199, Validation Loss Force: 3.381037837696281, time: 0.20787787437438965
Test Loss Energy: 9.792873455130266, Test Loss Force: 10.585179049044942, time: 8.685905694961548

Epoch 13, Batch 100/257, Loss: 1.9933730363845825, Variance: 0.18430818617343903
Epoch 13, Batch 200/257, Loss: 1.896954894065857, Variance: 0.1763935387134552

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.08439234953214, Training Loss Force: 3.298104719320694, time: 3.970839738845825
Validation Loss Energy: 1.9540920306122291, Validation Loss Force: 3.336494567338936, time: 0.19723796844482422
Test Loss Energy: 8.755288689691483, Test Loss Force: 10.701856804824205, time: 8.67114520072937

Epoch 14, Batch 100/257, Loss: 1.914945125579834, Variance: 0.18783748149871826
Epoch 14, Batch 200/257, Loss: 1.444229006767273, Variance: 0.17685893177986145

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.051116990772907, Training Loss Force: 3.3217220867604316, time: 4.131658554077148
Validation Loss Energy: 2.9379939287203447, Validation Loss Force: 3.2949732735624644, time: 0.19978570938110352
Test Loss Energy: 9.477064197696945, Test Loss Force: 10.454263566934278, time: 8.662264108657837

Epoch 15, Batch 100/257, Loss: 0.8663406372070312, Variance: 0.18246924877166748
Epoch 15, Batch 200/257, Loss: 0.7373791337013245, Variance: 0.18392065167427063

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.030317956676972, Training Loss Force: 3.298315161591185, time: 4.0592041015625
Validation Loss Energy: 5.712301007171763, Validation Loss Force: 3.279019296459713, time: 1.413938045501709
Test Loss Energy: 11.013459732505089, Test Loss Force: 10.461356364986031, time: 8.666611194610596

Epoch 16, Batch 100/257, Loss: 1.0832716226577759, Variance: 0.18315540254116058
Epoch 16, Batch 200/257, Loss: 1.3148210048675537, Variance: 0.17960447072982788

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.051696385421197, Training Loss Force: 3.3006403447255663, time: 3.934257984161377
Validation Loss Energy: 5.548221813867713, Validation Loss Force: 3.4019244262534527, time: 0.20552706718444824
Test Loss Energy: 10.588633922891193, Test Loss Force: 10.397387692950204, time: 8.752302646636963

Epoch 17, Batch 100/257, Loss: 1.8615086078643799, Variance: 0.1786895990371704
Epoch 17, Batch 200/257, Loss: 1.60506272315979, Variance: 0.18767376244068146

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.013925924469555, Training Loss Force: 3.3167577128086476, time: 3.8600974082946777
Validation Loss Energy: 1.93674797456528, Validation Loss Force: 3.359383085119487, time: 0.2057504653930664
Test Loss Energy: 9.057686055189174, Test Loss Force: 10.406470803424433, time: 8.603851079940796

Epoch 18, Batch 100/257, Loss: 1.631941795349121, Variance: 0.1766345500946045
Epoch 18, Batch 200/257, Loss: 1.6465524435043335, Variance: 0.19004252552986145

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.077497203618265, Training Loss Force: 3.3126585939020123, time: 3.965540647506714
Validation Loss Energy: 3.379818367851693, Validation Loss Force: 3.3232364455859327, time: 0.19538426399230957
Test Loss Energy: 9.212149053627638, Test Loss Force: 10.510652040308255, time: 8.759321212768555

Epoch 19, Batch 100/257, Loss: 0.908194899559021, Variance: 0.17721566557884216
Epoch 19, Batch 200/257, Loss: 0.8679147958755493, Variance: 0.18388083577156067

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.0240163401368605, Training Loss Force: 3.3195534461005556, time: 3.8599226474761963
Validation Loss Energy: 5.660185738905656, Validation Loss Force: 3.194583334459213, time: 0.19984054565429688
Test Loss Energy: 10.087719272840088, Test Loss Force: 10.518133784233397, time: 8.622612953186035

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–ƒâ–‚â–…â–„â–â–ƒâ–ˆâ–‡â–ƒâ–â–…â–„â–â–ƒâ–ˆâ–†â–‚â–‚â–…
wandb:   test_error_force â–ˆâ–…â–„â–‡â–‡â–†â–ƒâ–â–‡â–„â–…â–†â–…â–ˆâ–ƒâ–ƒâ–‚â–‚â–„â–„
wandb:          test_loss â–ˆâ–ƒâ–â–„â–„â–‚â–‚â–„â–…â–ƒâ–â–ƒâ–„â–‚â–‚â–…â–ƒâ–‚â–â–ƒ
wandb: train_error_energy â–ˆâ–†â–†â–ˆâ–ƒâ–†â–„â–†â–ƒâ–†â–„â–‚â–â–…â–ƒâ–‚â–ƒâ–â–„â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–‚â–ƒâ–‡â–‡â–â–ƒâ–ˆâ–†â–‚â–ƒâ–†â–†â–â–ƒâ–‡â–‡â–â–ƒâ–‡
wandb:  valid_error_force â–…â–‚â–ƒâ–…â–ˆâ–‡â–‡â–ƒâ–‡â–…â–„â–„â–†â–…â–„â–ƒâ–†â–…â–„â–
wandb:         valid_loss â–ˆâ–â–‚â–†â–‡â–â–ƒâ–ˆâ–†â–â–‚â–†â–…â–â–‚â–‡â–†â–â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 8201
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.08772
wandb:   test_error_force 10.51813
wandb:          test_loss 8.46925
wandb: train_error_energy 4.02402
wandb:  train_error_force 3.31955
wandb:         train_loss 1.35882
wandb: valid_error_energy 5.66019
wandb:  valid_error_force 3.19458
wandb:         valid_loss 1.81535
wandb: 
wandb: ğŸš€ View run al_63_88 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/tp4sauwg
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_212028-tp4sauwg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.2453598976135254, Uncertainty Bias: -0.17550261318683624
6.389618e-05 1.5895052
1.8158783 5.69084
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 2571 steps.
Found uncertainty sample 2 after 1985 steps.
Found uncertainty sample 3 after 790 steps.
Found uncertainty sample 4 after 2011 steps.
Found uncertainty sample 5 after 464 steps.
Found uncertainty sample 6 after 3080 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1941 steps.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 96 steps.
Found uncertainty sample 12 after 754 steps.
Found uncertainty sample 13 after 48 steps.
Found uncertainty sample 14 after 878 steps.
Found uncertainty sample 15 after 2041 steps.
Found uncertainty sample 16 after 10 steps.
Found uncertainty sample 17 after 1267 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 3831 steps.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 3965 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 789 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 373 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 391 steps.
Found uncertainty sample 32 after 1805 steps.
Did not find any uncertainty samples for sample 33.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 565 steps.
Found uncertainty sample 36 after 1018 steps.
Found uncertainty sample 37 after 1792 steps.
Found uncertainty sample 38 after 915 steps.
Found uncertainty sample 39 after 696 steps.
Found uncertainty sample 40 after 1888 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 1196 steps.
Found uncertainty sample 43 after 1354 steps.
Found uncertainty sample 44 after 2091 steps.
Did not find any uncertainty samples for sample 45.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 152 steps.
Found uncertainty sample 48 after 555 steps.
Found uncertainty sample 49 after 389 steps.
Found uncertainty sample 50 after 942 steps.
Found uncertainty sample 51 after 114 steps.
Found uncertainty sample 52 after 1077 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 187 steps.
Found uncertainty sample 55 after 252 steps.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 1189 steps.
Found uncertainty sample 58 after 462 steps.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 495 steps.
Found uncertainty sample 62 after 533 steps.
Found uncertainty sample 63 after 2384 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 1089 steps.
Found uncertainty sample 66 after 19 steps.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 1 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 1296 steps.
Found uncertainty sample 72 after 1104 steps.
Found uncertainty sample 73 after 2042 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 297 steps.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 2773 steps.
Found uncertainty sample 79 after 116 steps.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 3054 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 1345 steps.
Found uncertainty sample 85 after 1621 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 376 steps.
Found uncertainty sample 88 after 2197 steps.
Found uncertainty sample 89 after 2282 steps.
Found uncertainty sample 90 after 3307 steps.
Found uncertainty sample 91 after 3179 steps.
Found uncertainty sample 92 after 1314 steps.
Found uncertainty sample 93 after 512 steps.
Found uncertainty sample 94 after 2562 steps.
Did not find any uncertainty samples for sample 95.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 97 steps.
Found uncertainty sample 98 after 108 steps.
Found uncertainty sample 99 after 1113 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_214735-xtxrmxzb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_89
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/xtxrmxzb
Training model 89. Added 65 samples to the dataset.
Epoch 0, Batch 100/259, Loss: 0.8094298839569092, Variance: 0.13970163464546204
Epoch 0, Batch 200/259, Loss: 0.8516790866851807, Variance: 0.13700154423713684

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.908333102961003, Training Loss Force: 3.3784474171328287, time: 3.9405083656311035
Validation Loss Energy: 2.4236206754100227, Validation Loss Force: 3.276787884414542, time: 0.2031240463256836
Test Loss Energy: 9.250340104393352, Test Loss Force: 10.595713705873074, time: 8.561766862869263

Epoch 1, Batch 100/259, Loss: 0.7547132968902588, Variance: 0.13200417160987854
Epoch 1, Batch 200/259, Loss: 1.279052734375, Variance: 0.1380685567855835

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.551340622530671, Training Loss Force: 3.2095612909670046, time: 3.9143381118774414
Validation Loss Energy: 3.781251737257469, Validation Loss Force: 3.272531741825449, time: 0.19196677207946777
Test Loss Energy: 9.918383892040433, Test Loss Force: 10.63830200361643, time: 8.564523935317993

Epoch 2, Batch 100/259, Loss: 1.1176292896270752, Variance: 0.13359911739826202
Epoch 2, Batch 200/259, Loss: 0.6495813131332397, Variance: 0.13427196443080902

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5713007819207254, Training Loss Force: 3.2258285075039126, time: 4.155246734619141
Validation Loss Energy: 2.047524605450223, Validation Loss Force: 3.340877809476525, time: 0.1980605125427246
Test Loss Energy: 9.203969224442908, Test Loss Force: 10.913735399277105, time: 9.830399990081787

Epoch 3, Batch 100/259, Loss: 0.524804949760437, Variance: 0.13078513741493225
Epoch 3, Batch 200/259, Loss: 0.7819114923477173, Variance: 0.13456928730010986

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5710825270401636, Training Loss Force: 3.218259444185354, time: 3.963697671890259
Validation Loss Energy: 2.1904111291891093, Validation Loss Force: 3.2872509294463717, time: 0.19973254203796387
Test Loss Energy: 8.904676079016332, Test Loss Force: 10.895419773160762, time: 8.592689990997314

Epoch 4, Batch 100/259, Loss: 0.8369888663291931, Variance: 0.1417076289653778
Epoch 4, Batch 200/259, Loss: 1.0510480403900146, Variance: 0.13198009133338928

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.611580509335795, Training Loss Force: 3.2216076522142822, time: 3.954134941101074
Validation Loss Energy: 3.1522452457216494, Validation Loss Force: 3.3955212632254956, time: 0.1945488452911377
Test Loss Energy: 9.036240017980605, Test Loss Force: 10.629712099471393, time: 8.686031341552734

Epoch 5, Batch 100/259, Loss: 1.468914270401001, Variance: 0.13814523816108704
Epoch 5, Batch 200/259, Loss: 0.8393391966819763, Variance: 0.131717711687088

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6016340290393267, Training Loss Force: 3.249692699237725, time: 3.9616618156433105
Validation Loss Energy: 1.7398364160964082, Validation Loss Force: 3.2829890419933143, time: 0.20007109642028809
Test Loss Energy: 8.796901405123915, Test Loss Force: 10.865889451649785, time: 8.515539407730103

Epoch 6, Batch 100/259, Loss: 0.682115912437439, Variance: 0.13809014856815338
Epoch 6, Batch 200/259, Loss: 0.8021003603935242, Variance: 0.1369105875492096

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.57272938589413, Training Loss Force: 3.2382613957370014, time: 3.9585390090942383
Validation Loss Energy: 2.76540725989166, Validation Loss Force: 3.392327608733111, time: 0.19719481468200684
Test Loss Energy: 9.405624250683259, Test Loss Force: 10.61536241369445, time: 8.781075239181519

Epoch 7, Batch 100/259, Loss: 0.6080442667007446, Variance: 0.13141247630119324
Epoch 7, Batch 200/259, Loss: 1.2351144552230835, Variance: 0.13880756497383118

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5938062822047336, Training Loss Force: 3.2364211349741745, time: 4.00381875038147
Validation Loss Energy: 3.6499944659352948, Validation Loss Force: 3.332089681760363, time: 0.19571948051452637
Test Loss Energy: 9.558681299813033, Test Loss Force: 10.565825882636371, time: 8.585520505905151

Epoch 8, Batch 100/259, Loss: 1.298033356666565, Variance: 0.1321864277124405
Epoch 8, Batch 200/259, Loss: 0.878649890422821, Variance: 0.13455528020858765

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5786159095624717, Training Loss Force: 3.2562254637900416, time: 3.9648544788360596
Validation Loss Energy: 2.3114695301541826, Validation Loss Force: 3.314746249187023, time: 0.19695520401000977
Test Loss Energy: 8.953340173707756, Test Loss Force: 10.560044370361112, time: 8.59606409072876

Epoch 9, Batch 100/259, Loss: 0.7261897921562195, Variance: 0.13466154038906097
Epoch 9, Batch 200/259, Loss: 0.62455153465271, Variance: 0.12904474139213562

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.576356237707667, Training Loss Force: 3.2398586642807703, time: 4.0839478969573975
Validation Loss Energy: 2.1145268202717946, Validation Loss Force: 3.352944339562009, time: 0.1961677074432373
Test Loss Energy: 8.677635289565266, Test Loss Force: 10.827491992414828, time: 8.631597757339478

Epoch 10, Batch 100/259, Loss: 0.9326000213623047, Variance: 0.14021123945713043
Epoch 10, Batch 200/259, Loss: 1.2476933002471924, Variance: 0.13186800479888916

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6152945103281695, Training Loss Force: 3.2200758673133967, time: 4.00283670425415
Validation Loss Energy: 3.521341399342405, Validation Loss Force: 3.331865765667218, time: 0.19779324531555176
Test Loss Energy: 9.29033712550809, Test Loss Force: 10.931068860935934, time: 8.648256063461304

Epoch 11, Batch 100/259, Loss: 1.4156277179718018, Variance: 0.13840484619140625
Epoch 11, Batch 200/259, Loss: 0.4398733377456665, Variance: 0.1335846185684204

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5880265460162564, Training Loss Force: 3.2360859018464843, time: 3.959036111831665
Validation Loss Energy: 1.6852069459698016, Validation Loss Force: 3.2083266522811926, time: 0.2077631950378418
Test Loss Energy: 8.825182209978964, Test Loss Force: 10.725331341630879, time: 8.799174785614014

Epoch 12, Batch 100/259, Loss: 0.7605006098747253, Variance: 0.13915395736694336
Epoch 12, Batch 200/259, Loss: 0.8236621618270874, Variance: 0.13750743865966797

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5882303145064363, Training Loss Force: 3.2222796874704, time: 3.9639792442321777
Validation Loss Energy: 2.559746696675566, Validation Loss Force: 3.259428954068645, time: 0.2037062644958496
Test Loss Energy: 9.507636242415838, Test Loss Force: 10.780355624904582, time: 8.637116193771362

Epoch 13, Batch 100/259, Loss: 0.6472183465957642, Variance: 0.13345757126808167
Epoch 13, Batch 200/259, Loss: 1.405799150466919, Variance: 0.13524575531482697

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6037892793814086, Training Loss Force: 3.2384356588458925, time: 3.956709146499634
Validation Loss Energy: 3.9853885755079785, Validation Loss Force: 3.2624425582896315, time: 0.19529294967651367
Test Loss Energy: 9.758544186839167, Test Loss Force: 10.759401250496675, time: 8.74206256866455

Epoch 14, Batch 100/259, Loss: 1.0325387716293335, Variance: 0.13352978229522705
Epoch 14, Batch 200/259, Loss: 0.8090775012969971, Variance: 0.138894185423851

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.591974647936927, Training Loss Force: 3.2223289497949623, time: 3.9371869564056396
Validation Loss Energy: 1.967083506062984, Validation Loss Force: 3.278063199680929, time: 0.20439600944519043
Test Loss Energy: 9.052273870586324, Test Loss Force: 10.483678646087075, time: 8.639764547348022

Epoch 15, Batch 100/259, Loss: 0.48183560371398926, Variance: 0.1297607421875
Epoch 15, Batch 200/259, Loss: 0.9191991686820984, Variance: 0.13681615889072418

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6090849285480067, Training Loss Force: 3.2173981659469906, time: 3.9099955558776855
Validation Loss Energy: 2.0186190774745825, Validation Loss Force: 3.3967462552649024, time: 0.20093536376953125
Test Loss Energy: 8.933520059607908, Test Loss Force: 10.708685743435451, time: 8.607975482940674

Epoch 16, Batch 100/259, Loss: 0.8410344123840332, Variance: 0.13720519840717316
Epoch 16, Batch 200/259, Loss: 1.158842921257019, Variance: 0.12829750776290894

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5738611706064223, Training Loss Force: 3.233729711514184, time: 3.9132423400878906
Validation Loss Energy: 3.2596447650048788, Validation Loss Force: 3.3254185586821956, time: 0.2911796569824219
Test Loss Energy: 9.34121491757345, Test Loss Force: 11.054348213625873, time: 8.64243221282959

Epoch 17, Batch 100/259, Loss: 1.2327934503555298, Variance: 0.1424780637025833
Epoch 17, Batch 200/259, Loss: 0.5943692922592163, Variance: 0.1357385218143463

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6149847994692084, Training Loss Force: 3.2239733064151475, time: 3.887545585632324
Validation Loss Energy: 1.59767997995319, Validation Loss Force: 3.2149397032461025, time: 0.20140910148620605
Test Loss Energy: 8.897964241059702, Test Loss Force: 10.77320634583303, time: 8.543076276779175

Epoch 18, Batch 100/259, Loss: 0.716320276260376, Variance: 0.13703277707099915
Epoch 18, Batch 200/259, Loss: 0.9356992244720459, Variance: 0.143568217754364

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.618685982370343, Training Loss Force: 3.2331488304724885, time: 3.8704872131347656
Validation Loss Energy: 2.492589459594599, Validation Loss Force: 3.246089246340574, time: 0.20552992820739746
Test Loss Energy: 9.11623629389638, Test Loss Force: 10.514976498957125, time: 8.764098644256592

Epoch 19, Batch 100/259, Loss: 0.8713394403457642, Variance: 0.1322384476661682
Epoch 19, Batch 200/259, Loss: 1.491236925125122, Variance: 0.14049798250198364

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.596767563904923, Training Loss Force: 3.2306995420843028, time: 3.9119341373443604
Validation Loss Energy: 3.757715653066731, Validation Loss Force: 3.2093992050232982, time: 0.20711278915405273
Test Loss Energy: 9.589431731662991, Test Loss Force: 10.444544816489115, time: 8.601369619369507

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–ˆâ–„â–‚â–ƒâ–‚â–…â–†â–ƒâ–â–„â–‚â–†â–‡â–ƒâ–‚â–…â–‚â–ƒâ–†
wandb:   test_error_force â–ƒâ–ƒâ–†â–†â–ƒâ–†â–ƒâ–‚â–‚â–…â–‡â–„â–…â–…â–â–„â–ˆâ–…â–‚â–
wandb:          test_loss â–„â–ˆâ–‡â–…â–„â–†â–…â–„â–‚â–ƒâ–†â–ƒâ–…â–…â–â–ƒâ–‡â–„â–â–ƒ
wandb: train_error_energy â–ˆâ–â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–‚â–â–â–ƒâ–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: valid_error_energy â–ƒâ–‡â–‚â–ƒâ–†â–â–„â–‡â–ƒâ–ƒâ–‡â–â–„â–ˆâ–‚â–‚â–†â–â–„â–‡
wandb:  valid_error_force â–„â–ƒâ–†â–„â–ˆâ–„â–ˆâ–†â–…â–†â–†â–â–ƒâ–ƒâ–„â–ˆâ–…â–â–‚â–
wandb:         valid_loss â–ƒâ–‡â–‚â–‚â–…â–â–„â–‡â–ƒâ–‚â–‡â–â–ƒâ–ˆâ–‚â–‚â–†â–â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 8259
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.58943
wandb:   test_error_force 10.44454
wandb:          test_loss 9.57101
wandb: train_error_energy 2.59677
wandb:  train_error_force 3.2307
wandb:         train_loss 0.88645
wandb: valid_error_energy 3.75772
wandb:  valid_error_force 3.2094
wandb:         valid_loss 1.36387
wandb: 
wandb: ğŸš€ View run al_63_89 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/xtxrmxzb
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_214735-xtxrmxzb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.563728094100952, Uncertainty Bias: -0.13532927632331848
8.010864e-05 0.052505493
1.8196455 5.7955413
(48745, 22, 3)
Found uncertainty sample 0 after 1278 steps.
Found uncertainty sample 1 after 2411 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 897 steps.
Found uncertainty sample 5 after 921 steps.
Found uncertainty sample 6 after 30 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 1090 steps.
Found uncertainty sample 10 after 1403 steps.
Found uncertainty sample 11 after 1496 steps.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1059 steps.
Found uncertainty sample 16 after 1438 steps.
Found uncertainty sample 17 after 49 steps.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 2039 steps.
Found uncertainty sample 21 after 507 steps.
Found uncertainty sample 22 after 2860 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 434 steps.
Found uncertainty sample 25 after 919 steps.
Found uncertainty sample 26 after 761 steps.
Did not find any uncertainty samples for sample 27.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 3386 steps.
Found uncertainty sample 30 after 1351 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 481 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 772 steps.
Found uncertainty sample 35 after 3110 steps.
Found uncertainty sample 36 after 2900 steps.
Found uncertainty sample 37 after 549 steps.
Found uncertainty sample 38 after 2300 steps.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 2395 steps.
Found uncertainty sample 44 after 2673 steps.
Found uncertainty sample 45 after 552 steps.
Found uncertainty sample 46 after 130 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 766 steps.
Found uncertainty sample 49 after 3486 steps.
Found uncertainty sample 50 after 1927 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1672 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 1024 steps.
Found uncertainty sample 55 after 1085 steps.
Found uncertainty sample 56 after 258 steps.
Found uncertainty sample 57 after 471 steps.
Found uncertainty sample 58 after 2748 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 1843 steps.
Found uncertainty sample 61 after 1295 steps.
Found uncertainty sample 62 after 1271 steps.
Found uncertainty sample 63 after 2322 steps.
Found uncertainty sample 64 after 25 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 96 steps.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 1913 steps.
Found uncertainty sample 71 after 913 steps.
Found uncertainty sample 72 after 2876 steps.
Found uncertainty sample 73 after 2258 steps.
Found uncertainty sample 74 after 3859 steps.
Found uncertainty sample 75 after 452 steps.
Found uncertainty sample 76 after 3977 steps.
Found uncertainty sample 77 after 982 steps.
Found uncertainty sample 78 after 212 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 1228 steps.
Found uncertainty sample 81 after 433 steps.
Found uncertainty sample 82 after 451 steps.
Found uncertainty sample 83 after 1969 steps.
Found uncertainty sample 84 after 2811 steps.
Found uncertainty sample 85 after 2099 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 1134 steps.
Found uncertainty sample 88 after 688 steps.
Found uncertainty sample 89 after 3591 steps.
Found uncertainty sample 90 after 1208 steps.
Found uncertainty sample 91 after 585 steps.
Found uncertainty sample 92 after 297 steps.
Found uncertainty sample 93 after 860 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 1836 steps.
Found uncertainty sample 96 after 2507 steps.
Found uncertainty sample 97 after 1265 steps.
Found uncertainty sample 98 after 1212 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_221429-1uv49f16
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_90
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/1uv49f16
Training model 90. Added 71 samples to the dataset.
Epoch 0, Batch 100/261, Loss: 0.760654628276825, Variance: 0.10054615885019302
Epoch 0, Batch 200/261, Loss: 0.3053000569343567, Variance: 0.0987517386674881

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.211634761453556, Training Loss Force: 3.5091502555833567, time: 4.021465063095093
Validation Loss Energy: 1.4924119833362695, Validation Loss Force: 3.2812307128155807, time: 0.19959068298339844
Test Loss Energy: 8.36815989445949, Test Loss Force: 10.6401373802824, time: 8.619479656219482

Epoch 1, Batch 100/261, Loss: 0.30244481563568115, Variance: 0.09380918741226196
Epoch 1, Batch 200/261, Loss: 0.520915687084198, Variance: 0.09663358330726624

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5649094521856963, Training Loss Force: 3.20905880276573, time: 3.852320432662964
Validation Loss Energy: 1.8306138617714445, Validation Loss Force: 3.297316153782087, time: 0.20054149627685547
Test Loss Energy: 8.571276698176382, Test Loss Force: 10.75691421100772, time: 8.626185655593872

Epoch 2, Batch 100/261, Loss: 0.35354214906692505, Variance: 0.09311240911483765
Epoch 2, Batch 200/261, Loss: 0.4185050129890442, Variance: 0.09151789546012878

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.574172922690499, Training Loss Force: 3.203477415491998, time: 4.1357080936431885
Validation Loss Energy: 1.6339963156428556, Validation Loss Force: 3.3327516771312062, time: 0.20785236358642578
Test Loss Energy: 9.140158076255236, Test Loss Force: 10.871919751812047, time: 8.648157119750977

Epoch 3, Batch 100/261, Loss: 0.5300960540771484, Variance: 0.09245869517326355
Epoch 3, Batch 200/261, Loss: 0.29149264097213745, Variance: 0.09270557016134262

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.586789809515306, Training Loss Force: 3.2177117112731293, time: 3.926663637161255
Validation Loss Energy: 1.534836245033307, Validation Loss Force: 3.321873933082, time: 0.20166707038879395
Test Loss Energy: 8.536039389771418, Test Loss Force: 10.839274552421934, time: 8.631347417831421

Epoch 4, Batch 100/261, Loss: 0.4991793632507324, Variance: 0.09182782471179962
Epoch 4, Batch 200/261, Loss: 0.441722571849823, Variance: 0.09341375529766083

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5812962114719171, Training Loss Force: 3.2218666066281383, time: 3.9862277507781982
Validation Loss Energy: 1.1989216685989315, Validation Loss Force: 3.3227207438439637, time: 0.19968366622924805
Test Loss Energy: 8.574761260464884, Test Loss Force: 10.895579348829319, time: 8.794421911239624

Epoch 5, Batch 100/261, Loss: 0.11009985208511353, Variance: 0.09267878532409668
Epoch 5, Batch 200/261, Loss: 0.2768847346305847, Variance: 0.09240253269672394

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6056896411493229, Training Loss Force: 3.233103893438194, time: 3.8935017585754395
Validation Loss Energy: 1.7017450733099797, Validation Loss Force: 3.3231791305477705, time: 0.2035505771636963
Test Loss Energy: 9.087492780224476, Test Loss Force: 10.835395035690523, time: 8.64611291885376

Epoch 6, Batch 100/261, Loss: 0.08055704832077026, Variance: 0.0896095260977745
Epoch 6, Batch 200/261, Loss: 0.3803103566169739, Variance: 0.08957859873771667

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.5840532468757855, Training Loss Force: 3.229127234070269, time: 3.9201486110687256
Validation Loss Energy: 2.001018506571525, Validation Loss Force: 3.2154054825655543, time: 0.2043139934539795
Test Loss Energy: 9.139615351105924, Test Loss Force: 10.716016813794642, time: 8.809998273849487

Epoch 7, Batch 100/261, Loss: 0.5103704929351807, Variance: 0.09248626977205276
Epoch 7, Batch 200/261, Loss: 0.5445335507392883, Variance: 0.09148728102445602

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5903832499509734, Training Loss Force: 3.232095038404551, time: 4.083285093307495
Validation Loss Energy: 1.2407082949296906, Validation Loss Force: 3.251039047853061, time: 0.20178723335266113
Test Loss Energy: 8.575316303531453, Test Loss Force: 10.996469232626499, time: 8.611751317977905

Epoch 8, Batch 100/261, Loss: 2.191148042678833, Variance: 0.1189752146601677
Epoch 8, Batch 200/261, Loss: 0.5661075711250305, Variance: 0.13025589287281036

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5381698748019788, Training Loss Force: 3.804684420892574, time: 4.0611889362335205
Validation Loss Energy: 3.832001865523722, Validation Loss Force: 3.3350833793597032, time: 0.20420479774475098
Test Loss Energy: 10.292370357905371, Test Loss Force: 10.951275248879423, time: 8.594784498214722

Epoch 9, Batch 100/261, Loss: 0.6960909962654114, Variance: 0.128224179148674
Epoch 9, Batch 200/261, Loss: 0.6669033765792847, Variance: 0.13270032405853271

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.576585152566839, Training Loss Force: 3.205570711070095, time: 4.204490900039673
Validation Loss Energy: 3.343978076529106, Validation Loss Force: 3.2374324488866812, time: 0.20360398292541504
Test Loss Energy: 9.264384469967982, Test Loss Force: 10.815101198228843, time: 9.878942251205444

Epoch 10, Batch 100/261, Loss: 1.3734740018844604, Variance: 0.13904309272766113
Epoch 10, Batch 200/261, Loss: 0.6904117465019226, Variance: 0.13448645174503326

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.576228130744914, Training Loss Force: 3.219524762801153, time: 4.081113815307617
Validation Loss Energy: 3.747495673674596, Validation Loss Force: 3.2509440126615794, time: 0.20657014846801758
Test Loss Energy: 9.820620157144923, Test Loss Force: 10.787493390465741, time: 8.639454364776611

Epoch 11, Batch 100/261, Loss: 1.2431840896606445, Variance: 0.13086989521980286
Epoch 11, Batch 200/261, Loss: 0.6248621344566345, Variance: 0.1320033073425293

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5701613413692597, Training Loss Force: 3.2154513379062606, time: 4.077473163604736
Validation Loss Energy: 3.170347000338361, Validation Loss Force: 3.3587369883256626, time: 0.20552420616149902
Test Loss Energy: 9.194645267559276, Test Loss Force: 10.9464579383067, time: 8.80260682106018

Epoch 12, Batch 100/261, Loss: 1.2802488803863525, Variance: 0.13632240891456604
Epoch 12, Batch 200/261, Loss: 0.6841113567352295, Variance: 0.13331536948680878

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5779610725687125, Training Loss Force: 3.214550561015965, time: 3.9849605560302734
Validation Loss Energy: 3.963080511657033, Validation Loss Force: 3.259468544642791, time: 0.20484089851379395
Test Loss Energy: 9.812231942282203, Test Loss Force: 10.631022900272153, time: 8.616545677185059

Epoch 13, Batch 100/261, Loss: 1.3240458965301514, Variance: 0.12848308682441711
Epoch 13, Batch 200/261, Loss: 0.7949560880661011, Variance: 0.13664521276950836

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.55779576610891, Training Loss Force: 3.218047130348776, time: 3.936781883239746
Validation Loss Energy: 3.2026048568397742, Validation Loss Force: 3.3322595783187063, time: 0.20146489143371582
Test Loss Energy: 9.113762101601079, Test Loss Force: 10.747178528381005, time: 8.825822591781616

Epoch 14, Batch 100/261, Loss: 1.219651699066162, Variance: 0.133371964097023
Epoch 14, Batch 200/261, Loss: 0.5423114895820618, Variance: 0.1327252835035324

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6108055893263042, Training Loss Force: 3.2126946324537036, time: 3.9602253437042236
Validation Loss Energy: 3.639014026875987, Validation Loss Force: 3.2835874875899096, time: 0.20110249519348145
Test Loss Energy: 9.8143994760287, Test Loss Force: 10.82191843213734, time: 8.616678953170776

Epoch 15, Batch 100/261, Loss: 1.1293889284133911, Variance: 0.13303402066230774
Epoch 15, Batch 200/261, Loss: 0.5898065567016602, Variance: 0.13903477787971497

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5990169046067138, Training Loss Force: 3.2203969092168587, time: 3.986929416656494
Validation Loss Energy: 3.8857527277343267, Validation Loss Force: 3.5204857708822623, time: 0.19959545135498047
Test Loss Energy: 9.352237660506223, Test Loss Force: 11.059204886507722, time: 8.64298963546753

Epoch 16, Batch 100/261, Loss: 0.25131434202194214, Variance: 0.09353998303413391
Epoch 16, Batch 200/261, Loss: 0.35780423879623413, Variance: 0.09315700829029083

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6746326898994608, Training Loss Force: 3.487985478186733, time: 4.231544017791748
Validation Loss Energy: 2.1845029596725487, Validation Loss Force: 3.3824233741899015, time: 0.2071073055267334
Test Loss Energy: 9.39111524105208, Test Loss Force: 10.86068165167514, time: 8.660954713821411

Epoch 17, Batch 100/261, Loss: 1.1804478168487549, Variance: 0.12716996669769287
Epoch 17, Batch 200/261, Loss: 0.7493205070495605, Variance: 0.13177490234375

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6398246682860496, Training Loss Force: 3.3000343185327248, time: 3.929443836212158
Validation Loss Energy: 3.035430696008814, Validation Loss Force: 3.439272944769067, time: 0.20052695274353027
Test Loss Energy: 9.1387057791216, Test Loss Force: 10.88624206443953, time: 8.63587498664856

Epoch 18, Batch 100/261, Loss: 1.223116159439087, Variance: 0.13299719989299774
Epoch 18, Batch 200/261, Loss: 0.6692096590995789, Variance: 0.13239681720733643

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5943531974561824, Training Loss Force: 3.2143717986424325, time: 4.050408124923706
Validation Loss Energy: 3.7759545273435053, Validation Loss Force: 3.1685509064734783, time: 0.20727062225341797
Test Loss Energy: 10.038500680679794, Test Loss Force: 10.8444539673266, time: 8.858406066894531

Epoch 19, Batch 100/261, Loss: 0.8420997858047485, Variance: 0.12782302498817444
Epoch 19, Batch 200/261, Loss: 0.6632043123245239, Variance: 0.13950058817863464

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.594511147089679, Training Loss Force: 3.202766718710444, time: 4.02605938911438
Validation Loss Energy: 2.8124173926490803, Validation Loss Force: 3.2351163780322274, time: 0.20021367073059082
Test Loss Energy: 9.121205649024137, Test Loss Force: 10.890011379762425, time: 8.585867404937744

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–„â–‚â–‚â–„â–„â–‚â–ˆâ–„â–†â–„â–†â–„â–†â–…â–…â–„â–‡â–„
wandb:   test_error_force â–â–ƒâ–…â–„â–…â–„â–‚â–‡â–†â–„â–„â–†â–â–ƒâ–„â–ˆâ–…â–…â–„â–…
wandb:          test_loss â–ƒâ–„â–‡â–…â–†â–‡â–‡â–‡â–„â–‚â–‚â–‚â–‚â–â–‚â–‚â–ˆâ–â–ƒâ–‚
wandb: train_error_energy â–…â–â–â–â–â–â–â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‚â–ˆâ–ˆâ–ˆ
wandb:  train_error_force â–…â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–„â–‚â–â–
wandb:         train_loss â–†â–â–â–â–â–â–â–â–ˆâ–…â–…â–…â–…â–…â–…â–…â–‚â–†â–…â–…
wandb: valid_error_energy â–‚â–ƒâ–‚â–‚â–â–‚â–ƒâ–â–ˆâ–†â–‡â–†â–ˆâ–†â–‡â–ˆâ–ƒâ–†â–ˆâ–…
wandb:  valid_error_force â–ƒâ–„â–„â–„â–„â–„â–‚â–ƒâ–„â–‚â–ƒâ–…â–ƒâ–„â–ƒâ–ˆâ–…â–†â–â–‚
wandb:         valid_loss â–‚â–ƒâ–‚â–‚â–â–‚â–ƒâ–â–‡â–†â–‡â–†â–ˆâ–†â–†â–ˆâ–„â–…â–‡â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 8322
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.12121
wandb:   test_error_force 10.89001
wandb:          test_loss 10.15497
wandb: train_error_energy 2.59451
wandb:  train_error_force 3.20277
wandb:         train_loss 0.87983
wandb: valid_error_energy 2.81242
wandb:  valid_error_force 3.23512
wandb:         valid_loss 0.96256
wandb: 
wandb: ğŸš€ View run al_63_90 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/1uv49f16
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_221429-1uv49f16/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.441892623901367, Uncertainty Bias: -0.09732721745967865
3.8146973e-06 0.029129028
1.9105246 5.67672
(48745, 22, 3)
Found uncertainty sample 0 after 2164 steps.
Found uncertainty sample 1 after 2791 steps.
Found uncertainty sample 2 after 509 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 1212 steps.
Found uncertainty sample 5 after 777 steps.
Found uncertainty sample 6 after 232 steps.
Found uncertainty sample 7 after 1459 steps.
Found uncertainty sample 8 after 1096 steps.
Found uncertainty sample 9 after 870 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 348 steps.
Found uncertainty sample 12 after 3343 steps.
Found uncertainty sample 13 after 359 steps.
Found uncertainty sample 14 after 9 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 3271 steps.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 2997 steps.
Found uncertainty sample 21 after 3858 steps.
Found uncertainty sample 22 after 1137 steps.
Found uncertainty sample 23 after 511 steps.
Found uncertainty sample 24 after 78 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 94 steps.
Found uncertainty sample 27 after 790 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1363 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 187 steps.
Found uncertainty sample 32 after 3042 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 1378 steps.
Found uncertainty sample 35 after 1891 steps.
Found uncertainty sample 36 after 1140 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 2107 steps.
Found uncertainty sample 39 after 1207 steps.
Found uncertainty sample 40 after 1723 steps.
Found uncertainty sample 41 after 771 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 1405 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 1834 steps.
Found uncertainty sample 46 after 223 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 274 steps.
Found uncertainty sample 49 after 1388 steps.
Did not find any uncertainty samples for sample 50.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 1662 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1167 steps.
Found uncertainty sample 56 after 43 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 2302 steps.
Found uncertainty sample 59 after 2765 steps.
Found uncertainty sample 60 after 1230 steps.
Found uncertainty sample 61 after 1212 steps.
Found uncertainty sample 62 after 1352 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 750 steps.
Found uncertainty sample 65 after 59 steps.
Found uncertainty sample 66 after 1636 steps.
Found uncertainty sample 67 after 2258 steps.
Found uncertainty sample 68 after 3686 steps.
Found uncertainty sample 69 after 96 steps.
Found uncertainty sample 70 after 976 steps.
Found uncertainty sample 71 after 340 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 1178 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 507 steps.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 2352 steps.
Found uncertainty sample 79 after 12 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 118 steps.
Found uncertainty sample 82 after 1199 steps.
Found uncertainty sample 83 after 2623 steps.
Found uncertainty sample 84 after 650 steps.
Found uncertainty sample 85 after 1433 steps.
Found uncertainty sample 86 after 3439 steps.
Found uncertainty sample 87 after 3922 steps.
Found uncertainty sample 88 after 2364 steps.
Found uncertainty sample 89 after 1034 steps.
Found uncertainty sample 90 after 1056 steps.
Found uncertainty sample 91 after 2756 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 2654 steps.
Found uncertainty sample 94 after 970 steps.
Found uncertainty sample 95 after 1297 steps.
Found uncertainty sample 96 after 3789 steps.
Found uncertainty sample 97 after 3407 steps.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_224057-763ogsu9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_91
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/763ogsu9
Training model 91. Added 73 samples to the dataset.
Epoch 0, Batch 100/263, Loss: 0.5837278366088867, Variance: 0.1386362612247467
Epoch 0, Batch 200/263, Loss: 0.8592970371246338, Variance: 0.13888338208198547

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.8483423706357582, Training Loss Force: 3.275160833176061, time: 4.0176215171813965
Validation Loss Energy: 3.18470007534955, Validation Loss Force: 3.288952433458064, time: 0.20349335670471191
Test Loss Energy: 9.08081649705116, Test Loss Force: 10.751567246314291, time: 8.954627513885498

Epoch 1, Batch 100/263, Loss: 1.4063794612884521, Variance: 0.14064273238182068
Epoch 1, Batch 200/263, Loss: 0.6425989866256714, Variance: 0.1304127275943756

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.57544709840893, Training Loss Force: 3.2203481974419756, time: 4.084145784378052
Validation Loss Energy: 2.4804804887699, Validation Loss Force: 3.2102037531813643, time: 0.20569229125976562
Test Loss Energy: 8.773916103907633, Test Loss Force: 10.763072759271578, time: 8.881499528884888

Epoch 2, Batch 100/263, Loss: 0.8198349475860596, Variance: 0.13720473647117615
Epoch 2, Batch 200/263, Loss: 1.0177173614501953, Variance: 0.1326521337032318

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.602379590726501, Training Loss Force: 3.212257308288559, time: 4.2831621170043945
Validation Loss Energy: 1.9072166572219242, Validation Loss Force: 3.364927190582455, time: 0.20302581787109375
Test Loss Energy: 9.00487339182125, Test Loss Force: 10.829811747919035, time: 8.856421709060669

Epoch 3, Batch 100/263, Loss: 0.5694125890731812, Variance: 0.14038005471229553
Epoch 3, Batch 200/263, Loss: 0.9144544005393982, Variance: 0.12989065051078796

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5829234092898012, Training Loss Force: 3.2214465066543836, time: 3.971989393234253
Validation Loss Energy: 3.6742226948992354, Validation Loss Force: 3.2886089885673853, time: 0.20585155487060547
Test Loss Energy: 10.189833314658312, Test Loss Force: 10.779335744257493, time: 8.824307441711426

Epoch 4, Batch 100/263, Loss: 1.1850734949111938, Variance: 0.13504162430763245
Epoch 4, Batch 200/263, Loss: 0.6582292318344116, Variance: 0.1367214173078537

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5870985925100425, Training Loss Force: 3.2202800251239903, time: 4.107600927352905
Validation Loss Energy: 2.7650922275840015, Validation Loss Force: 3.3135613375938826, time: 0.20441961288452148
Test Loss Energy: 9.280515355025816, Test Loss Force: 10.653715478997066, time: 9.031368017196655

Epoch 5, Batch 100/263, Loss: 0.6317122578620911, Variance: 0.1312047243118286
Epoch 5, Batch 200/263, Loss: 1.3164440393447876, Variance: 0.14519129693508148

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.588828782572946, Training Loss Force: 3.229952639052276, time: 3.940234899520874
Validation Loss Energy: 1.7566001295483258, Validation Loss Force: 3.2168813336647633, time: 0.2035198211669922
Test Loss Energy: 8.784656926717139, Test Loss Force: 10.930815890133246, time: 8.8786461353302

Epoch 6, Batch 100/263, Loss: 0.676891028881073, Variance: 0.13747772574424744
Epoch 6, Batch 200/263, Loss: 0.823456883430481, Variance: 0.13913804292678833

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5798341195721184, Training Loss Force: 3.229664870930567, time: 3.9693374633789062
Validation Loss Energy: 3.2562776067139874, Validation Loss Force: 3.2580749475260875, time: 0.22279000282287598
Test Loss Energy: 8.839134961772631, Test Loss Force: 10.655079188784837, time: 9.002894639968872

Epoch 7, Batch 100/263, Loss: 1.4102652072906494, Variance: 0.14331680536270142
Epoch 7, Batch 200/263, Loss: 0.5248628854751587, Variance: 0.13419097661972046

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5778112317635435, Training Loss Force: 3.212520678475749, time: 4.089370250701904
Validation Loss Energy: 1.9423853338665633, Validation Loss Force: 3.3737462693099003, time: 0.20223402976989746
Test Loss Energy: 8.640037802462555, Test Loss Force: 10.766833899955465, time: 8.787649154663086

Epoch 8, Batch 100/263, Loss: 0.6966688632965088, Variance: 0.14018841087818146
Epoch 8, Batch 200/263, Loss: 1.0362619161605835, Variance: 0.13472065329551697

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5875788047694086, Training Loss Force: 3.2324406484173385, time: 3.968545913696289
Validation Loss Energy: 2.0862169767953405, Validation Loss Force: 3.211802384047638, time: 0.20403051376342773
Test Loss Energy: 9.248117959581558, Test Loss Force: 10.903326099349096, time: 8.747803926467896

Epoch 9, Batch 100/263, Loss: 0.5244340896606445, Variance: 0.136790931224823
Epoch 9, Batch 200/263, Loss: 0.8374096155166626, Variance: 0.139915332198143

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.590089267100566, Training Loss Force: 3.2153670547815407, time: 4.158475160598755
Validation Loss Energy: 4.379064151869632, Validation Loss Force: 3.5835180175010883, time: 0.20430636405944824
Test Loss Energy: 9.82380007773081, Test Loss Force: 10.697584024070101, time: 8.78446340560913

Epoch 10, Batch 100/263, Loss: 0.719312310218811, Variance: 0.12758414447307587
Epoch 10, Batch 200/263, Loss: 1.2131844758987427, Variance: 0.1360958367586136

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.7345725211018013, Training Loss Force: 3.5443366907883562, time: 3.9707789421081543
Validation Loss Energy: 1.481697834308577, Validation Loss Force: 3.240782996059285, time: 0.2147834300994873
Test Loss Energy: 8.653274566863622, Test Loss Force: 10.736035035817558, time: 8.83734130859375

Epoch 11, Batch 100/263, Loss: 0.7157878279685974, Variance: 0.13158458471298218
Epoch 11, Batch 200/263, Loss: 0.8905304670333862, Variance: 0.13438482582569122

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.556200417312471, Training Loss Force: 3.222376855598703, time: 4.06006932258606
Validation Loss Energy: 3.45529456875835, Validation Loss Force: 3.1995464961204556, time: 0.2093794345855713
Test Loss Energy: 9.296870732551799, Test Loss Force: 10.83095363260518, time: 8.929694175720215

Epoch 12, Batch 100/263, Loss: 1.4142305850982666, Variance: 0.13457225263118744
Epoch 12, Batch 200/263, Loss: 0.47929972410202026, Variance: 0.13432711362838745

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5914956240483145, Training Loss Force: 3.210452994799583, time: 4.046559810638428
Validation Loss Energy: 1.9670934816407215, Validation Loss Force: 3.3231756123752647, time: 0.2059786319732666
Test Loss Energy: 8.473955550268098, Test Loss Force: 10.617980419947038, time: 8.848828554153442

Epoch 13, Batch 100/263, Loss: 0.7513412237167358, Variance: 0.13729706406593323
Epoch 13, Batch 200/263, Loss: 1.0674717426300049, Variance: 0.13103187084197998

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5767243773123893, Training Loss Force: 3.2130483975783917, time: 3.9858314990997314
Validation Loss Energy: 1.960524185162987, Validation Loss Force: 3.2743523292963843, time: 0.20313262939453125
Test Loss Energy: 9.08521644870115, Test Loss Force: 10.861942751789885, time: 8.980197191238403

Epoch 14, Batch 100/263, Loss: 0.5225998163223267, Variance: 0.13237623870372772
Epoch 14, Batch 200/263, Loss: 0.5726906061172485, Variance: 0.1330271065235138

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5883718760686367, Training Loss Force: 3.2279614680505837, time: 3.8964476585388184
Validation Loss Energy: 3.344134969209326, Validation Loss Force: 3.295148959518601, time: 0.21160459518432617
Test Loss Energy: 9.507805149203998, Test Loss Force: 10.535326318473187, time: 8.814610242843628

Epoch 15, Batch 100/263, Loss: 0.9496853947639465, Variance: 0.13131433725357056
Epoch 15, Batch 200/263, Loss: 0.5327779650688171, Variance: 0.13513003289699554

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5604904038337777, Training Loss Force: 3.221898642705071, time: 4.044453859329224
Validation Loss Energy: 2.5785373896297785, Validation Loss Force: 3.2607702239360554, time: 0.20728468894958496
Test Loss Energy: 8.964683639485546, Test Loss Force: 10.682508296980592, time: 8.899735450744629

Epoch 16, Batch 100/263, Loss: 0.9076727628707886, Variance: 0.1351013481616974
Epoch 16, Batch 200/263, Loss: 1.2793446779251099, Variance: 0.1355682611465454

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5898412850023274, Training Loss Force: 3.221301262025782, time: 4.2200305461883545
Validation Loss Energy: 1.506799616598906, Validation Loss Force: 3.338112481353247, time: 0.20199871063232422
Test Loss Energy: 8.569946246532043, Test Loss Force: 10.700164258174377, time: 8.8147451877594

Epoch 17, Batch 100/263, Loss: 0.8531997203826904, Variance: 0.13549324870109558
Epoch 17, Batch 200/263, Loss: 0.7385762929916382, Variance: 0.13992071151733398

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.581139862026973, Training Loss Force: 3.2216019788239847, time: 3.9862332344055176
Validation Loss Energy: 2.969020736581581, Validation Loss Force: 3.298470247965431, time: 0.20102953910827637
Test Loss Energy: 9.140573551233782, Test Loss Force: 10.98067448704606, time: 8.853582620620728

Epoch 18, Batch 100/263, Loss: 1.2743158340454102, Variance: 0.13858436048030853
Epoch 18, Batch 200/263, Loss: 0.37396180629730225, Variance: 0.13243353366851807

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.59627332584789, Training Loss Force: 3.2220311418533574, time: 3.9841294288635254
Validation Loss Energy: 1.8567224203381716, Validation Loss Force: 3.258423602874781, time: 0.20455217361450195
Test Loss Energy: 8.622319361005907, Test Loss Force: 10.569590935768703, time: 9.057495832443237

Epoch 19, Batch 100/263, Loss: 0.8434944748878479, Variance: 0.13955941796302795
Epoch 19, Batch 200/263, Loss: 1.0894172191619873, Variance: 0.12625835835933685

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5703374684547637, Training Loss Force: 3.2066639524744343, time: 4.00444221496582
Validation Loss Energy: 2.341320778526408, Validation Loss Force: 3.341402150781355, time: 0.20797109603881836
Test Loss Energy: 8.992090219625927, Test Loss Force: 10.583563189182648, time: 8.833441972732544

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–‚â–ƒâ–ˆâ–„â–‚â–‚â–‚â–„â–‡â–‚â–„â–â–ƒâ–…â–ƒâ–â–„â–‚â–ƒ
wandb:   test_error_force â–„â–…â–†â–…â–ƒâ–‡â–ƒâ–…â–‡â–„â–„â–†â–‚â–†â–â–ƒâ–„â–ˆâ–‚â–‚
wandb:          test_loss â–…â–„â–„â–ˆâ–„â–„â–ƒâ–ƒâ–†â–…â–ƒâ–…â–â–…â–ƒâ–ƒâ–â–„â–â–‚
wandb: train_error_energy â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–…â–â–‚â–â–‚â–â–‚â–‚â–‚â–
wandb:  train_error_force â–‚â–â–â–â–â–â–â–â–‚â–â–ˆâ–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–‡â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–ƒâ–‚â–†â–„â–‚â–…â–‚â–‚â–ˆâ–â–†â–‚â–‚â–…â–„â–â–…â–‚â–ƒ
wandb:  valid_error_force â–ƒâ–â–„â–ƒâ–ƒâ–â–‚â–„â–â–ˆâ–‚â–â–ƒâ–‚â–ƒâ–‚â–„â–ƒâ–‚â–„
wandb:         valid_loss â–„â–ƒâ–‚â–…â–ƒâ–â–„â–‚â–‚â–ˆâ–â–…â–‚â–‚â–„â–ƒâ–â–„â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 8387
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.99209
wandb:   test_error_force 10.58356
wandb:          test_loss 9.18559
wandb: train_error_energy 2.57034
wandb:  train_error_force 3.20666
wandb:         train_loss 0.87103
wandb: valid_error_energy 2.34132
wandb:  valid_error_force 3.3414
wandb:         valid_loss 0.80837
wandb: 
wandb: ğŸš€ View run al_63_91 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/763ogsu9
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_224057-763ogsu9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.532888889312744, Uncertainty Bias: -0.12510748207569122
0.00011062622 0.004966736
2.006332 5.8022895
(48745, 22, 3)
Found uncertainty sample 0 after 18 steps.
Found uncertainty sample 1 after 3780 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 1622 steps.
Found uncertainty sample 5 after 612 steps.
Found uncertainty sample 6 after 3951 steps.
Found uncertainty sample 7 after 1630 steps.
Found uncertainty sample 8 after 1070 steps.
Found uncertainty sample 9 after 3041 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 53 steps.
Found uncertainty sample 12 after 158 steps.
Found uncertainty sample 13 after 191 steps.
Found uncertainty sample 14 after 299 steps.
Found uncertainty sample 15 after 2034 steps.
Found uncertainty sample 16 after 2716 steps.
Found uncertainty sample 17 after 697 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 2267 steps.
Found uncertainty sample 20 after 2111 steps.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 1809 steps.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 1825 steps.
Found uncertainty sample 27 after 289 steps.
Found uncertainty sample 28 after 3179 steps.
Found uncertainty sample 29 after 189 steps.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 478 steps.
Found uncertainty sample 33 after 2789 steps.
Found uncertainty sample 34 after 984 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 1931 steps.
Did not find any uncertainty samples for sample 37.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 2507 steps.
Found uncertainty sample 40 after 329 steps.
Found uncertainty sample 41 after 1497 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 1222 steps.
Found uncertainty sample 46 after 3711 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 567 steps.
Found uncertainty sample 49 after 2473 steps.
Found uncertainty sample 50 after 1096 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 1541 steps.
Found uncertainty sample 54 after 487 steps.
Found uncertainty sample 55 after 2857 steps.
Found uncertainty sample 56 after 845 steps.
Found uncertainty sample 57 after 82 steps.
Found uncertainty sample 58 after 2478 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 932 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 1480 steps.
Found uncertainty sample 63 after 676 steps.
Found uncertainty sample 64 after 3938 steps.
Found uncertainty sample 65 after 2094 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 494 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 511 steps.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 2000 steps.
Found uncertainty sample 75 after 1571 steps.
Found uncertainty sample 76 after 1437 steps.
Found uncertainty sample 77 after 2286 steps.
Found uncertainty sample 78 after 835 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 1083 steps.
Found uncertainty sample 82 after 1525 steps.
Found uncertainty sample 83 after 3074 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 988 steps.
Found uncertainty sample 86 after 410 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 128 steps.
Found uncertainty sample 89 after 838 steps.
Found uncertainty sample 90 after 1972 steps.
Found uncertainty sample 91 after 2815 steps.
Found uncertainty sample 92 after 978 steps.
Found uncertainty sample 93 after 3212 steps.
Found uncertainty sample 94 after 400 steps.
Found uncertainty sample 95 after 389 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 853 steps.
Found uncertainty sample 98 after 43 steps.
Found uncertainty sample 99 after 2141 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_230843-5y06bgx2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_63_92
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5y06bgx2
Training model 92. Added 68 samples to the dataset.
Epoch 0, Batch 100/264, Loss: 0.7332885265350342, Variance: 0.17494705319404602
Epoch 0, Batch 200/264, Loss: 0.9249909520149231, Variance: 0.18094073235988617

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.323588143919453, Training Loss Force: 3.395169335652489, time: 3.956360340118408
Validation Loss Energy: 3.9925504168499417, Validation Loss Force: 3.239515919551298, time: 0.20245599746704102
Test Loss Energy: 9.43488637595152, Test Loss Force: 10.640584635161037, time: 8.653831720352173

Epoch 1, Batch 100/264, Loss: 1.0373700857162476, Variance: 0.1855837106704712
Epoch 1, Batch 200/264, Loss: 1.0236014127731323, Variance: 0.19001241028308868

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.056125985294023, Training Loss Force: 3.291616115646557, time: 4.06721568107605
Validation Loss Energy: 3.802143937077118, Validation Loss Force: 3.2032339417108497, time: 0.2096402645111084
Test Loss Energy: 9.131216699938522, Test Loss Force: 10.421206935584458, time: 8.65832781791687

Epoch 2, Batch 100/264, Loss: 0.8355867266654968, Variance: 0.18315383791923523
Epoch 2, Batch 200/264, Loss: 0.9413059949874878, Variance: 0.1894408017396927

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.044811058126087, Training Loss Force: 3.297328413405458, time: 4.19047474861145
Validation Loss Energy: 3.327509778233091, Validation Loss Force: 3.2871491251128537, time: 0.2117905616760254
Test Loss Energy: 9.059884510801778, Test Loss Force: 10.602086417554647, time: 8.70888614654541

Epoch 3, Batch 100/264, Loss: 1.0561273097991943, Variance: 0.17835009098052979
Epoch 3, Batch 200/264, Loss: 1.0226892232894897, Variance: 0.18985822796821594

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.0045358902213035, Training Loss Force: 3.2979796270350423, time: 4.011364459991455
Validation Loss Energy: 3.1228156881779823, Validation Loss Force: 3.258503808301145, time: 0.20545744895935059
Test Loss Energy: 9.055907803068495, Test Loss Force: 10.579289447803541, time: 8.68294358253479

Epoch 4, Batch 100/264, Loss: 0.6713862419128418, Variance: 0.17726552486419678
Epoch 4, Batch 200/264, Loss: 0.9305866956710815, Variance: 0.19244979321956635

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.002222407136571, Training Loss Force: 3.3113627410767146, time: 3.9984562397003174
Validation Loss Energy: 3.3922858805351646, Validation Loss Force: 3.3706337999881213, time: 0.2093358039855957
Test Loss Energy: 8.979531843008589, Test Loss Force: 10.450332778008955, time: 8.8463134765625

Epoch 5, Batch 100/264, Loss: 0.904124915599823, Variance: 0.18523767590522766
Epoch 5, Batch 200/264, Loss: 0.8697085380554199, Variance: 0.18869422376155853

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.001999508152226, Training Loss Force: 3.2936722172367423, time: 3.9147982597351074
Validation Loss Energy: 3.1651185951904894, Validation Loss Force: 3.238854013248113, time: 0.19918131828308105
Test Loss Energy: 9.13749593721109, Test Loss Force: 10.61268456889777, time: 8.686347484588623

Epoch 6, Batch 100/264, Loss: 1.147470474243164, Variance: 0.18385067582130432
Epoch 6, Batch 200/264, Loss: 0.9088035225868225, Variance: 0.1912984848022461

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.02436287433155, Training Loss Force: 3.2785290621639334, time: 3.9857177734375
Validation Loss Energy: 3.0042974077619955, Validation Loss Force: 3.3287286246040706, time: 0.20669007301330566
Test Loss Energy: 9.060260721398992, Test Loss Force: 10.475495921037641, time: 8.871464967727661

Epoch 7, Batch 100/264, Loss: 0.8979924321174622, Variance: 0.18773970007896423
Epoch 7, Batch 200/264, Loss: 0.7748121023178101, Variance: 0.18559905886650085

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.003582516247582, Training Loss Force: 3.342390881916429, time: 4.065386772155762
Validation Loss Energy: 2.5180998175007914, Validation Loss Force: 3.7819800434999697, time: 0.19985198974609375
Test Loss Energy: 8.878851021057487, Test Loss Force: 10.732583827268662, time: 8.700141429901123

Epoch 8, Batch 100/264, Loss: 0.8369470834732056, Variance: 0.1810140311717987
Epoch 8, Batch 200/264, Loss: 0.93548983335495, Variance: 0.18725299835205078

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.023514420541192, Training Loss Force: 3.2989033446836116, time: 4.004539728164673
Validation Loss Energy: 3.4163305410552014, Validation Loss Force: 3.352313517871596, time: 0.2003178596496582
Test Loss Energy: 9.16852460523405, Test Loss Force: 10.60304167052229, time: 8.680377006530762

Epoch 9, Batch 100/264, Loss: 0.896648108959198, Variance: 0.18658918142318726
Epoch 9, Batch 200/264, Loss: 0.9372495412826538, Variance: 0.19375640153884888

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 3.980691366838235, Training Loss Force: 3.2656687044276254, time: 4.2456374168396
Validation Loss Energy: 3.204199969533542, Validation Loss Force: 3.2700068518459444, time: 0.20082950592041016
Test Loss Energy: 8.928028795981465, Test Loss Force: 10.510665927401798, time: 9.84706163406372

Epoch 10, Batch 100/264, Loss: 1.0657265186309814, Variance: 0.1690526306629181
slurmstepd: error: *** JOB 5123810 ON aimat01 CANCELLED AT 2024-11-30T23:11:00 ***
Epoch 10, Batch 200/264, Loss: 0.7966381907463074, Variance: 0.1686599850654602
