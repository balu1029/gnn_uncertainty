wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241121_173054-nbogcdqn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-river-63
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/nbogcdqn
['H1', 'CH3', 'H2', 'H3', 'C', 'O', 'N', 'H', 'CA', 'HA', 'CB', 'HB1', 'HB2', 'HB3', 'C', 'O', 'N', 'H', 'C', 'H1', 'H2', 'H3']
Uncertainty Slope: 0.6569409966468811, Uncertainty Bias: 0.026700109243392944
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
2.5987625e-05 0.000451684
0.029495241 0.1928166

Training and Validation Results of Epoch Initital validation:
================================
Training Loss Energy: 0.0, Training Loss Force: 0.0, time: 0
Validation Loss Energy: 0.0, Validation Loss Force: 0.0, time: 0
Test Loss Energy: 12.4286433428503, Test Loss Force: 10.720735772221673, time: 14.440657615661621

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.051 MB of 0.051 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–
wandb:  test_error_energy â–
wandb:   test_error_force â–
wandb:          test_loss â–
wandb: train_error_energy â–
wandb:  train_error_force â–
wandb:         train_loss â–
wandb: valid_error_energy â–
wandb:  valid_error_force â–
wandb:         valid_loss â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 800
wandb:  test_error_energy 12.42864
wandb:   test_error_force 10.72074
wandb:          test_loss 6.04317
wandb: train_error_energy 0.0
wandb:  train_error_force 0.0
wandb:         train_loss 0.0
wandb: valid_error_energy 0.0
wandb:  valid_error_force 0.0
wandb:         valid_loss 0.0
wandb: 
wandb: ğŸš€ View run serene-river-63 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/nbogcdqn
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241121_173054-nbogcdqn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Found uncertainty sample 0 after 1465 steps.
Found uncertainty sample 1 after 1296 steps.
Found uncertainty sample 2 after 929 steps.
Found uncertainty sample 3 after 2460 steps.
Found uncertainty sample 4 after 191 steps.
Found uncertainty sample 6 after 92 steps.
Found uncertainty sample 7 after 25 steps.
Found uncertainty sample 8 after 9 steps.
Found uncertainty sample 9 after 8 steps.
Found uncertainty sample 10 after 145 steps.
Found uncertainty sample 11 after 6 steps.
Found uncertainty sample 12 after 196 steps.
Found uncertainty sample 13 after 252 steps.
Found uncertainty sample 14 after 76 steps.
Found uncertainty sample 15 after 3960 steps.
Found uncertainty sample 16 after 725 steps.
Found uncertainty sample 17 after 19 steps.
Found uncertainty sample 18 after 155 steps.
Found uncertainty sample 19 after 2121 steps.
Found uncertainty sample 20 after 427 steps.
Found uncertainty sample 21 after 845 steps.
Found uncertainty sample 22 after 80 steps.
Found uncertainty sample 23 after 812 steps.
Found uncertainty sample 24 after 326 steps.
Found uncertainty sample 25 after 1053 steps.
Found uncertainty sample 26 after 853 steps.
Found uncertainty sample 27 after 1062 steps.
Found uncertainty sample 28 after 590 steps.
Found uncertainty sample 29 after 157 steps.
Found uncertainty sample 30 after 1130 steps.
Found uncertainty sample 31 after 459 steps.
Found uncertainty sample 32 after 1345 steps.
Found uncertainty sample 33 after 2656 steps.
Found uncertainty sample 34 after 608 steps.
Found uncertainty sample 35 after 69 steps.
Found uncertainty sample 36 after 2787 steps.
Found uncertainty sample 37 after 2902 steps.
Found uncertainty sample 38 after 95 steps.
Found uncertainty sample 39 after 1360 steps.
Found uncertainty sample 40 after 3007 steps.
Found uncertainty sample 41 after 471 steps.
Found uncertainty sample 42 after 191 steps.
Found uncertainty sample 43 after 2218 steps.
Found uncertainty sample 44 after 2148 steps.
Found uncertainty sample 45 after 1678 steps.
Found uncertainty sample 46 after 901 steps.
Found uncertainty sample 47 after 358 steps.
Found uncertainty sample 48 after 777 steps.
Found uncertainty sample 49 after 854 steps.
Found uncertainty sample 50 after 57 steps.
Found uncertainty sample 51 after 3944 steps.
Found uncertainty sample 52 after 7 steps.
Found uncertainty sample 53 after 532 steps.
Found uncertainty sample 54 after 89 steps.
Found uncertainty sample 55 after 1399 steps.
Found uncertainty sample 56 after 1566 steps.
Found uncertainty sample 57 after 1390 steps.
Found uncertainty sample 58 after 13 steps.
Found uncertainty sample 59 after 200 steps.
Found uncertainty sample 60 after 303 steps.
Found uncertainty sample 61 after 188 steps.
Found uncertainty sample 62 after 261 steps.
Found uncertainty sample 63 after 79 steps.
Found uncertainty sample 64 after 1669 steps.
Found uncertainty sample 65 after 93 steps.
Found uncertainty sample 66 after 93 steps.
Found uncertainty sample 67 after 1767 steps.
Found uncertainty sample 68 after 876 steps.
Found uncertainty sample 69 after 242 steps.
Found uncertainty sample 70 after 119 steps.
Found uncertainty sample 71 after 359 steps.
Found uncertainty sample 72 after 76 steps.
Found uncertainty sample 73 after 220 steps.
Found uncertainty sample 74 after 535 steps.
Found uncertainty sample 75 after 49 steps.
Found uncertainty sample 76 after 434 steps.
Found uncertainty sample 77 after 2068 steps.
Found uncertainty sample 78 after 58 steps.
Found uncertainty sample 79 after 179 steps.
Found uncertainty sample 80 after 35 steps.
Found uncertainty sample 81 after 373 steps.
Found uncertainty sample 82 after 1407 steps.
Found uncertainty sample 83 after 367 steps.
Found uncertainty sample 84 after 21 steps.
Found uncertainty sample 85 after 110 steps.
Found uncertainty sample 86 after 235 steps.
Found uncertainty sample 87 after 528 steps.
Found uncertainty sample 88 after 1795 steps.
Found uncertainty sample 89 after 2200 steps.
Found uncertainty sample 90 after 404 steps.
Found uncertainty sample 92 after 112 steps.
Found uncertainty sample 93 after 217 steps.
Found uncertainty sample 94 after 1801 steps.
Found uncertainty sample 95 after 86 steps.
Found uncertainty sample 96 after 13 steps.
Found uncertainty sample 97 after 71 steps.
Found uncertainty sample 98 after 817 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241121_181124-dkuh4zhk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_0
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/dkuh4zhk
Training model 0. Added 98 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1581407616663695, Training Loss Force: 2.793587303160978, time: 1.043241024017334
Validation Loss Energy: 2.159412644843235, Validation Loss Force: 2.713356253941068, time: 0.07518768310546875
Test Loss Energy: 13.391193763641798, Test Loss Force: 10.628930677660875, time: 16.419189453125


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.3818660293364153, Training Loss Force: 2.530025867212605, time: 1.0456979274749756
Validation Loss Energy: 1.1891159206189656, Validation Loss Force: 2.6507425807827576, time: 0.07198739051818848
Test Loss Energy: 12.0510516000404, Test Loss Force: 10.552255268402941, time: 16.565202474594116


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.1844970209646426, Training Loss Force: 2.445880827222741, time: 1.0361807346343994
Validation Loss Energy: 1.1320440371645062, Validation Loss Force: 2.610457724239794, time: 0.07410573959350586
Test Loss Energy: 12.278882468585115, Test Loss Force: 10.522306021124558, time: 16.46475386619568


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.1090904805097943, Training Loss Force: 2.437269368387947, time: 1.022653579711914
Validation Loss Energy: 1.2464531143607982, Validation Loss Force: 2.6092446227327795, time: 0.07560205459594727
Test Loss Energy: 12.543762615126884, Test Loss Force: 10.480625122301191, time: 16.57561421394348


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.140439312092248, Training Loss Force: 2.4363103641863133, time: 0.9793875217437744
Validation Loss Energy: 1.1898872465404595, Validation Loss Force: 2.598900304889558, time: 0.06823277473449707
Test Loss Energy: 12.434316798802014, Test Loss Force: 10.5291284731854, time: 16.850389003753662


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2732386539318894, Training Loss Force: 2.432900504315209, time: 1.2262341976165771
Validation Loss Energy: 1.332777266184937, Validation Loss Force: 2.604849133792165, time: 0.0711982250213623
Test Loss Energy: 12.55851502195328, Test Loss Force: 10.472671734813991, time: 16.665144205093384


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.2205794765145066, Training Loss Force: 2.420763326081203, time: 1.0098521709442139
Validation Loss Energy: 1.7923660493761722, Validation Loss Force: 2.597346296920128, time: 0.06904387474060059
Test Loss Energy: 13.06024713590556, Test Loss Force: 10.465165658164175, time: 16.819592714309692


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.2899655939719803, Training Loss Force: 2.4385963731155726, time: 1.0528016090393066
Validation Loss Energy: 1.8816709617222733, Validation Loss Force: 2.593521286331381, time: 0.0730586051940918
Test Loss Energy: 13.016013268873127, Test Loss Force: 10.447394254093393, time: 16.65294361114502


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.2620064885959807, Training Loss Force: 2.4351921197354645, time: 1.0310287475585938
Validation Loss Energy: 1.3223452166982201, Validation Loss Force: 2.6145298975805216, time: 0.07358717918395996
Test Loss Energy: 11.93699032563811, Test Loss Force: 10.461350344758443, time: 16.214293718338013


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.1191468950998005, Training Loss Force: 2.4534667514960864, time: 1.006805181503296
Validation Loss Energy: 1.4609503026330362, Validation Loss Force: 2.5996551532942536, time: 0.07098102569580078
Test Loss Energy: 12.829766304005492, Test Loss Force: 10.536861168735419, time: 14.967876195907593


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.4626050504979418, Training Loss Force: 2.4494343325503762, time: 0.9831662178039551
Validation Loss Energy: 1.6960300247436821, Validation Loss Force: 2.598542920910344, time: 0.06388664245605469
Test Loss Energy: 13.094070593161675, Test Loss Force: 10.396332516328968, time: 14.810901641845703


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.204640254423661, Training Loss Force: 2.4304938961860216, time: 0.9701554775238037
Validation Loss Energy: 1.2709112046840794, Validation Loss Force: 2.5860692180246154, time: 0.06420063972473145
Test Loss Energy: 11.869786016766016, Test Loss Force: 10.53467995774065, time: 14.659782886505127


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.1177548192905116, Training Loss Force: 2.436744561068403, time: 0.9953913688659668
Validation Loss Energy: 1.287227286351938, Validation Loss Force: 2.5817285056938992, time: 0.06554627418518066
Test Loss Energy: 12.585015639447954, Test Loss Force: 10.486045537501791, time: 14.786765575408936


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.5785407115726022, Training Loss Force: 2.425124237684443, time: 0.9988317489624023
Validation Loss Energy: 1.6657394501465415, Validation Loss Force: 2.5931883959582187, time: 0.06621217727661133
Test Loss Energy: 11.703108214373348, Test Loss Force: 10.449645651335604, time: 14.66041374206543


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.8702818424721286, Training Loss Force: 2.430757305833185, time: 1.0138375759124756
Validation Loss Energy: 1.506688771898063, Validation Loss Force: 2.5854675659923183, time: 0.06608700752258301
Test Loss Energy: 11.78722046066898, Test Loss Force: 10.353906750740842, time: 14.765989303588867


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.9725448127992862, Training Loss Force: 2.4509064857512692, time: 0.9818747043609619
Validation Loss Energy: 1.5830556048546032, Validation Loss Force: 2.5940634857017146, time: 0.06266522407531738
Test Loss Energy: 11.889284335478367, Test Loss Force: 10.460412774752081, time: 15.131279468536377


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.4911795497735303, Training Loss Force: 2.414283749364338, time: 1.0307776927947998
Validation Loss Energy: 1.1964350554456122, Validation Loss Force: 2.5928013969825963, time: 0.07764101028442383
Test Loss Energy: 12.407950841347693, Test Loss Force: 10.442010361295951, time: 16.471930503845215


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.324301748851849, Training Loss Force: 2.42522680863602, time: 0.9902973175048828
Validation Loss Energy: 1.3564193854229658, Validation Loss Force: 2.584836298963613, time: 0.07277536392211914
Test Loss Energy: 11.877797456780915, Test Loss Force: 10.47271080333974, time: 16.93718695640564


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.3405462682348177, Training Loss Force: 2.4074361880264195, time: 1.0131831169128418
Validation Loss Energy: 1.1019882671817465, Validation Loss Force: 2.576325661537107, time: 0.07321310043334961
Test Loss Energy: 12.005366298571547, Test Loss Force: 10.46623276798634, time: 16.795960903167725


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.1529823525595246, Training Loss Force: 2.4084513207577656, time: 1.0688822269439697
Validation Loss Energy: 1.2167885740674333, Validation Loss Force: 2.5832248423545336, time: 0.07621312141418457
Test Loss Energy: 12.573277021040058, Test Loss Force: 10.360983362727552, time: 16.98912739753723

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‚â–ƒâ–„â–„â–…â–‡â–†â–‚â–†â–‡â–‚â–…â–â–â–‚â–„â–‚â–‚â–…
wandb:   test_error_force â–ˆâ–†â–…â–„â–…â–„â–„â–ƒâ–„â–†â–‚â–†â–„â–ƒâ–â–„â–ƒâ–„â–„â–
wandb:          test_loss â–ˆâ–…â–…â–…â–†â–„â–…â–„â–ƒâ–†â–„â–ƒâ–…â–‚â–â–ƒâ–„â–ƒâ–ƒâ–‚
wandb: train_error_energy â–ˆâ–‚â–â–â–â–‚â–â–‚â–‚â–â–‚â–â–â–ƒâ–„â–„â–‚â–‚â–‚â–
wandb:  train_error_force â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–‚â–â–‚â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–â–
wandb: valid_error_energy â–ˆâ–‚â–â–‚â–‚â–ƒâ–†â–†â–‚â–ƒâ–…â–‚â–‚â–…â–„â–„â–‚â–ƒâ–â–‚
wandb:  valid_error_force â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–
wandb:         valid_loss â–ˆâ–„â–‚â–ƒâ–…â–‚â–„â–„â–‚â–†â–ƒâ–â–ƒâ–…â–†â–…â–†â–‚â–â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 888
wandb:                 lr 0.0001
wandb:  test_error_energy 12.57328
wandb:   test_error_force 10.36098
wandb:          test_loss 5.8393
wandb: train_error_energy 1.15298
wandb:  train_error_force 2.40845
wandb:         train_loss 1.07835
wandb: valid_error_energy 1.21679
wandb:  valid_error_force 2.58322
wandb:         valid_loss 1.26708
wandb: 
wandb: ğŸš€ View run al_48_0 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/dkuh4zhk
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241121_181124-dkuh4zhk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6945012807846069, Uncertainty Bias: 0.02352568507194519
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
2.1457672e-06 0.0008931309
0.028302986 0.23415233
Found uncertainty sample 0 after 230 steps.
Found uncertainty sample 1 after 173 steps.
Found uncertainty sample 2 after 608 steps.
Found uncertainty sample 3 after 674 steps.
Found uncertainty sample 4 after 1035 steps.
Found uncertainty sample 5 after 2507 steps.
Found uncertainty sample 6 after 91 steps.
Found uncertainty sample 7 after 812 steps.
Found uncertainty sample 8 after 41 steps.
Found uncertainty sample 10 after 377 steps.
Found uncertainty sample 11 after 1123 steps.
Found uncertainty sample 12 after 972 steps.
Found uncertainty sample 14 after 1490 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 1063 steps.
Found uncertainty sample 17 after 2249 steps.
Found uncertainty sample 18 after 70 steps.
Found uncertainty sample 19 after 516 steps.
Found uncertainty sample 20 after 983 steps.
Found uncertainty sample 21 after 1364 steps.
Found uncertainty sample 22 after 87 steps.
Found uncertainty sample 23 after 758 steps.
Found uncertainty sample 24 after 71 steps.
Found uncertainty sample 25 after 744 steps.
Found uncertainty sample 26 after 7 steps.
Found uncertainty sample 27 after 2093 steps.
Found uncertainty sample 28 after 658 steps.
Found uncertainty sample 29 after 23 steps.
Found uncertainty sample 31 after 496 steps.
Found uncertainty sample 32 after 2603 steps.
Found uncertainty sample 33 after 26 steps.
Found uncertainty sample 35 after 70 steps.
Found uncertainty sample 36 after 1160 steps.
Found uncertainty sample 37 after 57 steps.
Found uncertainty sample 38 after 649 steps.
Found uncertainty sample 39 after 134 steps.
Found uncertainty sample 40 after 287 steps.
Found uncertainty sample 41 after 1377 steps.
Found uncertainty sample 42 after 1228 steps.
Found uncertainty sample 43 after 1021 steps.
Found uncertainty sample 44 after 431 steps.
Found uncertainty sample 45 after 2725 steps.
Found uncertainty sample 46 after 1507 steps.
Found uncertainty sample 47 after 438 steps.
Found uncertainty sample 49 after 15 steps.
Found uncertainty sample 51 after 64 steps.
Found uncertainty sample 53 after 1623 steps.
Found uncertainty sample 54 after 2077 steps.
Found uncertainty sample 56 after 668 steps.
Found uncertainty sample 57 after 2180 steps.
Found uncertainty sample 58 after 404 steps.
Found uncertainty sample 62 after 1530 steps.
Found uncertainty sample 63 after 7 steps.
Found uncertainty sample 64 after 1527 steps.
Found uncertainty sample 65 after 52 steps.
Found uncertainty sample 66 after 2322 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 126 steps.
Found uncertainty sample 69 after 642 steps.
Found uncertainty sample 70 after 819 steps.
Found uncertainty sample 71 after 159 steps.
Found uncertainty sample 72 after 139 steps.
Found uncertainty sample 73 after 1432 steps.
Found uncertainty sample 74 after 682 steps.
Found uncertainty sample 75 after 19 steps.
Found uncertainty sample 76 after 1336 steps.
Found uncertainty sample 77 after 362 steps.
Found uncertainty sample 78 after 3017 steps.
Found uncertainty sample 79 after 412 steps.
Found uncertainty sample 80 after 1870 steps.
Found uncertainty sample 81 after 3544 steps.
Found uncertainty sample 82 after 2109 steps.
Found uncertainty sample 83 after 18 steps.
Found uncertainty sample 84 after 115 steps.
Found uncertainty sample 85 after 20 steps.
Found uncertainty sample 86 after 525 steps.
Found uncertainty sample 87 after 3260 steps.
Found uncertainty sample 88 after 1447 steps.
Found uncertainty sample 89 after 3207 steps.
Found uncertainty sample 90 after 310 steps.
Found uncertainty sample 91 after 39 steps.
Found uncertainty sample 92 after 685 steps.
Found uncertainty sample 93 after 8 steps.
Found uncertainty sample 94 after 345 steps.
Found uncertainty sample 95 after 155 steps.
Found uncertainty sample 96 after 1256 steps.
Found uncertainty sample 97 after 1777 steps.
Found uncertainty sample 98 after 783 steps.
Found uncertainty sample 99 after 883 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241121_191509-8to6s453
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_1
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/8to6s453
Training model 1. Added 90 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.971700229281403, Training Loss Force: 3.082574207584646, time: 1.0882954597473145
Validation Loss Energy: 1.4128179173801396, Validation Loss Force: 2.71596437764732, time: 0.07110142707824707
Test Loss Energy: 11.607115466288995, Test Loss Force: 10.502157238038867, time: 14.902683734893799


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5375474634766928, Training Loss Force: 2.6368042679982517, time: 1.0778052806854248
Validation Loss Energy: 1.2935962919329105, Validation Loss Force: 2.6867922701701747, time: 0.07332134246826172
Test Loss Energy: 12.17868609075745, Test Loss Force: 10.303952375784297, time: 15.016519546508789


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3945123748836432, Training Loss Force: 2.5819822910572516, time: 1.115842342376709
Validation Loss Energy: 1.1221305841186553, Validation Loss Force: 2.644950065620592, time: 0.06773734092712402
Test Loss Energy: 11.837684568435417, Test Loss Force: 10.289606080174245, time: 14.927735567092896


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3428497624639861, Training Loss Force: 2.573928858033216, time: 1.070549488067627
Validation Loss Energy: 1.4639765989953408, Validation Loss Force: 2.642421657468582, time: 0.06822729110717773
Test Loss Energy: 11.702183841302752, Test Loss Force: 10.320048006837037, time: 15.76991868019104


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4272637739555671, Training Loss Force: 2.551627156773543, time: 1.0752544403076172
Validation Loss Energy: 2.293328541233958, Validation Loss Force: 2.620667995257167, time: 0.07590770721435547
Test Loss Energy: 13.082539427786132, Test Loss Force: 10.27152523360743, time: 15.965537786483765


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.3375699088916597, Training Loss Force: 2.5557673519469133, time: 1.0748229026794434
Validation Loss Energy: 1.36394315319472, Validation Loss Force: 2.6405547993491236, time: 0.07583189010620117
Test Loss Energy: 11.752027888943626, Test Loss Force: 10.309182288787003, time: 15.8133065700531


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.1913654751715173, Training Loss Force: 2.543098698632487, time: 1.071824550628662
Validation Loss Energy: 1.2482709852681526, Validation Loss Force: 2.6244740789685106, time: 0.07282400131225586
Test Loss Energy: 12.435309343605466, Test Loss Force: 10.321815920580823, time: 15.690608024597168


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.3602998334461736, Training Loss Force: 2.58932794506089, time: 1.0703377723693848
Validation Loss Energy: 1.8481003592710434, Validation Loss Force: 2.6336046272972795, time: 0.07316064834594727
Test Loss Energy: 12.872897863169399, Test Loss Force: 10.281743574341181, time: 15.894325017929077


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.4051826767650653, Training Loss Force: 2.5626424844965796, time: 1.042384386062622
Validation Loss Energy: 1.1313466373994605, Validation Loss Force: 2.62913721622253, time: 0.06985688209533691
Test Loss Energy: 12.12744112076788, Test Loss Force: 10.323050067236647, time: 15.914098739624023


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.4222998591698093, Training Loss Force: 2.544280920956974, time: 1.0962932109832764
Validation Loss Energy: 1.1241491912610637, Validation Loss Force: 2.6252908293957735, time: 0.07666444778442383
Test Loss Energy: 12.070562570090821, Test Loss Force: 10.269165635481597, time: 15.752790927886963


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6044730222899164, Training Loss Force: 2.5683005074946044, time: 1.0626134872436523
Validation Loss Energy: 1.2118875148309898, Validation Loss Force: 2.6532097549196374, time: 0.07944202423095703
Test Loss Energy: 11.917597083446498, Test Loss Force: 10.355793987167477, time: 15.830327272415161


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.5754279978406525, Training Loss Force: 2.5833690385837107, time: 1.076125144958496
Validation Loss Energy: 1.3102879051153764, Validation Loss Force: 2.6411453179241238, time: 0.07297444343566895
Test Loss Energy: 12.224064901553716, Test Loss Force: 10.287627957430244, time: 15.771392583847046


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.3247804719313188, Training Loss Force: 2.581031141843232, time: 1.105269432067871
Validation Loss Energy: 1.5344767679641214, Validation Loss Force: 2.6349555610056234, time: 0.07561039924621582
Test Loss Energy: 11.619972807128633, Test Loss Force: 10.276228173845642, time: 15.929004192352295


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.642049130221295, Training Loss Force: 2.552150967982149, time: 1.0701930522918701
Validation Loss Energy: 1.2679314461732736, Validation Loss Force: 2.6417476379638587, time: 0.07299351692199707
Test Loss Energy: 11.772156778697877, Test Loss Force: 10.273925187309995, time: 15.766715288162231


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.2963608191655496, Training Loss Force: 2.556145932592245, time: 1.1095645427703857
Validation Loss Energy: 1.2894542004803617, Validation Loss Force: 2.6162932145471234, time: 0.0711967945098877
Test Loss Energy: 12.23544675502594, Test Loss Force: 10.23968345713842, time: 15.908764123916626


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.1615960541189827, Training Loss Force: 2.5414121666068517, time: 1.0871739387512207
Validation Loss Energy: 1.5784720718202525, Validation Loss Force: 2.615089019805946, time: 0.07263040542602539
Test Loss Energy: 12.49531543282619, Test Loss Force: 10.226681514255938, time: 15.867993593215942


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6306796508130978, Training Loss Force: 2.5419565208398884, time: 1.0891172885894775
Validation Loss Energy: 1.4542232450654118, Validation Loss Force: 2.6190647774064866, time: 0.07609200477600098
Test Loss Energy: 12.515872080502026, Test Loss Force: 10.271271922389086, time: 15.94553542137146


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.9007675184024893, Training Loss Force: 2.566343777234699, time: 1.0995042324066162
Validation Loss Energy: 2.007070876626988, Validation Loss Force: 2.6530400810304404, time: 0.0754692554473877
Test Loss Energy: 11.52888287456904, Test Loss Force: 10.222782808340536, time: 16.20232605934143


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.348877945981332, Training Loss Force: 2.543796249237922, time: 1.103306531906128
Validation Loss Energy: 1.1393151867902052, Validation Loss Force: 2.6182925608084093, time: 0.07177400588989258
Test Loss Energy: 11.954957202908034, Test Loss Force: 10.208500296365626, time: 15.900619983673096


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.1897008563038478, Training Loss Force: 2.536590101045645, time: 1.0938234329223633
Validation Loss Energy: 1.1189471946738987, Validation Loss Force: 2.6320898090975327, time: 0.07133769989013672
Test Loss Energy: 12.064742376337753, Test Loss Force: 10.222051390325868, time: 15.938627243041992

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–„â–‚â–‚â–ˆâ–‚â–…â–‡â–„â–ƒâ–ƒâ–„â–â–‚â–„â–…â–…â–â–ƒâ–ƒ
wandb:   test_error_force â–ˆâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–„â–‚â–…â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–â–â–
wandb:          test_loss â–ˆâ–…â–…â–…â–†â–ƒâ–†â–†â–…â–ƒâ–†â–„â–‚â–ƒâ–ƒâ–ƒâ–…â–â–‚â–‚
wandb: train_error_energy â–ˆâ–‚â–â–â–â–â–â–â–â–â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–â–â–â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–‚â–â–
wandb: valid_error_energy â–ƒâ–‚â–â–ƒâ–ˆâ–‚â–‚â–…â–â–â–‚â–‚â–ƒâ–‚â–‚â–„â–ƒâ–†â–â–
wandb:  valid_error_force â–ˆâ–†â–ƒâ–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–„â–ƒâ–‚â–ƒâ–â–â–â–„â–â–‚
wandb:         valid_loss â–†â–ƒâ–†â–…â–„â–‚â–ƒâ–ƒâ–‚â–„â–„â–„â–ƒâ–‚â–‚â–ƒâ–‚â–ˆâ–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 969
wandb:                 lr 0.0001
wandb:  test_error_energy 12.06474
wandb:   test_error_force 10.22205
wandb:          test_loss 5.73149
wandb: train_error_energy 1.1897
wandb:  train_error_force 2.53659
wandb:         train_loss 1.13737
wandb: valid_error_energy 1.11895
wandb:  valid_error_force 2.63209
wandb:         valid_loss 1.27945
wandb: 
wandb: ğŸš€ View run al_48_1 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/8to6s453
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241121_191509-8to6s453/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.708550751209259, Uncertainty Bias: 0.02223677933216095
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
4.7683716e-07 0.00042659044
0.023927094 0.23285812
Found uncertainty sample 0 after 34 steps.
Found uncertainty sample 1 after 902 steps.
Found uncertainty sample 2 after 225 steps.
Found uncertainty sample 3 after 12 steps.
Found uncertainty sample 4 after 335 steps.
Found uncertainty sample 5 after 1832 steps.
Found uncertainty sample 6 after 2098 steps.
Found uncertainty sample 7 after 612 steps.
Found uncertainty sample 8 after 3693 steps.
Found uncertainty sample 9 after 249 steps.
Found uncertainty sample 10 after 3191 steps.
Found uncertainty sample 11 after 107 steps.
Found uncertainty sample 12 after 902 steps.
Found uncertainty sample 13 after 5 steps.
Found uncertainty sample 14 after 88 steps.
Found uncertainty sample 15 after 33 steps.
Found uncertainty sample 16 after 1361 steps.
Found uncertainty sample 17 after 95 steps.
Found uncertainty sample 18 after 55 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 18 steps.
Found uncertainty sample 21 after 574 steps.
Found uncertainty sample 22 after 1195 steps.
Found uncertainty sample 23 after 502 steps.
Found uncertainty sample 24 after 2045 steps.
Found uncertainty sample 25 after 874 steps.
Found uncertainty sample 26 after 455 steps.
Found uncertainty sample 27 after 401 steps.
Found uncertainty sample 28 after 542 steps.
Found uncertainty sample 29 after 3543 steps.
Found uncertainty sample 30 after 6 steps.
Found uncertainty sample 31 after 1410 steps.
Found uncertainty sample 34 after 49 steps.
Found uncertainty sample 35 after 1905 steps.
Found uncertainty sample 36 after 1306 steps.
Found uncertainty sample 37 after 2126 steps.
Found uncertainty sample 38 after 491 steps.
Found uncertainty sample 39 after 1686 steps.
Found uncertainty sample 40 after 268 steps.
Found uncertainty sample 41 after 1564 steps.
Found uncertainty sample 42 after 392 steps.
Found uncertainty sample 43 after 108 steps.
Found uncertainty sample 44 after 269 steps.
Found uncertainty sample 45 after 517 steps.
Found uncertainty sample 46 after 222 steps.
Found uncertainty sample 47 after 150 steps.
Found uncertainty sample 48 after 260 steps.
Found uncertainty sample 49 after 444 steps.
Found uncertainty sample 50 after 263 steps.
Found uncertainty sample 51 after 645 steps.
Found uncertainty sample 52 after 20 steps.
Found uncertainty sample 54 after 265 steps.
Found uncertainty sample 55 after 231 steps.
Found uncertainty sample 56 after 2182 steps.
Found uncertainty sample 57 after 326 steps.
Found uncertainty sample 58 after 445 steps.
Found uncertainty sample 59 after 12 steps.
Found uncertainty sample 60 after 1020 steps.
Found uncertainty sample 61 after 7 steps.
Found uncertainty sample 62 after 140 steps.
Found uncertainty sample 63 after 509 steps.
Found uncertainty sample 64 after 644 steps.
Found uncertainty sample 67 after 8 steps.
Found uncertainty sample 68 after 475 steps.
Found uncertainty sample 69 after 381 steps.
Found uncertainty sample 70 after 80 steps.
Found uncertainty sample 71 after 2650 steps.
Found uncertainty sample 72 after 336 steps.
Found uncertainty sample 73 after 379 steps.
Found uncertainty sample 74 after 2371 steps.
Found uncertainty sample 75 after 59 steps.
Found uncertainty sample 76 after 482 steps.
Found uncertainty sample 77 after 295 steps.
Found uncertainty sample 78 after 1760 steps.
Found uncertainty sample 80 after 77 steps.
Found uncertainty sample 81 after 598 steps.
Found uncertainty sample 82 after 2050 steps.
Found uncertainty sample 83 after 627 steps.
Found uncertainty sample 84 after 2347 steps.
Found uncertainty sample 85 after 1088 steps.
Found uncertainty sample 86 after 254 steps.
Found uncertainty sample 87 after 1955 steps.
Found uncertainty sample 88 after 4 steps.
Found uncertainty sample 89 after 1560 steps.
Found uncertainty sample 91 after 530 steps.
Found uncertainty sample 92 after 2415 steps.
Found uncertainty sample 93 after 1570 steps.
Found uncertainty sample 94 after 236 steps.
Found uncertainty sample 95 after 541 steps.
Found uncertainty sample 96 after 2669 steps.
Found uncertainty sample 97 after 19 steps.
Found uncertainty sample 99 after 640 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241121_201031-jpuvie6w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_2
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/jpuvie6w
Training model 2. Added 92 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.022022442797778, Training Loss Force: 3.0068936544309546, time: 1.2422401905059814
Validation Loss Energy: 1.4359693851551165, Validation Loss Force: 2.7770663499739263, time: 0.0739593505859375
Test Loss Energy: 11.795033671516878, Test Loss Force: 10.289079606375164, time: 14.916537523269653


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5012583218278386, Training Loss Force: 2.740147022863314, time: 1.1118574142456055
Validation Loss Energy: 1.2982331542277108, Validation Loss Force: 2.716128484807869, time: 0.07223033905029297
Test Loss Energy: 11.919012541077457, Test Loss Force: 10.123208341940027, time: 15.083296537399292


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5111096856066126, Training Loss Force: 2.6948563075129983, time: 1.141831874847412
Validation Loss Energy: 2.043304936067233, Validation Loss Force: 2.702144194436166, time: 0.07159256935119629
Test Loss Energy: 11.336512853267278, Test Loss Force: 10.178350185196322, time: 14.916473388671875


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7633142439657767, Training Loss Force: 2.6935176948307284, time: 1.1207659244537354
Validation Loss Energy: 1.3583929020633239, Validation Loss Force: 2.696504430308107, time: 0.07058429718017578
Test Loss Energy: 11.502218102115728, Test Loss Force: 10.115596512601382, time: 15.447298049926758


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6467151651126612, Training Loss Force: 2.6998511070861024, time: 1.1277105808258057
Validation Loss Energy: 1.2399373698126592, Validation Loss Force: 2.680521907020848, time: 0.0724034309387207
Test Loss Energy: 11.805770446644315, Test Loss Force: 10.04627399761055, time: 14.95570707321167


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.554512988113658, Training Loss Force: 2.692306563608278, time: 1.1306860446929932
Validation Loss Energy: 1.3525198920049055, Validation Loss Force: 2.6802687322019283, time: 0.0741569995880127
Test Loss Energy: 11.599887672444464, Test Loss Force: 10.053922354015585, time: 15.07642126083374


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.5138715454892533, Training Loss Force: 2.6711464833595078, time: 1.1508562564849854
Validation Loss Energy: 1.2519269899751408, Validation Loss Force: 2.7010718198293775, time: 0.07448649406433105
Test Loss Energy: 11.83803575148695, Test Loss Force: 10.011706601572085, time: 15.004982948303223


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.4552004927140396, Training Loss Force: 2.678548869456531, time: 1.1348357200622559
Validation Loss Energy: 1.4808577036209911, Validation Loss Force: 2.6712608745725652, time: 0.07258391380310059
Test Loss Energy: 12.188944474631645, Test Loss Force: 10.081322606155318, time: 15.14646029472351


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.241400432617965, Training Loss Force: 2.6644984868337867, time: 1.1189520359039307
Validation Loss Energy: 1.4494714854913129, Validation Loss Force: 2.6764256854123607, time: 0.07276201248168945
Test Loss Energy: 12.076412748401133, Test Loss Force: 9.995878998885846, time: 15.024442911148071


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.5560906428716306, Training Loss Force: 2.6685211938863693, time: 1.1552867889404297
Validation Loss Energy: 1.2193354637585554, Validation Loss Force: 2.6713541729447092, time: 0.07417821884155273
Test Loss Energy: 11.796796600269735, Test Loss Force: 10.043585222206998, time: 15.07869267463684


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.5650020257197452, Training Loss Force: 2.668446404043537, time: 1.1059234142303467
Validation Loss Energy: 1.2782734259904627, Validation Loss Force: 2.6823518020620827, time: 0.0726161003112793
Test Loss Energy: 11.749570456290426, Test Loss Force: 10.074713300928448, time: 14.963256359100342


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.748429142190448, Training Loss Force: 2.6770149759332833, time: 1.3566465377807617
Validation Loss Energy: 1.3579120480426927, Validation Loss Force: 2.6655799166525096, time: 0.07732772827148438
Test Loss Energy: 12.00985122356556, Test Loss Force: 9.983015633155395, time: 15.250051259994507


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.3838564633494792, Training Loss Force: 2.666978292384036, time: 1.116835117340088
Validation Loss Energy: 1.6712918166348238, Validation Loss Force: 2.6820104502613753, time: 0.0737912654876709
Test Loss Energy: 11.429307284330537, Test Loss Force: 10.060419140209529, time: 15.138534784317017


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.54474713603447, Training Loss Force: 2.653390656731829, time: 1.108466625213623
Validation Loss Energy: 1.4057020794837254, Validation Loss Force: 2.670060465206869, time: 0.07365059852600098
Test Loss Energy: 12.137518091075911, Test Loss Force: 9.987841873810094, time: 15.054571628570557


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.622936890439447, Training Loss Force: 2.6677821954763346, time: 1.1120245456695557
Validation Loss Energy: 1.4405302059365825, Validation Loss Force: 2.660660041873235, time: 0.07246589660644531
Test Loss Energy: 12.254029047846155, Test Loss Force: 10.034241982874816, time: 15.168193101882935


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.4679213600386358, Training Loss Force: 2.6559374182195037, time: 1.1411411762237549
Validation Loss Energy: 1.3137092165425568, Validation Loss Force: 2.6653340899154054, time: 0.07229042053222656
Test Loss Energy: 11.599156170491685, Test Loss Force: 9.997844474955002, time: 15.015433073043823


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.424676490002693, Training Loss Force: 2.6569705423286596, time: 1.1011416912078857
Validation Loss Energy: 1.2474841838410167, Validation Loss Force: 2.677826942347684, time: 0.07244420051574707
Test Loss Energy: 11.87412875993261, Test Loss Force: 10.064604671964254, time: 15.123512744903564


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.4557131445634643, Training Loss Force: 2.667291655347855, time: 1.1576294898986816
Validation Loss Energy: 1.2403435183348315, Validation Loss Force: 2.696380403593825, time: 0.0729069709777832
Test Loss Energy: 11.742997790047426, Test Loss Force: 10.048226952912026, time: 14.944996356964111


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.4797282396815845, Training Loss Force: 2.67422918249123, time: 1.1569461822509766
Validation Loss Energy: 1.496799701123691, Validation Loss Force: 2.668059782930708, time: 0.07565903663635254
Test Loss Energy: 12.061705738101137, Test Loss Force: 10.027034156973377, time: 15.136507511138916


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5762496043271974, Training Loss Force: 2.645237430274737, time: 1.1124398708343506
Validation Loss Energy: 1.3595029077557739, Validation Loss Force: 2.6550697102955723, time: 0.07292842864990234
Test Loss Energy: 12.03532233084694, Test Loss Force: 9.993850625194904, time: 15.26237440109253

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–…â–â–‚â–…â–ƒâ–…â–ˆâ–‡â–…â–„â–†â–‚â–‡â–ˆâ–ƒâ–…â–„â–‡â–†
wandb:   test_error_force â–ˆâ–„â–…â–„â–‚â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–â–ƒâ–â–‚â–â–ƒâ–‚â–‚â–
wandb:          test_loss â–ˆâ–…â–…â–„â–„â–‚â–ƒâ–„â–ƒâ–ƒâ–…â–â–‚â–ƒâ–ƒâ–â–„â–‚â–ƒâ–ƒ
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–‚â–ˆâ–‚â–â–‚â–â–ƒâ–ƒâ–â–‚â–‚â–…â–ƒâ–ƒâ–‚â–â–â–ƒâ–‚
wandb:  valid_error_force â–ˆâ–…â–„â–ƒâ–‚â–‚â–„â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–ƒâ–‚â–
wandb:         valid_loss â–ˆâ–„â–‡â–‚â–„â–ƒâ–ƒâ–‚â–‚â–‚â–…â–â–ƒâ–„â–‚â–â–‚â–â–‚â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1051
wandb:                 lr 0.0001
wandb:  test_error_energy 12.03532
wandb:   test_error_force 9.99385
wandb:          test_loss 5.62565
wandb: train_error_energy 1.57625
wandb:  train_error_force 2.64524
wandb:         train_loss 1.2127
wandb: valid_error_energy 1.3595
wandb:  valid_error_force 2.65507
wandb:         valid_loss 1.36013
wandb: 
wandb: ğŸš€ View run al_48_2 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/jpuvie6w
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241121_201031-jpuvie6w/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7418696880340576, Uncertainty Bias: 0.015594452619552612
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
1.4305115e-06 0.0011517704
0.0506291 0.34771013
Found uncertainty sample 0 after 260 steps.
Found uncertainty sample 1 after 301 steps.
Found uncertainty sample 2 after 245 steps.
Found uncertainty sample 3 after 1290 steps.
Found uncertainty sample 4 after 1734 steps.
Found uncertainty sample 5 after 1368 steps.
Found uncertainty sample 6 after 230 steps.
Found uncertainty sample 7 after 876 steps.
Found uncertainty sample 8 after 881 steps.
Found uncertainty sample 9 after 352 steps.
Found uncertainty sample 10 after 326 steps.
Found uncertainty sample 11 after 21 steps.
Found uncertainty sample 12 after 3641 steps.
Found uncertainty sample 13 after 99 steps.
Found uncertainty sample 14 after 100 steps.
Found uncertainty sample 15 after 1634 steps.
Found uncertainty sample 17 after 12 steps.
Found uncertainty sample 18 after 3837 steps.
Found uncertainty sample 19 after 3779 steps.
Found uncertainty sample 20 after 86 steps.
Found uncertainty sample 21 after 312 steps.
Found uncertainty sample 22 after 2290 steps.
Found uncertainty sample 23 after 2151 steps.
Found uncertainty sample 24 after 376 steps.
Found uncertainty sample 25 after 1463 steps.
Found uncertainty sample 27 after 1482 steps.
Found uncertainty sample 28 after 912 steps.
Found uncertainty sample 29 after 3477 steps.
Found uncertainty sample 30 after 3181 steps.
Found uncertainty sample 31 after 169 steps.
Found uncertainty sample 33 after 15 steps.
Found uncertainty sample 34 after 105 steps.
Found uncertainty sample 35 after 2576 steps.
Found uncertainty sample 36 after 182 steps.
Found uncertainty sample 37 after 3028 steps.
Found uncertainty sample 38 after 1035 steps.
Found uncertainty sample 39 after 32 steps.
Found uncertainty sample 40 after 3075 steps.
Found uncertainty sample 41 after 225 steps.
Found uncertainty sample 42 after 48 steps.
Found uncertainty sample 43 after 1582 steps.
Found uncertainty sample 44 after 2028 steps.
Found uncertainty sample 46 after 553 steps.
Found uncertainty sample 47 after 650 steps.
Found uncertainty sample 48 after 2976 steps.
Found uncertainty sample 50 after 203 steps.
Found uncertainty sample 52 after 638 steps.
Found uncertainty sample 53 after 1936 steps.
Found uncertainty sample 54 after 108 steps.
Found uncertainty sample 55 after 1628 steps.
Found uncertainty sample 56 after 1136 steps.
Found uncertainty sample 57 after 1860 steps.
Found uncertainty sample 58 after 192 steps.
Found uncertainty sample 59 after 3712 steps.
Found uncertainty sample 60 after 2044 steps.
Found uncertainty sample 61 after 2727 steps.
Found uncertainty sample 63 after 1482 steps.
Found uncertainty sample 64 after 119 steps.
Found uncertainty sample 65 after 506 steps.
Found uncertainty sample 66 after 1069 steps.
Found uncertainty sample 67 after 182 steps.
Found uncertainty sample 68 after 431 steps.
Found uncertainty sample 69 after 50 steps.
Found uncertainty sample 70 after 65 steps.
Found uncertainty sample 71 after 895 steps.
Found uncertainty sample 72 after 1335 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 9 steps.
Found uncertainty sample 75 after 771 steps.
Found uncertainty sample 76 after 628 steps.
Found uncertainty sample 77 after 289 steps.
Found uncertainty sample 78 after 3063 steps.
Found uncertainty sample 79 after 1033 steps.
Found uncertainty sample 80 after 103 steps.
Found uncertainty sample 82 after 1081 steps.
Found uncertainty sample 83 after 176 steps.
Found uncertainty sample 84 after 1239 steps.
Found uncertainty sample 85 after 787 steps.
Found uncertainty sample 86 after 1027 steps.
Found uncertainty sample 87 after 931 steps.
Found uncertainty sample 88 after 453 steps.
Found uncertainty sample 89 after 2565 steps.
Found uncertainty sample 90 after 320 steps.
Found uncertainty sample 91 after 1685 steps.
Found uncertainty sample 92 after 3605 steps.
Found uncertainty sample 93 after 1521 steps.
Found uncertainty sample 94 after 2319 steps.
Found uncertainty sample 95 after 1639 steps.
Found uncertainty sample 96 after 3 steps.
Found uncertainty sample 97 after 124 steps.
Found uncertainty sample 98 after 213 steps.
Found uncertainty sample 99 after 322 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241121_211752-2853l3a5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_3
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/2853l3a5
Training model 3. Added 93 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.332160360658433, Training Loss Force: 3.0743221137775776, time: 1.21108078956604
Validation Loss Energy: 2.2240430324562133, Validation Loss Force: 2.851800350083378, time: 0.08128523826599121
Test Loss Energy: 12.463690794914475, Test Loss Force: 9.87279759090599, time: 15.049749851226807


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5453383400480787, Training Loss Force: 2.8287647444873323, time: 1.236055612564087
Validation Loss Energy: 1.663240100318075, Validation Loss Force: 2.7227771628361266, time: 0.07645559310913086
Test Loss Energy: 11.248112050516584, Test Loss Force: 9.922589359095658, time: 15.234584093093872


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.2990272511620027, Training Loss Force: 2.7714213964177166, time: 1.2097492218017578
Validation Loss Energy: 1.2734754625685838, Validation Loss Force: 2.7240773279791988, time: 0.07504796981811523
Test Loss Energy: 11.7551071537491, Test Loss Force: 9.955231634641699, time: 15.138278245925903


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.4350325078162525, Training Loss Force: 2.7529754719985506, time: 1.2146720886230469
Validation Loss Energy: 1.479236449569491, Validation Loss Force: 2.727829748426374, time: 0.07406234741210938
Test Loss Energy: 11.398239911513626, Test Loss Force: 9.968491682541103, time: 15.213185548782349


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4927644537496636, Training Loss Force: 2.7676584115566185, time: 1.212867259979248
Validation Loss Energy: 1.4280067364967388, Validation Loss Force: 2.73795562783744, time: 0.0764925479888916
Test Loss Energy: 11.462095975915194, Test Loss Force: 9.964988138582267, time: 15.079476594924927


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5954748682267794, Training Loss Force: 2.770124701202168, time: 1.2056450843811035
Validation Loss Energy: 1.5277878981550235, Validation Loss Force: 2.725742447852447, time: 0.0784311294555664
Test Loss Energy: 12.155652877203165, Test Loss Force: 9.965641680686215, time: 15.172686338424683


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.4591501880437558, Training Loss Force: 2.7381335961523225, time: 1.1942579746246338
Validation Loss Energy: 1.274547725840799, Validation Loss Force: 2.711098834139652, time: 0.0780782699584961
Test Loss Energy: 11.759336067515648, Test Loss Force: 9.973220886080462, time: 15.364504098892212


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5644458639818288, Training Loss Force: 2.7343267949515093, time: 1.2198634147644043
Validation Loss Energy: 1.256252560803934, Validation Loss Force: 2.71379246279794, time: 0.07650542259216309
Test Loss Energy: 11.709855224728575, Test Loss Force: 9.916098761005518, time: 15.197707414627075


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.4709926254470929, Training Loss Force: 2.745667099513957, time: 1.2083394527435303
Validation Loss Energy: 1.8965632968710697, Validation Loss Force: 2.7063951711010734, time: 0.07516789436340332
Test Loss Energy: 12.258056925182336, Test Loss Force: 9.832063481291108, time: 15.107746124267578


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.7352607098825372, Training Loss Force: 2.766014779255952, time: 1.3778314590454102
Validation Loss Energy: 1.2521532218274036, Validation Loss Force: 2.715775999549865, time: 0.09633159637451172
Test Loss Energy: 11.66234267926371, Test Loss Force: 9.878977543387228, time: 15.181290626525879


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.8458990768354175, Training Loss Force: 2.7383348333929556, time: 1.253431797027588
Validation Loss Energy: 1.2476506290275977, Validation Loss Force: 2.702975863696206, time: 0.08039712905883789
Test Loss Energy: 11.64590077087549, Test Loss Force: 9.907595480987283, time: 15.299334526062012


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6538667792213908, Training Loss Force: 2.7635627837325196, time: 1.243788480758667
Validation Loss Energy: 1.2975870797008013, Validation Loss Force: 2.699844022158732, time: 0.07820248603820801
Test Loss Energy: 11.808229651324222, Test Loss Force: 9.881343203709706, time: 15.139465570449829


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.5340472809320915, Training Loss Force: 2.7518904794734933, time: 1.202744722366333
Validation Loss Energy: 1.3205183276904555, Validation Loss Force: 2.7212797128297623, time: 0.0792844295501709
Test Loss Energy: 11.58662445354989, Test Loss Force: 9.882930939642804, time: 15.183159828186035


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.3950383149170276, Training Loss Force: 2.7336893488540044, time: 1.2105295658111572
Validation Loss Energy: 1.2398938379946303, Validation Loss Force: 2.7124691400636913, time: 0.07646727561950684
Test Loss Energy: 11.526140024371433, Test Loss Force: 9.951730730070016, time: 15.082043409347534


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.5221005292191414, Training Loss Force: 2.7374750583384952, time: 1.2397172451019287
Validation Loss Energy: 1.6060534309188899, Validation Loss Force: 2.7019417881825176, time: 0.07565045356750488
Test Loss Energy: 11.33978649075004, Test Loss Force: 9.887964600687262, time: 15.429731845855713


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5383700776744418, Training Loss Force: 2.7449020145138032, time: 1.247225046157837
Validation Loss Energy: 1.2595721124314256, Validation Loss Force: 2.7240906881279545, time: 0.07960033416748047
Test Loss Energy: 11.718817319768124, Test Loss Force: 9.840996281931186, time: 15.233149290084839


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.4795729660478418, Training Loss Force: 2.756488535484361, time: 1.2617807388305664
Validation Loss Energy: 1.3451595633862623, Validation Loss Force: 2.6977852232354835, time: 0.07686877250671387
Test Loss Energy: 11.875384730496345, Test Loss Force: 9.922736213477721, time: 15.607889413833618


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.2342663856147649, Training Loss Force: 2.733120082811932, time: 1.265775442123413
Validation Loss Energy: 1.4019341835309083, Validation Loss Force: 2.7003993590086757, time: 0.07853126525878906
Test Loss Energy: 11.46676386733035, Test Loss Force: 9.873305267232249, time: 15.201205015182495


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6280711586352998, Training Loss Force: 2.74141291888745, time: 1.2562448978424072
Validation Loss Energy: 1.4690506721581702, Validation Loss Force: 2.7089756838332306, time: 0.0783548355102539
Test Loss Energy: 11.346768709671792, Test Loss Force: 9.904543416791824, time: 15.239709615707397


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.664806219014942, Training Loss Force: 2.7506060186904624, time: 1.2254114151000977
Validation Loss Energy: 1.4617640470268294, Validation Loss Force: 2.7111157569947424, time: 0.07695746421813965
Test Loss Energy: 11.37105376216964, Test Loss Force: 9.846954022262858, time: 15.0862557888031

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–â–„â–‚â–‚â–†â–„â–„â–‡â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–„â–…â–‚â–‚â–‚
wandb:   test_error_force â–ƒâ–…â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–…â–â–ƒâ–…â–ƒâ–„â–‡â–„â–â–…â–ƒâ–…â–‚
wandb:          test_loss â–ˆâ–„â–†â–†â–…â–‡â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–„â–„â–‚â–ƒâ–…â–‚â–ƒâ–
wandb: train_error_energy â–ˆâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:  train_error_force â–ˆâ–ƒâ–‚â–â–‚â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–
wandb: valid_error_energy â–ˆâ–„â–â–ƒâ–‚â–ƒâ–â–â–†â–â–â–â–‚â–â–„â–â–‚â–‚â–ƒâ–ƒ
wandb:  valid_error_force â–ˆâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–‚â–â–â–‚â–‚
wandb:         valid_loss â–ˆâ–‚â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–â–â–â–â–‚â–â–‚â–‚â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1134
wandb:                 lr 0.0001
wandb:  test_error_energy 11.37105
wandb:   test_error_force 9.84695
wandb:          test_loss 5.47617
wandb: train_error_energy 1.66481
wandb:  train_error_force 2.75061
wandb:         train_loss 1.24592
wandb: valid_error_energy 1.46176
wandb:  valid_error_force 2.71112
wandb:         valid_loss 1.37729
wandb: 
wandb: ğŸš€ View run al_48_3 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/2853l3a5
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241121_211752-2853l3a5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7356384992599487, Uncertainty Bias: 0.0171516090631485
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
4.351139e-06 3.3289194e-05
0.038198896 0.2874518
Found uncertainty sample 0 after 1237 steps.
Found uncertainty sample 1 after 2128 steps.
Found uncertainty sample 2 after 7 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 908 steps.
Found uncertainty sample 5 after 11 steps.
Found uncertainty sample 7 after 1033 steps.
Found uncertainty sample 8 after 805 steps.
Found uncertainty sample 9 after 1209 steps.
Found uncertainty sample 10 after 1774 steps.
Found uncertainty sample 11 after 1834 steps.
Found uncertainty sample 12 after 1707 steps.
Found uncertainty sample 14 after 757 steps.
Found uncertainty sample 16 after 256 steps.
Found uncertainty sample 17 after 175 steps.
Found uncertainty sample 18 after 179 steps.
Found uncertainty sample 20 after 194 steps.
Found uncertainty sample 21 after 772 steps.
Found uncertainty sample 23 after 4 steps.
Found uncertainty sample 24 after 1964 steps.
Found uncertainty sample 25 after 3468 steps.
Found uncertainty sample 26 after 1099 steps.
Found uncertainty sample 27 after 33 steps.
Found uncertainty sample 28 after 31 steps.
Found uncertainty sample 30 after 1222 steps.
Found uncertainty sample 31 after 226 steps.
Found uncertainty sample 32 after 2792 steps.
Found uncertainty sample 33 after 3873 steps.
Found uncertainty sample 34 after 189 steps.
Found uncertainty sample 35 after 996 steps.
Found uncertainty sample 36 after 2546 steps.
Found uncertainty sample 37 after 1868 steps.
Found uncertainty sample 38 after 1403 steps.
Found uncertainty sample 39 after 15 steps.
Found uncertainty sample 40 after 588 steps.
Found uncertainty sample 41 after 724 steps.
Found uncertainty sample 42 after 369 steps.
Found uncertainty sample 43 after 502 steps.
Found uncertainty sample 45 after 130 steps.
Found uncertainty sample 46 after 360 steps.
Found uncertainty sample 48 after 663 steps.
Found uncertainty sample 49 after 712 steps.
Found uncertainty sample 50 after 1248 steps.
Found uncertainty sample 52 after 266 steps.
Found uncertainty sample 53 after 1349 steps.
Found uncertainty sample 54 after 963 steps.
Found uncertainty sample 55 after 382 steps.
Found uncertainty sample 56 after 1492 steps.
Found uncertainty sample 57 after 30 steps.
Found uncertainty sample 58 after 1279 steps.
Found uncertainty sample 59 after 589 steps.
Found uncertainty sample 60 after 393 steps.
Found uncertainty sample 62 after 3453 steps.
Found uncertainty sample 63 after 1716 steps.
Found uncertainty sample 64 after 495 steps.
Found uncertainty sample 65 after 784 steps.
Found uncertainty sample 66 after 373 steps.
Found uncertainty sample 67 after 1479 steps.
Found uncertainty sample 68 after 3093 steps.
Found uncertainty sample 69 after 169 steps.
Found uncertainty sample 70 after 3196 steps.
Found uncertainty sample 72 after 91 steps.
Found uncertainty sample 73 after 1215 steps.
Found uncertainty sample 74 after 726 steps.
Found uncertainty sample 75 after 117 steps.
Found uncertainty sample 77 after 1742 steps.
Found uncertainty sample 78 after 1607 steps.
Found uncertainty sample 79 after 9 steps.
Found uncertainty sample 80 after 1597 steps.
Found uncertainty sample 81 after 2808 steps.
Found uncertainty sample 83 after 48 steps.
Found uncertainty sample 84 after 553 steps.
Found uncertainty sample 85 after 1481 steps.
Found uncertainty sample 87 after 5 steps.
Found uncertainty sample 89 after 1315 steps.
Found uncertainty sample 90 after 1393 steps.
Found uncertainty sample 91 after 2390 steps.
Found uncertainty sample 92 after 2946 steps.
Found uncertainty sample 93 after 398 steps.
Found uncertainty sample 94 after 945 steps.
Found uncertainty sample 95 after 811 steps.
Found uncertainty sample 96 after 354 steps.
Found uncertainty sample 99 after 1546 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241121_223428-38pbykmh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_4
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/38pbykmh
Training model 4. Added 84 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.6031577391191516, Training Loss Force: 3.041742469759569, time: 1.2871015071868896
Validation Loss Energy: 1.4410587111544422, Validation Loss Force: 2.7794858729135776, time: 0.08420062065124512
Test Loss Energy: 11.964442745311574, Test Loss Force: 9.69191176228224, time: 15.202971458435059


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6181466581560147, Training Loss Force: 2.8367534591084507, time: 1.2747831344604492
Validation Loss Energy: 1.5178643136630328, Validation Loss Force: 2.7414085582833425, time: 0.07992148399353027
Test Loss Energy: 11.999444106028346, Test Loss Force: 9.80164287490759, time: 15.366460084915161


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.545278313459648, Training Loss Force: 2.8225709931968717, time: 1.2871272563934326
Validation Loss Energy: 1.3121251482384266, Validation Loss Force: 2.7340377642148406, time: 0.07910656929016113
Test Loss Energy: 11.740942943210067, Test Loss Force: 9.781220378980231, time: 15.239703178405762


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3730506625915282, Training Loss Force: 2.798402519256156, time: 1.3063228130340576
Validation Loss Energy: 1.327593528467738, Validation Loss Force: 2.7329308758904336, time: 0.078125
Test Loss Energy: 11.409543870422215, Test Loss Force: 9.788419680500544, time: 15.425114870071411


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.3575000734995115, Training Loss Force: 2.8033072290946497, time: 1.2970812320709229
Validation Loss Energy: 1.3807855133889788, Validation Loss Force: 2.727306253264298, time: 0.08054327964782715
Test Loss Energy: 11.50345974257925, Test Loss Force: 9.829644930634535, time: 15.579901218414307


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.653493509963454, Training Loss Force: 2.797683907325103, time: 1.3055322170257568
Validation Loss Energy: 1.2894528644654861, Validation Loss Force: 2.7225947968056117, time: 0.08174991607666016
Test Loss Energy: 11.486485414776011, Test Loss Force: 9.818477613638278, time: 15.362275838851929


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6559768158939847, Training Loss Force: 2.8101754120230775, time: 1.277453899383545
Validation Loss Energy: 1.2828206639540272, Validation Loss Force: 2.749751748512111, time: 0.08219695091247559
Test Loss Energy: 11.476771630668864, Test Loss Force: 9.790315865662071, time: 15.211417436599731


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6379740652532953, Training Loss Force: 2.8057348794581274, time: 1.432344913482666
Validation Loss Energy: 1.462945974853414, Validation Loss Force: 2.729479058790261, time: 0.10474562644958496
Test Loss Energy: 12.055652597447217, Test Loss Force: 9.75406516549204, time: 15.291210412979126


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.346474811434114, Training Loss Force: 2.8079005771505643, time: 1.2722067832946777
Validation Loss Energy: 1.5950619252030294, Validation Loss Force: 2.7615554399377427, time: 0.08028531074523926
Test Loss Energy: 12.105227105835652, Test Loss Force: 9.721125879616313, time: 15.428264141082764


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.4303852311813243, Training Loss Force: 2.797769095852564, time: 1.2729084491729736
Validation Loss Energy: 2.2919600166630856, Validation Loss Force: 2.747375423386769, time: 0.07856154441833496
Test Loss Energy: 12.58226441409252, Test Loss Force: 9.728431917766597, time: 15.396641254425049


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6705109803969131, Training Loss Force: 2.8123975856425782, time: 1.2697241306304932
Validation Loss Energy: 1.428169396307839, Validation Loss Force: 2.7231534736927445, time: 0.078399658203125
Test Loss Energy: 11.277240397094614, Test Loss Force: 9.782516457969605, time: 15.383261442184448


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.5521434361522475, Training Loss Force: 2.7911649457937773, time: 1.306422472000122
Validation Loss Energy: 1.2403745806806885, Validation Loss Force: 2.711780201726191, time: 0.08140397071838379
Test Loss Energy: 11.453319682539524, Test Loss Force: 9.732727958702897, time: 15.31691288948059


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.5097306345915822, Training Loss Force: 2.793634241147258, time: 1.3042957782745361
Validation Loss Energy: 1.5536736317074207, Validation Loss Force: 2.7626607695781282, time: 0.08179640769958496
Test Loss Energy: 12.033617855659408, Test Loss Force: 9.771038966350494, time: 15.379605054855347


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.7256900032494122, Training Loss Force: 2.7756864165391826, time: 1.3058044910430908
Validation Loss Energy: 1.3286402961227446, Validation Loss Force: 2.7082932029009616, time: 0.08350348472595215
Test Loss Energy: 11.64320808302349, Test Loss Force: 9.724416785033053, time: 15.285771131515503


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6667513525010673, Training Loss Force: 2.793548355313787, time: 1.2937090396881104
Validation Loss Energy: 1.6085770516842623, Validation Loss Force: 2.7264438556621196, time: 0.07995390892028809
Test Loss Energy: 11.374777377856308, Test Loss Force: 9.755379099982143, time: 15.378575801849365


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6909672608295803, Training Loss Force: 2.790363641573584, time: 1.2574589252471924
Validation Loss Energy: 1.5384335986872582, Validation Loss Force: 2.729155297852082, time: 0.07861590385437012
Test Loss Energy: 12.188701431841764, Test Loss Force: 9.763966583911747, time: 15.220092296600342


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.5518884975593037, Training Loss Force: 2.8085695280023604, time: 1.3084359169006348
Validation Loss Energy: 2.129882262710292, Validation Loss Force: 2.71375037832936, time: 0.07895517349243164
Test Loss Energy: 11.123202526517048, Test Loss Force: 9.737404146000998, time: 15.43519401550293


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6369744038420813, Training Loss Force: 2.784587246485641, time: 1.28615140914917
Validation Loss Energy: 1.2452959028094115, Validation Loss Force: 2.701569485370526, time: 0.08101367950439453
Test Loss Energy: 11.597870975476901, Test Loss Force: 9.713637896291859, time: 15.618899822235107


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.4088940973578865, Training Loss Force: 2.794464381021848, time: 1.5590765476226807
Validation Loss Energy: 1.2407805065337147, Validation Loss Force: 2.6937802959768264, time: 0.08213591575622559
Test Loss Energy: 11.542298455118168, Test Loss Force: 9.6807277882349, time: 15.281053304672241


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5881487367384095, Training Loss Force: 2.7695562353366463, time: 1.294382095336914
Validation Loss Energy: 1.4984634849820635, Validation Loss Force: 2.726108961266644, time: 0.08184099197387695
Test Loss Energy: 11.228157164458901, Test Loss Force: 9.747907643138381, time: 15.399767398834229

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–…â–„â–‚â–ƒâ–ƒâ–ƒâ–…â–†â–ˆâ–‚â–ƒâ–…â–ƒâ–‚â–†â–â–ƒâ–ƒâ–‚
wandb:   test_error_force â–‚â–‡â–†â–†â–ˆâ–‡â–†â–„â–ƒâ–ƒâ–†â–ƒâ–…â–ƒâ–…â–…â–„â–ƒâ–â–„
wandb:          test_loss â–…â–ˆâ–†â–†â–ˆâ–†â–…â–†â–…â–‡â–…â–ƒâ–†â–„â–ƒâ–…â–ƒâ–ƒâ–‚â–
wandb: train_error_energy â–ˆâ–‚â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:  train_error_force â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–
wandb:         train_loss â–ˆâ–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–
wandb: valid_error_energy â–‚â–ƒâ–â–‚â–‚â–â–â–‚â–ƒâ–ˆâ–‚â–â–ƒâ–‚â–ƒâ–ƒâ–‡â–â–â–ƒ
wandb:  valid_error_force â–ˆâ–…â–„â–„â–„â–ƒâ–†â–„â–‡â–…â–ƒâ–‚â–‡â–‚â–„â–„â–ƒâ–‚â–â–„
wandb:         valid_loss â–†â–ˆâ–ƒâ–â–ƒâ–â–ƒâ–‚â–ƒâ–‡â–…â–‚â–ƒâ–‚â–…â–‚â–†â–â–‚â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1209
wandb:                 lr 0.0001
wandb:  test_error_energy 11.22816
wandb:   test_error_force 9.74791
wandb:          test_loss 5.37569
wandb: train_error_energy 1.58815
wandb:  train_error_force 2.76956
wandb:         train_loss 1.2645
wandb: valid_error_energy 1.49846
wandb:  valid_error_force 2.72611
wandb:         valid_loss 1.35828
wandb: 
wandb: ğŸš€ View run al_48_4 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/38pbykmh
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241121_223428-38pbykmh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7474347949028015, Uncertainty Bias: 0.01490338146686554
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
2.026558e-06 0.0005867481
0.026775708 0.25822705
Found uncertainty sample 0 after 3797 steps.
Found uncertainty sample 2 after 1136 steps.
Found uncertainty sample 3 after 182 steps.
Found uncertainty sample 4 after 1544 steps.
Found uncertainty sample 5 after 215 steps.
Found uncertainty sample 6 after 11 steps.
Found uncertainty sample 7 after 2508 steps.
Found uncertainty sample 8 after 63 steps.
Found uncertainty sample 9 after 1503 steps.
Found uncertainty sample 10 after 3898 steps.
Found uncertainty sample 11 after 1210 steps.
Found uncertainty sample 13 after 3294 steps.
Found uncertainty sample 15 after 1124 steps.
Found uncertainty sample 17 after 52 steps.
Found uncertainty sample 18 after 1208 steps.
Found uncertainty sample 20 after 594 steps.
Found uncertainty sample 21 after 4 steps.
Found uncertainty sample 22 after 227 steps.
Found uncertainty sample 23 after 827 steps.
Found uncertainty sample 24 after 1744 steps.
Found uncertainty sample 25 after 583 steps.
Found uncertainty sample 26 after 177 steps.
Found uncertainty sample 27 after 302 steps.
Found uncertainty sample 28 after 45 steps.
Found uncertainty sample 29 after 1266 steps.
Found uncertainty sample 30 after 1319 steps.
Found uncertainty sample 31 after 2884 steps.
Found uncertainty sample 32 after 2408 steps.
Found uncertainty sample 33 after 84 steps.
Found uncertainty sample 34 after 1884 steps.
Found uncertainty sample 36 after 12 steps.
Found uncertainty sample 37 after 934 steps.
Found uncertainty sample 39 after 2331 steps.
Found uncertainty sample 40 after 1680 steps.
Found uncertainty sample 41 after 2131 steps.
Found uncertainty sample 42 after 2430 steps.
Found uncertainty sample 43 after 52 steps.
Found uncertainty sample 45 after 1017 steps.
Found uncertainty sample 47 after 470 steps.
Found uncertainty sample 48 after 1817 steps.
Found uncertainty sample 49 after 2263 steps.
Found uncertainty sample 50 after 1368 steps.
Found uncertainty sample 51 after 3972 steps.
Found uncertainty sample 52 after 1334 steps.
Found uncertainty sample 53 after 1415 steps.
Found uncertainty sample 54 after 1188 steps.
Found uncertainty sample 55 after 2065 steps.
Found uncertainty sample 56 after 3404 steps.
Found uncertainty sample 57 after 3294 steps.
Found uncertainty sample 58 after 169 steps.
Found uncertainty sample 59 after 1364 steps.
Found uncertainty sample 60 after 82 steps.
Found uncertainty sample 62 after 371 steps.
Found uncertainty sample 63 after 343 steps.
Found uncertainty sample 64 after 646 steps.
Found uncertainty sample 65 after 1950 steps.
Found uncertainty sample 66 after 2976 steps.
Found uncertainty sample 67 after 1125 steps.
Found uncertainty sample 68 after 1324 steps.
Found uncertainty sample 69 after 232 steps.
Found uncertainty sample 70 after 794 steps.
Found uncertainty sample 71 after 903 steps.
Found uncertainty sample 72 after 1126 steps.
Found uncertainty sample 74 after 320 steps.
Found uncertainty sample 75 after 2886 steps.
Found uncertainty sample 76 after 235 steps.
Found uncertainty sample 77 after 461 steps.
Found uncertainty sample 79 after 2305 steps.
Found uncertainty sample 80 after 1321 steps.
Found uncertainty sample 81 after 3754 steps.
Found uncertainty sample 82 after 694 steps.
Found uncertainty sample 83 after 1700 steps.
Found uncertainty sample 84 after 2620 steps.
Found uncertainty sample 85 after 139 steps.
Found uncertainty sample 86 after 3612 steps.
Found uncertainty sample 87 after 257 steps.
Found uncertainty sample 88 after 593 steps.
Found uncertainty sample 89 after 246 steps.
Found uncertainty sample 90 after 3551 steps.
Found uncertainty sample 91 after 214 steps.
Found uncertainty sample 92 after 584 steps.
Found uncertainty sample 94 after 221 steps.
Found uncertainty sample 95 after 17 steps.
Found uncertainty sample 96 after 483 steps.
Found uncertainty sample 97 after 2642 steps.
Found uncertainty sample 98 after 2312 steps.
Found uncertainty sample 99 after 544 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241121_235528-qt4vzh2d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_5
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/qt4vzh2d
Training model 5. Added 87 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.5597388448606453, Training Loss Force: 3.1299223466746295, time: 1.39408540725708
Validation Loss Energy: 1.151995415308782, Validation Loss Force: 2.4033620583781956, time: 0.10877776145935059
Test Loss Energy: 12.023034500739323, Test Loss Force: 9.706828319404046, time: 15.289196729660034


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.811216213186046, Training Loss Force: 2.8819277027655987, time: 1.385486364364624
Validation Loss Energy: 1.0897406848137074, Validation Loss Force: 2.746600312089795, time: 0.11727762222290039
Test Loss Energy: 11.9728826940278, Test Loss Force: 9.589394887497084, time: 15.39118504524231


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.4452844488147538, Training Loss Force: 2.8560348492314866, time: 1.3816206455230713
Validation Loss Energy: 1.3798571500523193, Validation Loss Force: 2.4489123722149135, time: 0.10983657836914062
Test Loss Energy: 12.119811216756784, Test Loss Force: 9.655306754697792, time: 15.308109760284424


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.8001589902018853, Training Loss Force: 2.8601671541035243, time: 1.3688697814941406
Validation Loss Energy: 2.133303128799181, Validation Loss Force: 3.2275098886727616, time: 0.10838770866394043
Test Loss Energy: 12.301007080106539, Test Loss Force: 9.614952311132207, time: 15.46100926399231


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6602649676230024, Training Loss Force: 2.8334913795594656, time: 1.3839397430419922
Validation Loss Energy: 1.7846806238044746, Validation Loss Force: 2.3636594816518333, time: 0.1182096004486084
Test Loss Energy: 12.221796045185172, Test Loss Force: 9.627258243891648, time: 15.621132850646973


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5055663306769083, Training Loss Force: 2.850521707814094, time: 1.3715012073516846
Validation Loss Energy: 1.3417950325912935, Validation Loss Force: 2.353777536290131, time: 0.1100149154663086
Test Loss Energy: 11.878860383687396, Test Loss Force: 9.580481947545854, time: 15.661031484603882


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6117059061966084, Training Loss Force: 2.864355316345461, time: 1.390019178390503
Validation Loss Energy: 1.30560461630365, Validation Loss Force: 2.5772470664629563, time: 0.11024975776672363
Test Loss Energy: 11.397627676284962, Test Loss Force: 9.682054052300328, time: 15.336611032485962


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6467690979496759, Training Loss Force: 2.854900849580824, time: 1.6417009830474854
Validation Loss Energy: 0.8470122218731553, Validation Loss Force: 2.61946569320373, time: 0.11547589302062988
Test Loss Energy: 11.250596590515228, Test Loss Force: 9.626908063434174, time: 15.313507795333862


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.93199583563899, Training Loss Force: 2.858046789876904, time: 1.3934149742126465
Validation Loss Energy: 2.5429624197280223, Validation Loss Force: 2.85313881431955, time: 0.11281418800354004
Test Loss Energy: 11.173799307788627, Test Loss Force: 9.633378017413131, time: 15.35414171218872


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6014385395516468, Training Loss Force: 2.8605663944512174, time: 1.3876750469207764
Validation Loss Energy: 1.6610727501861868, Validation Loss Force: 3.5345707522417538, time: 0.11106395721435547
Test Loss Energy: 12.273316971029164, Test Loss Force: 9.579482459195628, time: 15.363395690917969


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.7373766641738766, Training Loss Force: 2.8450274552574215, time: 1.393388271331787
Validation Loss Energy: 1.6034708028298463, Validation Loss Force: 2.703710337874049, time: 0.11539363861083984
Test Loss Energy: 11.318594977199142, Test Loss Force: 9.575319791248706, time: 15.413924932479858


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.7032790866762961, Training Loss Force: 2.863965221725614, time: 1.36334228515625
Validation Loss Energy: 1.0511321929358612, Validation Loss Force: 1.9655347308184832, time: 0.11493349075317383
Test Loss Energy: 11.51457287286397, Test Loss Force: 9.524889188776791, time: 15.288365125656128


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.589671330564741, Training Loss Force: 2.862820783779868, time: 1.3842132091522217
Validation Loss Energy: 0.9873486160848464, Validation Loss Force: 2.448631141083607, time: 0.12653112411499023
Test Loss Energy: 12.036966673707004, Test Loss Force: 9.578650819080766, time: 15.403980016708374


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6864600524328113, Training Loss Force: 2.8491761724748, time: 1.3912005424499512
Validation Loss Energy: 1.2401456211313882, Validation Loss Force: 3.0365498311362473, time: 0.11015081405639648
Test Loss Energy: 11.60784865166715, Test Loss Force: 9.561856272713817, time: 15.261404275894165


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.465098140035741, Training Loss Force: 2.8332274840358127, time: 1.389763355255127
Validation Loss Energy: 1.1202525925419002, Validation Loss Force: 2.4683768842716183, time: 0.10968589782714844
Test Loss Energy: 12.533934833743476, Test Loss Force: 9.595940842769034, time: 15.414574146270752


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.440190077458439, Training Loss Force: 2.859220641873733, time: 1.3878602981567383
Validation Loss Energy: 0.62834238080809, Validation Loss Force: 2.3210983897646233, time: 0.11348199844360352
Test Loss Energy: 11.575711557620286, Test Loss Force: 9.584537888189264, time: 15.294901371002197


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6924087251954683, Training Loss Force: 2.8243945900456016, time: 1.6306004524230957
Validation Loss Energy: 2.559414774913189, Validation Loss Force: 3.6096964273788927, time: 0.11621332168579102
Test Loss Energy: 11.666051442568598, Test Loss Force: 9.61872914824217, time: 15.646615743637085


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6706836583187021, Training Loss Force: 2.814661118841117, time: 1.4345343112945557
Validation Loss Energy: 0.759423476302006, Validation Loss Force: 2.5140800604770153, time: 0.1145472526550293
Test Loss Energy: 11.68544630756362, Test Loss Force: 9.612665352111842, time: 15.434284210205078


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.5049667844973271, Training Loss Force: 2.846150690755712, time: 1.3988664150238037
Validation Loss Energy: 1.4146662382931998, Validation Loss Force: 2.3450775187550135, time: 0.12636637687683105
Test Loss Energy: 11.597972946287664, Test Loss Force: 9.653806314396352, time: 15.357451915740967


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5458475606998185, Training Loss Force: 2.8356422005804847, time: 1.3700032234191895
Validation Loss Energy: 1.2504930563426524, Validation Loss Force: 2.2367134635323693, time: 0.11702227592468262
Test Loss Energy: 11.381673620839976, Test Loss Force: 9.621478307787516, time: 15.423632621765137

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–…â–†â–‡â–†â–…â–‚â–â–â–‡â–‚â–ƒâ–…â–ƒâ–ˆâ–ƒâ–„â–„â–ƒâ–‚
wandb:   test_error_force â–ˆâ–ƒâ–†â–„â–…â–ƒâ–‡â–…â–…â–ƒâ–ƒâ–â–ƒâ–‚â–„â–ƒâ–…â–„â–†â–…
wandb:          test_loss â–ˆâ–…â–‡â–†â–‡â–„â–†â–ƒâ–â–†â–‚â–â–ƒâ–ƒâ–†â–‚â–ƒâ–ƒâ–ƒâ–‚
wandb: train_error_energy â–ˆâ–‚â–â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–
wandb:         train_loss â–ˆâ–‚â–â–‚â–â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–ƒâ–„â–†â–…â–„â–ƒâ–‚â–ˆâ–…â–…â–ƒâ–‚â–ƒâ–ƒâ–â–ˆâ–â–„â–ƒ
wandb:  valid_error_force â–ƒâ–„â–ƒâ–†â–ƒâ–ƒâ–„â–„â–…â–ˆâ–„â–â–ƒâ–†â–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–‚
wandb:         valid_loss â–ƒâ–„â–ƒâ–‡â–‚â–ƒâ–ƒâ–ƒâ–…â–ˆâ–„â–â–‚â–†â–ƒâ–‚â–ˆâ–ƒâ–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1287
wandb:                 lr 0.0001
wandb:  test_error_energy 11.38167
wandb:   test_error_force 9.62148
wandb:          test_loss 5.33777
wandb: train_error_energy 1.54585
wandb:  train_error_force 2.83564
wandb:         train_loss 1.28677
wandb: valid_error_energy 1.25049
wandb:  valid_error_force 2.23671
wandb:         valid_loss 1.19521
wandb: 
wandb: ğŸš€ View run al_48_5 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/qt4vzh2d
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241121_235528-qt4vzh2d/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7186832427978516, Uncertainty Bias: 0.02169199287891388
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
1.9311905e-05 2.4318695e-05
0.022608615 0.29117107
Found uncertainty sample 0 after 1782 steps.
Found uncertainty sample 3 after 954 steps.
Found uncertainty sample 5 after 737 steps.
Found uncertainty sample 7 after 191 steps.
Found uncertainty sample 8 after 5 steps.
Found uncertainty sample 9 after 431 steps.
Found uncertainty sample 10 after 2747 steps.
Found uncertainty sample 11 after 108 steps.
Found uncertainty sample 12 after 52 steps.
Found uncertainty sample 13 after 427 steps.
Found uncertainty sample 14 after 2997 steps.
Found uncertainty sample 16 after 3223 steps.
Found uncertainty sample 17 after 74 steps.
Found uncertainty sample 18 after 145 steps.
Found uncertainty sample 19 after 3475 steps.
Found uncertainty sample 20 after 14 steps.
Found uncertainty sample 21 after 1483 steps.
Found uncertainty sample 23 after 1071 steps.
Found uncertainty sample 24 after 475 steps.
Found uncertainty sample 25 after 3695 steps.
Found uncertainty sample 26 after 127 steps.
Found uncertainty sample 27 after 433 steps.
Found uncertainty sample 28 after 1928 steps.
Found uncertainty sample 29 after 1598 steps.
Found uncertainty sample 31 after 2144 steps.
Found uncertainty sample 32 after 1928 steps.
Found uncertainty sample 33 after 414 steps.
Found uncertainty sample 34 after 899 steps.
Found uncertainty sample 35 after 47 steps.
Found uncertainty sample 37 after 9 steps.
Found uncertainty sample 38 after 811 steps.
Found uncertainty sample 39 after 1060 steps.
Found uncertainty sample 40 after 212 steps.
Found uncertainty sample 42 after 1042 steps.
Found uncertainty sample 43 after 1960 steps.
Found uncertainty sample 44 after 937 steps.
Found uncertainty sample 45 after 2206 steps.
Found uncertainty sample 47 after 378 steps.
Found uncertainty sample 48 after 472 steps.
Found uncertainty sample 49 after 544 steps.
Found uncertainty sample 50 after 1428 steps.
Found uncertainty sample 52 after 125 steps.
Found uncertainty sample 53 after 2229 steps.
Found uncertainty sample 55 after 969 steps.
Found uncertainty sample 56 after 50 steps.
Found uncertainty sample 58 after 1797 steps.
Found uncertainty sample 60 after 2051 steps.
Found uncertainty sample 61 after 2320 steps.
Found uncertainty sample 62 after 3640 steps.
Found uncertainty sample 63 after 1437 steps.
Found uncertainty sample 64 after 1284 steps.
Found uncertainty sample 65 after 93 steps.
Found uncertainty sample 66 after 1771 steps.
Found uncertainty sample 67 after 2290 steps.
Found uncertainty sample 68 after 381 steps.
Found uncertainty sample 69 after 1668 steps.
Found uncertainty sample 70 after 1967 steps.
Found uncertainty sample 71 after 1177 steps.
Found uncertainty sample 72 after 570 steps.
Found uncertainty sample 73 after 523 steps.
Found uncertainty sample 74 after 1975 steps.
Found uncertainty sample 75 after 141 steps.
Found uncertainty sample 76 after 3700 steps.
Found uncertainty sample 78 after 55 steps.
Found uncertainty sample 79 after 2197 steps.
Found uncertainty sample 80 after 285 steps.
Found uncertainty sample 81 after 64 steps.
Found uncertainty sample 83 after 1855 steps.
Found uncertainty sample 84 after 1345 steps.
Found uncertainty sample 85 after 1661 steps.
Found uncertainty sample 87 after 1086 steps.
Found uncertainty sample 88 after 2237 steps.
Found uncertainty sample 89 after 210 steps.
Found uncertainty sample 90 after 2050 steps.
Found uncertainty sample 92 after 1805 steps.
Found uncertainty sample 93 after 948 steps.
Found uncertainty sample 94 after 360 steps.
Found uncertainty sample 95 after 167 steps.
Found uncertainty sample 96 after 5 steps.
Found uncertainty sample 97 after 131 steps.
Found uncertainty sample 98 after 31 steps.
Found uncertainty sample 99 after 853 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_011635-jin8n47j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_6
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/jin8n47j
Training model 6. Added 82 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.6273901023200588, Training Loss Force: 3.1067009377378487, time: 1.430417776107788
Validation Loss Energy: 1.2577046419718156, Validation Loss Force: 2.6773041151930457, time: 0.1143348217010498
Test Loss Energy: 11.224117083454557, Test Loss Force: 9.545086077724463, time: 15.400244951248169


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.515974933595134, Training Loss Force: 2.913989346575234, time: 1.4711673259735107
Validation Loss Energy: 1.1676995743299878, Validation Loss Force: 2.6790964904831913, time: 0.11513757705688477
Test Loss Energy: 11.887892012122443, Test Loss Force: 9.478525881908922, time: 15.482787847518921


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.619978418800845, Training Loss Force: 2.9116141606873835, time: 1.4622972011566162
Validation Loss Energy: 1.8143187221367603, Validation Loss Force: 2.5698741570375683, time: 0.11592650413513184
Test Loss Energy: 11.056180857640124, Test Loss Force: 9.5009600385174, time: 15.378113269805908


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.58104334637203, Training Loss Force: 2.9061988004532493, time: 1.4409923553466797
Validation Loss Energy: 2.0278332158007326, Validation Loss Force: 2.6818789641307053, time: 0.11315035820007324
Test Loss Energy: 11.063689531708047, Test Loss Force: 9.552506411597609, time: 15.477668762207031


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7139442805471483, Training Loss Force: 2.903391346434574, time: 1.4688997268676758
Validation Loss Energy: 2.0483992721223423, Validation Loss Force: 2.608961721582878, time: 0.11688995361328125
Test Loss Energy: 12.266125923422743, Test Loss Force: 9.47852264562856, time: 15.429271221160889


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6862459308203228, Training Loss Force: 2.915149685851368, time: 1.4520361423492432
Validation Loss Energy: 1.4043303818775237, Validation Loss Force: 2.516518287624925, time: 0.11761856079101562
Test Loss Energy: 11.176669669297784, Test Loss Force: 9.501794154806431, time: 15.82415246963501


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.790518149204231, Training Loss Force: 2.9064014086626346, time: 1.4994337558746338
Validation Loss Energy: 1.319807456442081, Validation Loss Force: 2.804251023989093, time: 0.1105036735534668
Test Loss Energy: 11.42279631259645, Test Loss Force: 9.506592938373567, time: 15.497439622879028


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6177018766392282, Training Loss Force: 2.8860051634565567, time: 1.4619653224945068
Validation Loss Energy: 1.5905929554442588, Validation Loss Force: 2.7543075592377932, time: 0.11461567878723145
Test Loss Energy: 11.494876632757682, Test Loss Force: 9.471777010925647, time: 15.422224998474121


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.8968344017345893, Training Loss Force: 2.910294638863672, time: 1.458744764328003
Validation Loss Energy: 1.6812585989411246, Validation Loss Force: 2.6819295100268308, time: 0.11741113662719727
Test Loss Energy: 11.116501738079604, Test Loss Force: 9.47765306082594, time: 15.494274139404297


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.8567495642248952, Training Loss Force: 2.8957840155928705, time: 1.4708061218261719
Validation Loss Energy: 0.9648647660766475, Validation Loss Force: 2.675430576999209, time: 0.1204068660736084
Test Loss Energy: 11.36817226020835, Test Loss Force: 9.493878385581016, time: 15.612361907958984


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.8744474503479227, Training Loss Force: 2.8924824483826765, time: 1.4408550262451172
Validation Loss Energy: 4.1976600965990105, Validation Loss Force: 2.5764705078165333, time: 0.10952901840209961
Test Loss Energy: 10.95577909725342, Test Loss Force: 9.505613651127847, time: 15.536083936691284


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.9753246516826903, Training Loss Force: 2.9347928484708623, time: 1.4768040180206299
Validation Loss Energy: 1.360012875767235, Validation Loss Force: 2.749639745931709, time: 0.11715221405029297
Test Loss Energy: 11.446734854597413, Test Loss Force: 9.485204925713326, time: 15.319182395935059


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.59362953213033, Training Loss Force: 2.9130504129270833, time: 1.4493873119354248
Validation Loss Energy: 1.5023883070154733, Validation Loss Force: 2.7237425449182964, time: 0.11358404159545898
Test Loss Energy: 11.935078304153663, Test Loss Force: 9.49856655271233, time: 15.550747156143188


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.62232046841277, Training Loss Force: 2.8741694957058987, time: 1.4403128623962402
Validation Loss Energy: 1.9294812003882988, Validation Loss Force: 2.6569308903518096, time: 0.11385631561279297
Test Loss Energy: 12.048285979299557, Test Loss Force: 9.46177628439847, time: 15.370363235473633


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.9751195591588224, Training Loss Force: 2.8915269906125576, time: 1.4668352603912354
Validation Loss Energy: 1.302115279452389, Validation Loss Force: 2.773400993162809, time: 0.11756610870361328
Test Loss Energy: 11.484681710755886, Test Loss Force: 9.442987858239478, time: 15.496932029724121


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.782083227054869, Training Loss Force: 2.900508174549579, time: 1.4356544017791748
Validation Loss Energy: 1.2383982806760063, Validation Loss Force: 2.5972301749605267, time: 0.11737346649169922
Test Loss Energy: 11.930742518523523, Test Loss Force: 9.413061558707007, time: 15.599589109420776


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.4860460345366187, Training Loss Force: 2.8810882662298147, time: 1.4449412822723389
Validation Loss Energy: 1.6135873858040701, Validation Loss Force: 2.7801944061359203, time: 0.11376404762268066
Test Loss Energy: 11.226363076015298, Test Loss Force: 9.454036715250101, time: 15.764394998550415


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.685307927980986, Training Loss Force: 2.8732398520216504, time: 1.4636588096618652
Validation Loss Energy: 1.8328962316506616, Validation Loss Force: 2.6299785715904185, time: 0.12403750419616699
Test Loss Energy: 12.023098746034057, Test Loss Force: 9.40909475567039, time: 15.593753814697266


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.517966689686637, Training Loss Force: 2.873335636002865, time: 1.4933602809906006
Validation Loss Energy: 1.7460587161105865, Validation Loss Force: 2.6032271003986542, time: 0.11263561248779297
Test Loss Energy: 12.501846955406899, Test Loss Force: 9.43569651858444, time: 15.410065412521362


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.4891379095515753, Training Loss Force: 2.8799012584396917, time: 1.499525547027588
Validation Loss Energy: 1.4215160978948034, Validation Loss Force: 2.7158046125351167, time: 0.12391781806945801
Test Loss Energy: 11.359506909694474, Test Loss Force: 9.44378444486806, time: 15.547404289245605

wandb: - 0.039 MB of 0.048 MB uploadedwandb: \ 0.057 MB of 0.058 MB uploadedwandb: | 0.057 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–…â–â–â–‡â–‚â–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–…â–†â–ƒâ–…â–‚â–†â–ˆâ–ƒ
wandb:   test_error_force â–ˆâ–„â–…â–ˆâ–„â–†â–†â–„â–„â–…â–†â–…â–…â–„â–ƒâ–â–ƒâ–â–‚â–ƒ
wandb:          test_loss â–…â–†â–ƒâ–„â–ˆâ–ƒâ–„â–…â–„â–ƒâ–ƒâ–„â–„â–„â–â–‚â–â–‚â–…â–‚
wandb: train_error_energy â–ˆâ–â–‚â–‚â–‚â–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–„â–‚â–‚â–„â–ƒâ–â–‚â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–ƒâ–‚â–â–‚â–â–
wandb: valid_error_energy â–‚â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–â–ˆâ–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚
wandb:  valid_error_force â–…â–…â–‚â–…â–ƒâ–â–ˆâ–‡â–…â–…â–‚â–‡â–†â–„â–‡â–ƒâ–‡â–„â–ƒâ–†
wandb:         valid_loss â–ƒâ–„â–â–…â–ƒâ–‚â–„â–†â–…â–ƒâ–ˆâ–†â–„â–ƒâ–„â–ƒâ–…â–ƒâ–‚â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1360
wandb:                 lr 0.0001
wandb:  test_error_energy 11.35951
wandb:   test_error_force 9.44378
wandb:          test_loss 5.25161
wandb: train_error_energy 1.48914
wandb:  train_error_force 2.8799
wandb:         train_loss 1.29271
wandb: valid_error_energy 1.42152
wandb:  valid_error_force 2.7158
wandb:         valid_loss 1.33273
wandb: 
wandb: ğŸš€ View run al_48_6 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/jin8n47j
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_011635-jin8n47j/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6995373368263245, Uncertainty Bias: 0.026828184723854065
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
1.9073486e-06 0.0016147494
0.032032743 0.2779827
Found uncertainty sample 0 after 1407 steps.
Found uncertainty sample 2 after 1258 steps.
Found uncertainty sample 3 after 1676 steps.
Found uncertainty sample 4 after 3224 steps.
Found uncertainty sample 5 after 1017 steps.
Found uncertainty sample 6 after 1213 steps.
Found uncertainty sample 8 after 1921 steps.
Found uncertainty sample 9 after 2312 steps.
Found uncertainty sample 10 after 28 steps.
Found uncertainty sample 11 after 1671 steps.
Found uncertainty sample 13 after 230 steps.
Found uncertainty sample 14 after 116 steps.
Found uncertainty sample 15 after 1792 steps.
Found uncertainty sample 16 after 2771 steps.
Found uncertainty sample 17 after 2739 steps.
Found uncertainty sample 18 after 91 steps.
Found uncertainty sample 19 after 1539 steps.
Found uncertainty sample 21 after 46 steps.
Found uncertainty sample 22 after 3985 steps.
Found uncertainty sample 23 after 1522 steps.
Found uncertainty sample 24 after 1175 steps.
Found uncertainty sample 25 after 3982 steps.
Found uncertainty sample 26 after 3056 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 29 after 2010 steps.
Found uncertainty sample 30 after 1264 steps.
Found uncertainty sample 32 after 3126 steps.
Found uncertainty sample 35 after 1148 steps.
Found uncertainty sample 36 after 1251 steps.
Found uncertainty sample 39 after 87 steps.
Found uncertainty sample 40 after 143 steps.
Found uncertainty sample 41 after 3102 steps.
Found uncertainty sample 42 after 2186 steps.
Found uncertainty sample 43 after 1413 steps.
Found uncertainty sample 44 after 691 steps.
Found uncertainty sample 45 after 1772 steps.
Found uncertainty sample 46 after 854 steps.
Found uncertainty sample 48 after 744 steps.
Found uncertainty sample 49 after 1222 steps.
Found uncertainty sample 50 after 2185 steps.
Found uncertainty sample 52 after 986 steps.
Found uncertainty sample 55 after 2635 steps.
Found uncertainty sample 56 after 375 steps.
Found uncertainty sample 57 after 1713 steps.
Found uncertainty sample 58 after 28 steps.
Found uncertainty sample 59 after 444 steps.
Found uncertainty sample 60 after 139 steps.
Found uncertainty sample 61 after 2734 steps.
Found uncertainty sample 62 after 61 steps.
Found uncertainty sample 63 after 1058 steps.
Found uncertainty sample 65 after 77 steps.
Found uncertainty sample 66 after 3422 steps.
Found uncertainty sample 67 after 1788 steps.
Found uncertainty sample 69 after 1018 steps.
Found uncertainty sample 70 after 22 steps.
Found uncertainty sample 71 after 915 steps.
Found uncertainty sample 73 after 857 steps.
Found uncertainty sample 75 after 7 steps.
Found uncertainty sample 76 after 3791 steps.
Found uncertainty sample 77 after 1055 steps.
Found uncertainty sample 80 after 1994 steps.
Found uncertainty sample 82 after 11 steps.
Found uncertainty sample 83 after 1123 steps.
Found uncertainty sample 85 after 985 steps.
Found uncertainty sample 86 after 1690 steps.
Found uncertainty sample 89 after 3144 steps.
Found uncertainty sample 90 after 384 steps.
Found uncertainty sample 91 after 186 steps.
Found uncertainty sample 94 after 1695 steps.
Found uncertainty sample 98 after 168 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_025905-ihg0v4mo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_7
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/ihg0v4mo
Training model 7. Added 70 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.910880125346124, Training Loss Force: 3.131747130704667, time: 1.5419700145721436
Validation Loss Energy: 1.928407935104927, Validation Loss Force: 2.8910757360532777, time: 0.12076425552368164
Test Loss Energy: 12.09303335635671, Test Loss Force: 9.32475422570209, time: 15.410118341445923


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6651535000001851, Training Loss Force: 2.9561194594536384, time: 1.5322902202606201
Validation Loss Energy: 1.0483908017457713, Validation Loss Force: 2.6188742839521453, time: 0.12020015716552734
Test Loss Energy: 11.584499476423824, Test Loss Force: 9.37821914878621, time: 15.692635297775269


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7110311115033499, Training Loss Force: 2.9410005559747594, time: 1.5022852420806885
Validation Loss Energy: 1.377531649159536, Validation Loss Force: 2.721968205828971, time: 0.11432099342346191
Test Loss Energy: 11.721041479094497, Test Loss Force: 9.359792211413046, time: 15.458781719207764


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5727674788074075, Training Loss Force: 2.9379289835527826, time: 1.5238227844238281
Validation Loss Energy: 1.829538881935649, Validation Loss Force: 2.6631279953521263, time: 0.11725997924804688
Test Loss Energy: 11.049582529652406, Test Loss Force: 9.363975915182323, time: 15.582121849060059


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5849226596651405, Training Loss Force: 2.938047027889344, time: 1.4862666130065918
Validation Loss Energy: 2.387979851108358, Validation Loss Force: 2.8165605083797005, time: 0.12207746505737305
Test Loss Energy: 12.487208904925332, Test Loss Force: 9.342207383102025, time: 15.463170528411865


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7111626817534755, Training Loss Force: 2.9423962164922153, time: 1.5357141494750977
Validation Loss Energy: 1.545817474568083, Validation Loss Force: 2.76966382555213, time: 0.15574240684509277
Test Loss Energy: 11.523505314435482, Test Loss Force: 9.346251031475932, time: 15.908103466033936


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.501837173664184, Training Loss Force: 2.92691071053148, time: 1.5148108005523682
Validation Loss Energy: 1.0943586202311038, Validation Loss Force: 2.6548474864879363, time: 0.11377382278442383
Test Loss Energy: 11.322367346936502, Test Loss Force: 9.336571228312582, time: 15.658270835876465


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7087812476082853, Training Loss Force: 2.9422419265669, time: 1.5563876628875732
Validation Loss Energy: 3.27820530712986, Validation Loss Force: 2.639805740342222, time: 0.11665511131286621
Test Loss Energy: 13.018754640827403, Test Loss Force: 9.330626913414862, time: 15.518030166625977


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.8648593669816769, Training Loss Force: 2.9432268911634036, time: 1.544198989868164
Validation Loss Energy: 1.6722290867373206, Validation Loss Force: 2.7359106344012396, time: 0.12178730964660645
Test Loss Energy: 11.964099839490071, Test Loss Force: 9.379177423525721, time: 15.708548069000244


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.9632908567234444, Training Loss Force: 2.934710370323242, time: 1.5460364818572998
Validation Loss Energy: 1.6233966853569135, Validation Loss Force: 2.7556145157898166, time: 0.12008357048034668
Test Loss Energy: 11.064885225384234, Test Loss Force: 9.329868234168211, time: 15.625116109848022


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.7831059781163698, Training Loss Force: 2.9501483586882356, time: 1.5625083446502686
Validation Loss Energy: 1.934030609376036, Validation Loss Force: 2.7026083482708523, time: 0.11891865730285645
Test Loss Energy: 11.076895178387419, Test Loss Force: 9.28529642298907, time: 15.669718503952026


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.854268726343976, Training Loss Force: 2.93121074863154, time: 1.5780761241912842
Validation Loss Energy: 2.143323129031358, Validation Loss Force: 2.6015913728526687, time: 0.11409544944763184
Test Loss Energy: 12.247586359762407, Test Loss Force: 9.3470892234264, time: 15.536383152008057


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6855798252474707, Training Loss Force: 2.947539250289539, time: 1.5415644645690918
Validation Loss Energy: 1.8128806134577857, Validation Loss Force: 2.620635708231007, time: 0.12029695510864258
Test Loss Energy: 11.013673238407627, Test Loss Force: 9.322448273353318, time: 15.86621642112732


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.9160658482402033, Training Loss Force: 2.9448479324421646, time: 1.5317816734313965
Validation Loss Energy: 1.627817948251169, Validation Loss Force: 2.7385007219066715, time: 0.12205386161804199
Test Loss Energy: 11.093552112890569, Test Loss Force: 9.308400419684007, time: 15.570624589920044


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.7796858295169509, Training Loss Force: 2.9281875349998563, time: 1.5548436641693115
Validation Loss Energy: 1.4742925821897428, Validation Loss Force: 2.6775232216326383, time: 0.12081718444824219
Test Loss Energy: 11.105695723340048, Test Loss Force: 9.309843068598525, time: 15.709985256195068


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5731471198048075, Training Loss Force: 2.8987887899177824, time: 1.5786070823669434
Validation Loss Energy: 1.6388056685915486, Validation Loss Force: 2.7190909864598654, time: 0.1136319637298584
Test Loss Energy: 10.979979278998265, Test Loss Force: 9.314906653578259, time: 15.611458539962769


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.7927593424663195, Training Loss Force: 2.942729562100075, time: 1.5295896530151367
Validation Loss Energy: 1.7983030768140449, Validation Loss Force: 2.7081307657590075, time: 0.11342716217041016
Test Loss Energy: 11.037187421980349, Test Loss Force: 9.314775770752735, time: 15.46747350692749


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.533913142873746, Training Loss Force: 2.913654251374766, time: 1.516672134399414
Validation Loss Energy: 1.8613077569926042, Validation Loss Force: 2.7883199372745437, time: 0.11714816093444824
Test Loss Energy: 12.015552159920977, Test Loss Force: 9.300314708433776, time: 15.595180988311768


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.7570478454518397, Training Loss Force: 2.920682115165726, time: 1.535299301147461
Validation Loss Energy: 1.4526908916704233, Validation Loss Force: 2.6927632546528013, time: 0.11063432693481445
Test Loss Energy: 11.701966171137443, Test Loss Force: 9.251984195439091, time: 15.504676103591919


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6348626718076282, Training Loss Force: 2.9182103045624084, time: 1.5235421657562256
Validation Loss Energy: 1.247406026970796, Validation Loss Force: 2.7308102862791905, time: 0.11791825294494629
Test Loss Energy: 11.51874995951245, Test Loss Force: 9.301928530465352, time: 15.675053596496582

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–ƒâ–„â–â–†â–ƒâ–‚â–ˆâ–„â–â–â–…â–â–â–â–â–â–…â–ƒâ–ƒ
wandb:   test_error_force â–…â–ˆâ–‡â–‡â–†â–†â–†â–…â–ˆâ–…â–ƒâ–†â–…â–„â–„â–„â–„â–„â–â–„
wandb:          test_loss â–†â–…â–†â–ƒâ–†â–…â–„â–ˆâ–‡â–‚â–â–†â–‚â–‚â–‚â–â–‚â–„â–‚â–ƒ
wandb: train_error_energy â–ˆâ–‚â–‚â–â–â–‚â–â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–â–‚â–â–‚â–‚
wandb:  train_error_force â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚
wandb:         train_loss â–ˆâ–‚â–‚â–‚â–â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚
wandb: valid_error_energy â–„â–â–‚â–ƒâ–…â–ƒâ–â–ˆâ–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–‚
wandb:  valid_error_force â–ˆâ–â–„â–‚â–†â–…â–‚â–‚â–„â–…â–ƒâ–â–â–„â–ƒâ–„â–„â–†â–ƒâ–„
wandb:         valid_loss â–ˆâ–â–„â–ƒâ–ˆâ–„â–â–†â–†â–†â–„â–‚â–‚â–ƒâ–†â–‚â–…â–…â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1423
wandb:                 lr 0.0001
wandb:  test_error_energy 11.51875
wandb:   test_error_force 9.30193
wandb:          test_loss 5.18241
wandb: train_error_energy 1.63486
wandb:  train_error_force 2.91821
wandb:         train_loss 1.3355
wandb: valid_error_energy 1.24741
wandb:  valid_error_force 2.73081
wandb:         valid_loss 1.35235
wandb: 
wandb: ğŸš€ View run al_48_7 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/ihg0v4mo
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_025905-ihg0v4mo/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6856378316879272, Uncertainty Bias: 0.030926093459129333
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
5.558133e-06 4.067272e-05
0.052246228 0.3356257
Found uncertainty sample 0 after 1521 steps.
Found uncertainty sample 4 after 1180 steps.
Found uncertainty sample 5 after 2309 steps.
Found uncertainty sample 6 after 3290 steps.
Found uncertainty sample 7 after 404 steps.
Found uncertainty sample 9 after 372 steps.
Found uncertainty sample 10 after 1241 steps.
Found uncertainty sample 11 after 256 steps.
Found uncertainty sample 12 after 1565 steps.
Found uncertainty sample 14 after 3886 steps.
Found uncertainty sample 15 after 1903 steps.
Found uncertainty sample 17 after 1960 steps.
Found uncertainty sample 18 after 1705 steps.
Found uncertainty sample 19 after 3984 steps.
Found uncertainty sample 20 after 367 steps.
Found uncertainty sample 22 after 851 steps.
Found uncertainty sample 25 after 989 steps.
Found uncertainty sample 26 after 1815 steps.
Found uncertainty sample 27 after 226 steps.
Found uncertainty sample 28 after 2681 steps.
Found uncertainty sample 29 after 517 steps.
Found uncertainty sample 30 after 6 steps.
Found uncertainty sample 33 after 1442 steps.
Found uncertainty sample 34 after 1173 steps.
Found uncertainty sample 35 after 764 steps.
Found uncertainty sample 36 after 3776 steps.
Found uncertainty sample 37 after 3999 steps.
Found uncertainty sample 39 after 260 steps.
Found uncertainty sample 40 after 25 steps.
Found uncertainty sample 41 after 3486 steps.
Found uncertainty sample 46 after 1323 steps.
Found uncertainty sample 49 after 776 steps.
Found uncertainty sample 51 after 876 steps.
Found uncertainty sample 52 after 1514 steps.
Found uncertainty sample 53 after 547 steps.
Found uncertainty sample 54 after 1677 steps.
Found uncertainty sample 56 after 449 steps.
Found uncertainty sample 57 after 875 steps.
Found uncertainty sample 58 after 1119 steps.
Found uncertainty sample 59 after 3803 steps.
Found uncertainty sample 60 after 982 steps.
Found uncertainty sample 61 after 3485 steps.
Found uncertainty sample 62 after 3769 steps.
Found uncertainty sample 64 after 2343 steps.
Found uncertainty sample 65 after 3106 steps.
Found uncertainty sample 67 after 34 steps.
Found uncertainty sample 69 after 2615 steps.
Found uncertainty sample 70 after 2964 steps.
Found uncertainty sample 74 after 2499 steps.
Found uncertainty sample 75 after 1933 steps.
Found uncertainty sample 76 after 2080 steps.
Found uncertainty sample 77 after 1484 steps.
Found uncertainty sample 78 after 803 steps.
Found uncertainty sample 79 after 3069 steps.
Found uncertainty sample 80 after 564 steps.
Found uncertainty sample 81 after 1266 steps.
Found uncertainty sample 83 after 275 steps.
Found uncertainty sample 84 after 413 steps.
Found uncertainty sample 85 after 641 steps.
Found uncertainty sample 86 after 2783 steps.
Found uncertainty sample 87 after 472 steps.
Found uncertainty sample 88 after 1000 steps.
Found uncertainty sample 89 after 168 steps.
Found uncertainty sample 90 after 3876 steps.
Found uncertainty sample 91 after 617 steps.
Found uncertainty sample 92 after 1338 steps.
Found uncertainty sample 94 after 719 steps.
Found uncertainty sample 95 after 80 steps.
Found uncertainty sample 96 after 2313 steps.
Found uncertainty sample 97 after 648 steps.
Found uncertainty sample 98 after 3020 steps.
Found uncertainty sample 99 after 1086 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_044531-myq75xny
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_8
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/myq75xny
Training model 8. Added 72 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.6774202224134656, Training Loss Force: 3.1930511158501225, time: 1.6045513153076172
Validation Loss Energy: 2.0685856775501446, Validation Loss Force: 2.8583301227877103, time: 0.12147283554077148
Test Loss Energy: 11.894271765278578, Test Loss Force: 9.231679683848094, time: 15.568947792053223


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.4949074989915483, Training Loss Force: 2.9881138057082093, time: 1.5755541324615479
Validation Loss Energy: 1.1923210203054504, Validation Loss Force: 2.732088741180535, time: 0.11337542533874512
Test Loss Energy: 11.394173082232403, Test Loss Force: 9.225783353568415, time: 16.069153785705566


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5262280936813373, Training Loss Force: 2.954611347122096, time: 1.5964360237121582
Validation Loss Energy: 1.2667928274955718, Validation Loss Force: 2.774169424385406, time: 0.12550687789916992
Test Loss Energy: 11.39265700612685, Test Loss Force: 9.264628244884683, time: 15.55477237701416


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.8287379990538535, Training Loss Force: 2.9966575876739685, time: 1.6115682125091553
Validation Loss Energy: 1.7671717591891674, Validation Loss Force: 2.730530391162759, time: 0.11386704444885254
Test Loss Energy: 11.798391946591233, Test Loss Force: 9.182183979352788, time: 15.734679698944092


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.53204741678738, Training Loss Force: 2.9779596094144782, time: 1.581094741821289
Validation Loss Energy: 1.2752928881376464, Validation Loss Force: 2.8102510667952636, time: 0.12420916557312012
Test Loss Energy: 11.295889924338828, Test Loss Force: 9.20866889575816, time: 15.58119249343872


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6392876845187343, Training Loss Force: 2.954729745518391, time: 1.8141396045684814
Validation Loss Energy: 1.3043808823449174, Validation Loss Force: 2.6848171948457953, time: 0.11612272262573242
Test Loss Energy: 11.536202963025854, Test Loss Force: 9.229362856851191, time: 15.603710889816284


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6995458852803365, Training Loss Force: 2.947638292262276, time: 1.6041576862335205
Validation Loss Energy: 2.0024068486963844, Validation Loss Force: 2.832087116590672, time: 0.12065792083740234
Test Loss Energy: 10.937873406200534, Test Loss Force: 9.229112324916844, time: 15.714311122894287


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7475549426210584, Training Loss Force: 2.9673618703949844, time: 1.5822782516479492
Validation Loss Energy: 1.3333249209493463, Validation Loss Force: 2.8043061346027107, time: 0.12297415733337402
Test Loss Energy: 11.106554512769415, Test Loss Force: 9.197909606565036, time: 15.551758050918579


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6052400288196884, Training Loss Force: 2.9695894193673555, time: 1.5965440273284912
Validation Loss Energy: 1.803433262601648, Validation Loss Force: 2.8057938985012267, time: 0.11124444007873535
Test Loss Energy: 11.861485134840784, Test Loss Force: 9.177780292457022, time: 15.623760223388672


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.7889387590455414, Training Loss Force: 2.9726759501024556, time: 1.593231201171875
Validation Loss Energy: 1.2595932660002886, Validation Loss Force: 2.8546695533632302, time: 0.11274909973144531
Test Loss Energy: 11.160278702014276, Test Loss Force: 9.242997497207627, time: 15.826998233795166


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.94784308918227, Training Loss Force: 2.971630039960543, time: 1.5830421447753906
Validation Loss Energy: 1.517715904677272, Validation Loss Force: 2.728946768196921, time: 0.11975622177124023
Test Loss Energy: 11.605796579450558, Test Loss Force: 9.188194740588948, time: 15.683120727539062


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6040765671348813, Training Loss Force: 2.9460499079668283, time: 1.5864596366882324
Validation Loss Energy: 1.5250456719556365, Validation Loss Force: 2.7607453695848574, time: 0.12709927558898926
Test Loss Energy: 11.640676873437146, Test Loss Force: 9.200022230799632, time: 15.542006254196167


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6991188650789648, Training Loss Force: 2.9562197763783185, time: 1.8557312488555908
Validation Loss Energy: 1.2903777764303839, Validation Loss Force: 2.7769683755497185, time: 0.11672616004943848
Test Loss Energy: 11.33872438457172, Test Loss Force: 9.190526783699424, time: 15.515936136245728


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.7209208370197637, Training Loss Force: 2.9631899318016, time: 1.5524640083312988
Validation Loss Energy: 1.3489966537761764, Validation Loss Force: 2.7560761089293244, time: 0.11650395393371582
Test Loss Energy: 11.110659743734763, Test Loss Force: 9.209255152142596, time: 15.612245559692383


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6259606281882646, Training Loss Force: 2.940677711612269, time: 1.6076810359954834
Validation Loss Energy: 1.650570782587649, Validation Loss Force: 2.8635911266985463, time: 0.11751246452331543
Test Loss Energy: 11.382436748806315, Test Loss Force: 9.196833170286498, time: 15.55487847328186


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5756552467193383, Training Loss Force: 2.960154856589775, time: 1.5678679943084717
Validation Loss Energy: 1.920643239317279, Validation Loss Force: 2.5817914097276233, time: 0.12308239936828613
Test Loss Energy: 11.007800961058939, Test Loss Force: 9.178693804869166, time: 15.706188440322876


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6646987454881197, Training Loss Force: 2.97015738676928, time: 1.5836586952209473
Validation Loss Energy: 2.087328964243189, Validation Loss Force: 2.749844490211389, time: 0.11966919898986816
Test Loss Energy: 10.80444890680647, Test Loss Force: 9.151067945749746, time: 15.529398679733276


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.8916273129266161, Training Loss Force: 2.967972647124624, time: 1.6250262260437012
Validation Loss Energy: 1.8399656430301499, Validation Loss Force: 2.5812285021267183, time: 0.11943674087524414
Test Loss Energy: 11.90592523780301, Test Loss Force: 9.25793970674769, time: 15.766945600509644


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.9271499266794665, Training Loss Force: 2.9666473156304196, time: 1.562757968902588
Validation Loss Energy: 1.848447055131253, Validation Loss Force: 2.864002173941929, time: 0.12258100509643555
Test Loss Energy: 10.81112086716726, Test Loss Force: 9.182598293187517, time: 15.614867210388184


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.745816604868752, Training Loss Force: 2.9515108782459896, time: 1.582747459411621
Validation Loss Energy: 1.2052981224628354, Validation Loss Force: 2.7393635648471415, time: 0.11620116233825684
Test Loss Energy: 11.354272039680207, Test Loss Force: 9.174593470293416, time: 16.00169348716736

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–…â–…â–‡â–„â–†â–‚â–ƒâ–ˆâ–ƒâ–†â–†â–„â–ƒâ–…â–‚â–â–ˆâ–â–„
wandb:   test_error_force â–†â–†â–ˆâ–ƒâ–…â–†â–†â–„â–ƒâ–‡â–ƒâ–„â–ƒâ–…â–„â–ƒâ–â–ˆâ–ƒâ–‚
wandb:          test_loss â–ˆâ–†â–‡â–†â–…â–†â–„â–ƒâ–ˆâ–„â–†â–‡â–…â–†â–…â–ƒâ–â–ˆâ–‚â–ƒ
wandb: train_error_energy â–ˆâ–â–â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–‚â–â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–â–‚â–‚â–
wandb: valid_error_energy â–ˆâ–â–‚â–…â–‚â–‚â–‡â–‚â–†â–‚â–„â–„â–‚â–‚â–…â–‡â–ˆâ–†â–†â–
wandb:  valid_error_force â–ˆâ–…â–†â–…â–‡â–„â–‡â–‡â–‡â–ˆâ–…â–…â–†â–…â–ˆâ–â–…â–â–ˆâ–…
wandb:         valid_loss â–‡â–‚â–„â–ƒâ–…â–‚â–†â–„â–‡â–…â–†â–„â–ƒâ–…â–‡â–â–†â–„â–ˆâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 1487
wandb:                 lr 0.0001
wandb:  test_error_energy 11.35427
wandb:   test_error_force 9.17459
wandb:          test_loss 5.08173
wandb: train_error_energy 1.74582
wandb:  train_error_force 2.95151
wandb:         train_loss 1.35133
wandb: valid_error_energy 1.2053
wandb:  valid_error_force 2.73936
wandb:         valid_loss 1.31337
wandb: 
wandb: ğŸš€ View run al_48_8 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/myq75xny
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_044531-myq75xny/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7052116990089417, Uncertainty Bias: 0.025835677981376648
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
2.4437904e-06 2.5622547e-05
0.03177713 0.20718428
Found uncertainty sample 0 after 3316 steps.
Found uncertainty sample 2 after 3758 steps.
Found uncertainty sample 3 after 1133 steps.
Found uncertainty sample 4 after 66 steps.
Found uncertainty sample 5 after 2275 steps.
Found uncertainty sample 6 after 441 steps.
Found uncertainty sample 7 after 2978 steps.
Found uncertainty sample 8 after 315 steps.
Found uncertainty sample 9 after 2599 steps.
Found uncertainty sample 10 after 2878 steps.
Found uncertainty sample 11 after 2276 steps.
Found uncertainty sample 13 after 1280 steps.
Found uncertainty sample 14 after 2268 steps.
Found uncertainty sample 15 after 223 steps.
Found uncertainty sample 17 after 686 steps.
Found uncertainty sample 19 after 2533 steps.
Found uncertainty sample 20 after 1498 steps.
Found uncertainty sample 21 after 3780 steps.
Found uncertainty sample 22 after 1475 steps.
Found uncertainty sample 23 after 73 steps.
Found uncertainty sample 24 after 1447 steps.
Found uncertainty sample 25 after 1806 steps.
Found uncertainty sample 28 after 2683 steps.
Found uncertainty sample 29 after 53 steps.
Found uncertainty sample 31 after 254 steps.
Found uncertainty sample 32 after 10 steps.
Found uncertainty sample 33 after 324 steps.
Found uncertainty sample 35 after 61 steps.
Found uncertainty sample 36 after 1555 steps.
Found uncertainty sample 37 after 1747 steps.
Found uncertainty sample 38 after 1415 steps.
Found uncertainty sample 39 after 1238 steps.
Found uncertainty sample 40 after 1409 steps.
Found uncertainty sample 41 after 324 steps.
Found uncertainty sample 42 after 664 steps.
Found uncertainty sample 44 after 1807 steps.
Found uncertainty sample 46 after 539 steps.
Found uncertainty sample 47 after 1150 steps.
Found uncertainty sample 49 after 2075 steps.
Found uncertainty sample 50 after 2967 steps.
Found uncertainty sample 51 after 194 steps.
Found uncertainty sample 52 after 401 steps.
Found uncertainty sample 53 after 806 steps.
Found uncertainty sample 54 after 1005 steps.
Found uncertainty sample 56 after 47 steps.
Found uncertainty sample 57 after 431 steps.
Found uncertainty sample 58 after 2581 steps.
Found uncertainty sample 59 after 1987 steps.
Found uncertainty sample 60 after 1359 steps.
Found uncertainty sample 62 after 419 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 66 after 609 steps.
Found uncertainty sample 68 after 127 steps.
Found uncertainty sample 69 after 1694 steps.
Found uncertainty sample 71 after 1428 steps.
Found uncertainty sample 72 after 2122 steps.
Found uncertainty sample 73 after 2817 steps.
Found uncertainty sample 74 after 3431 steps.
Found uncertainty sample 75 after 39 steps.
Found uncertainty sample 77 after 1105 steps.
Found uncertainty sample 78 after 2106 steps.
Found uncertainty sample 79 after 1983 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 83 after 1461 steps.
Found uncertainty sample 84 after 461 steps.
Found uncertainty sample 86 after 3577 steps.
Found uncertainty sample 87 after 3481 steps.
Found uncertainty sample 88 after 2509 steps.
Found uncertainty sample 89 after 1052 steps.
Found uncertainty sample 92 after 149 steps.
Found uncertainty sample 93 after 435 steps.
Found uncertainty sample 94 after 1599 steps.
Found uncertainty sample 95 after 3408 steps.
Found uncertainty sample 97 after 273 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 1142 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_062149-6ebxaj60
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_9
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/6ebxaj60
Training model 9. Added 76 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.8725149789844067, Training Loss Force: 3.191426464909737, time: 1.674994945526123
Validation Loss Energy: 1.7095225503066036, Validation Loss Force: 2.9286532697854786, time: 0.12116384506225586
Test Loss Energy: 10.989602554509531, Test Loss Force: 9.159816702762969, time: 15.788273334503174


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.599156838062251, Training Loss Force: 3.007820489895109, time: 1.6607937812805176
Validation Loss Energy: 1.3155534180764172, Validation Loss Force: 2.7668784572057192, time: 0.12558245658874512
Test Loss Energy: 11.076555700038634, Test Loss Force: 9.133850612462457, time: 15.949043989181519


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5567359095859996, Training Loss Force: 2.9984788921144148, time: 1.6836633682250977
Validation Loss Energy: 1.0677622937667646, Validation Loss Force: 2.7124199301824414, time: 0.11648440361022949
Test Loss Energy: 11.259066202025952, Test Loss Force: 9.149878034476506, time: 15.7245774269104


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6696764769484957, Training Loss Force: 3.006588016172399, time: 1.6592035293579102
Validation Loss Energy: 1.3916471473254768, Validation Loss Force: 2.80553782898341, time: 0.12424206733703613
Test Loss Energy: 11.039358709625537, Test Loss Force: 9.148128395923983, time: 15.897770166397095


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5099425815962864, Training Loss Force: 2.9938684592839557, time: 1.660003662109375
Validation Loss Energy: 1.3901893324270875, Validation Loss Force: 2.740582344417377, time: 0.12741446495056152
Test Loss Energy: 11.10519825482029, Test Loss Force: 9.14554908513889, time: 15.730368614196777


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.481730520094278, Training Loss Force: 3.0038242649102163, time: 1.905212163925171
Validation Loss Energy: 1.2332631404999064, Validation Loss Force: 2.7133514665544305, time: 0.11927008628845215
Test Loss Energy: 11.179278591581866, Test Loss Force: 9.151782193758232, time: 15.765584468841553


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.7577471887499383, Training Loss Force: 2.987038464951157, time: 1.6961324214935303
Validation Loss Energy: 1.3019810656246809, Validation Loss Force: 2.732698631971231, time: 0.11874175071716309
Test Loss Energy: 11.352537122939074, Test Loss Force: 9.074535846820453, time: 15.906215906143188


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.591117314017309, Training Loss Force: 2.99215944632335, time: 1.6518487930297852
Validation Loss Energy: 1.3763668111899012, Validation Loss Force: 2.8092007364339224, time: 0.12874841690063477
Test Loss Energy: 11.014008501198129, Test Loss Force: 9.10221685327641, time: 15.762251377105713


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.7355604350496938, Training Loss Force: 3.0046808685703317, time: 1.6277885437011719
Validation Loss Energy: 1.4332638993648075, Validation Loss Force: 2.6926107262878407, time: 0.11545372009277344
Test Loss Energy: 11.030048262113961, Test Loss Force: 9.057600219147936, time: 15.978013515472412


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6866619020690135, Training Loss Force: 2.994074878126499, time: 1.6642684936523438
Validation Loss Energy: 2.0392558646503955, Validation Loss Force: 2.7343892474616878, time: 0.11456632614135742
Test Loss Energy: 12.251373078252897, Test Loss Force: 9.111397546463682, time: 15.817288160324097


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.8250720593436425, Training Loss Force: 2.9976035751575045, time: 1.6495087146759033
Validation Loss Energy: 1.2452913937592065, Validation Loss Force: 2.7517633415764227, time: 0.11752438545227051
Test Loss Energy: 11.348585037050615, Test Loss Force: 9.088070997203522, time: 16.003663539886475


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.7666721634700648, Training Loss Force: 2.984084608769571, time: 1.6559467315673828
Validation Loss Energy: 1.446855902368672, Validation Loss Force: 2.7362254885735813, time: 0.12122440338134766
Test Loss Energy: 10.938477914460096, Test Loss Force: 9.087169735091825, time: 16.267446041107178


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.7827275222893832, Training Loss Force: 2.9920748002380195, time: 1.6608126163482666
Validation Loss Energy: 1.766755701890001, Validation Loss Force: 2.7148655054121664, time: 0.11530208587646484
Test Loss Energy: 11.580999294652512, Test Loss Force: 9.093287265600551, time: 15.742587089538574


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.9172561950811633, Training Loss Force: 2.985209537839067, time: 1.652033805847168
Validation Loss Energy: 1.664261149686727, Validation Loss Force: 2.7168104090672727, time: 0.11447739601135254
Test Loss Energy: 10.77846410297124, Test Loss Force: 9.040565628446839, time: 15.931730031967163


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.5902202640973289, Training Loss Force: 2.9930271334538117, time: 1.6661159992218018
Validation Loss Energy: 1.5887719671688614, Validation Loss Force: 2.778863178646991, time: 0.1247100830078125
Test Loss Energy: 11.503393991061813, Test Loss Force: 9.039030060047466, time: 15.80152416229248


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.8887723443012754, Training Loss Force: 2.9974587356944817, time: 1.6448216438293457
Validation Loss Energy: 2.839139493780497, Validation Loss Force: 2.743601292697866, time: 0.12384843826293945
Test Loss Energy: 12.226153622080625, Test Loss Force: 9.072994225550895, time: 15.90302324295044


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.7456468418103757, Training Loss Force: 2.9905401417629447, time: 1.6770336627960205
Validation Loss Energy: 1.3397907319415112, Validation Loss Force: 2.804264495472422, time: 0.1164395809173584
Test Loss Energy: 11.573238796936039, Test Loss Force: 9.03569567469111, time: 15.788404941558838


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.640637191616896, Training Loss Force: 2.9846386050603715, time: 1.7139384746551514
Validation Loss Energy: 1.276179835013163, Validation Loss Force: 2.6813258539722202, time: 0.1584932804107666
Test Loss Energy: 11.129398604441354, Test Loss Force: 9.048006056170241, time: 15.880128383636475


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.794739007198715, Training Loss Force: 2.9807268489343457, time: 1.6564624309539795
Validation Loss Energy: 1.4429548502665903, Validation Loss Force: 2.6596668261478245, time: 0.11591362953186035
Test Loss Energy: 11.619871046163134, Test Loss Force: 9.010384525463001, time: 15.888837099075317


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6410461984838192, Training Loss Force: 2.9832912067927526, time: 1.6454081535339355
Validation Loss Energy: 1.3538870248936155, Validation Loss Force: 2.761422840461343, time: 0.12074470520019531
Test Loss Energy: 11.148099949477182, Test Loss Force: 9.061197573268213, time: 15.777200698852539

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–„â–‚â–‚â–ˆâ–„â–‚â–…â–â–„â–ˆâ–…â–ƒâ–…â–ƒ
wandb:   test_error_force â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–„â–…â–ƒâ–†â–…â–…â–…â–‚â–‚â–„â–‚â–ƒâ–â–ƒ
wandb:          test_loss â–‡â–…â–‡â–†â–†â–‡â–…â–ƒâ–ƒâ–ˆâ–„â–ƒâ–…â–â–„â–ˆâ–…â–ƒâ–„â–…
wandb: train_error_energy â–ˆâ–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–â–‚â–â–â–‚â–â–‚â–â–â–â–â–‚â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–
wandb: valid_error_energy â–„â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–…â–‚â–‚â–„â–ƒâ–ƒâ–ˆâ–‚â–‚â–‚â–‚
wandb:  valid_error_force â–ˆâ–„â–‚â–…â–ƒâ–‚â–ƒâ–…â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–…â–‚â–â–„
wandb:         valid_loss â–ˆâ–ƒâ–‚â–…â–ƒâ–„â–‚â–…â–‚â–…â–ƒâ–ƒâ–ƒâ–„â–†â–ˆâ–„â–â–‚â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 1555
wandb:                 lr 0.0001
wandb:  test_error_energy 11.1481
wandb:   test_error_force 9.0612
wandb:          test_loss 5.04156
wandb: train_error_energy 1.64105
wandb:  train_error_force 2.98329
wandb:         train_loss 1.3468
wandb: valid_error_energy 1.35389
wandb:  valid_error_force 2.76142
wandb:         valid_loss 1.38682
wandb: 
wandb: ğŸš€ View run al_48_9 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/6ebxaj60
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_062149-6ebxaj60/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7026374936103821, Uncertainty Bias: 0.027578502893447876
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
1.7642975e-05 0.00057685375
0.06066362 0.38803247
Found uncertainty sample 0 after 286 steps.
Found uncertainty sample 1 after 2540 steps.
Found uncertainty sample 3 after 1134 steps.
Found uncertainty sample 5 after 1740 steps.
Found uncertainty sample 6 after 238 steps.
Found uncertainty sample 7 after 1038 steps.
Found uncertainty sample 8 after 1385 steps.
Found uncertainty sample 9 after 2601 steps.
Found uncertainty sample 10 after 678 steps.
Found uncertainty sample 11 after 347 steps.
Found uncertainty sample 13 after 2066 steps.
Found uncertainty sample 14 after 564 steps.
Found uncertainty sample 15 after 2387 steps.
Found uncertainty sample 16 after 919 steps.
Found uncertainty sample 17 after 3444 steps.
Found uncertainty sample 20 after 11 steps.
Found uncertainty sample 21 after 3927 steps.
Found uncertainty sample 23 after 752 steps.
Found uncertainty sample 24 after 661 steps.
Found uncertainty sample 25 after 1027 steps.
Found uncertainty sample 26 after 244 steps.
Found uncertainty sample 27 after 2750 steps.
Found uncertainty sample 28 after 850 steps.
Found uncertainty sample 29 after 536 steps.
Found uncertainty sample 30 after 2281 steps.
Found uncertainty sample 31 after 1398 steps.
Found uncertainty sample 32 after 195 steps.
Found uncertainty sample 34 after 1531 steps.
Found uncertainty sample 37 after 241 steps.
Found uncertainty sample 39 after 757 steps.
Found uncertainty sample 40 after 2984 steps.
Found uncertainty sample 41 after 1315 steps.
Found uncertainty sample 45 after 404 steps.
Found uncertainty sample 47 after 849 steps.
Found uncertainty sample 49 after 1226 steps.
Found uncertainty sample 50 after 3402 steps.
Found uncertainty sample 51 after 1645 steps.
Found uncertainty sample 52 after 3230 steps.
Found uncertainty sample 53 after 2510 steps.
Found uncertainty sample 56 after 69 steps.
Found uncertainty sample 57 after 2476 steps.
Found uncertainty sample 60 after 696 steps.
Found uncertainty sample 62 after 1567 steps.
Found uncertainty sample 63 after 411 steps.
Found uncertainty sample 65 after 3960 steps.
Found uncertainty sample 66 after 982 steps.
Found uncertainty sample 68 after 974 steps.
Found uncertainty sample 69 after 654 steps.
Found uncertainty sample 70 after 33 steps.
Found uncertainty sample 71 after 1019 steps.
Found uncertainty sample 72 after 1828 steps.
Found uncertainty sample 73 after 3281 steps.
Found uncertainty sample 75 after 1952 steps.
Found uncertainty sample 76 after 1836 steps.
Found uncertainty sample 78 after 1012 steps.
Found uncertainty sample 79 after 2012 steps.
Found uncertainty sample 80 after 3694 steps.
Found uncertainty sample 83 after 1187 steps.
Found uncertainty sample 84 after 398 steps.
Found uncertainty sample 85 after 1203 steps.
Found uncertainty sample 86 after 2017 steps.
Found uncertainty sample 88 after 228 steps.
Found uncertainty sample 90 after 3185 steps.
Found uncertainty sample 91 after 1171 steps.
Found uncertainty sample 92 after 3317 steps.
Found uncertainty sample 94 after 2162 steps.
Found uncertainty sample 95 after 3293 steps.
Found uncertainty sample 96 after 2588 steps.
Found uncertainty sample 98 after 2867 steps.
Found uncertainty sample 99 after 1750 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_081053-cqio8efd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_10
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/cqio8efd
Training model 10. Added 70 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1208746497972863, Training Loss Force: 3.2236815751583494, time: 1.690019130706787
Validation Loss Energy: 1.445278792475352, Validation Loss Force: 2.769328819822082, time: 0.1251964569091797
Test Loss Energy: 11.337931528525687, Test Loss Force: 9.014354681360683, time: 15.854823589324951


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7869717801645295, Training Loss Force: 3.018261621235647, time: 1.7304050922393799
Validation Loss Energy: 1.384119594178242, Validation Loss Force: 2.853638261213866, time: 0.12444615364074707
Test Loss Energy: 11.162656598647517, Test Loss Force: 9.020502672075926, time: 16.01596474647522


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.713961158472418, Training Loss Force: 3.0259981384611803, time: 1.7216987609863281
Validation Loss Energy: 1.3617030459193997, Validation Loss Force: 2.7691268589067253, time: 0.11736392974853516
Test Loss Energy: 10.914991672950675, Test Loss Force: 9.002630360410405, time: 15.871300458908081


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6918318173918132, Training Loss Force: 3.040452972398845, time: 1.7597901821136475
Validation Loss Energy: 1.3741455751246987, Validation Loss Force: 2.792962032294273, time: 0.13458824157714844
Test Loss Energy: 11.002188929148907, Test Loss Force: 8.991190370472374, time: 15.926395893096924


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6576006068858402, Training Loss Force: 3.0009339842002696, time: 1.7331464290618896
Validation Loss Energy: 1.3844438561219996, Validation Loss Force: 2.7665366600667203, time: 0.12516355514526367
Test Loss Energy: 11.357178769705994, Test Loss Force: 8.986473384590864, time: 15.919659614562988


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7354515384321125, Training Loss Force: 3.0120928476096025, time: 1.7782564163208008
Validation Loss Energy: 1.638023153545472, Validation Loss Force: 2.854030715583564, time: 0.11739754676818848
Test Loss Energy: 11.305685459499323, Test Loss Force: 8.991600739216056, time: 15.714359283447266


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6299717712324424, Training Loss Force: 3.008822222069352, time: 1.7424170970916748
Validation Loss Energy: 1.3149185326740906, Validation Loss Force: 2.780814762376475, time: 0.12266063690185547
Test Loss Energy: 11.098929784941609, Test Loss Force: 9.00978659701841, time: 15.994761228561401


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.606794757630547, Training Loss Force: 3.016922275055014, time: 1.7608020305633545
Validation Loss Energy: 1.6251125181281465, Validation Loss Force: 2.7119361814629133, time: 0.1183319091796875
Test Loss Energy: 11.389578384846867, Test Loss Force: 9.036191203307942, time: 15.94290542602539


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.7033169871532885, Training Loss Force: 3.002567271117773, time: 1.7198381423950195
Validation Loss Energy: 1.3443126969570125, Validation Loss Force: 2.7698974054862084, time: 0.11625123023986816
Test Loss Energy: 11.153874187116465, Test Loss Force: 9.002167674909694, time: 16.014954805374146


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.5721805616511557, Training Loss Force: 3.0270332661957347, time: 1.754420518875122
Validation Loss Energy: 1.2838158837017515, Validation Loss Force: 2.73253218678465, time: 0.12181425094604492
Test Loss Energy: 11.213548165888941, Test Loss Force: 8.990202750037882, time: 16.487082719802856


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.685623892199127, Training Loss Force: 3.021624711230259, time: 1.954869270324707
Validation Loss Energy: 1.5269840625382243, Validation Loss Force: 2.7819960221956217, time: 0.1326150894165039
Test Loss Energy: 11.34558209063765, Test Loss Force: 8.970936730037668, time: 15.761430740356445


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6374670177062398, Training Loss Force: 2.9962690262985268, time: 1.7512798309326172
Validation Loss Energy: 1.433083537356606, Validation Loss Force: 2.704687966759245, time: 0.1199796199798584
Test Loss Energy: 11.01810237720284, Test Loss Force: 8.948874685048827, time: 15.968387603759766


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.8411722489288926, Training Loss Force: 3.004083189565065, time: 1.759704351425171
Validation Loss Energy: 1.8083161743026985, Validation Loss Force: 2.6945842429281988, time: 0.12388420104980469
Test Loss Energy: 10.733072500940061, Test Loss Force: 8.991160399904816, time: 15.846060276031494


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.8127197186271216, Training Loss Force: 3.009861370946718, time: 1.7252862453460693
Validation Loss Energy: 1.4302684983465004, Validation Loss Force: 2.792978287141926, time: 0.122222900390625
Test Loss Energy: 11.083415084866715, Test Loss Force: 9.0180304942952, time: 15.935360670089722


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.4990692170990048, Training Loss Force: 3.0059576402657937, time: 1.7266478538513184
Validation Loss Energy: 1.383840422736535, Validation Loss Force: 2.7602374612629963, time: 0.11975717544555664
Test Loss Energy: 10.936550896033799, Test Loss Force: 9.030165626995835, time: 15.85856819152832


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6849415640790169, Training Loss Force: 3.0000856846112725, time: 1.7432525157928467
Validation Loss Energy: 1.5994100969501344, Validation Loss Force: 2.706920336281744, time: 0.12099981307983398
Test Loss Energy: 11.417868828594582, Test Loss Force: 8.947799214058522, time: 16.064427852630615


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.7871624002346596, Training Loss Force: 3.0156949052584134, time: 1.7700715065002441
Validation Loss Energy: 1.3008686105716256, Validation Loss Force: 2.79718818134941, time: 0.12717986106872559
Test Loss Energy: 10.871918539372718, Test Loss Force: 8.983767313273965, time: 15.933470487594604


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.7453320757389215, Training Loss Force: 3.0063871428519175, time: 1.7446894645690918
Validation Loss Energy: 1.4336336414816206, Validation Loss Force: 2.7545468172350924, time: 0.12195849418640137
Test Loss Energy: 11.412466194283057, Test Loss Force: 8.957150618703261, time: 15.932694911956787


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6992173283627394, Training Loss Force: 2.997482018654, time: 1.7215149402618408
Validation Loss Energy: 1.606024929268211, Validation Loss Force: 2.8802012429736026, time: 0.1176292896270752
Test Loss Energy: 11.250661390734116, Test Loss Force: 8.925288897607297, time: 15.995291948318481


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.7477272164070938, Training Loss Force: 2.9913931395907283, time: 1.7265794277191162
Validation Loss Energy: 2.3634732189112646, Validation Loss Force: 2.6725214046076626, time: 0.1218419075012207
Test Loss Energy: 12.22843522305181, Test Loss Force: 8.954483891042576, time: 15.87827181816101

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–ƒâ–‚â–‚â–„â–„â–ƒâ–„â–ƒâ–ƒâ–„â–‚â–â–ƒâ–‚â–„â–‚â–„â–ƒâ–ˆ
wandb:   test_error_force â–‡â–‡â–†â–…â–…â–…â–†â–ˆâ–†â–…â–„â–‚â–…â–‡â–ˆâ–‚â–…â–ƒâ–â–ƒ
wandb:          test_loss â–†â–…â–„â–„â–…â–…â–…â–‡â–…â–†â–†â–„â–ƒâ–‡â–…â–†â–ƒâ–„â–â–ˆ
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–‚â–â–â–
wandb: valid_error_energy â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–ƒâ–â–â–ƒâ–‚â–„â–‚â–‚â–ƒâ–â–‚â–ƒâ–ˆ
wandb:  valid_error_force â–„â–‡â–„â–…â–„â–‡â–…â–‚â–„â–ƒâ–…â–‚â–‚â–…â–„â–‚â–…â–„â–ˆâ–
wandb:         valid_loss â–„â–…â–‚â–„â–„â–ˆâ–„â–â–ƒâ–ƒâ–„â–‚â–‚â–„â–ƒâ–„â–„â–‚â–‡â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1618
wandb:                 lr 0.0001
wandb:  test_error_energy 12.22844
wandb:   test_error_force 8.95448
wandb:          test_loss 5.01253
wandb: train_error_energy 1.74773
wandb:  train_error_force 2.99139
wandb:         train_loss 1.36148
wandb: valid_error_energy 2.36347
wandb:  valid_error_force 2.67252
wandb:         valid_loss 1.35597
wandb: 
wandb: ğŸš€ View run al_48_10 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/cqio8efd
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_081053-cqio8efd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.701991856098175, Uncertainty Bias: 0.028104513883590698
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
2.0861626e-05 0.0014760494
0.03075184 0.37557665
Found uncertainty sample 0 after 136 steps.
Found uncertainty sample 1 after 468 steps.
Found uncertainty sample 2 after 3927 steps.
Found uncertainty sample 4 after 2501 steps.
Found uncertainty sample 6 after 1029 steps.
Found uncertainty sample 8 after 1225 steps.
Found uncertainty sample 9 after 407 steps.
Found uncertainty sample 10 after 943 steps.
Found uncertainty sample 11 after 1296 steps.
Found uncertainty sample 13 after 787 steps.
Found uncertainty sample 14 after 2375 steps.
Found uncertainty sample 15 after 3711 steps.
Found uncertainty sample 16 after 3917 steps.
Found uncertainty sample 17 after 1960 steps.
Found uncertainty sample 18 after 26 steps.
Found uncertainty sample 19 after 441 steps.
Found uncertainty sample 21 after 558 steps.
Found uncertainty sample 22 after 838 steps.
Found uncertainty sample 23 after 4 steps.
Found uncertainty sample 24 after 1051 steps.
Found uncertainty sample 25 after 645 steps.
Found uncertainty sample 26 after 3133 steps.
Found uncertainty sample 27 after 5 steps.
Found uncertainty sample 28 after 3586 steps.
Found uncertainty sample 29 after 1369 steps.
Found uncertainty sample 30 after 211 steps.
Found uncertainty sample 33 after 583 steps.
Found uncertainty sample 36 after 685 steps.
Found uncertainty sample 37 after 16 steps.
Found uncertainty sample 38 after 567 steps.
Found uncertainty sample 42 after 1996 steps.
Found uncertainty sample 43 after 1339 steps.
Found uncertainty sample 44 after 988 steps.
Found uncertainty sample 45 after 2467 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 20 steps.
Found uncertainty sample 50 after 154 steps.
Found uncertainty sample 52 after 1746 steps.
Found uncertainty sample 53 after 987 steps.
Found uncertainty sample 56 after 1727 steps.
Found uncertainty sample 57 after 1607 steps.
Found uncertainty sample 58 after 1117 steps.
Found uncertainty sample 60 after 1531 steps.
Found uncertainty sample 61 after 1172 steps.
Found uncertainty sample 62 after 1568 steps.
Found uncertainty sample 63 after 1422 steps.
Found uncertainty sample 64 after 12 steps.
Found uncertainty sample 65 after 1109 steps.
Found uncertainty sample 66 after 1495 steps.
Found uncertainty sample 67 after 1646 steps.
Found uncertainty sample 70 after 229 steps.
Found uncertainty sample 71 after 13 steps.
Found uncertainty sample 72 after 930 steps.
Found uncertainty sample 77 after 732 steps.
Found uncertainty sample 78 after 705 steps.
Found uncertainty sample 79 after 12 steps.
Found uncertainty sample 80 after 1607 steps.
Found uncertainty sample 82 after 3706 steps.
Found uncertainty sample 83 after 627 steps.
Found uncertainty sample 84 after 1299 steps.
Found uncertainty sample 85 after 1421 steps.
Found uncertainty sample 86 after 1147 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 89 after 2751 steps.
Found uncertainty sample 90 after 516 steps.
Found uncertainty sample 91 after 604 steps.
Found uncertainty sample 93 after 1710 steps.
Found uncertainty sample 95 after 23 steps.
Found uncertainty sample 96 after 1314 steps.
Found uncertainty sample 97 after 5 steps.
Found uncertainty sample 98 after 496 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_094602-b0mfzrxx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_11
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/b0mfzrxx
Training model 11. Added 73 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.266661277883929, Training Loss Force: 3.275614576824818, time: 1.811356544494629
Validation Loss Energy: 1.346828357300418, Validation Loss Force: 2.801153362165519, time: 0.1228952407836914
Test Loss Energy: 11.211597682098406, Test Loss Force: 8.916873202340872, time: 15.963429927825928


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.8433782441024857, Training Loss Force: 3.0498187695681773, time: 1.8297843933105469
Validation Loss Energy: 1.757127877691089, Validation Loss Force: 2.800351641905606, time: 0.11903095245361328
Test Loss Energy: 11.598514696822285, Test Loss Force: 8.953652563364, time: 16.448561429977417


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.91666359445962, Training Loss Force: 3.0402509536537665, time: 1.7987258434295654
Validation Loss Energy: 1.4772869811968932, Validation Loss Force: 2.728112872245421, time: 0.12459635734558105
Test Loss Energy: 11.667343180092528, Test Loss Force: 8.857528638670752, time: 15.985211372375488


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6595210634375708, Training Loss Force: 3.0284284319807377, time: 1.8289437294006348
Validation Loss Energy: 1.4154033845007936, Validation Loss Force: 2.7663354784933745, time: 0.11801362037658691
Test Loss Energy: 11.030391181925152, Test Loss Force: 8.915119282944872, time: 15.99285078048706


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6780666882002317, Training Loss Force: 3.023296462739314, time: 1.7995917797088623
Validation Loss Energy: 2.1315236683194985, Validation Loss Force: 2.880754241797514, time: 0.12049221992492676
Test Loss Energy: 10.692014562770192, Test Loss Force: 8.865064513348687, time: 16.098457098007202


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.805156675576778, Training Loss Force: 3.0472506842951463, time: 1.7804338932037354
Validation Loss Energy: 1.6787400438988218, Validation Loss Force: 2.6696016554316846, time: 0.12793898582458496
Test Loss Energy: 11.7127858677885, Test Loss Force: 8.879049469396216, time: 15.944847106933594


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.7989714802247907, Training Loss Force: 3.0342889956741597, time: 1.7916722297668457
Validation Loss Energy: 1.2828975404766587, Validation Loss Force: 2.8520643243558754, time: 0.12925148010253906
Test Loss Energy: 11.149792496127011, Test Loss Force: 8.916706669135857, time: 16.29679822921753


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6188403074551045, Training Loss Force: 3.0166142835078555, time: 1.80364990234375
Validation Loss Energy: 1.6206167167372914, Validation Loss Force: 2.751883248911504, time: 0.12377023696899414
Test Loss Energy: 10.750254612864858, Test Loss Force: 8.90068646579904, time: 15.865776300430298


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6360096152771118, Training Loss Force: 3.025569418965289, time: 1.8181993961334229
Validation Loss Energy: 3.6737104667959426, Validation Loss Force: 2.8100041267124296, time: 0.11982536315917969
Test Loss Energy: 12.954270999176433, Test Loss Force: 8.856981488117892, time: 16.110822439193726


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.2268772829821124, Training Loss Force: 3.037940164679506, time: 1.787792682647705
Validation Loss Energy: 1.4916594952245212, Validation Loss Force: 2.8358919756217134, time: 0.12630558013916016
Test Loss Energy: 10.863129174968442, Test Loss Force: 8.899980327144373, time: 16.168590545654297


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.7672013353235074, Training Loss Force: 3.0426432983537, time: 1.8730542659759521
Validation Loss Energy: 1.2106616098480845, Validation Loss Force: 2.7787979365872335, time: 0.12256884574890137
Test Loss Energy: 11.041609710493354, Test Loss Force: 8.83085746826837, time: 15.94037389755249


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.646285888790381, Training Loss Force: 3.045163631598192, time: 1.777843713760376
Validation Loss Energy: 1.5614609284133802, Validation Loss Force: 2.8450245279727957, time: 0.12455630302429199
Test Loss Energy: 11.321311372903248, Test Loss Force: 8.80807583121154, time: 16.14584517478943


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.7693582599179314, Training Loss Force: 3.014336348735882, time: 1.7770190238952637
Validation Loss Energy: 1.7619660885610946, Validation Loss Force: 2.7765715791316747, time: 0.12366628646850586
Test Loss Energy: 10.663806331735413, Test Loss Force: 8.834099106679313, time: 15.932639122009277


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6193549945820027, Training Loss Force: 3.0167330880004704, time: 1.8099417686462402
Validation Loss Energy: 1.4512744932331767, Validation Loss Force: 2.7763646081605353, time: 0.1215813159942627
Test Loss Energy: 10.845730987702373, Test Loss Force: 8.848204919611762, time: 16.390082597732544


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.8273514096551866, Training Loss Force: 3.0276528879493845, time: 1.791236162185669
Validation Loss Energy: 1.3633369364449313, Validation Loss Force: 2.689024639692674, time: 0.11687111854553223
Test Loss Energy: 10.854364036034596, Test Loss Force: 8.868202334310505, time: 16.04367733001709


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7903569497861647, Training Loss Force: 3.0227104941772382, time: 2.039825439453125
Validation Loss Energy: 1.7478435763170568, Validation Loss Force: 2.822509671286026, time: 0.12039732933044434
Test Loss Energy: 10.734556288853643, Test Loss Force: 8.873988994789682, time: 16.022754192352295


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6572913722467797, Training Loss Force: 3.0292730916747246, time: 1.7844865322113037
Validation Loss Energy: 1.6035457866647378, Validation Loss Force: 2.792351250826994, time: 0.12345623970031738
Test Loss Energy: 10.767582133570977, Test Loss Force: 8.85780501421571, time: 16.054192066192627


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.7598448445966532, Training Loss Force: 3.0185241209737823, time: 1.8166582584381104
Validation Loss Energy: 1.8829291537313386, Validation Loss Force: 2.802303448304236, time: 0.11849665641784668
Test Loss Energy: 10.631135507906412, Test Loss Force: 8.78749826895728, time: 16.10389733314514


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.629184241929277, Training Loss Force: 3.013031238657248, time: 1.8583333492279053
Validation Loss Energy: 1.2427167260921297, Validation Loss Force: 2.659917885609858, time: 0.12009429931640625
Test Loss Energy: 10.985340667056475, Test Loss Force: 8.860189546622662, time: 16.114925622940063


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5678494007394028, Training Loss Force: 3.007790569060871, time: 1.7540152072906494
Validation Loss Energy: 1.428341352555782, Validation Loss Force: 2.763330781038225, time: 0.12267255783081055
Test Loss Energy: 11.444213778205917, Test Loss Force: 8.791061287728416, time: 15.935842037200928

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–„â–„â–‚â–â–„â–ƒâ–â–ˆâ–‚â–‚â–ƒâ–â–‚â–‚â–â–â–â–‚â–ƒ
wandb:   test_error_force â–†â–ˆâ–„â–†â–„â–…â–†â–†â–„â–†â–ƒâ–‚â–ƒâ–„â–„â–…â–„â–â–„â–
wandb:          test_loss â–…â–†â–…â–„â–ƒâ–…â–…â–ƒâ–ˆâ–ƒâ–„â–ƒâ–â–â–‚â–â–ƒâ–â–‚â–ƒ
wandb: train_error_energy â–ˆâ–‚â–‚â–â–â–‚â–‚â–â–â–ƒâ–‚â–â–‚â–â–‚â–‚â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–‚â–‚â–â–â–‚â–‚â–â–â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–
wandb: valid_error_energy â–â–ƒâ–‚â–‚â–„â–‚â–â–‚â–ˆâ–‚â–â–‚â–ƒâ–‚â–â–ƒâ–‚â–ƒâ–â–‚
wandb:  valid_error_force â–…â–…â–ƒâ–„â–ˆâ–â–‡â–„â–†â–‡â–…â–‡â–…â–…â–‚â–†â–…â–†â–â–„
wandb:         valid_loss â–„â–…â–ƒâ–ƒâ–†â–‚â–†â–„â–ˆâ–…â–„â–„â–„â–„â–â–„â–…â–…â–â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1683
wandb:                 lr 0.0001
wandb:  test_error_energy 11.44421
wandb:   test_error_force 8.79106
wandb:          test_loss 4.89643
wandb: train_error_energy 1.56785
wandb:  train_error_force 3.00779
wandb:         train_loss 1.35231
wandb: valid_error_energy 1.42834
wandb:  valid_error_force 2.76333
wandb:         valid_loss 1.3833
wandb: 
wandb: ğŸš€ View run al_48_11 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/b0mfzrxx
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_094602-b0mfzrxx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6891111135482788, Uncertainty Bias: 0.02999541163444519
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
9.536743e-07 0.00027678162
0.04040803 0.3978549
Found uncertainty sample 0 after 792 steps.
Found uncertainty sample 1 after 1356 steps.
Found uncertainty sample 4 after 1700 steps.
Found uncertainty sample 6 after 2202 steps.
Found uncertainty sample 8 after 1136 steps.
Found uncertainty sample 11 after 160 steps.
Found uncertainty sample 12 after 3372 steps.
Found uncertainty sample 13 after 2206 steps.
Found uncertainty sample 14 after 178 steps.
Found uncertainty sample 16 after 3707 steps.
Found uncertainty sample 17 after 2017 steps.
Found uncertainty sample 19 after 45 steps.
Found uncertainty sample 20 after 1177 steps.
Found uncertainty sample 22 after 1395 steps.
Found uncertainty sample 23 after 2143 steps.
Found uncertainty sample 24 after 3201 steps.
Found uncertainty sample 26 after 1188 steps.
Found uncertainty sample 27 after 2141 steps.
Found uncertainty sample 29 after 1049 steps.
Found uncertainty sample 30 after 1645 steps.
Found uncertainty sample 31 after 2892 steps.
Found uncertainty sample 32 after 1748 steps.
Found uncertainty sample 35 after 3705 steps.
Found uncertainty sample 36 after 757 steps.
Found uncertainty sample 38 after 2016 steps.
Found uncertainty sample 39 after 1373 steps.
Found uncertainty sample 42 after 1473 steps.
Found uncertainty sample 43 after 3949 steps.
Found uncertainty sample 45 after 5 steps.
Found uncertainty sample 46 after 934 steps.
Found uncertainty sample 47 after 34 steps.
Found uncertainty sample 49 after 970 steps.
Found uncertainty sample 50 after 2369 steps.
Found uncertainty sample 51 after 147 steps.
Found uncertainty sample 52 after 3095 steps.
Found uncertainty sample 53 after 104 steps.
Found uncertainty sample 54 after 1981 steps.
Found uncertainty sample 55 after 1656 steps.
Found uncertainty sample 62 after 87 steps.
Found uncertainty sample 66 after 492 steps.
Found uncertainty sample 69 after 191 steps.
Found uncertainty sample 70 after 3037 steps.
Found uncertainty sample 71 after 82 steps.
Found uncertainty sample 74 after 3213 steps.
Found uncertainty sample 75 after 678 steps.
Found uncertainty sample 76 after 3628 steps.
Found uncertainty sample 77 after 1531 steps.
Found uncertainty sample 78 after 3003 steps.
Found uncertainty sample 79 after 1103 steps.
Found uncertainty sample 82 after 1243 steps.
Found uncertainty sample 83 after 1257 steps.
Found uncertainty sample 85 after 1435 steps.
Found uncertainty sample 86 after 21 steps.
Found uncertainty sample 87 after 1055 steps.
Found uncertainty sample 88 after 235 steps.
Found uncertainty sample 89 after 761 steps.
Found uncertainty sample 95 after 2907 steps.
Found uncertainty sample 97 after 766 steps.
Found uncertainty sample 98 after 3261 steps.
Found uncertainty sample 99 after 117 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_114454-5qm8br3q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_12
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/5qm8br3q
Training model 12. Added 60 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.404133417539895, Training Loss Force: 3.267794755337049, time: 1.8562681674957275
Validation Loss Energy: 1.342314742377889, Validation Loss Force: 2.7980313180704655, time: 0.11887812614440918
Test Loss Energy: 10.934002078097821, Test Loss Force: 8.829993297473672, time: 15.947410583496094


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5859739846153338, Training Loss Force: 3.070130399043891, time: 1.889359951019287
Validation Loss Energy: 1.83502133031211, Validation Loss Force: 2.8008198037849192, time: 0.1276843547821045
Test Loss Energy: 10.590723510778545, Test Loss Force: 8.813898463832953, time: 16.038411140441895


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7808260771656526, Training Loss Force: 3.0539931368943303, time: 1.8959274291992188
Validation Loss Energy: 1.5845915209559265, Validation Loss Force: 2.7903968837331856, time: 0.12613606452941895
Test Loss Energy: 11.388222346069465, Test Loss Force: 8.781372876245833, time: 15.861186027526855


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7400583646040093, Training Loss Force: 3.043140477536546, time: 1.8718783855438232
Validation Loss Energy: 1.4425920108933008, Validation Loss Force: 2.853319955669762, time: 0.12366676330566406
Test Loss Energy: 10.911362301105669, Test Loss Force: 8.787021052837542, time: 16.11211633682251


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6139843391026478, Training Loss Force: 3.051153485858673, time: 1.892716884613037
Validation Loss Energy: 1.2730392537117112, Validation Loss Force: 2.877354529277489, time: 0.1176152229309082
Test Loss Energy: 11.107518383382752, Test Loss Force: 8.792863821278264, time: 16.369593858718872


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.007785927982511, Training Loss Force: 3.0550595480165383, time: 1.855543851852417
Validation Loss Energy: 1.457398841090061, Validation Loss Force: 2.841341246295431, time: 0.12440872192382812
Test Loss Energy: 11.230649981687584, Test Loss Force: 8.79205991338143, time: 15.967007160186768


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.682952362163599, Training Loss Force: 3.0473678512719853, time: 1.8757543563842773
Validation Loss Energy: 2.187035086399296, Validation Loss Force: 2.7616118865662354, time: 0.12134146690368652
Test Loss Energy: 10.60311342827763, Test Loss Force: 8.778122839360924, time: 16.063357830047607


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7776087246033472, Training Loss Force: 3.0517727813843876, time: 1.8495595455169678
Validation Loss Energy: 1.3611861194971284, Validation Loss Force: 2.7768808665753686, time: 0.12262558937072754
Test Loss Energy: 10.90710753906528, Test Loss Force: 8.805842648538627, time: 15.90504503250122


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6000821346994591, Training Loss Force: 3.041681209215913, time: 1.8893296718597412
Validation Loss Energy: 2.0757028514614344, Validation Loss Force: 2.810976634207269, time: 0.12272214889526367
Test Loss Energy: 10.651383263347146, Test Loss Force: 8.775931597762325, time: 16.04214835166931


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.0056899129479184, Training Loss Force: 3.0700042751911063, time: 1.857994556427002
Validation Loss Energy: 1.6828730059163877, Validation Loss Force: 2.7708136890217014, time: 0.12375950813293457
Test Loss Energy: 10.634045394937678, Test Loss Force: 8.815336272316612, time: 16.07297992706299


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.7394682184937624, Training Loss Force: 3.032547673072059, time: 1.9354605674743652
Validation Loss Energy: 1.1833190077411682, Validation Loss Force: 2.7198878079985676, time: 0.12992501258850098
Test Loss Energy: 10.896720450093127, Test Loss Force: 8.747229852625292, time: 15.934712171554565


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.699421587572967, Training Loss Force: 3.0346212450810084, time: 1.8990428447723389
Validation Loss Energy: 1.2137996304543583, Validation Loss Force: 2.8010081818823744, time: 0.12045121192932129
Test Loss Energy: 10.872312064536189, Test Loss Force: 8.791285243925325, time: 16.038917541503906


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6350291077725574, Training Loss Force: 3.046950472127799, time: 1.8638865947723389
Validation Loss Energy: 2.714619678675062, Validation Loss Force: 2.8496038302933737, time: 0.12331986427307129
Test Loss Energy: 10.405149095332947, Test Loss Force: 8.761135790271192, time: 15.897431373596191


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.007109684822726, Training Loss Force: 3.049305538422498, time: 1.8872191905975342
Validation Loss Energy: 1.2495625219818203, Validation Loss Force: 2.802165616102907, time: 0.12476944923400879
Test Loss Energy: 10.809139983792173, Test Loss Force: 8.715240279802126, time: 16.408987998962402


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.92207268313008, Training Loss Force: 3.030339823471058, time: 1.9009089469909668
Validation Loss Energy: 1.3421214098919372, Validation Loss Force: 2.7390251077453316, time: 0.11826872825622559
Test Loss Energy: 11.107673370434775, Test Loss Force: 8.692336163056966, time: 15.915484428405762


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.716019373038001, Training Loss Force: 3.037587266929774, time: 2.1113061904907227
Validation Loss Energy: 1.189567827650626, Validation Loss Force: 2.736238848722337, time: 0.12434530258178711
Test Loss Energy: 10.885965273867265, Test Loss Force: 8.698472327825527, time: 15.897154808044434


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.747227194720757, Training Loss Force: 3.029823413236351, time: 1.8649466037750244
Validation Loss Energy: 1.275491063677522, Validation Loss Force: 2.787109841800999, time: 0.11689090728759766
Test Loss Energy: 11.172809285791622, Test Loss Force: 8.749999768199299, time: 16.121018648147583


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.683346541207045, Training Loss Force: 3.0508414858999298, time: 1.8944296836853027
Validation Loss Energy: 1.5658249096698462, Validation Loss Force: 2.722649239411791, time: 0.12509536743164062
Test Loss Energy: 10.645183156393685, Test Loss Force: 8.694644647538643, time: 15.876323938369751


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6782439704420669, Training Loss Force: 3.0153547000648877, time: 1.8580923080444336
Validation Loss Energy: 1.2788112276451649, Validation Loss Force: 2.8455152907704204, time: 0.1256411075592041
Test Loss Energy: 10.929225789943459, Test Loss Force: 8.71553397457657, time: 16.070701122283936


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6158923505288039, Training Loss Force: 3.0380817948602306, time: 1.87799072265625
Validation Loss Energy: 1.3589383858035604, Validation Loss Force: 2.7839084161554224, time: 0.12236547470092773
Test Loss Energy: 10.9263981109619, Test Loss Force: 8.705161490835097, time: 15.847833395004272

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–‚â–ˆâ–…â–†â–‡â–‚â–…â–ƒâ–ƒâ–…â–„â–â–„â–†â–„â–†â–ƒâ–…â–…
wandb:   test_error_force â–ˆâ–‡â–†â–†â–†â–†â–…â–‡â–…â–‡â–„â–†â–„â–‚â–â–â–„â–â–‚â–‚
wandb:          test_loss â–…â–„â–…â–„â–ˆâ–…â–ƒâ–„â–ƒâ–„â–â–ƒâ–‚â–‚â–‚â–‚â–„â–‚â–â–‚
wandb: train_error_energy â–ˆâ–â–‚â–‚â–â–ƒâ–â–‚â–â–ƒâ–‚â–â–â–ƒâ–‚â–‚â–‚â–â–â–
wandb:  train_error_force â–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–‚â–â–â–‚â–â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–â–â–
wandb: valid_error_energy â–‚â–„â–ƒâ–‚â–â–‚â–†â–‚â–…â–ƒâ–â–â–ˆâ–â–‚â–â–â–ƒâ–â–‚
wandb:  valid_error_force â–„â–…â–„â–‡â–ˆâ–†â–ƒâ–„â–…â–ƒâ–â–…â–‡â–…â–‚â–‚â–„â–â–‡â–„
wandb:         valid_loss â–ƒâ–…â–„â–„â–ˆâ–ƒâ–„â–â–„â–‚â–‚â–‚â–†â–‚â–â–â–‚â–‚â–„â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1737
wandb:                 lr 0.0001
wandb:  test_error_energy 10.9264
wandb:   test_error_force 8.70516
wandb:          test_loss 4.81686
wandb: train_error_energy 1.61589
wandb:  train_error_force 3.03808
wandb:         train_loss 1.36501
wandb: valid_error_energy 1.35894
wandb:  valid_error_force 2.78391
wandb:         valid_loss 1.3657
wandb: 
wandb: ğŸš€ View run al_48_12 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/5qm8br3q
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_114454-5qm8br3q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6758363246917725, Uncertainty Bias: 0.0337664932012558
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
2.5629997e-06 0.00014461204
0.03948575 0.38477576
Found uncertainty sample 0 after 2249 steps.
Found uncertainty sample 1 after 1059 steps.
Found uncertainty sample 3 after 3090 steps.
Found uncertainty sample 4 after 3053 steps.
Found uncertainty sample 5 after 990 steps.
Found uncertainty sample 6 after 981 steps.
Found uncertainty sample 8 after 384 steps.
Found uncertainty sample 9 after 1031 steps.
Found uncertainty sample 10 after 392 steps.
Found uncertainty sample 11 after 898 steps.
Found uncertainty sample 13 after 3973 steps.
Found uncertainty sample 14 after 2064 steps.
Found uncertainty sample 15 after 844 steps.
Found uncertainty sample 18 after 509 steps.
Found uncertainty sample 19 after 1754 steps.
Found uncertainty sample 22 after 85 steps.
Found uncertainty sample 23 after 513 steps.
Found uncertainty sample 24 after 3641 steps.
Found uncertainty sample 25 after 1367 steps.
Found uncertainty sample 26 after 208 steps.
Found uncertainty sample 27 after 496 steps.
Found uncertainty sample 28 after 235 steps.
Found uncertainty sample 30 after 299 steps.
Found uncertainty sample 31 after 140 steps.
Found uncertainty sample 32 after 1261 steps.
Found uncertainty sample 34 after 2491 steps.
Found uncertainty sample 36 after 1145 steps.
Found uncertainty sample 37 after 2328 steps.
Found uncertainty sample 38 after 55 steps.
Found uncertainty sample 40 after 2146 steps.
Found uncertainty sample 41 after 717 steps.
Found uncertainty sample 42 after 8 steps.
Found uncertainty sample 43 after 35 steps.
Found uncertainty sample 44 after 1894 steps.
Found uncertainty sample 46 after 42 steps.
Found uncertainty sample 47 after 570 steps.
Found uncertainty sample 48 after 63 steps.
Found uncertainty sample 49 after 3419 steps.
Found uncertainty sample 50 after 3495 steps.
Found uncertainty sample 51 after 16 steps.
Found uncertainty sample 53 after 1748 steps.
Found uncertainty sample 54 after 2682 steps.
Found uncertainty sample 55 after 1407 steps.
Found uncertainty sample 57 after 382 steps.
Found uncertainty sample 58 after 1862 steps.
Found uncertainty sample 59 after 2504 steps.
Found uncertainty sample 60 after 1029 steps.
Found uncertainty sample 62 after 67 steps.
Found uncertainty sample 63 after 3023 steps.
Found uncertainty sample 66 after 1318 steps.
Found uncertainty sample 68 after 2976 steps.
Found uncertainty sample 69 after 1103 steps.
Found uncertainty sample 70 after 2822 steps.
Found uncertainty sample 71 after 3811 steps.
Found uncertainty sample 72 after 559 steps.
Found uncertainty sample 74 after 1308 steps.
Found uncertainty sample 75 after 3393 steps.
Found uncertainty sample 76 after 487 steps.
Found uncertainty sample 78 after 293 steps.
Found uncertainty sample 80 after 807 steps.
Found uncertainty sample 81 after 1408 steps.
Found uncertainty sample 82 after 1459 steps.
Found uncertainty sample 83 after 551 steps.
Found uncertainty sample 85 after 780 steps.
Found uncertainty sample 86 after 834 steps.
Found uncertainty sample 87 after 2205 steps.
Found uncertainty sample 88 after 421 steps.
Found uncertainty sample 90 after 1172 steps.
Found uncertainty sample 91 after 1830 steps.
Found uncertainty sample 94 after 363 steps.
Found uncertainty sample 95 after 482 steps.
Found uncertainty sample 97 after 2705 steps.
Found uncertainty sample 98 after 836 steps.
Found uncertainty sample 99 after 133 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_132226-cbcq9g6q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_13
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/cbcq9g6q
Training model 13. Added 74 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1523564892737164, Training Loss Force: 3.3514130235317876, time: 2.020159959793091
Validation Loss Energy: 1.7672187980462446, Validation Loss Force: 2.7818637567229407, time: 0.13956165313720703
Test Loss Energy: 10.613457214512842, Test Loss Force: 8.742742185653107, time: 17.518796682357788


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7225466053785885, Training Loss Force: 3.069118443916422, time: 2.1561830043792725
Validation Loss Energy: 2.59517182137557, Validation Loss Force: 2.782904957649299, time: 0.1371920108795166
Test Loss Energy: 11.95160963460559, Test Loss Force: 8.686518944443481, time: 18.200204849243164


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.036698789799085, Training Loss Force: 3.068161122847982, time: 2.118398666381836
Validation Loss Energy: 1.3137217973493018, Validation Loss Force: 2.9288174882806004, time: 0.13465547561645508
Test Loss Energy: 10.929497838012658, Test Loss Force: 8.663756687762998, time: 18.257219314575195


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.691646897847143, Training Loss Force: 3.0965899294642107, time: 2.073430061340332
Validation Loss Energy: 1.4753536563373753, Validation Loss Force: 2.8014200084677676, time: 0.1329667568206787
Test Loss Energy: 11.228403373580717, Test Loss Force: 8.713435717748016, time: 18.14119601249695


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7087528065079884, Training Loss Force: 3.06876014191529, time: 2.144545555114746
Validation Loss Energy: 1.3699850578003243, Validation Loss Force: 2.8216358062228326, time: 0.1316370964050293
Test Loss Energy: 10.918087315013539, Test Loss Force: 8.717088629568911, time: 18.49400782585144


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8863392535716974, Training Loss Force: 3.0746007028287847, time: 2.180936813354492
Validation Loss Energy: 1.4620139931431333, Validation Loss Force: 2.8541493982383432, time: 0.13404297828674316
Test Loss Energy: 11.189487535910818, Test Loss Force: 8.70947300902575, time: 18.22042679786682


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.9217753371095099, Training Loss Force: 3.074139711286618, time: 2.1149473190307617
Validation Loss Energy: 1.3419007447683229, Validation Loss Force: 2.772886738770289, time: 0.1378462314605713
Test Loss Energy: 11.068992293661486, Test Loss Force: 8.686453603290124, time: 18.172468662261963


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.9259571046880346, Training Loss Force: 3.0653415181437658, time: 2.1005818843841553
Validation Loss Energy: 1.3971633857448325, Validation Loss Force: 2.7536031453946506, time: 0.14186882972717285
Test Loss Energy: 10.744258442869695, Test Loss Force: 8.695574290057086, time: 18.263447523117065


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6375257604810416, Training Loss Force: 3.0625900931694425, time: 2.0663599967956543
Validation Loss Energy: 1.8317776531960934, Validation Loss Force: 2.7300402963725725, time: 0.1355125904083252
Test Loss Energy: 10.537874091846051, Test Loss Force: 8.644550758121198, time: 18.305094480514526


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6645799669792187, Training Loss Force: 3.078845272872651, time: 2.0745604038238525
Validation Loss Energy: 1.5709991282810565, Validation Loss Force: 2.7734999695981744, time: 0.13962054252624512
Test Loss Energy: 11.427961033669101, Test Loss Force: 8.6325595466321, time: 18.16575050354004


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.0133589126748435, Training Loss Force: 3.076014253444849, time: 2.0906262397766113
Validation Loss Energy: 2.60263157176787, Validation Loss Force: 2.7917324532704604, time: 0.14478445053100586
Test Loss Energy: 11.960978246562345, Test Loss Force: 8.669710156577649, time: 18.24910855293274


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.9983110001563782, Training Loss Force: 3.0677982854279304, time: 2.1264071464538574
Validation Loss Energy: 1.807479717656021, Validation Loss Force: 2.84110644168105, time: 0.14007973670959473
Test Loss Energy: 10.480329333737291, Test Loss Force: 8.698818493243532, time: 18.250758171081543


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.9551636594727024, Training Loss Force: 3.0647195094637594, time: 2.0617611408233643
Validation Loss Energy: 2.571925496544428, Validation Loss Force: 2.764743394099991, time: 0.14107203483581543
Test Loss Energy: 10.5083935223166, Test Loss Force: 8.650192536763834, time: 18.221330165863037


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.0528722538322155, Training Loss Force: 3.0628034961537187, time: 2.115450143814087
Validation Loss Energy: 3.486732067592336, Validation Loss Force: 2.792396007325325, time: 0.13710355758666992
Test Loss Energy: 10.457149872020606, Test Loss Force: 8.645914990190525, time: 18.61291265487671


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.7948045870889429, Training Loss Force: 3.0651914039811246, time: 2.069714069366455
Validation Loss Energy: 1.7374227716222095, Validation Loss Force: 2.8662070438249057, time: 0.13472843170166016
Test Loss Energy: 10.68133101372928, Test Loss Force: 8.60854864531936, time: 18.314558506011963


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7224812637036875, Training Loss Force: 3.0529100052014755, time: 2.0881378650665283
Validation Loss Energy: 1.4763684153026544, Validation Loss Force: 2.7979455904492836, time: 0.13191437721252441
Test Loss Energy: 10.804444957052198, Test Loss Force: 8.653015361324849, time: 18.15141797065735


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.7515783687901794, Training Loss Force: 3.0659029764419845, time: 2.1198108196258545
Validation Loss Energy: 3.3461143865820517, Validation Loss Force: 2.762922405824593, time: 0.13293886184692383
Test Loss Energy: 10.406865666934392, Test Loss Force: 8.647205580560323, time: 18.229204416275024


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.081631087374519, Training Loss Force: 3.0512039806154734, time: 2.077221155166626
Validation Loss Energy: 1.6073826543855056, Validation Loss Force: 2.7825032625100454, time: 0.14090633392333984
Test Loss Energy: 10.743095471065688, Test Loss Force: 8.613017720000766, time: 18.258214712142944


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.0046943483648816, Training Loss Force: 3.0656175497551916, time: 2.0788111686706543
Validation Loss Energy: 1.4570353337093338, Validation Loss Force: 2.7617396986559983, time: 0.13163352012634277
Test Loss Energy: 11.265763003403297, Test Loss Force: 8.65224335923527, time: 18.202476024627686


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.9451896726475881, Training Loss Force: 3.062579483179962, time: 2.1121010780334473
Validation Loss Energy: 1.4523667410612389, Validation Loss Force: 2.745548311709859, time: 0.14017009735107422
Test Loss Energy: 10.726896829302298, Test Loss Force: 8.623185581289391, time: 18.240601301193237

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ˆâ–ƒâ–…â–ƒâ–…â–„â–ƒâ–‚â–†â–ˆâ–â–â–â–‚â–ƒâ–â–ƒâ–…â–‚
wandb:   test_error_force â–ˆâ–…â–„â–†â–‡â–†â–…â–†â–ƒâ–‚â–„â–†â–ƒâ–ƒâ–â–ƒâ–ƒâ–â–ƒâ–‚
wandb:          test_loss â–†â–ˆâ–†â–‡â–…â–†â–†â–„â–‚â–…â–ˆâ–…â–‚â–â–‚â–‚â–‚â–„â–ƒâ–‚
wandb: train_error_energy â–ˆâ–â–ƒâ–â–â–‚â–‚â–‚â–â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–â–‚â–ƒâ–ƒâ–‚
wandb:  train_error_force â–ˆâ–â–â–‚â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚
wandb: valid_error_energy â–‚â–…â–â–‚â–â–â–â–â–ƒâ–‚â–…â–ƒâ–…â–ˆâ–‚â–‚â–ˆâ–‚â–â–
wandb:  valid_error_force â–ƒâ–ƒâ–ˆâ–„â–„â–…â–ƒâ–‚â–â–ƒâ–ƒâ–…â–‚â–ƒâ–†â–ƒâ–‚â–ƒâ–‚â–‚
wandb:         valid_loss â–ƒâ–…â–…â–‚â–‚â–„â–„â–â–‚â–‚â–‡â–„â–„â–ˆâ–ƒâ–â–‡â–†â–ƒâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 1803
wandb:                 lr 0.0001
wandb:  test_error_energy 10.7269
wandb:   test_error_force 8.62319
wandb:          test_loss 4.75002
wandb: train_error_energy 1.94519
wandb:  train_error_force 3.06258
wandb:         train_loss 1.41063
wandb: valid_error_energy 1.45237
wandb:  valid_error_force 2.74555
wandb:         valid_loss 1.35489
wandb: 
wandb: ğŸš€ View run al_48_13 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/cbcq9g6q
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_132226-cbcq9g6q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6786383390426636, Uncertainty Bias: 0.03376837074756622
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
2.1457672e-06 0.0010488033
0.05282408 0.36824185
Found uncertainty sample 0 after 6 steps.
Found uncertainty sample 1 after 3684 steps.
Found uncertainty sample 3 after 918 steps.
Found uncertainty sample 5 after 1022 steps.
Found uncertainty sample 6 after 2607 steps.
Found uncertainty sample 7 after 3138 steps.
Found uncertainty sample 9 after 689 steps.
Found uncertainty sample 12 after 1736 steps.
Found uncertainty sample 13 after 1610 steps.
Found uncertainty sample 14 after 279 steps.
Found uncertainty sample 15 after 1000 steps.
Found uncertainty sample 16 after 2528 steps.
Found uncertainty sample 18 after 2491 steps.
Found uncertainty sample 19 after 2325 steps.
Found uncertainty sample 21 after 3970 steps.
Found uncertainty sample 22 after 3553 steps.
Found uncertainty sample 23 after 451 steps.
Found uncertainty sample 24 after 1629 steps.
Found uncertainty sample 25 after 763 steps.
Found uncertainty sample 26 after 2267 steps.
Found uncertainty sample 27 after 3545 steps.
Found uncertainty sample 28 after 779 steps.
Found uncertainty sample 30 after 460 steps.
Found uncertainty sample 31 after 1512 steps.
Found uncertainty sample 32 after 1307 steps.
Found uncertainty sample 33 after 2872 steps.
Found uncertainty sample 34 after 408 steps.
Found uncertainty sample 35 after 1796 steps.
Found uncertainty sample 37 after 2649 steps.
Found uncertainty sample 39 after 1155 steps.
Found uncertainty sample 41 after 1434 steps.
Found uncertainty sample 42 after 1720 steps.
Found uncertainty sample 43 after 1358 steps.
Found uncertainty sample 44 after 2061 steps.
Found uncertainty sample 46 after 3871 steps.
Found uncertainty sample 48 after 765 steps.
Found uncertainty sample 50 after 2837 steps.
Found uncertainty sample 53 after 3844 steps.
Found uncertainty sample 54 after 9 steps.
Found uncertainty sample 55 after 326 steps.
Found uncertainty sample 57 after 1822 steps.
Found uncertainty sample 59 after 834 steps.
Found uncertainty sample 60 after 1136 steps.
Found uncertainty sample 63 after 3451 steps.
Found uncertainty sample 64 after 1010 steps.
Found uncertainty sample 65 after 3478 steps.
Found uncertainty sample 66 after 2745 steps.
Found uncertainty sample 67 after 3067 steps.
Found uncertainty sample 69 after 932 steps.
Found uncertainty sample 71 after 3925 steps.
Found uncertainty sample 72 after 663 steps.
Found uncertainty sample 75 after 1664 steps.
Found uncertainty sample 76 after 954 steps.
Found uncertainty sample 77 after 534 steps.
Found uncertainty sample 78 after 798 steps.
Found uncertainty sample 79 after 1901 steps.
Found uncertainty sample 81 after 2993 steps.
Found uncertainty sample 83 after 3805 steps.
Found uncertainty sample 84 after 2689 steps.
Found uncertainty sample 85 after 1875 steps.
Found uncertainty sample 87 after 1393 steps.
Found uncertainty sample 88 after 1315 steps.
Found uncertainty sample 89 after 804 steps.
Found uncertainty sample 91 after 1787 steps.
Found uncertainty sample 92 after 1934 steps.
Found uncertainty sample 93 after 7 steps.
Found uncertainty sample 94 after 1219 steps.
Found uncertainty sample 96 after 3509 steps.
Found uncertainty sample 98 after 222 steps.
Found uncertainty sample 99 after 2506 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_152434-bhcoin9p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_14
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/bhcoin9p
Training model 14. Added 70 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.4945803680961736, Training Loss Force: 3.3093024686286707, time: 2.1830873489379883
Validation Loss Energy: 1.708029943354163, Validation Loss Force: 2.8361048473252204, time: 0.15142488479614258
Test Loss Energy: 11.374481701209884, Test Loss Force: 8.622385413301561, time: 18.502328395843506


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.668153544110491, Training Loss Force: 3.086582799449077, time: 2.2137622833251953
Validation Loss Energy: 1.4674485676532196, Validation Loss Force: 2.8544919747193536, time: 0.1412339210510254
Test Loss Energy: 10.60073083111038, Test Loss Force: 8.651044331350922, time: 18.361915111541748


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7032237326822655, Training Loss Force: 3.0750746843314243, time: 2.1512465476989746
Validation Loss Energy: 1.3819328718306583, Validation Loss Force: 2.7685382330194654, time: 0.1351795196533203
Test Loss Energy: 11.04018952901224, Test Loss Force: 8.585338233757195, time: 18.362385272979736


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5574150130486324, Training Loss Force: 3.089631083446028, time: 2.1621158123016357
Validation Loss Energy: 1.3417210507675592, Validation Loss Force: 2.81286609324504, time: 0.13371062278747559
Test Loss Energy: 10.676799018109948, Test Loss Force: 8.60072954481426, time: 18.22477126121521


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.9885308019479757, Training Loss Force: 3.0891094375700794, time: 2.1705994606018066
Validation Loss Energy: 1.5312408286009258, Validation Loss Force: 2.7790272858075395, time: 0.14460301399230957
Test Loss Energy: 11.155413416149965, Test Loss Force: 8.614211595017965, time: 18.339977502822876


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.792122956940699, Training Loss Force: 3.077343339489577, time: 2.1770410537719727
Validation Loss Energy: 1.3364331038900659, Validation Loss Force: 2.8099102716674214, time: 0.1435995101928711
Test Loss Energy: 10.636474004177455, Test Loss Force: 8.617561648821027, time: 18.304218530654907


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.7491999059781362, Training Loss Force: 3.072798028180232, time: 2.2040340900421143
Validation Loss Energy: 1.3455785153843256, Validation Loss Force: 2.815593344944363, time: 0.13900399208068848
Test Loss Energy: 10.883628334367192, Test Loss Force: 8.560207150422297, time: 18.21110510826111


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7281235514154, Training Loss Force: 3.070184560414478, time: 2.1180672645568848
Validation Loss Energy: 1.3662880819704855, Validation Loss Force: 2.7498971514644017, time: 0.137617826461792
Test Loss Energy: 10.984162273956855, Test Loss Force: 8.534561955506238, time: 18.406835794448853


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.670456299727423, Training Loss Force: 3.0611381885709763, time: 2.200512170791626
Validation Loss Energy: 1.9451154134640403, Validation Loss Force: 2.7629040356200543, time: 0.1361234188079834
Test Loss Energy: 11.40371070356833, Test Loss Force: 8.535517947595551, time: 18.395853757858276


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.8613668624416615, Training Loss Force: 3.0593182494824016, time: 2.144319772720337
Validation Loss Energy: 1.310796425777389, Validation Loss Force: 2.808566240702601, time: 0.13637518882751465
Test Loss Energy: 10.756081073996604, Test Loss Force: 8.591819332851522, time: 18.277987241744995


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.681845739738403, Training Loss Force: 3.065301678905854, time: 2.202695369720459
Validation Loss Energy: 1.9422548942808788, Validation Loss Force: 2.817549493391339, time: 0.13942933082580566
Test Loss Energy: 11.395123508758871, Test Loss Force: 8.545339818337231, time: 18.295644760131836


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6785964345653717, Training Loss Force: 3.0673794348989327, time: 2.151561737060547
Validation Loss Energy: 1.5280379556059005, Validation Loss Force: 2.8410340742086237, time: 0.13707184791564941
Test Loss Energy: 10.69335967094072, Test Loss Force: 8.561488162521398, time: 18.326435089111328


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.687555823914962, Training Loss Force: 3.0717819586749378, time: 2.180408477783203
Validation Loss Energy: 1.353782092058597, Validation Loss Force: 2.779289812730588, time: 0.13622641563415527
Test Loss Energy: 10.75521287417276, Test Loss Force: 8.495481822980853, time: 18.209616661071777


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6899009876255466, Training Loss Force: 3.063039971779865, time: 2.19585919380188
Validation Loss Energy: 1.4510679676003284, Validation Loss Force: 2.7802991719690793, time: 0.1419661045074463
Test Loss Energy: 10.71812691055956, Test Loss Force: 8.572599921817034, time: 18.6850528717041


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.000958025480032, Training Loss Force: 3.0799098504839133, time: 2.173394203186035
Validation Loss Energy: 1.3987271911566832, Validation Loss Force: 2.820510881697605, time: 0.13896536827087402
Test Loss Energy: 10.994756065181623, Test Loss Force: 8.508894423727167, time: 18.400577783584595


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7952049602170455, Training Loss Force: 3.05665771172363, time: 2.1473886966705322
Validation Loss Energy: 2.889024062542699, Validation Loss Force: 2.7225283300655523, time: 0.14198017120361328
Test Loss Energy: 10.346583533257919, Test Loss Force: 8.528830078631803, time: 18.193198680877686


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.0189210757116927, Training Loss Force: 3.0608276141751003, time: 2.386134624481201
Validation Loss Energy: 1.232895680741839, Validation Loss Force: 2.716622921646401, time: 0.14072823524475098
Test Loss Energy: 10.974571804255886, Test Loss Force: 8.50741133529781, time: 18.246199131011963


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.7983273515250044, Training Loss Force: 3.0712144203106067, time: 2.158837080001831
Validation Loss Energy: 2.652751056479026, Validation Loss Force: 2.7338542848385963, time: 0.13402152061462402
Test Loss Energy: 12.022467060480624, Test Loss Force: 8.463069551838677, time: 18.429049968719482


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6804312566332165, Training Loss Force: 3.0545963728624965, time: 2.121877670288086
Validation Loss Energy: 1.3941275702765394, Validation Loss Force: 2.7868405234689995, time: 0.13616418838500977
Test Loss Energy: 10.925804449371102, Test Loss Force: 8.415863613627108, time: 18.23513126373291


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.7907072755735831, Training Loss Force: 3.0767787750792635, time: 2.359703779220581
Validation Loss Energy: 1.546525172781128, Validation Loss Force: 2.8246796934476652, time: 0.14094042778015137
Test Loss Energy: 10.55088384563575, Test Loss Force: 8.522397491100309, time: 18.20132327079773

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–‚â–„â–‚â–„â–‚â–ƒâ–„â–…â–ƒâ–…â–‚â–ƒâ–ƒâ–„â–â–„â–ˆâ–ƒâ–‚
wandb:   test_error_force â–‡â–ˆâ–†â–‡â–‡â–‡â–…â–…â–…â–†â–…â–…â–ƒâ–†â–„â–„â–„â–‚â–â–„
wandb:          test_loss â–†â–†â–„â–„â–†â–„â–„â–…â–†â–ƒâ–…â–„â–„â–ˆâ–ƒâ–â–‚â–„â–â–
wandb: train_error_energy â–ˆâ–â–‚â–â–ƒâ–‚â–‚â–‚â–â–‚â–â–â–â–â–ƒâ–‚â–ƒâ–‚â–â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–‚â–‚â–â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–â–â–
wandb: valid_error_energy â–ƒâ–‚â–‚â–â–‚â–â–â–‚â–„â–â–„â–‚â–‚â–‚â–‚â–ˆâ–â–‡â–‚â–‚
wandb:  valid_error_force â–‡â–ˆâ–„â–†â–„â–†â–†â–ƒâ–ƒâ–†â–†â–‡â–„â–„â–†â–â–â–‚â–…â–†
wandb:         valid_loss â–„â–…â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–…â–‚â–„â–„â–…â–ˆâ–„â–…â–â–„â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1866
wandb:                 lr 0.0001
wandb:  test_error_energy 10.55088
wandb:   test_error_force 8.5224
wandb:          test_loss 4.65954
wandb: train_error_energy 1.79071
wandb:  train_error_force 3.07678
wandb:         train_loss 1.39793
wandb: valid_error_energy 1.54653
wandb:  valid_error_force 2.82468
wandb:         valid_loss 1.38602
wandb: 
wandb: ğŸš€ View run al_48_14 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/bhcoin9p
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_152434-bhcoin9p/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6723268628120422, Uncertainty Bias: 0.03600709140300751
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
1.0192394e-05 0.0007711649
0.04163338 0.33182478
Found uncertainty sample 0 after 3379 steps.
Found uncertainty sample 1 after 1713 steps.
Found uncertainty sample 2 after 2851 steps.
Found uncertainty sample 3 after 3856 steps.
Found uncertainty sample 4 after 2309 steps.
Found uncertainty sample 6 after 2533 steps.
Found uncertainty sample 7 after 250 steps.
Found uncertainty sample 8 after 2229 steps.
Found uncertainty sample 9 after 1029 steps.
Found uncertainty sample 10 after 2003 steps.
Found uncertainty sample 11 after 1311 steps.
Found uncertainty sample 12 after 2364 steps.
Found uncertainty sample 14 after 2405 steps.
Found uncertainty sample 16 after 1493 steps.
Found uncertainty sample 18 after 3819 steps.
Found uncertainty sample 19 after 1789 steps.
Found uncertainty sample 20 after 1325 steps.
Found uncertainty sample 21 after 2146 steps.
Found uncertainty sample 22 after 15 steps.
Found uncertainty sample 24 after 1527 steps.
Found uncertainty sample 25 after 52 steps.
Found uncertainty sample 26 after 2645 steps.
Found uncertainty sample 27 after 221 steps.
Found uncertainty sample 28 after 3835 steps.
Found uncertainty sample 29 after 1393 steps.
Found uncertainty sample 31 after 310 steps.
Found uncertainty sample 32 after 820 steps.
Found uncertainty sample 35 after 1407 steps.
Found uncertainty sample 36 after 284 steps.
Found uncertainty sample 43 after 3617 steps.
Found uncertainty sample 44 after 3001 steps.
Found uncertainty sample 45 after 1437 steps.
Found uncertainty sample 46 after 961 steps.
Found uncertainty sample 49 after 1398 steps.
Found uncertainty sample 50 after 2140 steps.
Found uncertainty sample 53 after 5 steps.
Found uncertainty sample 54 after 3918 steps.
Found uncertainty sample 55 after 564 steps.
Found uncertainty sample 56 after 1652 steps.
Found uncertainty sample 57 after 1122 steps.
Found uncertainty sample 58 after 1179 steps.
Found uncertainty sample 59 after 2894 steps.
Found uncertainty sample 60 after 174 steps.
Found uncertainty sample 61 after 715 steps.
Found uncertainty sample 62 after 1335 steps.
Found uncertainty sample 64 after 866 steps.
Found uncertainty sample 67 after 811 steps.
Found uncertainty sample 68 after 566 steps.
Found uncertainty sample 69 after 2602 steps.
Found uncertainty sample 71 after 16 steps.
Found uncertainty sample 73 after 1873 steps.
Found uncertainty sample 74 after 3052 steps.
Found uncertainty sample 75 after 3153 steps.
Found uncertainty sample 76 after 67 steps.
Found uncertainty sample 77 after 3244 steps.
Found uncertainty sample 78 after 259 steps.
Found uncertainty sample 79 after 1224 steps.
Found uncertainty sample 80 after 389 steps.
Found uncertainty sample 81 after 1797 steps.
Found uncertainty sample 82 after 778 steps.
Found uncertainty sample 84 after 15 steps.
Found uncertainty sample 85 after 3 steps.
Found uncertainty sample 88 after 1469 steps.
Found uncertainty sample 90 after 1698 steps.
Found uncertainty sample 94 after 187 steps.
Found uncertainty sample 95 after 3489 steps.
Found uncertainty sample 96 after 294 steps.
Found uncertainty sample 98 after 1176 steps.
Found uncertainty sample 99 after 214 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_171938-6yft0895
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_15
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/6yft0895
Training model 15. Added 69 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.5420842772327954, Training Loss Force: 3.3084833757516896, time: 2.2034149169921875
Validation Loss Energy: 1.3905396466609186, Validation Loss Force: 2.8082634106641393, time: 0.141357421875
Test Loss Energy: 10.614588812117603, Test Loss Force: 8.469803216034835, time: 18.229210376739502


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6361127909853383, Training Loss Force: 3.0860677039685593, time: 2.189067840576172
Validation Loss Energy: 1.8817071454584866, Validation Loss Force: 2.7827918417231676, time: 0.13794183731079102
Test Loss Energy: 11.451514836028498, Test Loss Force: 8.443634713651635, time: 18.5894558429718


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.1190546186713726, Training Loss Force: 3.109995113486916, time: 2.256929397583008
Validation Loss Energy: 1.3054616627119646, Validation Loss Force: 2.827567044262911, time: 0.14420318603515625
Test Loss Energy: 10.683899090797668, Test Loss Force: 8.483986098139532, time: 18.33273220062256


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.8389050178711235, Training Loss Force: 3.1008488505569467, time: 2.1996829509735107
Validation Loss Energy: 1.4362009610668816, Validation Loss Force: 2.7792505116263317, time: 0.14076685905456543
Test Loss Energy: 10.518767223160628, Test Loss Force: 8.445153802202595, time: 18.252655029296875


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7109529212327572, Training Loss Force: 3.0932089882029454, time: 2.194021224975586
Validation Loss Energy: 1.376100665893231, Validation Loss Force: 2.7936008700739405, time: 0.13849377632141113
Test Loss Energy: 10.603209155025855, Test Loss Force: 8.4176186242185, time: 18.378523111343384


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7436127347834436, Training Loss Force: 3.0930028914819765, time: 2.2512967586517334
Validation Loss Energy: 1.6144949852428718, Validation Loss Force: 2.780941127116789, time: 0.1456005573272705
Test Loss Energy: 11.12131244085799, Test Loss Force: 8.421215493363043, time: 18.33206820487976


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.8146369282098112, Training Loss Force: 3.0758996743832627, time: 2.2263102531433105
Validation Loss Energy: 1.3542579916907314, Validation Loss Force: 2.7522194793218553, time: 0.14123058319091797
Test Loss Energy: 10.851041080250527, Test Loss Force: 8.455732587159059, time: 18.10921621322632


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7415831913006874, Training Loss Force: 3.085230303715746, time: 2.190443754196167
Validation Loss Energy: 1.3395786952473012, Validation Loss Force: 2.7650282992722053, time: 0.1523573398590088
Test Loss Energy: 10.667485716702723, Test Loss Force: 8.443432567372959, time: 18.339112520217896


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.77971614036458, Training Loss Force: 3.087536641373303, time: 2.2616422176361084
Validation Loss Energy: 1.964314058560507, Validation Loss Force: 2.7802825831177076, time: 0.13573765754699707
Test Loss Energy: 10.264296185354514, Test Loss Force: 8.425668866955142, time: 18.372867345809937


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.8309478939627843, Training Loss Force: 3.0954042541101363, time: 2.279388189315796
Validation Loss Energy: 1.3492073544555108, Validation Loss Force: 2.7484508040270272, time: 0.13538837432861328
Test Loss Energy: 10.619270184925448, Test Loss Force: 8.406714355261045, time: 18.64103102684021


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.762946058210058, Training Loss Force: 3.088154825677106, time: 2.1878204345703125
Validation Loss Energy: 1.5672885139660293, Validation Loss Force: 2.8015207662563, time: 0.137908935546875
Test Loss Energy: 10.252781108116217, Test Loss Force: 8.41306595184539, time: 18.259380340576172


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.7203842353474956, Training Loss Force: 3.0850498029956177, time: 2.2497873306274414
Validation Loss Energy: 1.3039517989007146, Validation Loss Force: 2.75572585036278, time: 0.1413729190826416
Test Loss Energy: 10.805979569489788, Test Loss Force: 8.420086423227959, time: 18.273178577423096


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.170234865428325, Training Loss Force: 3.075967836694114, time: 2.2767889499664307
Validation Loss Energy: 1.3439232486207848, Validation Loss Force: 2.811085408085055, time: 0.13699626922607422
Test Loss Energy: 10.595375230118352, Test Loss Force: 8.411828443002044, time: 18.221559524536133


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6509773855738978, Training Loss Force: 3.0805925288424794, time: 2.1751315593719482
Validation Loss Energy: 1.2724767357818112, Validation Loss Force: 2.808468043609247, time: 0.17647671699523926
Test Loss Energy: 10.595573785711323, Test Loss Force: 8.39352837807816, time: 18.283382177352905


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.7199098633717151, Training Loss Force: 3.07774587172623, time: 2.279776096343994
Validation Loss Energy: 2.0438739670696515, Validation Loss Force: 2.822864828573781, time: 0.13431382179260254
Test Loss Energy: 11.441412306566768, Test Loss Force: 8.369010210803244, time: 18.342851638793945


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7960812712998786, Training Loss Force: 3.0879144415579787, time: 2.2502634525299072
Validation Loss Energy: 1.3657702092043438, Validation Loss Force: 2.797859640158955, time: 0.13639163970947266
Test Loss Energy: 10.549659836838005, Test Loss Force: 8.411652774533012, time: 18.23995065689087


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.7165604868547681, Training Loss Force: 3.082390122256299, time: 2.4456398487091064
Validation Loss Energy: 1.7532675740427122, Validation Loss Force: 2.8069364138889825, time: 0.14031291007995605
Test Loss Energy: 10.272860297184447, Test Loss Force: 8.355649684326092, time: 18.16817307472229


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.9661366767010464, Training Loss Force: 3.0755648566443785, time: 2.297130823135376
Validation Loss Energy: 2.6735836477683113, Validation Loss Force: 2.807743255539252, time: 0.13657712936401367
Test Loss Energy: 11.679905935481138, Test Loss Force: 8.381103579632235, time: 18.417766571044922


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.9099909016018544, Training Loss Force: 3.0779512164825427, time: 2.202903985977173
Validation Loss Energy: 1.4189477762983858, Validation Loss Force: 2.8570777201764397, time: 0.13910651206970215
Test Loss Energy: 10.485604452074044, Test Loss Force: 8.38519875667761, time: 18.274588584899902


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.7172427943971278, Training Loss Force: 3.0758306469480248, time: 2.482903242111206
Validation Loss Energy: 1.6117404009058856, Validation Loss Force: 2.8251013174754798, time: 0.1452188491821289
Test Loss Energy: 10.726703552483631, Test Loss Force: 8.374153863410946, time: 18.257132053375244

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–‡â–ƒâ–‚â–ƒâ–…â–„â–ƒâ–â–ƒâ–â–„â–ƒâ–ƒâ–‡â–‚â–â–ˆâ–‚â–ƒ
wandb:   test_error_force â–‡â–†â–ˆâ–†â–„â–…â–†â–†â–…â–„â–„â–…â–„â–ƒâ–‚â–„â–â–‚â–ƒâ–‚
wandb:          test_loss â–‡â–ˆâ–ˆâ–†â–…â–‡â–†â–…â–ƒâ–ƒâ–„â–„â–…â–ƒâ–†â–ƒâ–â–†â–ƒâ–‡
wandb: train_error_energy â–ˆâ–â–ƒâ–‚â–â–â–‚â–â–‚â–‚â–â–â–ƒâ–â–â–‚â–â–‚â–‚â–
wandb:  train_error_force â–ˆâ–â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–ƒâ–‚â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–‚â–
wandb: valid_error_energy â–‚â–„â–â–‚â–‚â–ƒâ–â–â–„â–â–‚â–â–â–â–…â–â–ƒâ–ˆâ–‚â–ƒ
wandb:  valid_error_force â–…â–ƒâ–†â–ƒâ–„â–ƒâ–â–‚â–ƒâ–â–„â–â–…â–…â–†â–„â–…â–…â–ˆâ–†
wandb:         valid_loss â–ƒâ–†â–‡â–ƒâ–ƒâ–„â–‚â–‚â–‡â–â–„â–‚â–ƒâ–‚â–†â–‚â–…â–†â–„â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1928
wandb:                 lr 0.0001
wandb:  test_error_energy 10.7267
wandb:   test_error_force 8.37415
wandb:          test_loss 4.66198
wandb: train_error_energy 1.71724
wandb:  train_error_force 3.07583
wandb:         train_loss 1.3946
wandb: valid_error_energy 1.61174
wandb:  valid_error_force 2.8251
wandb:         valid_loss 1.48173
wandb: 
wandb: ğŸš€ View run al_48_15 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/6yft0895
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_171938-6yft0895/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6690755486488342, Uncertainty Bias: 0.036132052540779114
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
7.2717667e-06 0.0003260076
0.1002958 0.5114211
Found uncertainty sample 0 after 1673 steps.
Found uncertainty sample 2 after 2130 steps.
Found uncertainty sample 6 after 3471 steps.
Found uncertainty sample 7 after 640 steps.
Found uncertainty sample 9 after 1378 steps.
Found uncertainty sample 10 after 21 steps.
Found uncertainty sample 11 after 3428 steps.
Found uncertainty sample 14 after 13 steps.
Found uncertainty sample 15 after 159 steps.
Found uncertainty sample 16 after 3356 steps.
Found uncertainty sample 19 after 1720 steps.
Found uncertainty sample 20 after 2201 steps.
Found uncertainty sample 21 after 3064 steps.
Found uncertainty sample 22 after 3267 steps.
Found uncertainty sample 23 after 492 steps.
Found uncertainty sample 25 after 583 steps.
Found uncertainty sample 26 after 1104 steps.
Found uncertainty sample 28 after 175 steps.
Found uncertainty sample 29 after 2370 steps.
Found uncertainty sample 30 after 1050 steps.
Found uncertainty sample 32 after 3051 steps.
Found uncertainty sample 35 after 2425 steps.
Found uncertainty sample 38 after 883 steps.
Found uncertainty sample 39 after 645 steps.
Found uncertainty sample 40 after 2591 steps.
Found uncertainty sample 41 after 766 steps.
Found uncertainty sample 42 after 3424 steps.
Found uncertainty sample 43 after 598 steps.
Found uncertainty sample 45 after 432 steps.
Found uncertainty sample 47 after 1067 steps.
Found uncertainty sample 49 after 2666 steps.
Found uncertainty sample 50 after 2107 steps.
Found uncertainty sample 51 after 3786 steps.
Found uncertainty sample 53 after 2279 steps.
Found uncertainty sample 54 after 45 steps.
Found uncertainty sample 55 after 2152 steps.
Found uncertainty sample 56 after 3736 steps.
Found uncertainty sample 57 after 849 steps.
Found uncertainty sample 61 after 1975 steps.
Found uncertainty sample 62 after 774 steps.
Found uncertainty sample 63 after 3862 steps.
Found uncertainty sample 67 after 2609 steps.
Found uncertainty sample 72 after 2865 steps.
Found uncertainty sample 73 after 1060 steps.
Found uncertainty sample 74 after 290 steps.
Found uncertainty sample 76 after 1320 steps.
Found uncertainty sample 78 after 1174 steps.
Found uncertainty sample 80 after 839 steps.
Found uncertainty sample 83 after 5 steps.
Found uncertainty sample 85 after 1255 steps.
Found uncertainty sample 87 after 750 steps.
Found uncertainty sample 88 after 652 steps.
Found uncertainty sample 89 after 460 steps.
Found uncertainty sample 90 after 396 steps.
Found uncertainty sample 91 after 5 steps.
Found uncertainty sample 95 after 902 steps.
Found uncertainty sample 98 after 2415 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241122_192744-1v5zl17z
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_48_16
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/1v5zl17z
Training model 16. Added 57 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.887618648518079, Training Loss Force: 3.243125607639359, time: 2.2038822174072266
Validation Loss Energy: 1.4225492827319082, Validation Loss Force: 2.8029862632402236, time: 0.14228439331054688
Test Loss Energy: 10.252002209112147, Test Loss Force: 8.354089340195364, time: 18.441731452941895


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.72732499880366, Training Loss Force: 3.0852923765376783, time: 2.2785096168518066
Validation Loss Energy: 1.5881115304820395, Validation Loss Force: 2.8082415890878383, time: 0.14068388938903809
Test Loss Energy: 10.744250594656657, Test Loss Force: 8.347120066395059, time: 18.215794563293457


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.361715964291733, Training Loss Force: 3.084978732679887, time: 2.271949529647827
Validation Loss Energy: 1.34054346498932, Validation Loss Force: 2.7631750239706485, time: 0.1372694969177246
Test Loss Energy: 10.75525921733448, Test Loss Force: 8.350896026818122, time: 18.20796275138855


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.8612030127080723, Training Loss Force: 3.0874840945752933, time: 2.2963953018188477
Validation Loss Energy: 1.2916998188170434, Validation Loss Force: 2.7425055378307555, time: 0.13712787628173828
Test Loss Energy: 10.395501062743234, Test Loss Force: 8.328886573716774, time: 18.08462429046631


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.644639637924954, Training Loss Force: 3.0838480977684988, time: 2.217636823654175
Validation Loss Energy: 1.4841483639267985, Validation Loss Force: 2.7858234821449743, time: 0.14053058624267578
Test Loss Energy: 10.828982257552369, Test Loss Force: 8.279283512382149, time: 18.239587545394897


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8513506805470594, Training Loss Force: 3.098483630746159, time: 2.224289894104004
Validation Loss Energy: 1.3660968091741332, Validation Loss Force: 2.7949384436335283, time: 0.13591957092285156
Test Loss Energy: 10.496526655807106, Test Loss Force: 8.291389933586835, time: 18.291804313659668


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.8578551946690804, Training Loss Force: 3.0947007154003052, time: 2.2337136268615723
Validation Loss Energy: 1.4204323671615726, Validation Loss Force: 2.8752923903170515, time: 0.13933014869689941
Test Loss Energy: 10.461112133072968, Test Loss Force: 8.338620447010952, time: 18.12515616416931


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.9300903728914278, Training Loss Force: 3.09464543598911, time: 2.214872121810913
Validation Loss Energy: 1.2843690495275224, Validation Loss Force: 2.8149227768114016, time: 0.13770508766174316
Test Loss Energy: 10.46064899523912, Test Loss Force: 8.296799753932996, time: 18.250684022903442


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6996840739127679, Training Loss Force: 3.078437406320032, time: 2.308985710144043
Validation Loss Energy: 1.3664686109805464, Validation Loss Force: 2.824252280022057, time: 0.14565753936767578
Test Loss Energy: 10.596142568979745, Test Loss Force: 8.314200608565583, time: 18.241507291793823


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.8183273413935146, Training Loss Force: 3.0822787364023125, time: 2.21402907371521
Validation Loss Energy: 1.3950857156114669, Validation Loss Force: 2.8050789078736527, time: 0.1435537338256836
Test Loss Energy: 10.569982586314444, Test Loss Force: 8.290504985778954, time: 18.10137701034546


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.7574952799861507, Training Loss Force: 3.092091941643804, time: 2.2852041721343994
Validation Loss Energy: 1.6721757018095844, Validation Loss Force: 2.7859951600564847, time: 0.13362979888916016
Test Loss Energy: 10.811073484105654, Test Loss Force: 8.278074812962544, time: 18.6074857711792


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6986407576295237, Training Loss Force: 3.077264281107587, time: 2.248704433441162
Validation Loss Energy: 1.902076473589669, Validation Loss Force: 2.81894841230063, time: 0.1391282081604004
Test Loss Energy: 10.197971407950766, Test Loss Force: 8.263127977500117, time: 18.246098279953003


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.790146129572937, Training Loss Force: 3.0766184831213907, time: 2.2094321250915527
Validation Loss Energy: 1.3632266038831242, Validation Loss Force: 2.801799659361574, time: 0.13790369033813477
Test Loss Energy: 10.389536504771952, Test Loss Force: 8.261780976058942, time: 18.17088747024536


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.022362251046757, Training Loss Force: 3.070102684701684, time: 2.21336030960083
Validation Loss Energy: 1.5649438635266963, Validation Loss Force: 2.744672553958925, time: 0.13735222816467285
Test Loss Energy: 10.195502746244093, Test Loss Force: 8.239214929367655, time: 18.229028940200806


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6732686607423384, Training Loss Force: 3.078043949939177, time: 2.2214696407318115
Validation Loss Energy: 1.3334685425484698, Validation Loss Force: 2.7520911105592276, time: 0.13965821266174316
Test Loss Energy: 10.433267134871356, Test Loss Force: 8.229454997954367, time: 18.27970862388611


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7394044767408372, Training Loss Force: 3.0626371520944655, time: 2.273502826690674
Validation Loss Energy: 1.3797930770055786, Validation Loss Force: 2.8354525380622246, time: 0.1357715129852295
Test Loss Energy: 10.560809135120545, Test Loss Force: 8.272868943564369, time: 18.160265922546387


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.7751509553973812, Training Loss Force: 3.066683815452234, time: 2.275390148162842
Validation Loss Energy: 1.7498840050357662, Validation Loss Force: 2.7718789382158198, time: 0.1829688549041748
Test Loss Energy: 11.119026475105224, Test Loss Force: 8.253378002079009, time: 18.212121725082397


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.96438789313529, Training Loss Force: 3.076390592024847, time: 2.2446389198303223
Validation Loss Energy: 1.3515787251923552, Validation Loss Force: 2.777122128594981, time: 0.14455938339233398
Test Loss Energy: 10.627717412940875, Test Loss Force: 8.218117557067396, time: 18.25468683242798


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.7119236922674024, Training Loss Force: 3.060612514927928, time: 2.323667287826538
Validation Loss Energy: 2.05399016037287, Validation Loss Force: 2.8349735767293343, time: 0.14525294303894043
Test Loss Energy: 10.02134320925274, Test Loss Force: 8.257369259084237, time: 18.172775983810425


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.7809468985869625, Training Loss Force: 3.0519937500824477, time: 2.4490182399749756
Validation Loss Energy: 1.4160662148809342, Validation Loss Force: 2.7633236556255554, time: 0.13907551765441895
Test Loss Energy: 10.638408889469703, Test Loss Force: 8.262691172915845, time: 18.169628858566284

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–†â–†â–ƒâ–†â–„â–„â–„â–…â–„â–†â–‚â–ƒâ–‚â–„â–„â–ˆâ–…â–â–…
wandb:   test_error_force â–ˆâ–ˆâ–ˆâ–‡â–„â–…â–‡â–…â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–â–ƒâ–ƒ
wandb:          test_loss â–…â–ˆâ–ˆâ–†â–‡â–‡â–‡â–„â–…â–„â–†â–ƒâ–ƒâ–â–ƒâ–„â–†â–„â–â–„
wandb: train_error_energy â–ˆâ–â–…â–‚â–â–‚â–‚â–ƒâ–â–‚â–‚â–â–‚â–ƒâ–â–‚â–‚â–ƒâ–â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:         train_loss â–ˆâ–â–ƒâ–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–
wandb: valid_error_energy â–‚â–„â–‚â–â–ƒâ–‚â–‚â–â–‚â–‚â–…â–‡â–‚â–„â–â–‚â–…â–‚â–ˆâ–‚
wandb:  valid_error_force â–„â–„â–‚â–â–ƒâ–„â–ˆâ–…â–…â–„â–ƒâ–…â–„â–â–‚â–†â–ƒâ–ƒâ–†â–‚
wandb:         valid_loss â–‚â–ƒâ–…â–â–‚â–„â–ˆâ–†â–‚â–‚â–‚â–„â–„â–ƒâ–â–„â–ƒâ–‚â–„â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 1979
wandb:                 lr 0.0001
wandb:  test_error_energy 10.63841
wandb:   test_error_force 8.26269
wandb:          test_loss 4.54882
wandb: train_error_energy 1.78095
wandb:  train_error_force 3.05199
wandb:         train_loss 1.39183
wandb: valid_error_energy 1.41607
wandb:  valid_error_force 2.76332
wandb:         valid_loss 1.36
wandb: 
wandb: ğŸš€ View run al_48_16 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/1v5zl17z
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241122_192744-1v5zl17z/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6821814775466919, Uncertainty Bias: 0.033032938838005066
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
1.50203705e-05 0.00020241737
0.035964534 0.38057595
Found uncertainty sample 1 after 1440 steps.
Found uncertainty sample 2 after 786 steps.
Found uncertainty sample 3 after 1267 steps.
Found uncertainty sample 4 after 804 steps.
Found uncertainty sample 8 after 3138 steps.
Found uncertainty sample 9 after 560 steps.
Found uncertainty sample 10 after 2855 steps.
Found uncertainty sample 11 after 3212 steps.
Found uncertainty sample 12 after 84 steps.
Found uncertainty sample 13 after 2488 steps.
Found uncertainty sample 14 after 2961 steps.
Found uncertainty sample 15 after 2822 steps.
Found uncertainty sample 16 after 1466 steps.
Found uncertainty sample 20 after 261 steps.
Found uncertainty sample 21 after 2251 steps.
Found uncertainty sample 23 after 1505 steps.
Found uncertainty sample 25 after 1000 steps.
Found uncertainty sample 29 after 2598 steps.
Found uncertainty sample 34 after 1324 steps.
Found uncertainty sample 36 after 72 steps.
Found uncertainty sample 39 after 502 steps.
Found uncertainty sample 41 after 1876 steps.
Found uncertainty sample 43 after 11 steps.
Found uncertainty sample 44 after 3737 steps.
Found uncertainty sample 45 after 181 steps.
Found uncertainty sample 46 after 408 steps.
Found uncertainty sample 47 after 1917 steps.
Found uncertainty sample 48 after 826 steps.
Found uncertainty sample 49 after 3788 steps.
Found uncertainty sample 53 after 587 steps.
Found uncertainty sample 56 after 6 steps.
Found uncertainty sample 59 after 1083 steps.
Found uncertainty sample 62 after 2845 steps.
Found uncertainty sample 64 after 2594 steps.
Found uncertainty sample 67 after 359 steps.
Found uncertainty sample 68 after 1042 steps.
Found uncertainty sample 69 after 1292 steps.
Found uncertainty sample 70 after 2019 steps.
Found uncertainty sample 72 after 3975 steps.
slurmstepd: error: *** JOB 5122567 ON aimat01 CANCELLED AT 2024-11-22T21:08:40 ***
