wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_100422-5o4cpnyn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/5o4cpnyn
['H1', 'CH3', 'H2', 'H3', 'C', 'O', 'N', 'H', 'CA', 'HA', 'CB', 'HB1', 'HB2', 'HB3', 'C', 'O', 'N', 'H', 'C', 'H1', 'H2', 'H3']
70
Uncertainty Slope: 0.044185154139995575, Uncertainty Bias: 0.2551637291908264
0.0013961792 0.001633048
3.8687828 7.085002
(48745, 22, 3)

Training and Validation Results of Epoch Initital validation:
================================
Training Loss Energy: 0.0, Training Loss Force: 0.0, time: 0
Validation Loss Energy: 0.0, Validation Loss Force: 0.0, time: 0
Test Loss Energy: 11.451026409508195, Test Loss Force: 12.623542335388883, time: 8.295928478240967

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.047 MB uploadedwandb: | 0.050 MB of 0.050 MB uploadedwandb: / 0.050 MB of 0.050 MB uploadedwandb: - 0.050 MB of 0.050 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–
wandb:    max_uncertainty â–
wandb:  test_error_energy â–
wandb:   test_error_force â–
wandb:          test_loss â–
wandb: train_error_energy â–
wandb:  train_error_force â–
wandb:         train_loss â–
wandb: valid_error_energy â–
wandb:  valid_error_force â–
wandb:         valid_loss â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 800
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.45103
wandb:   test_error_force 12.62354
wandb:          test_loss 10.20482
wandb: train_error_energy 0.0
wandb:  train_error_force 0.0
wandb:         train_loss 0.0
wandb: valid_error_energy 0.0
wandb:  valid_error_force 0.0
wandb:         valid_loss 0.0
wandb: 
wandb: ğŸš€ View run al_71 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/5o4cpnyn
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_100422-5o4cpnyn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Found uncertainty sample 0 after 27 steps.
Found uncertainty sample 1 after 1699 steps.
Found uncertainty sample 2 after 94 steps.
Found uncertainty sample 3 after 218 steps.
Found uncertainty sample 4 after 390 steps.
Found uncertainty sample 5 after 1018 steps.
Found uncertainty sample 6 after 2177 steps.
Found uncertainty sample 7 after 410 steps.
Found uncertainty sample 8 after 172 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 223 steps.
Found uncertainty sample 11 after 206 steps.
Found uncertainty sample 12 after 14 steps.
Found uncertainty sample 13 after 154 steps.
Found uncertainty sample 14 after 1115 steps.
Found uncertainty sample 15 after 382 steps.
Found uncertainty sample 16 after 1954 steps.
Found uncertainty sample 17 after 102 steps.
Found uncertainty sample 18 after 2263 steps.
Found uncertainty sample 19 after 633 steps.
Found uncertainty sample 20 after 2607 steps.
Found uncertainty sample 21 after 34 steps.
Found uncertainty sample 22 after 913 steps.
Found uncertainty sample 23 after 964 steps.
Found uncertainty sample 24 after 539 steps.
Found uncertainty sample 25 after 325 steps.
Found uncertainty sample 26 after 1254 steps.
Found uncertainty sample 27 after 2769 steps.
Found uncertainty sample 28 after 700 steps.
Found uncertainty sample 29 after 1241 steps.
Found uncertainty sample 30 after 134 steps.
Found uncertainty sample 31 after 411 steps.
Found uncertainty sample 32 after 27 steps.
Found uncertainty sample 33 after 921 steps.
Found uncertainty sample 34 after 180 steps.
Found uncertainty sample 35 after 450 steps.
Found uncertainty sample 36 after 5 steps.
Found uncertainty sample 37 after 62 steps.
Found uncertainty sample 38 after 524 steps.
Found uncertainty sample 39 after 529 steps.
Found uncertainty sample 40 after 649 steps.
Found uncertainty sample 41 after 185 steps.
Found uncertainty sample 42 after 28 steps.
Found uncertainty sample 43 after 233 steps.
Found uncertainty sample 44 after 2842 steps.
Found uncertainty sample 45 after 45 steps.
Found uncertainty sample 46 after 967 steps.
Found uncertainty sample 47 after 532 steps.
Found uncertainty sample 48 after 1471 steps.
Found uncertainty sample 49 after 760 steps.
Found uncertainty sample 50 after 210 steps.
Found uncertainty sample 51 after 197 steps.
Found uncertainty sample 52 after 1509 steps.
Found uncertainty sample 53 after 121 steps.
Found uncertainty sample 54 after 179 steps.
Found uncertainty sample 55 after 1113 steps.
Found uncertainty sample 56 after 221 steps.
Found uncertainty sample 57 after 108 steps.
Found uncertainty sample 58 after 2753 steps.
Found uncertainty sample 59 after 286 steps.
Found uncertainty sample 60 after 1273 steps.
Found uncertainty sample 61 after 3999 steps.
Found uncertainty sample 62 after 878 steps.
Found uncertainty sample 63 after 1023 steps.
Found uncertainty sample 64 after 2286 steps.
Found uncertainty sample 65 after 1427 steps.
Found uncertainty sample 66 after 966 steps.
Found uncertainty sample 67 after 59 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 863 steps.
Found uncertainty sample 70 after 13 steps.
Found uncertainty sample 71 after 606 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 384 steps.
Found uncertainty sample 74 after 35 steps.
Found uncertainty sample 75 after 3091 steps.
Found uncertainty sample 76 after 89 steps.
Found uncertainty sample 77 after 1746 steps.
Found uncertainty sample 78 after 375 steps.
Found uncertainty sample 79 after 323 steps.
Found uncertainty sample 80 after 18 steps.
Found uncertainty sample 81 after 39 steps.
Found uncertainty sample 82 after 674 steps.
Found uncertainty sample 83 after 786 steps.
Found uncertainty sample 84 after 151 steps.
Found uncertainty sample 85 after 257 steps.
Found uncertainty sample 86 after 69 steps.
Found uncertainty sample 87 after 38 steps.
Found uncertainty sample 88 after 1299 steps.
Found uncertainty sample 89 after 3398 steps.
Found uncertainty sample 90 after 831 steps.
Found uncertainty sample 91 after 1371 steps.
Found uncertainty sample 92 after 1284 steps.
Found uncertainty sample 93 after 231 steps.
Found uncertainty sample 94 after 38 steps.
Found uncertainty sample 95 after 220 steps.
Found uncertainty sample 96 after 645 steps.
Found uncertainty sample 97 after 494 steps.
Found uncertainty sample 98 after 1392 steps.
Found uncertainty sample 99 after 49 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_101555-kryeoq1c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_0
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/kryeoq1c
Training model 0. Added 97 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.44261615062445, Training Loss Force: 3.791586160638142, time: 1.9063291549682617
Validation Loss Energy: 1.3877033984146634, Validation Loss Force: 4.0033507611040235, time: 0.03654289245605469
Test Loss Energy: 10.423163542193604, Test Loss Force: 11.95212856330147, time: 7.769222021102905


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.670994382346916, Training Loss Force: 3.65880904992698, time: 0.4658162593841553
Validation Loss Energy: 1.875128496876623, Validation Loss Force: 3.956633438273498, time: 0.0362248420715332
Test Loss Energy: 11.860822126383615, Test Loss Force: 12.46015302286075, time: 7.78860330581665


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6115975228626602, Training Loss Force: 3.382361201783371, time: 0.4430375099182129
Validation Loss Energy: 1.9160421710876867, Validation Loss Force: 3.8966726453197715, time: 0.03425908088684082
Test Loss Energy: 11.755104934052481, Test Loss Force: 12.693531813254383, time: 7.723182439804077


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6316007272506508, Training Loss Force: 3.371378531261122, time: 0.4785654544830322
Validation Loss Energy: 1.7046881804793654, Validation Loss Force: 3.918930207808421, time: 0.05664253234863281
Test Loss Energy: 11.625568699605907, Test Loss Force: 12.868025750339026, time: 7.919205665588379


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6616346597521703, Training Loss Force: 3.365646367389973, time: 0.4277644157409668
Validation Loss Energy: 1.8803436309433978, Validation Loss Force: 3.9129876136419, time: 0.0364224910736084
Test Loss Energy: 11.560973182447505, Test Loss Force: 12.555684209282381, time: 7.740234375


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6223191495974214, Training Loss Force: 3.366836987265718, time: 0.4312119483947754
Validation Loss Energy: 1.7342408295269014, Validation Loss Force: 3.8885844112630905, time: 0.037529706954956055
Test Loss Energy: 11.450175752748782, Test Loss Force: 12.700440521046993, time: 7.702134847640991


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6165018624927714, Training Loss Force: 3.3737059725564635, time: 0.4662349224090576
Validation Loss Energy: 1.6881052298401122, Validation Loss Force: 3.9130486249878844, time: 0.038779258728027344
Test Loss Energy: 11.329031650276606, Test Loss Force: 12.594544176815376, time: 7.9690022468566895


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6640100345574071, Training Loss Force: 3.3674988394921104, time: 0.447437047958374
Validation Loss Energy: 1.605569125526497, Validation Loss Force: 3.9082674730865232, time: 0.04092240333557129
Test Loss Energy: 11.453611148290895, Test Loss Force: 12.627304212862823, time: 7.742284059524536


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6446079056476748, Training Loss Force: 3.3484591881183055, time: 0.4644918441772461
Validation Loss Energy: 1.6126731619581771, Validation Loss Force: 3.9047256976513958, time: 0.03728342056274414
Test Loss Energy: 11.48922697325941, Test Loss Force: 12.75101849085607, time: 7.768930912017822


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.5492608779555275, Training Loss Force: 3.3509043498196163, time: 0.44115161895751953
Validation Loss Energy: 1.7072470943043676, Validation Loss Force: 3.892874355028535, time: 0.03442263603210449
Test Loss Energy: 11.346391202697143, Test Loss Force: 12.763222009798069, time: 7.80552077293396


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6147521641301836, Training Loss Force: 3.336255582906587, time: 0.47464632987976074
Validation Loss Energy: 1.5944911128474435, Validation Loss Force: 3.9057477490312045, time: 0.037412166595458984
Test Loss Energy: 11.429748808101738, Test Loss Force: 12.611490310740995, time: 8.309206008911133


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6235212727202946, Training Loss Force: 3.3515356009433823, time: 0.4693624973297119
Validation Loss Energy: 1.7246748516832706, Validation Loss Force: 3.9368390418771013, time: 0.03692340850830078
Test Loss Energy: 11.582618819812161, Test Loss Force: 12.574199393585756, time: 7.748905658721924


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6207058957302356, Training Loss Force: 3.341887291183045, time: 0.4416837692260742
Validation Loss Energy: 1.904360725023169, Validation Loss Force: 3.903266769407277, time: 0.04318857192993164
Test Loss Energy: 11.623257930142419, Test Loss Force: 12.748599879577531, time: 7.716502904891968


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6506958431229648, Training Loss Force: 3.380687437575779, time: 0.43767714500427246
Validation Loss Energy: 1.8364471921880585, Validation Loss Force: 3.900974613219096, time: 0.0363917350769043
Test Loss Energy: 11.310537646355657, Test Loss Force: 12.583352191342009, time: 7.791980504989624


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6386279627081592, Training Loss Force: 3.3599510314050662, time: 0.5070996284484863
Validation Loss Energy: 1.670631602617148, Validation Loss Force: 3.876041012934686, time: 0.05830574035644531
Test Loss Energy: 11.12472036998608, Test Loss Force: 12.576257793802995, time: 8.010501146316528


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6279063220565764, Training Loss Force: 3.362012709122272, time: 0.4538869857788086
Validation Loss Energy: 1.6105795153135911, Validation Loss Force: 3.892227723828761, time: 0.03420209884643555
Test Loss Energy: 11.269762327793284, Test Loss Force: 12.656952980369748, time: 7.7887537479400635


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6120998008361174, Training Loss Force: 3.350471544619688, time: 0.44399523735046387
Validation Loss Energy: 1.6965412731027325, Validation Loss Force: 3.8808622452823145, time: 0.03662419319152832
Test Loss Energy: 11.228712844056119, Test Loss Force: 12.541311855553221, time: 7.7853782176971436


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.601588620301666, Training Loss Force: 3.351623634780719, time: 0.45266103744506836
Validation Loss Energy: 1.7490015672104544, Validation Loss Force: 3.9153024820829656, time: 0.038083553314208984
Test Loss Energy: 11.382353617437428, Test Loss Force: 12.578856834706544, time: 7.914483070373535


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6241553072083852, Training Loss Force: 3.3804967691671095, time: 0.4520263671875
Validation Loss Energy: 1.7211134813633016, Validation Loss Force: 3.894981250487304, time: 0.036881208419799805
Test Loss Energy: 11.505817965052769, Test Loss Force: 12.725389747809059, time: 7.699612617492676


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6642167908118706, Training Loss Force: 3.359535554636173, time: 0.4533846378326416
Validation Loss Energy: 1.6715800618442274, Validation Loss Force: 3.9807422720413914, time: 0.041929006576538086
Test Loss Energy: 11.602106296518977, Test Loss Force: 12.801211231384626, time: 7.774499177932739

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.045 MB of 0.061 MB uploaded (0.003 MB deduped)wandb: - 0.045 MB of 0.061 MB uploaded (0.003 MB deduped)wandb: \ 0.064 MB of 0.064 MB uploaded (0.003 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 4.7%             
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–ˆâ–‡â–‡â–‡â–†â–…â–†â–†â–…â–†â–‡â–‡â–…â–„â–…â–…â–†â–†â–‡
wandb:   test_error_force â–â–…â–‡â–ˆâ–†â–‡â–†â–†â–‡â–‡â–†â–†â–‡â–†â–†â–†â–†â–†â–‡â–‡
wandb:          test_loss â–ƒâ–â–ƒâ–â–ƒâ–ƒâ–„â–„â–ƒâ–„â–†â–†â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–†â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–‡â–ˆâ–…â–ˆâ–†â–…â–„â–„â–…â–„â–…â–ˆâ–‡â–…â–„â–…â–†â–…â–…
wandb:  valid_error_force â–ˆâ–…â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–‚â–‚â–â–‚â–â–ƒâ–‚â–‡
wandb:         valid_loss â–ˆâ–‡â–‡â–„â–†â–„â–ƒâ–â–â–ƒâ–â–„â–ˆâ–†â–ƒâ–â–„â–…â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 887
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.60211
wandb:   test_error_force 12.80121
wandb:          test_loss 12.64255
wandb: train_error_energy 1.66422
wandb:  train_error_force 3.35954
wandb:         train_loss 0.50385
wandb: valid_error_energy 1.67158
wandb:  valid_error_force 3.98074
wandb:         valid_loss 0.82478
wandb: 
wandb: ğŸš€ View run al_71_0 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/kryeoq1c
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_101555-kryeoq1c/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.043029848486185074, Uncertainty Bias: 0.26179298758506775
0.0004310608 0.007123947
3.9482832 7.069803
(48745, 22, 3)
Found uncertainty sample 0 after 16 steps.
Found uncertainty sample 1 after 3432 steps.
Found uncertainty sample 2 after 37 steps.
Found uncertainty sample 3 after 39 steps.
Found uncertainty sample 4 after 397 steps.
Found uncertainty sample 5 after 2496 steps.
Found uncertainty sample 6 after 316 steps.
Found uncertainty sample 7 after 205 steps.
Found uncertainty sample 8 after 13 steps.
Found uncertainty sample 9 after 141 steps.
Found uncertainty sample 10 after 20 steps.
Found uncertainty sample 11 after 71 steps.
Found uncertainty sample 12 after 87 steps.
Found uncertainty sample 13 after 18 steps.
Found uncertainty sample 14 after 57 steps.
Found uncertainty sample 15 after 113 steps.
Found uncertainty sample 16 after 49 steps.
Found uncertainty sample 17 after 23 steps.
Found uncertainty sample 18 after 662 steps.
Found uncertainty sample 19 after 190 steps.
Found uncertainty sample 20 after 1487 steps.
Found uncertainty sample 21 after 190 steps.
Found uncertainty sample 22 after 807 steps.
Found uncertainty sample 23 after 44 steps.
Found uncertainty sample 24 after 183 steps.
Found uncertainty sample 25 after 178 steps.
Found uncertainty sample 26 after 262 steps.
Found uncertainty sample 27 after 804 steps.
Found uncertainty sample 28 after 608 steps.
Found uncertainty sample 29 after 210 steps.
Found uncertainty sample 30 after 129 steps.
Found uncertainty sample 31 after 1321 steps.
Found uncertainty sample 32 after 155 steps.
Found uncertainty sample 33 after 1123 steps.
Found uncertainty sample 34 after 31 steps.
Found uncertainty sample 35 after 181 steps.
Found uncertainty sample 36 after 97 steps.
Found uncertainty sample 37 after 567 steps.
Found uncertainty sample 38 after 63 steps.
Found uncertainty sample 39 after 368 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 542 steps.
Found uncertainty sample 42 after 313 steps.
Found uncertainty sample 43 after 47 steps.
Found uncertainty sample 44 after 3697 steps.
Found uncertainty sample 45 after 248 steps.
Found uncertainty sample 46 after 1764 steps.
Found uncertainty sample 47 after 56 steps.
Found uncertainty sample 48 after 108 steps.
Found uncertainty sample 49 after 15 steps.
Found uncertainty sample 50 after 34 steps.
Found uncertainty sample 51 after 155 steps.
Found uncertainty sample 52 after 482 steps.
Found uncertainty sample 53 after 8 steps.
Found uncertainty sample 54 after 12 steps.
Found uncertainty sample 55 after 125 steps.
Found uncertainty sample 56 after 85 steps.
Found uncertainty sample 57 after 190 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 1757 steps.
Found uncertainty sample 60 after 185 steps.
Found uncertainty sample 61 after 479 steps.
Found uncertainty sample 62 after 810 steps.
Found uncertainty sample 63 after 1132 steps.
Found uncertainty sample 64 after 31 steps.
Found uncertainty sample 65 after 1144 steps.
Found uncertainty sample 66 after 23 steps.
Found uncertainty sample 67 after 23 steps.
Found uncertainty sample 68 after 2165 steps.
Found uncertainty sample 69 after 8 steps.
Found uncertainty sample 70 after 475 steps.
Found uncertainty sample 71 after 161 steps.
Found uncertainty sample 72 after 1488 steps.
Found uncertainty sample 73 after 1533 steps.
Found uncertainty sample 74 after 91 steps.
Found uncertainty sample 75 after 213 steps.
Found uncertainty sample 76 after 3723 steps.
Found uncertainty sample 77 after 1546 steps.
Found uncertainty sample 78 after 1653 steps.
Found uncertainty sample 79 after 382 steps.
Found uncertainty sample 80 after 191 steps.
Found uncertainty sample 81 after 153 steps.
Found uncertainty sample 82 after 196 steps.
Found uncertainty sample 83 after 1419 steps.
Found uncertainty sample 84 after 7 steps.
Found uncertainty sample 85 after 768 steps.
Found uncertainty sample 86 after 363 steps.
Found uncertainty sample 87 after 84 steps.
Found uncertainty sample 88 after 715 steps.
Found uncertainty sample 89 after 229 steps.
Found uncertainty sample 90 after 65 steps.
Found uncertainty sample 91 after 566 steps.
Found uncertainty sample 92 after 13 steps.
Found uncertainty sample 93 after 107 steps.
Found uncertainty sample 94 after 2498 steps.
Found uncertainty sample 95 after 793 steps.
Found uncertainty sample 96 after 756 steps.
Found uncertainty sample 97 after 1437 steps.
Found uncertainty sample 98 after 199 steps.
Found uncertainty sample 99 after 88 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_102651-8htkpo90
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_1
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8htkpo90
Training model 1. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.759670769993622, Training Loss Force: 4.179907985694685, time: 0.5020155906677246
Validation Loss Energy: 4.162628227877629, Validation Loss Force: 5.074011706145189, time: 0.040961265563964844
Test Loss Energy: 11.917339552006643, Test Loss Force: 12.490491034146622, time: 8.342161893844604


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.012749004434889, Training Loss Force: 3.9433924462424064, time: 0.48728275299072266
Validation Loss Energy: 4.003388169520539, Validation Loss Force: 4.135992989980039, time: 0.040177106857299805
Test Loss Energy: 12.029658812234501, Test Loss Force: 12.23270823230409, time: 8.321435451507568


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.213761011634584, Training Loss Force: 3.616896290554954, time: 0.4984261989593506
Validation Loss Energy: 2.284506167003153, Validation Loss Force: 4.054603854437098, time: 0.038591623306274414
Test Loss Energy: 9.554339988257496, Test Loss Force: 12.071278622768617, time: 8.35429835319519


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.9765029100864506, Training Loss Force: 3.563371576875254, time: 0.7001955509185791
Validation Loss Energy: 5.509911498243178, Validation Loss Force: 4.050478685839639, time: 0.040686845779418945
Test Loss Energy: 9.793983378050417, Test Loss Force: 12.017584693211463, time: 8.458010196685791


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.002423459037245, Training Loss Force: 3.526666011927069, time: 0.4885842800140381
Validation Loss Energy: 6.226731130181956, Validation Loss Force: 4.027475627050424, time: 0.041367292404174805
Test Loss Energy: 10.025448183740535, Test Loss Force: 11.946283542521535, time: 8.332551002502441


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.11819064563468, Training Loss Force: 3.521958218734977, time: 0.5011723041534424
Validation Loss Energy: 3.8741527785772187, Validation Loss Force: 3.993489635307465, time: 0.04323577880859375
Test Loss Energy: 9.75460769012254, Test Loss Force: 11.924766698856171, time: 8.38106918334961


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.15630091888995, Training Loss Force: 3.5058326341128976, time: 0.496412992477417
Validation Loss Energy: 1.8240324966260006, Validation Loss Force: 3.985942041937097, time: 0.04323768615722656
Test Loss Energy: 10.610751618845217, Test Loss Force: 11.77649429438567, time: 8.883162498474121


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.120983721206691, Training Loss Force: 3.491607135593149, time: 0.4942929744720459
Validation Loss Energy: 4.971648473070726, Validation Loss Force: 4.038921711827696, time: 0.04017043113708496
Test Loss Energy: 12.60375431362542, Test Loss Force: 11.807369747383317, time: 8.275678634643555


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 3.9586899961395354, Training Loss Force: 3.4841331236011968, time: 0.5040285587310791
Validation Loss Energy: 5.556721896776701, Validation Loss Force: 3.998452930570196, time: 0.04343771934509277
Test Loss Energy: 12.98374913450077, Test Loss Force: 11.813065684758117, time: 8.336575508117676


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.151686538996014, Training Loss Force: 3.4972729735599093, time: 0.4989006519317627
Validation Loss Energy: 2.9470027660939495, Validation Loss Force: 4.099605288829104, time: 0.04450225830078125
Test Loss Energy: 11.18665087088854, Test Loss Force: 11.767328061869007, time: 8.506661176681519


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.012448569885465, Training Loss Force: 3.538255524365821, time: 0.47901439666748047
Validation Loss Energy: 2.1739464826670876, Validation Loss Force: 4.089376313603473, time: 0.03969120979309082
Test Loss Energy: 9.713649088039508, Test Loss Force: 11.750768938577076, time: 8.347479820251465


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.060133440435176, Training Loss Force: 3.477864290404389, time: 0.5030198097229004
Validation Loss Energy: 5.29347753373966, Validation Loss Force: 3.9441960304585555, time: 0.0441136360168457
Test Loss Energy: 9.947162683033572, Test Loss Force: 11.606698923431194, time: 8.329473972320557


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.028527142586252, Training Loss Force: 3.478963672623716, time: 0.5496647357940674
Validation Loss Energy: 5.692628455332018, Validation Loss Force: 3.9936437223564476, time: 0.04282379150390625
Test Loss Energy: 10.008038454984703, Test Loss Force: 11.608470530451704, time: 8.35671091079712


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.116390107006329, Training Loss Force: 3.4790216671619167, time: 0.4774179458618164
Validation Loss Energy: 3.830587560176447, Validation Loss Force: 3.9714814622622545, time: 0.0420377254486084
Test Loss Energy: 9.669353386601319, Test Loss Force: 11.71278111129575, time: 8.529869318008423


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.218994997158568, Training Loss Force: 3.4541038548382406, time: 0.4949679374694824
Validation Loss Energy: 1.9171492820812397, Validation Loss Force: 3.914388647908078, time: 0.04065227508544922
Test Loss Energy: 10.722301465075132, Test Loss Force: 11.619862848207982, time: 8.349464416503906


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.05666286166374, Training Loss Force: 3.4539983599432005, time: 0.538489580154419
Validation Loss Energy: 4.739133788186595, Validation Loss Force: 3.945943092577505, time: 0.03967690467834473
Test Loss Energy: 12.266286837437791, Test Loss Force: 11.596309638078568, time: 8.405155420303345


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.068948157720512, Training Loss Force: 3.441882788055755, time: 0.4777872562408447
Validation Loss Energy: 5.345410658644398, Validation Loss Force: 3.954937590058112, time: 0.040944814682006836
Test Loss Energy: 12.658144206673338, Test Loss Force: 11.694537308219942, time: 8.62854814529419


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.091488114966328, Training Loss Force: 3.455889050844149, time: 0.5093128681182861
Validation Loss Energy: 3.408741085885399, Validation Loss Force: 3.918791707599654, time: 0.04126262664794922
Test Loss Energy: 11.596934594593728, Test Loss Force: 11.571515333083333, time: 8.700841903686523


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.138333264252368, Training Loss Force: 3.4673578622400574, time: 0.485898494720459
Validation Loss Energy: 2.43938346878408, Validation Loss Force: 3.953543681204604, time: 0.041445255279541016
Test Loss Energy: 9.712521524127128, Test Loss Force: 11.626784645682115, time: 8.363756656646729


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.07466638039957, Training Loss Force: 3.4509457742559246, time: 0.49040985107421875
Validation Loss Energy: 5.296952508431009, Validation Loss Force: 3.980041309570011, time: 0.04307055473327637
Test Loss Energy: 9.8472432496647, Test Loss Force: 11.701646800628604, time: 8.482510566711426

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–†â–â–â–‚â–â–ƒâ–‡â–ˆâ–„â–â–‚â–‚â–â–ƒâ–‡â–‡â–…â–â–‚
wandb:   test_error_force â–ˆâ–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–â–‚â–â–â–‚â–â–â–‚
wandb:          test_loss â–ˆâ–‡â–ƒâ–‚â–‚â–‚â–„â–…â–†â–„â–‚â–â–â–â–ƒâ–…â–…â–„â–‚â–
wandb: train_error_energy â–ˆâ–â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–â–â–‚â–‚â–
wandb:  train_error_force â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–„â–‚â–‡â–ˆâ–„â–â–†â–‡â–ƒâ–‚â–‡â–‡â–„â–â–†â–‡â–„â–‚â–‡
wandb:  valid_error_force â–ˆâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:         valid_loss â–ˆâ–…â–â–‡â–ˆâ–ƒâ–â–†â–‡â–‚â–‚â–…â–†â–ƒâ–â–…â–†â–ƒâ–‚â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 977
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.84724
wandb:   test_error_force 11.70165
wandb:          test_loss 9.39407
wandb: train_error_energy 4.07467
wandb:  train_error_force 3.45095
wandb:         train_loss 1.41517
wandb: valid_error_energy 5.29695
wandb:  valid_error_force 3.98004
wandb:         valid_loss 1.86552
wandb: 
wandb: ğŸš€ View run al_71_1 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8htkpo90
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_102651-8htkpo90/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.101405382156372, Uncertainty Bias: -0.06904283165931702
0.00018501282 0.053926468
3.251715 4.576208
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 116 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 95 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 4 steps.
Found uncertainty sample 7 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 51 steps.
Found uncertainty sample 19 after 5 steps.
Found uncertainty sample 20 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 30 steps.
Found uncertainty sample 23 after 24 steps.
Found uncertainty sample 24 after 8 steps.
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 43 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 16 steps.
Found uncertainty sample 32 after 4 steps.
Found uncertainty sample 33 after 8 steps.
Found uncertainty sample 34 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 11 steps.
Found uncertainty sample 37 after 23 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 6 steps.
Found uncertainty sample 40 after 54 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 9 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 71 steps.
Found uncertainty sample 49 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 8 steps.
Found uncertainty sample 58 after 39 steps.
Found uncertainty sample 59 after 15 steps.
Found uncertainty sample 60 after 8 steps.
Found uncertainty sample 61 after 28 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 2 steps.
Found uncertainty sample 64 after 9 steps.
Found uncertainty sample 65 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 112 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 7 steps.
Found uncertainty sample 80 after 26 steps.
Found uncertainty sample 81 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 60 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 20 steps.
Found uncertainty sample 86 after 16 steps.
Found uncertainty sample 87 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 33 steps.
Found uncertainty sample 90 after 13 steps.
Found uncertainty sample 91 after 21 steps.
Found uncertainty sample 92 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 49 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_103246-97itgrce
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_2
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/97itgrce
Training model 2. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 6.073791197325182, Training Loss Force: 4.040763146369963, time: 0.7967031002044678
Validation Loss Energy: 4.85961249229716, Validation Loss Force: 4.18209485529122, time: 0.05245518684387207
Test Loss Energy: 12.0974330608556, Test Loss Force: 11.406604311269998, time: 8.713823795318604


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.054197987525905, Training Loss Force: 3.5381642851551245, time: 0.5682961940765381
Validation Loss Energy: 3.834783982900602, Validation Loss Force: 3.9739397296332974, time: 0.044101715087890625
Test Loss Energy: 9.783283069367402, Test Loss Force: 11.414581593138426, time: 8.653929471969604


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.0648473457263155, Training Loss Force: 3.4574094161252655, time: 0.5612740516662598
Validation Loss Energy: 5.442409346655169, Validation Loss Force: 3.9479943207497916, time: 0.04487133026123047
Test Loss Energy: 9.971793742399667, Test Loss Force: 11.464576155795488, time: 8.864521265029907


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.9162806447192025, Training Loss Force: 3.463661939546548, time: 0.5398204326629639
Validation Loss Energy: 3.4631560810906477, Validation Loss Force: 3.937913643175349, time: 0.04452776908874512
Test Loss Energy: 11.428808738605069, Test Loss Force: 11.509474434916768, time: 9.008431673049927


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.097429654095169, Training Loss Force: 3.440050240843309, time: 0.5744950771331787
Validation Loss Energy: 4.962912271799395, Validation Loss Force: 3.9522717950430644, time: 0.04396533966064453
Test Loss Energy: 12.18275002417397, Test Loss Force: 11.518558361455934, time: 8.761678457260132


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 3.99361834789745, Training Loss Force: 3.4322847526154647, time: 0.5418391227722168
Validation Loss Energy: 3.6165708919211212, Validation Loss Force: 3.9029933316960777, time: 0.04492449760437012
Test Loss Energy: 9.838575469608942, Test Loss Force: 11.469861962251244, time: 8.701135873794556


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.005332431364579, Training Loss Force: 3.4454197239227664, time: 0.6222052574157715
Validation Loss Energy: 5.323981870716891, Validation Loss Force: 3.9249017489639124, time: 0.06890535354614258
Test Loss Energy: 10.035735251131303, Test Loss Force: 11.581618333531104, time: 8.863009214401245


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 3.991776736529684, Training Loss Force: 3.432426710745086, time: 0.5250251293182373
Validation Loss Energy: 3.2648360289392095, Validation Loss Force: 3.9012703178448884, time: 0.04637956619262695
Test Loss Energy: 11.274623211570187, Test Loss Force: 11.506644648156072, time: 8.899105548858643


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.062527211814858, Training Loss Force: 3.438727972512957, time: 0.6424152851104736
Validation Loss Energy: 5.091631961000747, Validation Loss Force: 3.9204626168706964, time: 0.05605363845825195
Test Loss Energy: 12.48320494994975, Test Loss Force: 11.523152382673379, time: 9.963823318481445


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.140449660543044, Training Loss Force: 3.4131357758367256, time: 0.5824799537658691
Validation Loss Energy: 3.549964092968888, Validation Loss Force: 3.915619562946767, time: 0.05626177787780762
Test Loss Energy: 9.618036747457742, Test Loss Force: 11.511302247826617, time: 9.93962812423706


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.117010857882525, Training Loss Force: 3.424216761803786, time: 0.6029770374298096
Validation Loss Energy: 5.459879522506371, Validation Loss Force: 3.9203490556062732, time: 0.053231000900268555
Test Loss Energy: 9.965450991959194, Test Loss Force: 11.518543639644548, time: 9.910959482192993


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.012477078719019, Training Loss Force: 3.440831855389163, time: 0.5995259284973145
Validation Loss Energy: 3.5082178588212, Validation Loss Force: 3.9430083132341767, time: 0.049764156341552734
Test Loss Energy: 11.166417342957006, Test Loss Force: 11.52516922886424, time: 9.878685235977173


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.090799025365916, Training Loss Force: 3.5036533247042714, time: 0.7708017826080322
Validation Loss Energy: 4.720183753191557, Validation Loss Force: 3.9726816489588055, time: 0.051143646240234375
Test Loss Energy: 12.408437994806702, Test Loss Force: 11.530846424724958, time: 9.778658628463745


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.043011204243611, Training Loss Force: 3.4379243071725627, time: 0.5612766742706299
Validation Loss Energy: 3.5557091796029705, Validation Loss Force: 3.93229792731505, time: 0.0546870231628418
Test Loss Energy: 9.642879004428423, Test Loss Force: 11.538058657377034, time: 10.14557147026062


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.127902578350093, Training Loss Force: 3.430258430642059, time: 0.5793008804321289
Validation Loss Energy: 5.369580949096568, Validation Loss Force: 3.8895026988208965, time: 0.0472259521484375
Test Loss Energy: 9.958116540759567, Test Loss Force: 11.52110267937671, time: 9.934583187103271


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.054760345003038, Training Loss Force: 3.4295059857543215, time: 0.5649893283843994
Validation Loss Energy: 3.2746891386465142, Validation Loss Force: 3.9468627161501866, time: 0.05097222328186035
Test Loss Energy: 11.367851840473167, Test Loss Force: 11.496288257215074, time: 9.887146711349487


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.086336833917911, Training Loss Force: 3.4312050364759537, time: 0.5963103771209717
Validation Loss Energy: 5.172169164391368, Validation Loss Force: 3.9272491271002834, time: 0.04973101615905762
Test Loss Energy: 12.441610745201608, Test Loss Force: 11.519728547273942, time: 9.805571794509888


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 3.9778108327352526, Training Loss Force: 3.4354226651617124, time: 0.5858697891235352
Validation Loss Energy: 3.807137381742208, Validation Loss Force: 3.897621215881424, time: 0.04991507530212402
Test Loss Energy: 9.662953786746018, Test Loss Force: 11.658525318700804, time: 10.05698561668396


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.022492275191828, Training Loss Force: 3.5197110184755585, time: 0.5905811786651611
Validation Loss Energy: 5.327504051267177, Validation Loss Force: 3.936149212863017, time: 0.04906296730041504
Test Loss Energy: 9.934240551301507, Test Loss Force: 11.527898383160666, time: 9.852835178375244


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.045837088550942, Training Loss Force: 3.4370310305991185, time: 0.5440890789031982
Validation Loss Energy: 2.9954208358610717, Validation Loss Force: 3.9097856313234587, time: 0.05326056480407715
Test Loss Energy: 11.122392225334364, Test Loss Force: 11.546790025212347, time: 9.796478509902954

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–â–‚â–…â–‡â–‚â–‚â–…â–ˆâ–â–‚â–…â–ˆâ–â–‚â–…â–ˆâ–â–‚â–…
wandb:   test_error_force â–â–â–ƒâ–„â–„â–ƒâ–†â–„â–„â–„â–„â–„â–„â–…â–„â–ƒâ–„â–ˆâ–„â–…
wandb:          test_loss â–„â–â–â–†â–‡â–‚â–‚â–†â–ˆâ–â–â–…â–ˆâ–‚â–‚â–…â–‡â–â–‚â–…
wandb: train_error_energy â–ˆâ–â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–â–
wandb:  train_error_force â–ˆâ–‚â–â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–ƒâ–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–†â–ƒâ–ˆâ–‚â–‡â–ƒâ–ˆâ–
wandb:  valid_error_force â–ˆâ–ƒâ–‚â–‚â–ƒâ–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–â–‚â–
wandb:         valid_loss â–†â–ƒâ–ˆâ–‚â–‡â–‚â–‡â–‚â–‡â–‚â–ˆâ–‚â–†â–‚â–‡â–‚â–‡â–ƒâ–ˆâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 1067
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.12239
wandb:   test_error_force 11.54679
wandb:          test_loss 11.5443
wandb: train_error_energy 4.04584
wandb:  train_error_force 3.43703
wandb:         train_loss 1.40288
wandb: valid_error_energy 2.99542
wandb:  valid_error_force 3.90979
wandb:         valid_loss 1.28236
wandb: 
wandb: ğŸš€ View run al_71_2 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/97itgrce
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_103246-97itgrce/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.3593335151672363, Uncertainty Bias: -0.09766316413879395
4.9591064e-05 0.0075969696
3.0764241 4.6735463
(48745, 22, 3)
Found uncertainty sample 0 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 59 steps.
Found uncertainty sample 3 after 88 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 7 steps.
Found uncertainty sample 6 after 27 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 75 steps.
Found uncertainty sample 9 after 97 steps.
Found uncertainty sample 10 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 12 steps.
Found uncertainty sample 14 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 43 steps.
Found uncertainty sample 22 after 29 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 61 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 6 steps.
Found uncertainty sample 27 after 50 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 9 steps.
Found uncertainty sample 30 after 90 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 38 steps.
Found uncertainty sample 34 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 132 steps.
Found uncertainty sample 37 after 7 steps.
Found uncertainty sample 38 after 5 steps.
Found uncertainty sample 39 after 6 steps.
Found uncertainty sample 40 after 20 steps.
Found uncertainty sample 41 after 7 steps.
Found uncertainty sample 42 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 42 steps.
Found uncertainty sample 45 after 75 steps.
Found uncertainty sample 46 after 40 steps.
Found uncertainty sample 47 after 196 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 33 steps.
Found uncertainty sample 50 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 9 steps.
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 22 steps.
Found uncertainty sample 60 after 4 steps.
Found uncertainty sample 61 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 4 steps.
Found uncertainty sample 67 after 61 steps.
Found uncertainty sample 68 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 32 steps.
Found uncertainty sample 73 after 38 steps.
Found uncertainty sample 74 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 136 steps.
Found uncertainty sample 77 after 34 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 102 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 17 steps.
Found uncertainty sample 84 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 24 steps.
Found uncertainty sample 90 after 15 steps.
Found uncertainty sample 91 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 10 steps.
Found uncertainty sample 94 after 5 steps.
Found uncertainty sample 95 after 35 steps.
Found uncertainty sample 96 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_103910-t5qvq1ws
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_3
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t5qvq1ws
Training model 3. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 6.0023869134773316, Training Loss Force: 3.8473371425629175, time: 0.5814769268035889
Validation Loss Energy: 4.397167420636536, Validation Loss Force: 4.049071416837376, time: 0.048961639404296875
Test Loss Energy: 12.044394370817455, Test Loss Force: 11.438603107958517, time: 8.59928846359253


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.012272177969129, Training Loss Force: 3.513908812780482, time: 0.5789816379547119
Validation Loss Energy: 5.527873772906881, Validation Loss Force: 3.8842423629174987, time: 0.04722952842712402
Test Loss Energy: 10.067703391757723, Test Loss Force: 11.481760939750032, time: 9.816970109939575


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.038490350538311, Training Loss Force: 3.457558058400322, time: 0.6292803287506104
Validation Loss Energy: 1.9191074345505286, Validation Loss Force: 3.8949629916173376, time: 0.05379033088684082
Test Loss Energy: 10.546513845900405, Test Loss Force: 11.452619876548733, time: 10.641043186187744


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.979356908868618, Training Loss Force: 3.4220566346129453, time: 0.6665751934051514
Validation Loss Energy: 3.4569905951090534, Validation Loss Force: 3.90027231473284, time: 0.049918413162231445
Test Loss Energy: 11.321620773430222, Test Loss Force: 11.467458972787128, time: 8.717577934265137


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.028414941797307, Training Loss Force: 3.4556842493926605, time: 0.5794587135314941
Validation Loss Energy: 6.116633706992836, Validation Loss Force: 3.8852034029513227, time: 0.046561479568481445
Test Loss Energy: 10.30780067686622, Test Loss Force: 11.544148464994187, time: 8.698664903640747


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.057262797862344, Training Loss Force: 3.4571936392797142, time: 0.6219770908355713
Validation Loss Energy: 4.600245689767305, Validation Loss Force: 3.9882431048911147, time: 0.0468289852142334
Test Loss Energy: 12.121818386749158, Test Loss Force: 11.524993630343635, time: 9.432019472122192


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.091442731422363, Training Loss Force: 3.4339800317678106, time: 0.6183969974517822
Validation Loss Energy: 2.7712388757558504, Validation Loss Force: 3.8834536688026224, time: 0.05226874351501465
Test Loss Energy: 9.889843074908136, Test Loss Force: 11.488815723165603, time: 10.278795003890991


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 3.91086713536523, Training Loss Force: 3.4552833065140347, time: 0.6708590984344482
Validation Loss Energy: 3.824124365546747, Validation Loss Force: 3.8615577210069594, time: 0.05460047721862793
Test Loss Energy: 9.82526360858115, Test Loss Force: 11.462032318187216, time: 10.205394268035889


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.003324176213426, Training Loss Force: 3.4028300084301537, time: 0.670335054397583
Validation Loss Energy: 5.717990916391486, Validation Loss Force: 3.895933383755291, time: 0.05268287658691406
Test Loss Energy: 13.025709980901887, Test Loss Force: 11.526335287725306, time: 10.444463968276978


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.014363290999919, Training Loss Force: 3.449222950459743, time: 0.6383097171783447
Validation Loss Energy: 5.2177401864586574, Validation Loss Force: 3.9064807758595985, time: 0.05243492126464844
Test Loss Energy: 10.202262594466523, Test Loss Force: 11.523481816428648, time: 10.281957864761353


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.062112414507994, Training Loss Force: 3.4406959859685635, time: 0.622614860534668
Validation Loss Energy: 1.9984794103005101, Validation Loss Force: 3.8794416161312952, time: 0.061945438385009766
Test Loss Energy: 10.636157869948974, Test Loss Force: 11.520226211649284, time: 10.299602746963501


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 3.995025607076075, Training Loss Force: 3.420071485014234, time: 0.6176443099975586
Validation Loss Energy: 3.1195001000612885, Validation Loss Force: 3.891755219901102, time: 0.05382084846496582
Test Loss Energy: 11.281312789607064, Test Loss Force: 11.579258646803934, time: 10.783458709716797


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.026997797117517, Training Loss Force: 3.4709574330341835, time: 0.6265723705291748
Validation Loss Energy: 6.0021122932124245, Validation Loss Force: 3.8898113182571525, time: 0.05538654327392578
Test Loss Energy: 10.215270095917008, Test Loss Force: 11.56933548375119, time: 10.3935546875


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.090908554159367, Training Loss Force: 3.4238938777359955, time: 0.6513433456420898
Validation Loss Energy: 4.731537207604123, Validation Loss Force: 3.903372314582447, time: 0.055303335189819336
Test Loss Energy: 11.969277020708438, Test Loss Force: 11.511892253446518, time: 10.305300951004028


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 3.98010886558346, Training Loss Force: 3.4326581955005637, time: 0.651207685470581
Validation Loss Energy: 2.3771396490771557, Validation Loss Force: 3.8654669005328675, time: 0.07502055168151855
Test Loss Energy: 9.728594454585053, Test Loss Force: 11.556116271741017, time: 10.441436767578125


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 3.992502549001653, Training Loss Force: 3.4322980612564926, time: 0.6443614959716797
Validation Loss Energy: 3.335423484213194, Validation Loss Force: 3.917567917973635, time: 0.05248117446899414
Test Loss Energy: 9.768537288988258, Test Loss Force: 11.593619375105224, time: 10.402931928634644


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.060268358912694, Training Loss Force: 3.435811178570731, time: 0.6426329612731934
Validation Loss Energy: 5.5280407747663265, Validation Loss Force: 3.9105792241595454, time: 0.05161476135253906
Test Loss Energy: 12.89766724787321, Test Loss Force: 11.667682271393375, time: 10.475370168685913


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.119838517967444, Training Loss Force: 3.4322658825558636, time: 0.6047549247741699
Validation Loss Energy: 5.200733162430986, Validation Loss Force: 3.9451860174813502, time: 0.047660112380981445
Test Loss Energy: 10.123299955720986, Test Loss Force: 11.667786578481461, time: 10.386835813522339


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.010895920158999, Training Loss Force: 3.434377448048612, time: 0.6512560844421387
Validation Loss Energy: 1.9198905619367568, Validation Loss Force: 3.962788458805234, time: 0.05309748649597168
Test Loss Energy: 10.80989803816625, Test Loss Force: 11.584149962807986, time: 10.245102167129517


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.067624763739901, Training Loss Force: 3.4424661394797544, time: 0.5933294296264648
Validation Loss Energy: 3.1303069017206018, Validation Loss Force: 3.911349214066164, time: 0.05608701705932617
Test Loss Energy: 11.33534965296509, Test Loss Force: 11.612021779234894, time: 10.607603788375854

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.040 MB of 0.058 MB uploadedwandb: / 0.040 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–‚â–ƒâ–„â–‚â–†â–â–â–ˆâ–‚â–ƒâ–„â–‚â–†â–â–â–ˆâ–‚â–ƒâ–„
wandb:   test_error_force â–â–‚â–â–‚â–„â–„â–ƒâ–‚â–„â–„â–ƒâ–…â–…â–ƒâ–…â–†â–ˆâ–ˆâ–…â–†
wandb:          test_loss â–„â–â–„â–„â–â–†â–‚â–â–ˆâ–â–ƒâ–…â–â–…â–‚â–â–‡â–â–„â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–‚â–â–â–â–‚â–â–â–‚â–â–â–‚â–‚â–â–‚
wandb:  train_error_force â–ˆâ–ƒâ–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–‡â–â–„â–ˆâ–…â–‚â–„â–‡â–‡â–â–ƒâ–ˆâ–†â–‚â–ƒâ–‡â–†â–â–ƒ
wandb:  valid_error_force â–ˆâ–‚â–‚â–‚â–‚â–†â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–„â–…â–ƒ
wandb:         valid_loss â–…â–‡â–â–ƒâ–ˆâ–…â–‚â–ƒâ–ˆâ–†â–â–‚â–ˆâ–…â–â–ƒâ–‡â–†â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1157
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.33535
wandb:   test_error_force 11.61202
wandb:          test_loss 11.51665
wandb: train_error_energy 4.06762
wandb:  train_error_force 3.44247
wandb:         train_loss 1.4115
wandb: valid_error_energy 3.13031
wandb:  valid_error_force 3.91135
wandb:         valid_loss 1.30786
wandb: 
wandb: ğŸš€ View run al_71_3 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t5qvq1ws
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_103910-t5qvq1ws/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.5750107765197754, Uncertainty Bias: -0.13801372051239014
9.1552734e-05 0.0595603
3.0021408 4.7165823
(48745, 22, 3)
Found uncertainty sample 0 after 15 steps.
Found uncertainty sample 1 after 9 steps.
Found uncertainty sample 2 after 47 steps.
Found uncertainty sample 3 after 26 steps.
Found uncertainty sample 4 after 13 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 22 steps.
Found uncertainty sample 9 after 80 steps.
Found uncertainty sample 10 after 52 steps.
Found uncertainty sample 11 after 84 steps.
Found uncertainty sample 12 after 25 steps.
Found uncertainty sample 13 after 9 steps.
Found uncertainty sample 14 after 55 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 89 steps.
Found uncertainty sample 17 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 799 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 8 steps.
Found uncertainty sample 24 after 4 steps.
Found uncertainty sample 25 after 12 steps.
Found uncertainty sample 26 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 62 steps.
Found uncertainty sample 29 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 26 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 15 steps.
Found uncertainty sample 35 after 3 steps.
Found uncertainty sample 36 after 11 steps.
Found uncertainty sample 37 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 40 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 25 steps.
Found uncertainty sample 42 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 60 steps.
Found uncertainty sample 45 after 20 steps.
Found uncertainty sample 46 after 5 steps.
Found uncertainty sample 47 after 15 steps.
Found uncertainty sample 48 after 44 steps.
Found uncertainty sample 49 after 17 steps.
Found uncertainty sample 50 after 8 steps.
Found uncertainty sample 51 after 28 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 18 steps.
Found uncertainty sample 57 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 15 steps.
Found uncertainty sample 60 after 10 steps.
Found uncertainty sample 61 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 207 steps.
Found uncertainty sample 64 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 7 steps.
Found uncertainty sample 67 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 299 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 48 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 56 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 15 steps.
Found uncertainty sample 80 after 11 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 19 steps.
Found uncertainty sample 84 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 5 steps.
Found uncertainty sample 87 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 59 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 65 steps.
Found uncertainty sample 92 after 34 steps.
Found uncertainty sample 93 after 27 steps.
Found uncertainty sample 94 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 3 steps.
Found uncertainty sample 97 after 8 steps.
Found uncertainty sample 98 after 8 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_104555-4jy9sdwg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_4
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/4jy9sdwg
Training model 4. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.919532528617812, Training Loss Force: 3.801933466774708, time: 0.6400222778320312
Validation Loss Energy: 2.752010949666694, Validation Loss Force: 3.9300569850304328, time: 0.05089426040649414
Test Loss Energy: 11.19082013730335, Test Loss Force: 11.435608107686381, time: 8.785287141799927


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.952010632682217, Training Loss Force: 3.4584802117290527, time: 0.6249344348907471
Validation Loss Energy: 4.457048943374312, Validation Loss Force: 3.91592996373619, time: 0.053049325942993164
Test Loss Energy: 12.29023179604074, Test Loss Force: 11.541469342472983, time: 8.827155351638794


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.973104566180506, Training Loss Force: 3.459194111848759, time: 0.5997440814971924
Validation Loss Energy: 5.484926684055204, Validation Loss Force: 3.9291119771751153, time: 0.04977560043334961
Test Loss Energy: 12.778142885069176, Test Loss Force: 11.66714336542795, time: 8.973058223724365


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.084406503840858, Training Loss Force: 3.4252143434229563, time: 0.6308245658874512
Validation Loss Energy: 3.3325755458367774, Validation Loss Force: 3.8604216630244355, time: 0.05295443534851074
Test Loss Energy: 11.201868299492972, Test Loss Force: 11.479289046426226, time: 8.907472372055054


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.067098585283453, Training Loss Force: 3.4315320037826633, time: 0.6035962104797363
Validation Loss Energy: 2.6212240001145486, Validation Loss Force: 3.9003146218705664, time: 0.05552053451538086
Test Loss Energy: 9.851214165668575, Test Loss Force: 11.477103829718445, time: 8.906057357788086


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.04501016570373, Training Loss Force: 3.4681830083439253, time: 0.6236052513122559
Validation Loss Energy: 5.060880015933244, Validation Loss Force: 3.89175477456281, time: 0.050608158111572266
Test Loss Energy: 10.043121655356162, Test Loss Force: 11.62063656170231, time: 8.885749101638794


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.01755905663653, Training Loss Force: 3.4923117624361826, time: 0.826134204864502
Validation Loss Energy: 5.984101476674909, Validation Loss Force: 3.9231631482725082, time: 0.05347466468811035
Test Loss Energy: 10.473571229562205, Test Loss Force: 11.57641006723364, time: 8.849531412124634


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.095090506320925, Training Loss Force: 3.4530564423820103, time: 0.6319365501403809
Validation Loss Energy: 3.804259160361945, Validation Loss Force: 3.9383220183889804, time: 0.05089068412780762
Test Loss Energy: 10.010734253209906, Test Loss Force: 11.500769689451175, time: 8.801072597503662


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.0928792589652305, Training Loss Force: 3.45793254269098, time: 0.6222071647644043
Validation Loss Energy: 2.020573310966903, Validation Loss Force: 3.8865768262434055, time: 0.05562424659729004
Test Loss Energy: 10.692015691271411, Test Loss Force: 11.670129668779513, time: 8.758923768997192


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 3.982731906502235, Training Loss Force: 3.462633185422426, time: 0.703514575958252
Validation Loss Energy: 4.800613184715549, Validation Loss Force: 3.9344832023131855, time: 0.05202913284301758
Test Loss Energy: 12.23531514473682, Test Loss Force: 11.778501464378044, time: 9.436680555343628


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 3.9453618204815246, Training Loss Force: 3.45719396055287, time: 0.604048490524292
Validation Loss Energy: 5.75795201199627, Validation Loss Force: 3.90594592457108, time: 0.04918932914733887
Test Loss Energy: 12.807100991175785, Test Loss Force: 11.648349418006783, time: 8.849908590316772


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.135335510796232, Training Loss Force: 3.4714445632231015, time: 0.6708693504333496
Validation Loss Energy: 3.0241411476411293, Validation Loss Force: 3.9304617975377294, time: 0.05271005630493164
Test Loss Energy: 11.363646130930075, Test Loss Force: 11.738045576946481, time: 8.824118614196777


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.139997500447731, Training Loss Force: 3.443808638764781, time: 0.6097931861877441
Validation Loss Energy: 2.1685077887778017, Validation Loss Force: 3.9192450619807633, time: 0.05039858818054199
Test Loss Energy: 9.864914189147422, Test Loss Force: 11.648193363075505, time: 9.068408250808716


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.039832468600798, Training Loss Force: 3.455954241518718, time: 0.6211178302764893
Validation Loss Energy: 5.30421508529459, Validation Loss Force: 3.99297482424208, time: 0.050772905349731445
Test Loss Energy: 10.190052038713192, Test Loss Force: 11.672121426802732, time: 8.890923023223877


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.032644177589977, Training Loss Force: 3.4810161734542313, time: 0.643038272857666
Validation Loss Energy: 5.699159341382081, Validation Loss Force: 3.895760592498051, time: 0.05116987228393555
Test Loss Energy: 10.208318630985147, Test Loss Force: 11.660067807330808, time: 8.846065998077393


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.051647967200629, Training Loss Force: 3.450029106897024, time: 0.6444535255432129
Validation Loss Energy: 3.7046082601540284, Validation Loss Force: 3.8583931471050334, time: 0.053043365478515625
Test Loss Energy: 9.955805929845836, Test Loss Force: 11.608530061222742, time: 9.173015594482422


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.068446709934879, Training Loss Force: 3.425308286964676, time: 0.6176137924194336
Validation Loss Energy: 1.9688359122414356, Validation Loss Force: 3.987271376738286, time: 0.05093502998352051
Test Loss Energy: 10.675336942186805, Test Loss Force: 11.667039524662679, time: 8.984862327575684


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 3.969742975760157, Training Loss Force: 3.4448782956660957, time: 0.6676907539367676
Validation Loss Energy: 5.023456903253744, Validation Loss Force: 3.896466899028934, time: 0.04962420463562012
Test Loss Energy: 12.444016859028606, Test Loss Force: 11.806844486023564, time: 8.958095073699951


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.041392528603424, Training Loss Force: 3.4586404764194683, time: 0.6040480136871338
Validation Loss Energy: 5.508983858581243, Validation Loss Force: 3.941764483385024, time: 0.05476880073547363
Test Loss Energy: 12.59245176767497, Test Loss Force: 11.803829653041953, time: 9.159237384796143


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 3.979897879237974, Training Loss Force: 3.4373798607917903, time: 0.6270253658294678
Validation Loss Energy: 3.30687574102123, Validation Loss Force: 3.871113344735304, time: 0.0509798526763916
Test Loss Energy: 11.665876450267653, Test Loss Force: 11.76243712358644, time: 9.03390622138977

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–‡â–ˆâ–„â–â–â–‚â–â–ƒâ–‡â–ˆâ–…â–â–‚â–‚â–â–ƒâ–‡â–‡â–…
wandb:   test_error_force â–â–ƒâ–…â–‚â–‚â–„â–„â–‚â–…â–‡â–…â–‡â–…â–…â–…â–„â–…â–ˆâ–ˆâ–‡
wandb:          test_loss â–ƒâ–†â–ˆâ–„â–‚â–â–â–â–„â–‡â–ˆâ–…â–‚â–â–â–â–„â–‡â–‡â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–…â–‡â–ƒâ–‚â–†â–ˆâ–„â–â–†â–ˆâ–ƒâ–â–‡â–ˆâ–„â–â–†â–‡â–ƒ
wandb:  valid_error_force â–…â–„â–…â–â–ƒâ–ƒâ–„â–…â–‚â–…â–ƒâ–…â–„â–ˆâ–ƒâ–â–ˆâ–ƒâ–…â–‚
wandb:         valid_loss â–‚â–…â–‡â–ƒâ–‚â–†â–ˆâ–ƒâ–â–†â–ˆâ–‚â–â–†â–‡â–ƒâ–â–†â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1247
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.66588
wandb:   test_error_force 11.76244
wandb:          test_loss 12.00604
wandb: train_error_energy 3.9799
wandb:  train_error_force 3.43738
wandb:         train_loss 1.38335
wandb: valid_error_energy 3.30688
wandb:  valid_error_force 3.87111
wandb:         valid_loss 1.33322
wandb: 
wandb: ğŸš€ View run al_71_4 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/4jy9sdwg
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_104555-4jy9sdwg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.426001787185669, Uncertainty Bias: -0.1128285825252533
0.00015258789 0.17673606
3.037028 4.6258335
(48745, 22, 3)
Found uncertainty sample 0 after 33 steps.
Found uncertainty sample 1 after 93 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 16 steps.
Found uncertainty sample 5 after 45 steps.
Found uncertainty sample 6 after 23 steps.
Found uncertainty sample 7 after 17 steps.
Found uncertainty sample 8 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 10 steps.
Found uncertainty sample 11 after 43 steps.
Found uncertainty sample 12 after 56 steps.
Found uncertainty sample 13 after 44 steps.
Found uncertainty sample 14 after 109 steps.
Found uncertainty sample 15 after 17 steps.
Found uncertainty sample 16 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 97 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 114 steps.
Found uncertainty sample 21 after 203 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 131 steps.
Found uncertainty sample 26 after 132 steps.
Found uncertainty sample 27 after 88 steps.
Found uncertainty sample 28 after 19 steps.
Found uncertainty sample 29 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 41 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 152 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 11 steps.
Found uncertainty sample 36 after 8 steps.
Found uncertainty sample 37 after 36 steps.
Found uncertainty sample 38 after 117 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 35 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 59 steps.
Found uncertainty sample 44 after 26 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 36 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 5 steps.
Found uncertainty sample 49 after 24 steps.
Found uncertainty sample 50 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 5 steps.
Found uncertainty sample 56 after 36 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 4 steps.
Found uncertainty sample 59 after 29 steps.
Found uncertainty sample 60 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 14 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 10 steps.
Found uncertainty sample 71 after 32 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 12 steps.
Found uncertainty sample 74 after 248 steps.
Found uncertainty sample 75 after 26 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 48 steps.
Found uncertainty sample 78 after 17 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 31 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 30 steps.
Found uncertainty sample 83 after 32 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 61 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 24 steps.
Found uncertainty sample 89 after 17 steps.
Found uncertainty sample 90 after 15 steps.
Found uncertainty sample 91 after 24 steps.
Found uncertainty sample 92 after 38 steps.
Found uncertainty sample 93 after 25 steps.
Found uncertainty sample 94 after 7 steps.
Found uncertainty sample 95 after 5 steps.
Found uncertainty sample 96 after 24 steps.
Found uncertainty sample 97 after 9 steps.
Found uncertainty sample 98 after 5 steps.
Found uncertainty sample 99 after 9 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_105217-yt95j0vf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_5
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/yt95j0vf
Training model 5. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.7100749169191785, Training Loss Force: 3.7452450876953014, time: 0.7006800174713135
Validation Loss Energy: 4.838126701068296, Validation Loss Force: 4.1231391908622115, time: 0.05698680877685547
Test Loss Energy: 10.190217233571717, Test Loss Force: 11.50641213493544, time: 8.922878503799438


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.009047046083624, Training Loss Force: 3.4683744173518067, time: 0.6750695705413818
Validation Loss Energy: 4.133557546196454, Validation Loss Force: 4.228867622739082, time: 0.059804439544677734
Test Loss Energy: 11.747144579702372, Test Loss Force: 11.708887357728685, time: 8.930155038833618


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.096751820835562, Training Loss Force: 3.458212311791357, time: 0.7071189880371094
Validation Loss Energy: 4.522406123079598, Validation Loss Force: 3.4779220514991303, time: 0.05707359313964844
Test Loss Energy: 12.452211251474967, Test Loss Force: 11.760936799865704, time: 9.096237659454346


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.031100383853933, Training Loss Force: 3.453678433976548, time: 0.6749975681304932
Validation Loss Energy: 4.213498998287146, Validation Loss Force: 4.071855369186999, time: 0.06135249137878418
Test Loss Energy: 10.034266469486026, Test Loss Force: 11.709603666884028, time: 8.96094822883606


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.086303781583511, Training Loss Force: 3.458291385846393, time: 0.683283805847168
Validation Loss Energy: 5.124580759861974, Validation Loss Force: 3.7344221001097377, time: 0.05950665473937988
Test Loss Energy: 10.289534322418902, Test Loss Force: 11.667421153931045, time: 9.003366470336914


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.087367368711504, Training Loss Force: 3.453769007302477, time: 0.7023582458496094
Validation Loss Energy: 2.990084792448058, Validation Loss Force: 3.864909782329756, time: 0.062291622161865234
Test Loss Energy: 11.738768941566072, Test Loss Force: 11.797146057926835, time: 9.148232460021973


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.0491120831155625, Training Loss Force: 3.45481014460908, time: 0.6882116794586182
Validation Loss Energy: 5.2248362068010845, Validation Loss Force: 4.345436701985446, time: 0.057290077209472656
Test Loss Energy: 12.502342171587447, Test Loss Force: 11.840728975610194, time: 9.074296474456787


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 3.9851178851099087, Training Loss Force: 3.46865131703806, time: 0.6670923233032227
Validation Loss Energy: 3.6797780896879018, Validation Loss Force: 3.9706157246228875, time: 0.0565953254699707
Test Loss Energy: 10.266466796831617, Test Loss Force: 11.793398836979092, time: 9.089834928512573


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.07990615771564, Training Loss Force: 3.458811095632989, time: 0.7031183242797852
Validation Loss Energy: 5.3512982535318745, Validation Loss Force: 3.6172754170988366, time: 0.06380152702331543
Test Loss Energy: 10.362283638622296, Test Loss Force: 11.798835069849702, time: 9.501336097717285


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.123855464488197, Training Loss Force: 3.4676171354914453, time: 0.7050588130950928
Validation Loss Energy: 3.48822762491096, Validation Loss Force: 4.026205299572906, time: 0.060602426528930664
Test Loss Energy: 11.477003365130386, Test Loss Force: 11.751089223078642, time: 9.037761449813843


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.130701538714321, Training Loss Force: 3.456765025962583, time: 0.6928386688232422
Validation Loss Energy: 4.483000586993904, Validation Loss Force: 3.587503216273416, time: 0.05927729606628418
Test Loss Energy: 12.226948506630919, Test Loss Force: 11.90001564530237, time: 9.039119005203247


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.0195031093774, Training Loss Force: 3.5091132538701464, time: 0.6423726081848145
Validation Loss Energy: 4.306612332370623, Validation Loss Force: 3.609178610280037, time: 0.057274818420410156
Test Loss Energy: 10.06411976618123, Test Loss Force: 11.911768822308666, time: 9.065332174301147


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.070200303886018, Training Loss Force: 3.4846954719639247, time: 0.8299789428710938
Validation Loss Energy: 5.38595425673493, Validation Loss Force: 3.935217119818163, time: 0.0831460952758789
Test Loss Energy: 10.278720151693246, Test Loss Force: 11.916207282897021, time: 9.036499261856079


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.087005812336628, Training Loss Force: 3.480803718949508, time: 0.6618144512176514
Validation Loss Energy: 2.25936453806916, Validation Loss Force: 3.624871329673871, time: 0.05977296829223633
Test Loss Energy: 11.77909015963082, Test Loss Force: 11.91747468303305, time: 9.144834041595459


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 3.953685415417633, Training Loss Force: 3.4781171255759022, time: 0.6543996334075928
Validation Loss Energy: 4.669043998453759, Validation Loss Force: 3.6913571079452927, time: 0.059209585189819336
Test Loss Energy: 12.600004513912484, Test Loss Force: 11.98744212714536, time: 9.023764848709106


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 3.9715139454493285, Training Loss Force: 3.4475245851086513, time: 0.6912329196929932
Validation Loss Energy: 4.069068887494859, Validation Loss Force: 3.7520445816567545, time: 0.057318687438964844
Test Loss Energy: 10.091641835830893, Test Loss Force: 11.877656720554228, time: 9.278140306472778


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 3.9795582023496885, Training Loss Force: 3.455202503549143, time: 0.6504502296447754
Validation Loss Energy: 4.787465907663389, Validation Loss Force: 3.9789048062491954, time: 0.056330204010009766
Test Loss Energy: 10.342711193234685, Test Loss Force: 11.887303242258014, time: 9.053434133529663


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 3.9501412330350716, Training Loss Force: 3.469569705327147, time: 0.7084934711456299
Validation Loss Energy: 3.223215380189825, Validation Loss Force: 4.307728127460867, time: 0.056044816970825195
Test Loss Energy: 11.616192509903355, Test Loss Force: 12.195246650323659, time: 8.991050243377686


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.0046546692094065, Training Loss Force: 3.4628158055241456, time: 0.6893882751464844
Validation Loss Energy: 4.519954535782934, Validation Loss Force: 3.6907987650618788, time: 0.06442475318908691
Test Loss Energy: 12.240352265896593, Test Loss Force: 12.189126900118318, time: 9.15939450263977


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 3.913902118475048, Training Loss Force: 3.457382058070887, time: 0.6834301948547363
Validation Loss Energy: 3.47952337666207, Validation Loss Force: 3.809786253902165, time: 0.06573987007141113
Test Loss Energy: 10.210662210922159, Test Loss Force: 12.018369388608168, time: 9.349128723144531

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–†â–ˆâ–â–‚â–†â–ˆâ–‚â–‚â–…â–‡â–â–‚â–†â–ˆâ–â–‚â–…â–‡â–
wandb:   test_error_force â–â–ƒâ–„â–ƒâ–ƒâ–„â–„â–„â–„â–ƒâ–…â–…â–…â–…â–†â–…â–…â–ˆâ–ˆâ–†
wandb:          test_loss â–â–†â–‡â–‚â–‚â–†â–ˆâ–ƒâ–‚â–†â–‡â–‚â–ƒâ–‡â–ˆâ–‚â–‚â–†â–‡â–ƒ
wandb: train_error_energy â–ˆâ–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–…â–†â–…â–‡â–ƒâ–ˆâ–„â–ˆâ–„â–†â–†â–ˆâ–â–†â–…â–‡â–ƒâ–†â–„
wandb:  valid_error_force â–†â–‡â–â–†â–ƒâ–„â–ˆâ–…â–‚â–…â–‚â–‚â–…â–‚â–ƒâ–ƒâ–…â–ˆâ–ƒâ–„
wandb:         valid_loss â–†â–†â–„â–…â–†â–‚â–ˆâ–„â–†â–„â–„â–„â–‡â–â–…â–„â–†â–„â–…â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1337
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.21066
wandb:   test_error_force 12.01837
wandb:          test_loss 9.69991
wandb: train_error_energy 3.9139
wandb:  train_error_force 3.45738
wandb:         train_loss 1.37582
wandb: valid_error_energy 3.47952
wandb:  valid_error_force 3.80979
wandb:         valid_loss 1.39132
wandb: 
wandb: ğŸš€ View run al_71_5 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/yt95j0vf
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_105217-yt95j0vf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.6282026767730713, Uncertainty Bias: -0.1518615186214447
/home/ws/fq0795/git/gnn_uncertainty/uncertainty/base_uncertainty.py:925: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(10, 8))
0.00022888184 0.08466816
3.002661 4.6349425
(48745, 22, 3)
Found uncertainty sample 0 after 30 steps.
Found uncertainty sample 1 after 113 steps.
Found uncertainty sample 2 after 3 steps.
Found uncertainty sample 3 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 27 steps.
Found uncertainty sample 6 after 96 steps.
Found uncertainty sample 7 after 44 steps.
Found uncertainty sample 8 after 10 steps.
Found uncertainty sample 9 after 19 steps.
Found uncertainty sample 10 after 21 steps.
Found uncertainty sample 11 after 31 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 134 steps.
Found uncertainty sample 14 after 12 steps.
Found uncertainty sample 15 after 21 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 137 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 7 steps.
Found uncertainty sample 23 after 48 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 150 steps.
Found uncertainty sample 26 after 37 steps.
Found uncertainty sample 27 after 24 steps.
Found uncertainty sample 28 after 88 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 6 steps.
Found uncertainty sample 31 after 20 steps.
Found uncertainty sample 32 after 85 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 27 steps.
Found uncertainty sample 35 after 114 steps.
Found uncertainty sample 36 after 37 steps.
Found uncertainty sample 37 after 260 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 13 steps.
Found uncertainty sample 40 after 8 steps.
Found uncertainty sample 41 after 281 steps.
Found uncertainty sample 42 after 11 steps.
Found uncertainty sample 43 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 33 steps.
Found uncertainty sample 46 after 136 steps.
Found uncertainty sample 47 after 4 steps.
Found uncertainty sample 48 after 109 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1097 steps.
Found uncertainty sample 51 after 5 steps.
Found uncertainty sample 52 after 749 steps.
Found uncertainty sample 53 after 7 steps.
Found uncertainty sample 54 after 384 steps.
Found uncertainty sample 55 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 381 steps.
Found uncertainty sample 58 after 15 steps.
Found uncertainty sample 59 after 152 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 50 steps.
Found uncertainty sample 62 after 219 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 82 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 45 steps.
Found uncertainty sample 67 after 208 steps.
Found uncertainty sample 68 after 9 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 63 steps.
Found uncertainty sample 71 after 17 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 208 steps.
Found uncertainty sample 74 after 35 steps.
Found uncertainty sample 75 after 82 steps.
Found uncertainty sample 76 after 34 steps.
Found uncertainty sample 77 after 4 steps.
Found uncertainty sample 78 after 41 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 27 steps.
Found uncertainty sample 81 after 47 steps.
Found uncertainty sample 82 after 3 steps.
Found uncertainty sample 83 after 30 steps.
Found uncertainty sample 84 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 38 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 245 steps.
Found uncertainty sample 89 after 65 steps.
Found uncertainty sample 90 after 16 steps.
Found uncertainty sample 91 after 216 steps.
Found uncertainty sample 92 after 71 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 10 steps.
Found uncertainty sample 96 after 92 steps.
Found uncertainty sample 97 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 7 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_105907-hjlupb0i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_6
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hjlupb0i
Training model 6. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.725419095720374, Training Loss Force: 4.064200058380238, time: 0.7708632946014404
Validation Loss Energy: 1.307521185309937, Validation Loss Force: 3.9783993472879393, time: 0.06359124183654785
Test Loss Energy: 10.455640413035852, Test Loss Force: 11.893727711109232, time: 8.999913454055786


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.8576389908694526, Training Loss Force: 4.084214303446713, time: 0.7056636810302734
Validation Loss Energy: 1.2287668381034726, Validation Loss Force: 4.005779190809512, time: 0.06343269348144531
Test Loss Energy: 10.811908766201732, Test Loss Force: 13.830206759520005, time: 9.002760410308838


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.379119407875905, Training Loss Force: 3.7772725950859107, time: 0.7349607944488525
Validation Loss Energy: 2.715162212049115, Validation Loss Force: 4.777015821975851, time: 0.060326576232910156
Test Loss Energy: 11.409910143680161, Test Loss Force: 13.524044415501628, time: 9.220794677734375


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.613218388681515, Training Loss Force: 3.843406489306022, time: 0.7178950309753418
Validation Loss Energy: 2.3482910798690435, Validation Loss Force: 3.825922418900114, time: 0.06201291084289551
Test Loss Energy: 10.27655994854146, Test Loss Force: 12.476812713331757, time: 9.080029726028442


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.4609830315760046, Training Loss Force: 3.4513425625333753, time: 0.7160871028900146
Validation Loss Energy: 2.332417553128716, Validation Loss Force: 3.792366623947088, time: 0.05899333953857422
Test Loss Energy: 11.269611980653309, Test Loss Force: 12.479350619313776, time: 9.076875448226929


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6215989823785955, Training Loss Force: 3.4264493346392233, time: 0.7578966617584229
Validation Loss Energy: 2.2807480684927564, Validation Loss Force: 3.815642007101774, time: 0.060518741607666016
Test Loss Energy: 10.178744367874966, Test Loss Force: 12.160669032489007, time: 9.218560457229614


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5619163724629783, Training Loss Force: 3.4250909192475776, time: 0.7138047218322754
Validation Loss Energy: 2.078956381687158, Validation Loss Force: 3.907749989991387, time: 0.061855316162109375
Test Loss Energy: 10.908912851899128, Test Loss Force: 12.24429545434567, time: 9.048117637634277


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5741545730376694, Training Loss Force: 3.4256255627115593, time: 0.7514703273773193
Validation Loss Energy: 2.4831573281745856, Validation Loss Force: 3.6377037525536897, time: 0.058187007904052734
Test Loss Energy: 10.084332239637504, Test Loss Force: 12.071614925459004, time: 9.519231796264648


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.528518291591912, Training Loss Force: 3.4206997006063604, time: 0.7082452774047852
Validation Loss Energy: 2.3076868044362464, Validation Loss Force: 3.85871267732944, time: 0.06147289276123047
Test Loss Energy: 11.097707571486715, Test Loss Force: 12.28281599948862, time: 9.22124719619751


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5235780475339884, Training Loss Force: 3.421930422665123, time: 0.7292914390563965
Validation Loss Energy: 2.7838618783039237, Validation Loss Force: 3.91122474201359, time: 0.06224513053894043
Test Loss Energy: 10.118470320236165, Test Loss Force: 12.173133333140905, time: 9.074134588241577


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6160973547663655, Training Loss Force: 3.415094232474978, time: 0.7357292175292969
Validation Loss Energy: 2.092565029209681, Validation Loss Force: 3.657468199949959, time: 0.05817413330078125
Test Loss Energy: 10.900282741400664, Test Loss Force: 12.269337488675108, time: 9.121934652328491


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5251539438804627, Training Loss Force: 3.418854708597033, time: 0.714977502822876
Validation Loss Energy: 2.670459599650944, Validation Loss Force: 3.7405236800464507, time: 0.06043219566345215
Test Loss Energy: 10.057812605498281, Test Loss Force: 12.295894764392255, time: 9.310671329498291


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.646222501032691, Training Loss Force: 3.4136614505367935, time: 0.7275876998901367
Validation Loss Energy: 1.7221397077895964, Validation Loss Force: 3.7050179713825355, time: 0.05934429168701172
Test Loss Energy: 10.907281109081627, Test Loss Force: 12.35397621809628, time: 9.116403818130493


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5438180048913863, Training Loss Force: 3.4264768367528475, time: 0.7231714725494385
Validation Loss Energy: 2.5304712929816247, Validation Loss Force: 3.77518435930157, time: 0.058760643005371094
Test Loss Energy: 10.01545336086024, Test Loss Force: 12.194938145136494, time: 9.06094741821289


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.583401067664824, Training Loss Force: 3.418810822982472, time: 0.7079238891601562
Validation Loss Energy: 2.1624950538303134, Validation Loss Force: 3.87976070101741, time: 0.05840587615966797
Test Loss Energy: 11.162001760166135, Test Loss Force: 12.446877716878715, time: 9.243537187576294


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6260475803846153, Training Loss Force: 3.414935231860176, time: 0.7797040939331055
Validation Loss Energy: 2.387827656747117, Validation Loss Force: 3.894799329795081, time: 0.06475543975830078
Test Loss Energy: 9.965006945381546, Test Loss Force: 12.239741009945842, time: 9.052932977676392


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.573140389301017, Training Loss Force: 3.4430759259569768, time: 0.7127690315246582
Validation Loss Energy: 1.8663373519935347, Validation Loss Force: 3.775891222505318, time: 0.06159400939941406
Test Loss Energy: 10.992805600043999, Test Loss Force: 12.423581455285632, time: 9.075994968414307


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.534653242211294, Training Loss Force: 3.4280608159893196, time: 0.7375435829162598
Validation Loss Energy: 2.7255429362976957, Validation Loss Force: 4.050433929341308, time: 0.06234169006347656
Test Loss Energy: 10.055136546717995, Test Loss Force: 12.303271823507826, time: 9.180458784103394


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6325260278508114, Training Loss Force: 3.4199480833783835, time: 0.8671133518218994
Validation Loss Energy: 2.335991838259149, Validation Loss Force: 3.8592989651906677, time: 0.08677124977111816
Test Loss Energy: 10.978820881821111, Test Loss Force: 12.34090150087471, time: 9.516719102859497


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5612658099157275, Training Loss Force: 3.426376581206943, time: 0.7389788627624512
Validation Loss Energy: 2.6550038002283776, Validation Loss Force: 3.8474460638837855, time: 0.05849790573120117
Test Loss Energy: 10.073576699679842, Test Loss Force: 12.299966191616472, time: 9.127373456954956

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–…â–ˆâ–ƒâ–‡â–‚â–†â–‚â–†â–‚â–†â–â–†â–â–‡â–â–†â–â–†â–‚
wandb:   test_error_force â–â–ˆâ–‡â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚
wandb:          test_loss â–â–†â–ˆâ–…â–‡â–„â–…â–ƒâ–…â–ƒâ–…â–ƒâ–…â–ƒâ–…â–‚â–…â–ƒâ–…â–ƒ
wandb: train_error_energy â–ˆâ–â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:  train_error_force â–ˆâ–ˆâ–…â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–â–ˆâ–†â–†â–†â–…â–‡â–†â–ˆâ–…â–‡â–ƒâ–‡â–…â–†â–„â–ˆâ–†â–‡
wandb:  valid_error_force â–ƒâ–ƒâ–ˆâ–‚â–‚â–‚â–ƒâ–â–‚â–ƒâ–â–‚â–â–‚â–‚â–ƒâ–‚â–„â–‚â–‚
wandb:         valid_loss â–ƒâ–â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–ƒâ–„â–‚â–„â–ƒâ–„â–ƒâ–…â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1427
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.07358
wandb:   test_error_force 12.29997
wandb:          test_loss 12.74585
wandb: train_error_energy 2.56127
wandb:  train_error_force 3.42638
wandb:         train_loss 0.96447
wandb: valid_error_energy 2.655
wandb:  valid_error_force 3.84745
wandb:         valid_loss 1.10894
wandb: 
wandb: ğŸš€ View run al_71_6 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hjlupb0i
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_105907-hjlupb0i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.3687384128570557, Uncertainty Bias: -0.02415105700492859
0.000120162964 0.052257538
3.0239239 4.6628885
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 94 steps.
Found uncertainty sample 4 after 124 steps.
Found uncertainty sample 5 after 15 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 12 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 21 steps.
Found uncertainty sample 10 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 34 steps.
Found uncertainty sample 14 after 55 steps.
Found uncertainty sample 15 after 76 steps.
Found uncertainty sample 16 after 13 steps.
Found uncertainty sample 17 after 17 steps.
Found uncertainty sample 18 after 16 steps.
Found uncertainty sample 19 after 5 steps.
Found uncertainty sample 20 after 7 steps.
Found uncertainty sample 21 after 30 steps.
Found uncertainty sample 22 after 196 steps.
Found uncertainty sample 23 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 37 steps.
Found uncertainty sample 29 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 429 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 27 steps.
Found uncertainty sample 34 after 50 steps.
Found uncertainty sample 35 after 20 steps.
Found uncertainty sample 36 after 24 steps.
Found uncertainty sample 37 after 30 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 71 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 291 steps.
Found uncertainty sample 42 after 9 steps.
Found uncertainty sample 43 after 15 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 11 steps.
Found uncertainty sample 46 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 83 steps.
Found uncertainty sample 50 after 27 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 10 steps.
Found uncertainty sample 53 after 9 steps.
Found uncertainty sample 54 after 9 steps.
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 21 steps.
Found uncertainty sample 58 after 38 steps.
Found uncertainty sample 59 after 4 steps.
Found uncertainty sample 60 after 80 steps.
Found uncertainty sample 61 after 17 steps.
Found uncertainty sample 62 after 45 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 75 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 14 steps.
Found uncertainty sample 69 after 21 steps.
Found uncertainty sample 70 after 195 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 98 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 20 steps.
Found uncertainty sample 75 after 137 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 148 steps.
Found uncertainty sample 78 after 55 steps.
Found uncertainty sample 79 after 5 steps.
Found uncertainty sample 80 after 40 steps.
Found uncertainty sample 81 after 16 steps.
Found uncertainty sample 82 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 62 steps.
Found uncertainty sample 85 after 9 steps.
Found uncertainty sample 86 after 11 steps.
Found uncertainty sample 87 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 7 steps.
Found uncertainty sample 92 after 14 steps.
Found uncertainty sample 93 after 53 steps.
Found uncertainty sample 94 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 55 steps.
Found uncertainty sample 97 after 311 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_110539-6dtlwbun
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_7
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/6dtlwbun
Training model 7. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.636767486452965, Training Loss Force: 3.7971383031318666, time: 0.7703168392181396
Validation Loss Energy: 3.855619357554211, Validation Loss Force: 3.9223561952876658, time: 0.06382870674133301
Test Loss Energy: 11.899853546827359, Test Loss Force: 11.916531260675312, time: 9.253746747970581


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.022786541882493, Training Loss Force: 3.486016896046081, time: 0.7745001316070557
Validation Loss Energy: 3.558054553717028, Validation Loss Force: 3.9097655911003253, time: 0.060369253158569336
Test Loss Energy: 11.613338371765064, Test Loss Force: 11.856860429581864, time: 9.152008056640625


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.984579494641111, Training Loss Force: 3.494326556012625, time: 0.742743968963623
Validation Loss Energy: 3.2832415265378816, Validation Loss Force: 3.9328913405889474, time: 0.06670427322387695
Test Loss Energy: 11.606033536727653, Test Loss Force: 11.719205864603891, time: 9.40484356880188


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.0291119669365605, Training Loss Force: 3.505376016012653, time: 0.7806832790374756
Validation Loss Energy: 2.9693135465084763, Validation Loss Force: 3.820211289310785, time: 0.06313848495483398
Test Loss Energy: 11.451656295055548, Test Loss Force: 11.807761481867166, time: 9.219552040100098


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.113296975706766, Training Loss Force: 3.47263352939301, time: 0.76546311378479
Validation Loss Energy: 2.812913633107695, Validation Loss Force: 3.891251096954721, time: 0.06438374519348145
Test Loss Energy: 11.382452389273656, Test Loss Force: 11.70148371354922, time: 9.238306283950806


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.014350978750218, Training Loss Force: 3.4881334312232926, time: 0.8047077655792236
Validation Loss Energy: 3.173096229482388, Validation Loss Force: 3.989611852131133, time: 0.06051135063171387
Test Loss Energy: 11.558725282626432, Test Loss Force: 11.814758030889132, time: 9.497795581817627


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.089986478091661, Training Loss Force: 3.4847013760098213, time: 0.8091263771057129
Validation Loss Energy: 3.530315099524782, Validation Loss Force: 3.981215443976488, time: 0.06452035903930664
Test Loss Energy: 11.574586493198213, Test Loss Force: 11.851329216079542, time: 9.233048915863037


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.076094850557248, Training Loss Force: 3.485548608915373, time: 0.7950572967529297
Validation Loss Energy: 3.358382677180662, Validation Loss Force: 3.7869017664331555, time: 0.06441807746887207
Test Loss Energy: 11.530777133817327, Test Loss Force: 11.707304257044402, time: 9.581783056259155


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.045938190576151, Training Loss Force: 3.5212101998982597, time: 0.7911198139190674
Validation Loss Energy: 3.48573940853979, Validation Loss Force: 3.9567790638949347, time: 0.059699058532714844
Test Loss Energy: 11.650134016781022, Test Loss Force: 11.794209030907522, time: 9.494755983352661


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.094945118177559, Training Loss Force: 3.4748271637619985, time: 0.7591986656188965
Validation Loss Energy: 3.164092713901371, Validation Loss Force: 3.8563158666426736, time: 0.06197190284729004
Test Loss Energy: 11.278501306017004, Test Loss Force: 11.770044238401386, time: 9.249420642852783


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.108059294069496, Training Loss Force: 3.488552786809182, time: 0.7590689659118652
Validation Loss Energy: 3.364728747839604, Validation Loss Force: 4.13288875941666, time: 0.06016278266906738
Test Loss Energy: 11.28032066449415, Test Loss Force: 11.77521595431632, time: 9.265726566314697


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.078924843192184, Training Loss Force: 3.4898495608867193, time: 0.7761855125427246
Validation Loss Energy: 3.2690418037674935, Validation Loss Force: 3.7542335306957977, time: 0.061583757400512695
Test Loss Energy: 11.438153652036174, Test Loss Force: 11.78520358183552, time: 9.498141050338745


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.124411188358324, Training Loss Force: 3.4881435580304916, time: 0.7584147453308105
Validation Loss Energy: 2.7014575941246974, Validation Loss Force: 4.010147736783469, time: 0.06096601486206055
Test Loss Energy: 11.354803181370704, Test Loss Force: 11.874667693877402, time: 9.168391227722168


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.041090780687054, Training Loss Force: 3.510588199713245, time: 0.7528555393218994
Validation Loss Energy: 3.546256429020044, Validation Loss Force: 3.8364958632944948, time: 0.06159830093383789
Test Loss Energy: 11.739779239279239, Test Loss Force: 11.79413534723894, time: 9.160563230514526


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.104843399568375, Training Loss Force: 3.4807177646829452, time: 0.7926149368286133
Validation Loss Energy: 3.0280535558696537, Validation Loss Force: 3.7213754695115364, time: 0.06100869178771973
Test Loss Energy: 11.662282066623739, Test Loss Force: 11.867260911345372, time: 9.38414478302002


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.082420347514894, Training Loss Force: 3.475180497946121, time: 0.7974972724914551
Validation Loss Energy: 2.930101509910593, Validation Loss Force: 3.992662419430344, time: 0.0607607364654541
Test Loss Energy: 11.40207277264703, Test Loss Force: 11.788142422850596, time: 9.266082763671875


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.014324119284491, Training Loss Force: 3.4999435568594532, time: 0.7524406909942627
Validation Loss Energy: 3.2612494970057506, Validation Loss Force: 3.904104228065111, time: 0.06039261817932129
Test Loss Energy: 11.707474310986699, Test Loss Force: 11.880790661710197, time: 9.355360269546509


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.045473756085561, Training Loss Force: 3.494040815831113, time: 0.7907943725585938
Validation Loss Energy: 3.4116900047194934, Validation Loss Force: 3.9259975038310233, time: 0.0633094310760498
Test Loss Energy: 11.570762254373731, Test Loss Force: 11.901331081351948, time: 9.469703674316406


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.136961598445355, Training Loss Force: 3.506922699094971, time: 0.7825396060943604
Validation Loss Energy: 3.1494993120809824, Validation Loss Force: 3.8920714100883194, time: 0.062220096588134766
Test Loss Energy: 11.611571092220311, Test Loss Force: 11.840389525240777, time: 10.084399938583374


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.0337331310565725, Training Loss Force: 3.514491381089035, time: 0.80393385887146
Validation Loss Energy: 3.376891604597618, Validation Loss Force: 3.9749208098902553, time: 0.06800031661987305
Test Loss Energy: 11.445790947263946, Test Loss Force: 11.825053818516613, time: 10.554734706878662

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–…â–…â–ƒâ–‚â–„â–„â–„â–…â–â–â–ƒâ–‚â–†â–…â–‚â–†â–„â–…â–ƒ
wandb:   test_error_force â–ˆâ–†â–‚â–„â–â–…â–†â–â–„â–ƒâ–ƒâ–„â–‡â–„â–†â–„â–‡â–ˆâ–†â–…
wandb:          test_loss â–ˆâ–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: train_error_energy â–ˆâ–â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–ƒâ–‚
wandb:  train_error_force â–ˆâ–â–â–‚â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–‚â–â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–†â–…â–ƒâ–‚â–„â–†â–…â–†â–„â–…â–„â–â–†â–ƒâ–‚â–„â–…â–„â–…
wandb:  valid_error_force â–„â–„â–…â–ƒâ–„â–†â–…â–‚â–…â–ƒâ–ˆâ–‚â–†â–ƒâ–â–†â–„â–„â–„â–…
wandb:         valid_loss â–ˆâ–„â–„â–‚â–â–„â–…â–ƒâ–„â–‚â–…â–‚â–‚â–„â–â–‚â–ƒâ–„â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1517
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.44579
wandb:   test_error_force 11.82505
wandb:          test_loss 11.46056
wandb: train_error_energy 4.03373
wandb:  train_error_force 3.51449
wandb:         train_loss 1.42782
wandb: valid_error_energy 3.37689
wandb:  valid_error_force 3.97492
wandb:         valid_loss 1.39967
wandb: 
wandb: ğŸš€ View run al_71_7 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/6dtlwbun
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_110539-6dtlwbun/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.86444354057312, Uncertainty Bias: -0.17880713939666748
3.8146973e-05 0.04794693
2.9062514 4.797232
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 62 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 20 steps.
Found uncertainty sample 5 after 211 steps.
Found uncertainty sample 6 after 22 steps.
Found uncertainty sample 7 after 40 steps.
Found uncertainty sample 8 after 50 steps.
Found uncertainty sample 9 after 285 steps.
Found uncertainty sample 10 after 14 steps.
Found uncertainty sample 11 after 5 steps.
Found uncertainty sample 12 after 235 steps.
Found uncertainty sample 13 after 7 steps.
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 65 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 10 steps.
Found uncertainty sample 21 after 96 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 25 steps.
Found uncertainty sample 24 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 12 steps.
Found uncertainty sample 28 after 10 steps.
Found uncertainty sample 29 after 15 steps.
Found uncertainty sample 30 after 265 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 341 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 47 steps.
Found uncertainty sample 35 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 35 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 5 steps.
Found uncertainty sample 44 after 53 steps.
Found uncertainty sample 45 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 15 steps.
Found uncertainty sample 49 after 45 steps.
Found uncertainty sample 50 after 53 steps.
Found uncertainty sample 51 after 164 steps.
Found uncertainty sample 52 after 55 steps.
Found uncertainty sample 53 after 274 steps.
Found uncertainty sample 54 after 121 steps.
Found uncertainty sample 55 after 54 steps.
Found uncertainty sample 56 after 77 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 85 steps.
Found uncertainty sample 61 after 135 steps.
Found uncertainty sample 62 after 49 steps.
Found uncertainty sample 63 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 22 steps.
Found uncertainty sample 66 after 87 steps.
Found uncertainty sample 67 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 20 steps.
Found uncertainty sample 70 after 278 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 79 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 15 steps.
Found uncertainty sample 82 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 20 steps.
Found uncertainty sample 87 after 60 steps.
Found uncertainty sample 88 after 7 steps.
Found uncertainty sample 89 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 13 steps.
Found uncertainty sample 94 after 17 steps.
Found uncertainty sample 95 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 48 steps.
Found uncertainty sample 99 after 72 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_111227-nawavc16
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_8
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/nawavc16
Training model 8. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.481773977108937, Training Loss Force: 3.730798382428909, time: 0.9677627086639404
Validation Loss Energy: 2.1879434651801084, Validation Loss Force: 3.7674530638858115, time: 0.07525038719177246
Test Loss Energy: 10.035335761698981, Test Loss Force: 11.589019660086159, time: 10.809908628463745


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.056201702031905, Training Loss Force: 3.5038717544043037, time: 0.911247730255127
Validation Loss Energy: 4.817428713277724, Validation Loss Force: 3.9915924941841605, time: 0.06905961036682129
Test Loss Energy: 12.693998234000867, Test Loss Force: 11.876110422066805, time: 10.68205189704895


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.005841293670916, Training Loss Force: 3.4925200071218554, time: 0.9362990856170654
Validation Loss Energy: 5.701854083386099, Validation Loss Force: 3.939931916314038, time: 0.0652620792388916
Test Loss Energy: 10.606967427824404, Test Loss Force: 11.706686537859389, time: 10.75657343864441


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.112979963471997, Training Loss Force: 3.4745162636042703, time: 0.9141597747802734
Validation Loss Energy: 3.123301062382276, Validation Loss Force: 3.916879870312718, time: 0.07137751579284668
Test Loss Energy: 11.575227388627317, Test Loss Force: 12.003642573578059, time: 10.545128583908081


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.05524991149976, Training Loss Force: 3.5278411754565377, time: 0.9310760498046875
Validation Loss Energy: 2.0095321500315007, Validation Loss Force: 3.757286213351895, time: 0.07265043258666992
Test Loss Energy: 11.050070569124772, Test Loss Force: 11.992492762340417, time: 10.95286750793457


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.076515633572431, Training Loss Force: 3.4700036768892675, time: 0.9114742279052734
Validation Loss Energy: 5.268680206972558, Validation Loss Force: 3.7436969380450678, time: 0.07460260391235352
Test Loss Energy: 10.437318104513519, Test Loss Force: 11.764935200936515, time: 10.568045616149902


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.041493802426863, Training Loss Force: 3.4719500933847485, time: 0.933448314666748
Validation Loss Energy: 4.817750470193589, Validation Loss Force: 3.772678329398726, time: 0.0681452751159668
Test Loss Energy: 12.812355875765435, Test Loss Force: 12.182488953343924, time: 9.770532369613647


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.044959946758455, Training Loss Force: 3.504969129080299, time: 0.852226734161377
Validation Loss Energy: 3.334069655805953, Validation Loss Force: 3.9331229165007127, time: 0.05674004554748535
Test Loss Energy: 10.22750099267169, Test Loss Force: 11.901173398954032, time: 10.876332521438599


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.176289091314873, Training Loss Force: 3.4725965019153766, time: 0.9002737998962402
Validation Loss Energy: 2.29687877809663, Validation Loss Force: 3.8354272740631874, time: 0.06941342353820801
Test Loss Energy: 10.143642743088943, Test Loss Force: 12.008016439129573, time: 8.487471342086792


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.077174411155812, Training Loss Force: 3.5034521453139558, time: 0.8356237411499023
Validation Loss Energy: 5.28932787153615, Validation Loss Force: 3.9248451910008466, time: 0.05666184425354004
Test Loss Energy: 12.725095693966045, Test Loss Force: 12.193691832118335, time: 8.285556316375732


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.013200877876042, Training Loss Force: 3.4800332584438127, time: 0.9699337482452393
Validation Loss Energy: 5.872857752722553, Validation Loss Force: 3.820250590415041, time: 0.06665778160095215
Test Loss Energy: 10.536937519770522, Test Loss Force: 11.869871587003246, time: 10.05589771270752


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.09842854570077, Training Loss Force: 3.5118747192762787, time: 0.937938928604126
Validation Loss Energy: 3.210990287405525, Validation Loss Force: 3.8114275481767983, time: 0.07123064994812012
Test Loss Energy: 11.998238708174297, Test Loss Force: 12.19486665318516, time: 9.631509065628052


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.106402423869802, Training Loss Force: 3.566134256489177, time: 0.8275573253631592
Validation Loss Energy: 1.8733325588801426, Validation Loss Force: 3.8060657864774305, time: 0.05916404724121094
Test Loss Energy: 11.118830307085085, Test Loss Force: 12.146356871709289, time: 8.666538953781128


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 3.9960434342912037, Training Loss Force: 3.49890772564321, time: 0.8340263366699219
Validation Loss Energy: 4.872576735311367, Validation Loss Force: 3.9991817266848173, time: 0.06177544593811035
Test Loss Energy: 10.502397548177024, Test Loss Force: 11.980311095285709, time: 8.858924150466919


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.105657388188742, Training Loss Force: 3.4845093667134375, time: 0.8472387790679932
Validation Loss Energy: 5.598747358033382, Validation Loss Force: 3.9530034858565832, time: 0.05928754806518555
Test Loss Energy: 13.002502096809398, Test Loss Force: 12.256522429325447, time: 8.658900737762451


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.0890886076026876, Training Loss Force: 3.5034970022317067, time: 0.8606946468353271
Validation Loss Energy: 3.8082888038958007, Validation Loss Force: 3.8459795648887054, time: 0.059749603271484375
Test Loss Energy: 10.227655452778924, Test Loss Force: 12.014928639680097, time: 8.717658758163452


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.087039953223764, Training Loss Force: 3.5036696625070967, time: 0.804882287979126
Validation Loss Energy: 2.4188185258082004, Validation Loss Force: 3.828274250419403, time: 0.0647127628326416
Test Loss Energy: 10.289944555928965, Test Loss Force: 12.06095134902442, time: 8.947632551193237


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.00162952423994, Training Loss Force: 3.4715808162535042, time: 0.8219106197357178
Validation Loss Energy: 4.767773271066681, Validation Loss Force: 3.9239582997926163, time: 0.06036639213562012
Test Loss Energy: 12.684863599518406, Test Loss Force: 12.405544643582587, time: 8.698297262191772


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.019116472317105, Training Loss Force: 3.4597519108216415, time: 0.8109095096588135
Validation Loss Energy: 5.908187775430332, Validation Loss Force: 3.9054587244797903, time: 0.06074380874633789
Test Loss Energy: 10.68961200232307, Test Loss Force: 12.090231861401419, time: 9.069414377212524


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.1088029937356785, Training Loss Force: 3.490814562302886, time: 0.8217597007751465
Validation Loss Energy: 3.2978769014922773, Validation Loss Force: 3.933663111848733, time: 0.06961464881896973
Test Loss Energy: 11.648532048675499, Test Loss Force: 12.51142281521396, time: 8.709639549255371

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‡â–‚â–…â–ƒâ–‚â–ˆâ–â–â–‡â–‚â–†â–„â–‚â–ˆâ–â–‚â–‡â–ƒâ–…
wandb:   test_error_force â–â–ƒâ–‚â–„â–„â–‚â–†â–ƒâ–„â–†â–ƒâ–†â–…â–„â–†â–„â–…â–‡â–…â–ˆ
wandb:          test_loss â–â–‡â–â–†â–…â–â–ˆâ–‚â–‚â–‡â–â–†â–…â–‚â–ˆâ–‚â–‚â–ˆâ–â–…
wandb: train_error_energy â–ˆâ–â–â–‚â–â–â–â–â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–â–ƒâ–â–â–‚â–â–‚â–‚â–‚â–„â–‚â–‚â–‚â–‚â–â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–†â–ˆâ–ƒâ–â–‡â–†â–„â–‚â–‡â–ˆâ–ƒâ–â–†â–‡â–„â–‚â–†â–ˆâ–ƒ
wandb:  valid_error_force â–‚â–ˆâ–†â–†â–â–â–‚â–†â–„â–†â–ƒâ–ƒâ–ƒâ–ˆâ–‡â–„â–ƒâ–†â–…â–†
wandb:         valid_loss â–‚â–†â–ˆâ–ƒâ–â–†â–†â–ƒâ–‚â–‡â–ˆâ–ƒâ–â–†â–ˆâ–„â–‚â–†â–ˆâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1607
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.64853
wandb:   test_error_force 12.51142
wandb:          test_loss 11.18457
wandb: train_error_energy 4.1088
wandb:  train_error_force 3.49081
wandb:         train_loss 1.4368
wandb: valid_error_energy 3.29788
wandb:  valid_error_force 3.93366
wandb:         valid_loss 1.36463
wandb: 
wandb: ğŸš€ View run al_71_8 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/nawavc16
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_111227-nawavc16/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1348655223846436, Uncertainty Bias: -0.23185542225837708
4.5776367e-05 0.004675865
2.8080637 4.795289
(48745, 22, 3)
Found uncertainty sample 0 after 54 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 11 steps.
Found uncertainty sample 4 after 15 steps.
Found uncertainty sample 5 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 98 steps.
Found uncertainty sample 9 after 39 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 16 steps.
Found uncertainty sample 12 after 7 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 129 steps.
Found uncertainty sample 18 after 90 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 94 steps.
Found uncertainty sample 21 after 27 steps.
Found uncertainty sample 22 after 15 steps.
Found uncertainty sample 23 after 33 steps.
Found uncertainty sample 24 after 30 steps.
Found uncertainty sample 25 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 17 steps.
Found uncertainty sample 28 after 6 steps.
Found uncertainty sample 29 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 125 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 387 steps.
Found uncertainty sample 34 after 60 steps.
Found uncertainty sample 35 after 15 steps.
Found uncertainty sample 36 after 62 steps.
Found uncertainty sample 37 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 79 steps.
Found uncertainty sample 40 after 48 steps.
Found uncertainty sample 41 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 63 steps.
Found uncertainty sample 46 after 148 steps.
Found uncertainty sample 47 after 11 steps.
Found uncertainty sample 48 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 9 steps.
Found uncertainty sample 51 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 5 steps.
Found uncertainty sample 55 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 7 steps.
Found uncertainty sample 59 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 8 steps.
Found uncertainty sample 66 after 51 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 12 steps.
Found uncertainty sample 69 after 141 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 25 steps.
Found uncertainty sample 74 after 180 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 12 steps.
Found uncertainty sample 78 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 12 steps.
Found uncertainty sample 84 after 4 steps.
Found uncertainty sample 85 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 19 steps.
Found uncertainty sample 88 after 7 steps.
Found uncertainty sample 89 after 3 steps.
Found uncertainty sample 90 after 41 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 26 steps.
Found uncertainty sample 94 after 39 steps.
Found uncertainty sample 95 after 97 steps.
Found uncertainty sample 96 after 31 steps.
Found uncertainty sample 97 after 105 steps.
Found uncertainty sample 98 after 9 steps.
Found uncertainty sample 99 after 3 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_111906-34wl12rb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_9
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/34wl12rb
Training model 9. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.9592741809478773, Training Loss Force: 3.8221695572381496, time: 1.0221827030181885
Validation Loss Energy: 2.143054590041369, Validation Loss Force: 3.8940329025947973, time: 0.07541418075561523
Test Loss Energy: 10.586535153107539, Test Loss Force: 13.024571649595622, time: 9.422030210494995


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7234550804106352, Training Loss Force: 3.5381887794888534, time: 0.8948304653167725
Validation Loss Energy: 1.7110791189712118, Validation Loss Force: 3.853884542238288, time: 0.06495165824890137
Test Loss Energy: 11.45582750137328, Test Loss Force: 14.6272531311064, time: 9.43367886543274


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7323728415677027, Training Loss Force: 3.545112899250624, time: 0.8966522216796875
Validation Loss Energy: 6.589185953856269, Validation Loss Force: 3.8886950778286167, time: 0.06810808181762695
Test Loss Energy: 11.262690864010493, Test Loss Force: 13.297842869558982, time: 9.594335556030273


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.257724946143431, Training Loss Force: 3.570888737332501, time: 0.8815503120422363
Validation Loss Energy: 5.528023851911236, Validation Loss Force: 3.8950264523239273, time: 0.06904864311218262
Test Loss Energy: 12.839121532497344, Test Loss Force: 12.504154404651919, time: 9.505297422409058


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.036819212022808, Training Loss Force: 3.5118969716373325, time: 0.9012811183929443
Validation Loss Energy: 3.4258023298501046, Validation Loss Force: 3.855872532373131, time: 0.06349563598632812
Test Loss Energy: 11.448577361339275, Test Loss Force: 11.949458230965394, time: 9.450565814971924


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.13454222420952, Training Loss Force: 3.4980471363182457, time: 0.867943525314331
Validation Loss Energy: 5.536106073900977, Validation Loss Force: 3.8957412202823556, time: 0.06339192390441895
Test Loss Energy: 10.480363948880125, Test Loss Force: 11.760862468008403, time: 9.68237853050232


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 3.9576199677392774, Training Loss Force: 3.4916680858846907, time: 0.904557466506958
Validation Loss Energy: 3.6552639980744197, Validation Loss Force: 3.7445034456916186, time: 0.06852054595947266
Test Loss Energy: 10.170385522023365, Test Loss Force: 11.862419482556463, time: 9.561993837356567


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.125638670581013, Training Loss Force: 3.4785377316876205, time: 0.8850719928741455
Validation Loss Energy: 4.779603905458971, Validation Loss Force: 3.878830166656578, time: 0.06507325172424316
Test Loss Energy: 12.500547262415681, Test Loss Force: 12.147906812177931, time: 9.508207559585571


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.03913906577864, Training Loss Force: 3.5006282964403277, time: 0.888561487197876
Validation Loss Energy: 3.4969396665799577, Validation Loss Force: 3.8224171612049185, time: 0.06479549407958984
Test Loss Energy: 11.543528544765467, Test Loss Force: 12.003491340424729, time: 10.144771099090576


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.0335098619410035, Training Loss Force: 3.5215002053278797, time: 0.8824937343597412
Validation Loss Energy: 4.776468501215161, Validation Loss Force: 3.8687236594612076, time: 0.06702232360839844
Test Loss Energy: 10.430431108711344, Test Loss Force: 11.899567546379426, time: 9.505399703979492


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 3.9855572614059276, Training Loss Force: 3.5440080644305625, time: 0.8943278789520264
Validation Loss Energy: 4.092124273533368, Validation Loss Force: 3.9166701159772543, time: 0.07176494598388672
Test Loss Energy: 10.224639118237835, Test Loss Force: 11.980856720962953, time: 9.470482587814331


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.06562627611447, Training Loss Force: 3.5373047788560172, time: 0.8750948905944824
Validation Loss Energy: 4.800343532379831, Validation Loss Force: 3.896186335905065, time: 0.07480955123901367
Test Loss Energy: 12.68152733021786, Test Loss Force: 12.246002461666105, time: 9.657937049865723


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 3.9968198338189347, Training Loss Force: 3.4975449019361027, time: 0.8438661098480225
Validation Loss Energy: 3.130670186432183, Validation Loss Force: 3.857741171845757, time: 0.06630110740661621
Test Loss Energy: 11.614661912505232, Test Loss Force: 12.127295292296015, time: 9.460377931594849


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.063882488056665, Training Loss Force: 3.528714293923183, time: 0.8619427680969238
Validation Loss Energy: 5.608627410707344, Validation Loss Force: 3.9457549371491956, time: 0.06519412994384766
Test Loss Energy: 10.545714256152872, Test Loss Force: 11.97826866662501, time: 9.437174797058105


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.013128262265777, Training Loss Force: 3.5664687795268497, time: 0.8443949222564697
Validation Loss Energy: 3.3075987477380573, Validation Loss Force: 3.839069695952274, time: 0.06366610527038574
Test Loss Energy: 10.081211961740125, Test Loss Force: 11.98670425988611, time: 9.639854431152344


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 3.982903841022491, Training Loss Force: 3.5395203409815017, time: 0.843536376953125
Validation Loss Energy: 4.665168441968886, Validation Loss Force: 3.9568349538505627, time: 0.06324219703674316
Test Loss Energy: 12.60849374828742, Test Loss Force: 12.251415061296266, time: 9.510984659194946


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.1004717357109355, Training Loss Force: 3.6187508269832573, time: 0.8453490734100342
Validation Loss Energy: 2.333632435988898, Validation Loss Force: 4.0313367100408115, time: 0.0670311450958252
Test Loss Energy: 10.200712280939623, Test Loss Force: 12.177053056225745, time: 9.471961259841919


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.25328562296505, Training Loss Force: 4.696729269820496, time: 0.941889762878418
Validation Loss Energy: 6.139966761456318, Validation Loss Force: 4.477023479109764, time: 0.09423351287841797
Test Loss Energy: 10.950108131465559, Test Loss Force: 14.133452921149061, time: 9.589515924453735


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 3.830074596440273, Training Loss Force: 4.975646856330594, time: 0.9029674530029297
Validation Loss Energy: 5.870006696978095, Validation Loss Force: 4.2447454915277545, time: 0.06401658058166504
Test Loss Energy: 13.485053304002333, Test Loss Force: 12.793080958325838, time: 9.55406904220581


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.054378762733332, Training Loss Force: 3.5937834964467417, time: 0.8480069637298584
Validation Loss Energy: 2.5219042089266264, Validation Loss Force: 3.8444694227410228, time: 0.06373095512390137
Test Loss Energy: 10.030430320776754, Test Loss Force: 11.82873326410589, time: 10.02118182182312

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.039 MB of 0.048 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–ƒâ–‡â–„â–‚â–â–†â–„â–‚â–â–†â–„â–‚â–â–†â–â–ƒâ–ˆâ–
wandb:   test_error_force â–„â–ˆâ–…â–ƒâ–â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‡â–„â–
wandb:          test_loss â–„â–‡â–†â–†â–ƒâ–â–â–„â–ƒâ–â–â–„â–ƒâ–â–â–„â–â–†â–ˆâ–‚
wandb: train_error_energy â–‡â–â–â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‚â–‡â–‡
wandb:  train_error_force â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‡â–ˆâ–‚
wandb:         train_loss â–†â–â–â–†â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–…
wandb: valid_error_energy â–‚â–â–ˆâ–†â–ƒâ–†â–„â–…â–„â–…â–„â–…â–ƒâ–‡â–ƒâ–…â–‚â–‡â–‡â–‚
wandb:  valid_error_force â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–„â–ˆâ–†â–‚
wandb:         valid_loss â–â–â–ˆâ–„â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–ˆâ–…â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 1697
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.03043
wandb:   test_error_force 11.82873
wandb:          test_loss 10.25818
wandb: train_error_energy 4.05438
wandb:  train_error_force 3.59378
wandb:         train_loss 1.54944
wandb: valid_error_energy 2.5219
wandb:  valid_error_force 3.84447
wandb:         valid_loss 1.10389
wandb: 
wandb: ğŸš€ View run al_71_9 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/34wl12rb
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_111906-34wl12rb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.165679931640625, Uncertainty Bias: -0.20225608348846436
0.00015449524 0.034893036
2.6918156 4.7637978
(48745, 22, 3)
Found uncertainty sample 0 after 18 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 99 steps.
Found uncertainty sample 5 after 151 steps.
Found uncertainty sample 6 after 29 steps.
Found uncertainty sample 7 after 36 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 47 steps.
Found uncertainty sample 10 after 95 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 34 steps.
Found uncertainty sample 13 after 89 steps.
Found uncertainty sample 14 after 23 steps.
Found uncertainty sample 15 after 603 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 22 steps.
Found uncertainty sample 19 after 18 steps.
Found uncertainty sample 20 after 10 steps.
Found uncertainty sample 21 after 50 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 58 steps.
Found uncertainty sample 24 after 5 steps.
Found uncertainty sample 25 after 4 steps.
Found uncertainty sample 26 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 11 steps.
Found uncertainty sample 30 after 3 steps.
Found uncertainty sample 31 after 44 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 17 steps.
Found uncertainty sample 34 after 251 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 84 steps.
Found uncertainty sample 40 after 3 steps.
Found uncertainty sample 41 after 407 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 190 steps.
Found uncertainty sample 45 after 61 steps.
Found uncertainty sample 46 after 116 steps.
Found uncertainty sample 47 after 56 steps.
Found uncertainty sample 48 after 49 steps.
Found uncertainty sample 49 after 7 steps.
Found uncertainty sample 50 after 30 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 341 steps.
Found uncertainty sample 53 after 209 steps.
Found uncertainty sample 54 after 10 steps.
Found uncertainty sample 55 after 36 steps.
Found uncertainty sample 56 after 117 steps.
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 10 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 5 steps.
Found uncertainty sample 63 after 5 steps.
Found uncertainty sample 64 after 80 steps.
Found uncertainty sample 65 after 17 steps.
Found uncertainty sample 66 after 15 steps.
Found uncertainty sample 67 after 59 steps.
Found uncertainty sample 68 after 13 steps.
Found uncertainty sample 69 after 21 steps.
Found uncertainty sample 70 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 9 steps.
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 296 steps.
Found uncertainty sample 77 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 36 steps.
Found uncertainty sample 80 after 33 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 54 steps.
Found uncertainty sample 83 after 49 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 25 steps.
Found uncertainty sample 86 after 25 steps.
Found uncertainty sample 87 after 16 steps.
Found uncertainty sample 88 after 52 steps.
Found uncertainty sample 89 after 141 steps.
Found uncertainty sample 90 after 142 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 130 steps.
Found uncertainty sample 95 after 40 steps.
Found uncertainty sample 96 after 6 steps.
Found uncertainty sample 97 after 96 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_112600-24rpvpnh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_10
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/24rpvpnh
Training model 10. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.452798134702825, Training Loss Force: 3.7806253275971327, time: 1.012899398803711
Validation Loss Energy: 2.112668490380625, Validation Loss Force: 3.9532428551884555, time: 0.07824492454528809
Test Loss Energy: 10.494418484955428, Test Loss Force: 13.264197183887816, time: 10.510764360427856


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7009153042925256, Training Loss Force: 3.5648851524375087, time: 1.0132229328155518
Validation Loss Energy: 2.1758062153738766, Validation Loss Force: 3.8204089081777957, time: 0.07921957969665527
Test Loss Energy: 10.457275625295122, Test Loss Force: 13.094491412199723, time: 10.667251825332642


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7592321369062194, Training Loss Force: 3.497736086518951, time: 1.0093650817871094
Validation Loss Energy: 1.8286015004985794, Validation Loss Force: 3.8757384055653703, time: 0.07216358184814453
Test Loss Energy: 10.372812680923689, Test Loss Force: 12.863561805208624, time: 10.812450408935547


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6729681494900634, Training Loss Force: 3.4838622798059755, time: 1.0282979011535645
Validation Loss Energy: 2.092471508168391, Validation Loss Force: 3.7920718099978794, time: 0.06944775581359863
Test Loss Energy: 10.4409576468592, Test Loss Force: 12.92352570154071, time: 10.663487672805786


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7033630505464232, Training Loss Force: 3.497498327562152, time: 0.9413690567016602
Validation Loss Energy: 2.031243059766898, Validation Loss Force: 3.908123406149108, time: 0.07159638404846191
Test Loss Energy: 10.394189133441252, Test Loss Force: 12.72646134831997, time: 10.845998525619507


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7227040527993636, Training Loss Force: 3.495816288810048, time: 0.9950962066650391
Validation Loss Energy: 1.9888794754121255, Validation Loss Force: 3.815758017726802, time: 0.07650232315063477
Test Loss Energy: 10.476541146329923, Test Loss Force: 12.73752972094055, time: 10.589049577713013


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6476867638571224, Training Loss Force: 3.5046263645467755, time: 0.9497561454772949
Validation Loss Energy: 2.0100386110039135, Validation Loss Force: 3.8431106956125713, time: 0.07360959053039551
Test Loss Energy: 10.368791737807822, Test Loss Force: 12.726963675923498, time: 10.64608383178711


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6875546247095194, Training Loss Force: 3.5130685073534993, time: 0.9713809490203857
Validation Loss Energy: 2.11792982829518, Validation Loss Force: 3.7663402748290373, time: 0.06858444213867188
Test Loss Energy: 10.175430337509647, Test Loss Force: 12.529061349924014, time: 10.761914014816284


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.708045907936679, Training Loss Force: 3.5065857416963664, time: 0.9700803756713867
Validation Loss Energy: 2.052149799881777, Validation Loss Force: 3.800683539216637, time: 0.07971596717834473
Test Loss Energy: 10.074058523069032, Test Loss Force: 12.481124268828236, time: 10.662768125534058


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.7331131176611154, Training Loss Force: 3.4868854110854204, time: 0.9947450160980225
Validation Loss Energy: 2.207762577851703, Validation Loss Force: 3.857605789005033, time: 0.07104754447937012
Test Loss Energy: 10.21844962479173, Test Loss Force: 12.72211257559933, time: 10.624867916107178


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.7218471483654905, Training Loss Force: 3.493913378170123, time: 0.9875614643096924
Validation Loss Energy: 2.131023108079453, Validation Loss Force: 3.775132922728861, time: 0.07682514190673828
Test Loss Energy: 10.12701086877407, Test Loss Force: 12.60698445717888, time: 10.836608409881592


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.704097439235219, Training Loss Force: 3.49418091912519, time: 0.9880580902099609
Validation Loss Energy: 1.8844895077714274, Validation Loss Force: 3.8274305570254827, time: 0.06821465492248535
Test Loss Energy: 10.197534323572802, Test Loss Force: 12.601446757126151, time: 11.00193452835083


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.7156547353221776, Training Loss Force: 3.5190218850686583, time: 0.9655680656433105
Validation Loss Energy: 1.7988255699649645, Validation Loss Force: 3.772501752766005, time: 0.06970047950744629
Test Loss Energy: 10.08135333216584, Test Loss Force: 12.594614479643525, time: 10.819064855575562


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6989139639495134, Training Loss Force: 3.5339170433441267, time: 0.9571032524108887
Validation Loss Energy: 2.2368910421762465, Validation Loss Force: 3.801169848631343, time: 0.0757286548614502
Test Loss Energy: 10.035427645947177, Test Loss Force: 12.41915660666035, time: 10.57843542098999


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.7007323159812724, Training Loss Force: 3.5296494818040824, time: 0.900611400604248
Validation Loss Energy: 2.206150453235186, Validation Loss Force: 3.8408425876921513, time: 0.07133698463439941
Test Loss Energy: 10.078623329161887, Test Loss Force: 12.55877266531061, time: 10.524456262588501


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6938526266665843, Training Loss Force: 3.4918901904052837, time: 0.9267065525054932
Validation Loss Energy: 1.8312179742978039, Validation Loss Force: 3.8365726841498398, time: 0.07542943954467773
Test Loss Energy: 10.081564427179408, Test Loss Force: 12.581825650952409, time: 10.609130859375


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.677469652801548, Training Loss Force: 3.489722864130804, time: 0.9149014949798584
Validation Loss Energy: 2.0583888780161, Validation Loss Force: 3.8533993461693106, time: 0.06685376167297363
Test Loss Energy: 9.98770628578733, Test Loss Force: 12.535289594796353, time: 10.274555206298828


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.7276261284245713, Training Loss Force: 3.511932592515427, time: 0.9777266979217529
Validation Loss Energy: 2.165437737928322, Validation Loss Force: 3.8119440292607782, time: 0.07364678382873535
Test Loss Energy: 10.135295741836938, Test Loss Force: 12.663761979017266, time: 10.241037607192993


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.7024085762702277, Training Loss Force: 3.5197561445298255, time: 1.0791904926300049
Validation Loss Energy: 2.0063267163412974, Validation Loss Force: 3.8061035288976655, time: 0.07284760475158691
Test Loss Energy: 9.991544864038488, Test Loss Force: 12.590401803237523, time: 9.210588455200195


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.7164205004793753, Training Loss Force: 3.5162775872509675, time: 0.8963792324066162
Validation Loss Energy: 2.177247441420894, Validation Loss Force: 3.86479911576423, time: 0.06280136108398438
Test Loss Energy: 9.998199593771126, Test Loss Force: 12.588873322944991, time: 9.12184739112854

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‡â–†â–‡â–‡â–ˆâ–†â–„â–‚â–„â–ƒâ–„â–‚â–‚â–‚â–‚â–â–ƒâ–â–
wandb:   test_error_force â–ˆâ–‡â–…â–…â–„â–„â–„â–‚â–‚â–„â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚
wandb:          test_loss â–â–‚â–„â–…â–†â–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–ƒâ–â–â–â–â–â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–‡â–â–†â–…â–„â–„â–†â–…â–ˆâ–†â–‚â–â–ˆâ–ˆâ–‚â–…â–‡â–„â–‡
wandb:  valid_error_force â–ˆâ–ƒâ–…â–‚â–†â–ƒâ–„â–â–‚â–„â–â–ƒâ–â–‚â–„â–„â–„â–ƒâ–‚â–…
wandb:         valid_loss â–‡â–…â–ƒâ–…â–…â–„â–…â–†â–„â–ˆâ–…â–ƒâ–â–‡â–‡â–‚â–†â–†â–„â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 1787
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.9982
wandb:   test_error_force 12.58887
wandb:          test_loss 16.43206
wandb: train_error_energy 1.71642
wandb:  train_error_force 3.51628
wandb:         train_loss 0.63918
wandb: valid_error_energy 2.17725
wandb:  valid_error_force 3.8648
wandb:         valid_loss 0.98107
wandb: 
wandb: ğŸš€ View run al_71_10 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/24rpvpnh
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_112600-24rpvpnh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.5672857761383057, Uncertainty Bias: -0.05572667717933655
1.835823e-05 0.03168297
2.8621016 5.1167674
(48745, 22, 3)
Found uncertainty sample 0 after 16 steps.
Found uncertainty sample 1 after 42 steps.
Found uncertainty sample 2 after 63 steps.
Found uncertainty sample 3 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 14 steps.
Found uncertainty sample 6 after 5 steps.
Found uncertainty sample 7 after 131 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 17 steps.
Found uncertainty sample 11 after 87 steps.
Found uncertainty sample 12 after 30 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 9 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 85 steps.
Found uncertainty sample 25 after 81 steps.
Found uncertainty sample 26 after 53 steps.
Found uncertainty sample 27 after 247 steps.
Found uncertainty sample 28 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 90 steps.
Found uncertainty sample 31 after 49 steps.
Found uncertainty sample 32 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 105 steps.
Found uncertainty sample 35 after 9 steps.
Found uncertainty sample 36 after 90 steps.
Found uncertainty sample 37 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 9 steps.
Found uncertainty sample 40 after 17 steps.
Found uncertainty sample 41 after 53 steps.
Found uncertainty sample 42 after 84 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 55 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 9 steps.
Found uncertainty sample 54 after 57 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 21 steps.
Found uncertainty sample 57 after 161 steps.
Found uncertainty sample 58 after 18 steps.
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 18 steps.
Found uncertainty sample 63 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 36 steps.
Found uncertainty sample 66 after 24 steps.
Found uncertainty sample 67 after 55 steps.
Found uncertainty sample 68 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 3 steps.
Found uncertainty sample 72 after 15 steps.
Found uncertainty sample 73 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 13 steps.
Found uncertainty sample 76 after 13 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 9 steps.
Found uncertainty sample 82 after 18 steps.
Found uncertainty sample 83 after 64 steps.
Found uncertainty sample 84 after 13 steps.
Found uncertainty sample 85 after 104 steps.
Found uncertainty sample 86 after 11 steps.
Found uncertainty sample 87 after 19 steps.
Found uncertainty sample 88 after 53 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 132 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 89 steps.
Found uncertainty sample 93 after 48 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 49 steps.
Found uncertainty sample 99 after 42 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_113301-9ehg5xck
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_11
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/9ehg5xck
Training model 11. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.958711330618604, Training Loss Force: 3.7503108608162945, time: 0.9372706413269043
Validation Loss Energy: 1.9422694234426507, Validation Loss Force: 3.8460098478925513, time: 0.07266521453857422
Test Loss Energy: 10.80038533245852, Test Loss Force: 12.356426091656587, time: 9.661051511764526


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.698947820847067, Training Loss Force: 3.497410000426957, time: 0.9421408176422119
Validation Loss Energy: 2.1123118857434218, Validation Loss Force: 3.8452051215991685, time: 0.07731342315673828
Test Loss Energy: 10.087430698290191, Test Loss Force: 12.403256994404448, time: 10.005659103393555


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7033127918490159, Training Loss Force: 3.5349209843899256, time: 0.9350640773773193
Validation Loss Energy: 2.084044928344473, Validation Loss Force: 3.7605870607711283, time: 0.06994986534118652
Test Loss Energy: 10.09689489709832, Test Loss Force: 12.492342177187089, time: 9.712358236312866


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6950647634312186, Training Loss Force: 3.526572570871361, time: 0.9830784797668457
Validation Loss Energy: 1.618053850534949, Validation Loss Force: 3.760529834800625, time: 0.06847500801086426
Test Loss Energy: 10.498679714893312, Test Loss Force: 12.490919612796375, time: 9.553977727890015


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6994371230860565, Training Loss Force: 3.5060551790802963, time: 0.9650712013244629
Validation Loss Energy: 1.581906799063475, Validation Loss Force: 3.8622526714114, time: 0.0660560131072998
Test Loss Energy: 10.584881768248025, Test Loss Force: 12.637836388853557, time: 9.561973094940186


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.728706184105987, Training Loss Force: 3.4991145063206606, time: 1.0104920864105225
Validation Loss Energy: 2.080850071438701, Validation Loss Force: 3.8648249453851578, time: 0.06994318962097168
Test Loss Energy: 10.099232904477647, Test Loss Force: 12.580794998248209, time: 9.768650770187378


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.729053206421809, Training Loss Force: 3.5475809443551425, time: 0.9563910961151123
Validation Loss Energy: 2.1283666651685342, Validation Loss Force: 3.87288957651237, time: 0.0683293342590332
Test Loss Energy: 10.131574905434276, Test Loss Force: 12.58259143229559, time: 9.798132419586182


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7534762452248727, Training Loss Force: 3.5074395509010627, time: 0.9886837005615234
Validation Loss Energy: 1.3520738300383255, Validation Loss Force: 3.749544786489996, time: 0.0723724365234375
Test Loss Energy: 10.612852174645264, Test Loss Force: 12.639522794031864, time: 9.68501353263855


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6860844092154312, Training Loss Force: 3.5013105977556145, time: 0.9330196380615234
Validation Loss Energy: 1.573171154465009, Validation Loss Force: 3.7503979433226187, time: 0.0744326114654541
Test Loss Energy: 10.778938530480529, Test Loss Force: 12.734313310594098, time: 9.786806583404541


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.7309333548700363, Training Loss Force: 3.5216378565144293, time: 0.9624700546264648
Validation Loss Energy: 1.8363215511224689, Validation Loss Force: 3.875175943302757, time: 0.06711244583129883
Test Loss Energy: 10.1843937895486, Test Loss Force: 12.636185265659709, time: 9.58812952041626


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6675988318663717, Training Loss Force: 3.521006219628498, time: 0.9869060516357422
Validation Loss Energy: 1.9816845899689066, Validation Loss Force: 3.802565316168873, time: 0.06914138793945312
Test Loss Energy: 10.066908544513254, Test Loss Force: 12.707717903660068, time: 9.59616732597351


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.7071226411220604, Training Loss Force: 3.5233459892733743, time: 1.1511435508728027
Validation Loss Energy: 1.517149712706464, Validation Loss Force: 3.834312814987819, time: 0.07052469253540039
Test Loss Energy: 10.652912014106912, Test Loss Force: 12.746819939368248, time: 9.612800359725952


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.711074390082401, Training Loss Force: 3.527383901758055, time: 0.9631671905517578
Validation Loss Energy: 1.469458045360645, Validation Loss Force: 3.8094275339080754, time: 0.06814789772033691
Test Loss Energy: 10.441794570411602, Test Loss Force: 12.559372538321355, time: 9.583211898803711


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6977096294335754, Training Loss Force: 3.511519600691071, time: 0.9487652778625488
Validation Loss Energy: 1.916000253620966, Validation Loss Force: 3.7837341864977594, time: 0.06625223159790039
Test Loss Energy: 9.88531937050852, Test Loss Force: 12.487942626574728, time: 10.23130464553833


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6694660334246825, Training Loss Force: 3.5058235503317854, time: 0.9473090171813965
Validation Loss Energy: 2.248459149645487, Validation Loss Force: 3.898413027364343, time: 0.06792283058166504
Test Loss Energy: 10.007521004532165, Test Loss Force: 12.604888387404348, time: 9.659028053283691


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7110748373077194, Training Loss Force: 3.506989717711703, time: 0.9467349052429199
Validation Loss Energy: 1.476253072685064, Validation Loss Force: 3.8409274246367495, time: 0.0684502124786377
Test Loss Energy: 10.600902955525989, Test Loss Force: 12.66313425720779, time: 9.528675079345703


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6922237674258858, Training Loss Force: 3.505616501992552, time: 0.9340972900390625
Validation Loss Energy: 1.5666251825803106, Validation Loss Force: 3.7735997343245753, time: 0.07142162322998047
Test Loss Energy: 10.67144063783392, Test Loss Force: 12.633396893028543, time: 9.788074254989624


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.696723495719221, Training Loss Force: 3.506615859989743, time: 0.931302547454834
Validation Loss Energy: 1.9677895899247209, Validation Loss Force: 3.828502040955687, time: 0.07386016845703125
Test Loss Energy: 10.07470278068566, Test Loss Force: 12.691284002034646, time: 9.650838375091553


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.7182283629005952, Training Loss Force: 3.5205313040807673, time: 0.932199239730835
Validation Loss Energy: 1.935569253174397, Validation Loss Force: 3.8114377909575112, time: 0.06961441040039062
Test Loss Energy: 9.904918284379319, Test Loss Force: 12.642045110842055, time: 9.573115587234497


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6708628775481602, Training Loss Force: 3.530473885270148, time: 0.9826235771179199
Validation Loss Energy: 1.4270476448179412, Validation Loss Force: 3.7842861833105146, time: 0.06675052642822266
Test Loss Energy: 10.482040155578387, Test Loss Force: 12.659689824329453, time: 9.737840175628662

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–ƒâ–ƒâ–†â–†â–ƒâ–ƒâ–‡â–ˆâ–ƒâ–‚â–‡â–…â–â–‚â–†â–‡â–‚â–â–†
wandb:   test_error_force â–â–‚â–ƒâ–ƒâ–†â–…â–…â–†â–ˆâ–†â–‡â–ˆâ–…â–ƒâ–…â–†â–†â–‡â–†â–†
wandb:          test_loss â–†â–‚â–‚â–…â–†â–‚â–‚â–†â–‡â–‚â–‚â–‡â–†â–‚â–ƒâ–‡â–ˆâ–‚â–â–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–‚â–‚â–â–â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–‡â–‡â–ƒâ–ƒâ–‡â–‡â–â–ƒâ–…â–†â–‚â–‚â–…â–ˆâ–‚â–ƒâ–†â–†â–‚
wandb:  valid_error_force â–†â–…â–‚â–‚â–†â–†â–‡â–â–â–‡â–ƒâ–…â–„â–ƒâ–ˆâ–…â–‚â–…â–„â–ƒ
wandb:         valid_loss â–†â–†â–…â–ƒâ–ƒâ–†â–†â–â–ƒâ–„â–…â–ƒâ–‚â–„â–ˆâ–‚â–ƒâ–…â–…â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1877
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.48204
wandb:   test_error_force 12.65969
wandb:          test_loss 18.85868
wandb: train_error_energy 1.67086
wandb:  train_error_force 3.53047
wandb:         train_loss 0.61646
wandb: valid_error_energy 1.42705
wandb:  valid_error_force 3.78429
wandb:         valid_loss 0.65358
wandb: 
wandb: ğŸš€ View run al_71_11 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/9ehg5xck
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_113301-9ehg5xck/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.514925718307495, Uncertainty Bias: -0.027008086442947388
7.6293945e-05 0.003235817
2.8623445 5.228971
(48745, 22, 3)
Found uncertainty sample 0 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 11 steps.
Found uncertainty sample 4 after 29 steps.
Found uncertainty sample 5 after 133 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 5 steps.
Found uncertainty sample 8 after 6 steps.
Found uncertainty sample 9 after 635 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 235 steps.
Found uncertainty sample 12 after 50 steps.
Found uncertainty sample 13 after 55 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 59 steps.
Found uncertainty sample 18 after 109 steps.
Found uncertainty sample 19 after 21 steps.
Found uncertainty sample 20 after 172 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 54 steps.
Found uncertainty sample 25 after 71 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 33 steps.
Found uncertainty sample 28 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 136 steps.
Found uncertainty sample 33 after 12 steps.
Found uncertainty sample 34 after 29 steps.
Found uncertainty sample 35 after 53 steps.
Found uncertainty sample 36 after 28 steps.
Found uncertainty sample 37 after 8 steps.
Found uncertainty sample 38 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 26 steps.
Found uncertainty sample 42 after 7 steps.
Found uncertainty sample 43 after 40 steps.
Found uncertainty sample 44 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 8 steps.
Found uncertainty sample 47 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 42 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 86 steps.
Found uncertainty sample 54 after 479 steps.
Found uncertainty sample 55 after 101 steps.
Found uncertainty sample 56 after 517 steps.
Found uncertainty sample 57 after 11 steps.
Found uncertainty sample 58 after 8 steps.
Found uncertainty sample 59 after 181 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 28 steps.
Found uncertainty sample 62 after 93 steps.
Found uncertainty sample 63 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 8 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 21 steps.
Found uncertainty sample 68 after 20 steps.
Found uncertainty sample 69 after 54 steps.
Found uncertainty sample 70 after 21 steps.
Found uncertainty sample 71 after 103 steps.
Found uncertainty sample 72 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 47 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 51 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 5 steps.
Found uncertainty sample 80 after 9 steps.
Found uncertainty sample 81 after 83 steps.
Found uncertainty sample 82 after 30 steps.
Found uncertainty sample 83 after 15 steps.
Found uncertainty sample 84 after 6 steps.
Found uncertainty sample 85 after 82 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 12 steps.
Found uncertainty sample 88 after 14 steps.
Found uncertainty sample 89 after 57 steps.
Found uncertainty sample 90 after 6 steps.
Found uncertainty sample 91 after 74 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 29 steps.
Found uncertainty sample 98 after 8 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_113956-tnfgbr68
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_12
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/tnfgbr68
Training model 12. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 6.371576041270166, Training Loss Force: 4.629507590373264, time: 0.9895508289337158
Validation Loss Energy: 5.558275904746131, Validation Loss Force: 4.213099752508497, time: 0.06937265396118164
Test Loss Energy: 10.307483899112253, Test Loss Force: 11.780633911995666, time: 9.600560665130615


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.175861465096572, Training Loss Force: 3.8363659753536616, time: 0.9659042358398438
Validation Loss Energy: 1.868983273787308, Validation Loss Force: 3.8228609408127525, time: 0.06851077079772949
Test Loss Energy: 10.535943888434902, Test Loss Force: 12.014673411874966, time: 9.537523984909058


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.037775314095637, Training Loss Force: 3.5759835070204735, time: 1.0014300346374512
Validation Loss Energy: 5.615196373181361, Validation Loss Force: 3.90366467917105, time: 0.06973600387573242
Test Loss Energy: 12.565866941605693, Test Loss Force: 11.878786588101336, time: 9.68736457824707


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.10455158581655, Training Loss Force: 3.5645989758962995, time: 1.0471999645233154
Validation Loss Energy: 2.4004938571095, Validation Loss Force: 3.8204416405422474, time: 0.0671072006225586
Test Loss Energy: 9.806465496897216, Test Loss Force: 11.648233009841553, time: 9.564920663833618


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.066331928139061, Training Loss Force: 3.54429978136321, time: 1.0036273002624512
Validation Loss Energy: 5.753072217663263, Validation Loss Force: 3.8401171316147185, time: 0.08247232437133789
Test Loss Energy: 10.316746926256394, Test Loss Force: 11.583680683502617, time: 9.96795392036438


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.154064627517333, Training Loss Force: 3.557098362124284, time: 1.04594087600708
Validation Loss Energy: 1.8442993412827695, Validation Loss Force: 3.8499715773368983, time: 0.06869006156921387
Test Loss Energy: 10.66833144909317, Test Loss Force: 11.613555752810226, time: 9.723864555358887


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.035377546290688, Training Loss Force: 3.559112731370045, time: 0.9877724647521973
Validation Loss Energy: 5.503520225747613, Validation Loss Force: 3.851761837270158, time: 0.06683540344238281
Test Loss Energy: 12.434131845845661, Test Loss Force: 11.556169008188585, time: 9.57276463508606


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.097291086599757, Training Loss Force: 3.537532636166442, time: 0.9675807952880859
Validation Loss Energy: 2.148733098600819, Validation Loss Force: 3.834466679367655, time: 0.06979990005493164
Test Loss Energy: 9.671651439441542, Test Loss Force: 11.583532131705493, time: 9.674683570861816


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.0602146661890295, Training Loss Force: 3.5153105965894436, time: 1.0407614707946777
Validation Loss Energy: 5.849287778287806, Validation Loss Force: 3.7957867106939656, time: 0.06743597984313965
Test Loss Energy: 10.28781562904863, Test Loss Force: 11.529631373988561, time: 9.717110395431519


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.15767673767107, Training Loss Force: 3.5273642377228174, time: 1.0047669410705566
Validation Loss Energy: 1.8297720721987218, Validation Loss Force: 3.811315545596397, time: 0.07498002052307129
Test Loss Energy: 10.470609459454133, Test Loss Force: 11.585190366322323, time: 9.553692817687988


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.120849004577685, Training Loss Force: 3.5290617703225498, time: 0.9937140941619873
Validation Loss Energy: 5.046424112310467, Validation Loss Force: 3.8413437046050616, time: 0.06804180145263672
Test Loss Energy: 12.320067787496141, Test Loss Force: 11.53809238650651, time: 9.554440975189209


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.158550468055346, Training Loss Force: 3.526489851901137, time: 1.2075390815734863
Validation Loss Energy: 2.6271364226117964, Validation Loss Force: 3.786756474815437, time: 0.06787681579589844
Test Loss Energy: 9.810719237690632, Test Loss Force: 11.484333153101826, time: 9.452309608459473


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.0672847096615765, Training Loss Force: 3.5294683354515106, time: 1.0043773651123047
Validation Loss Energy: 6.125281063274942, Validation Loss Force: 3.8485400373977288, time: 0.07091856002807617
Test Loss Energy: 10.39610718447975, Test Loss Force: 11.55781792101227, time: 9.556262016296387


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.135818363498064, Training Loss Force: 3.5276473830904913, time: 0.9993412494659424
Validation Loss Energy: 2.1779427258290536, Validation Loss Force: 3.7968560792672843, time: 0.06782722473144531
Test Loss Energy: 10.655742141283266, Test Loss Force: 11.534288521325973, time: 9.835578680038452


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.101546317322783, Training Loss Force: 3.5236558303463412, time: 1.0044171810150146
Validation Loss Energy: 5.382070684160803, Validation Loss Force: 3.772982495452063, time: 0.07041573524475098
Test Loss Energy: 12.649047899263337, Test Loss Force: 11.603443080827196, time: 9.593312978744507


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.19897411750926, Training Loss Force: 3.5317364756948715, time: 0.9841580390930176
Validation Loss Energy: 2.4976297093141624, Validation Loss Force: 3.805654405230329, time: 0.06833243370056152
Test Loss Energy: 9.868897943018496, Test Loss Force: 11.545952019791008, time: 9.994257688522339


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.064719765812446, Training Loss Force: 3.519292915746914, time: 1.0069007873535156
Validation Loss Energy: 5.856323900629977, Validation Loss Force: 3.8117731306912788, time: 0.06892538070678711
Test Loss Energy: 10.418963899510386, Test Loss Force: 11.536577326984702, time: 9.73491883277893


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.1554328845260855, Training Loss Force: 3.5276871762217317, time: 0.9976985454559326
Validation Loss Energy: 1.689327015443818, Validation Loss Force: 4.017450171424171, time: 0.09412932395935059
Test Loss Energy: 10.412758443834194, Test Loss Force: 11.65871748567116, time: 9.556102275848389


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.1021586969799015, Training Loss Force: 3.531077489948936, time: 1.0240345001220703
Validation Loss Energy: 5.705670187209009, Validation Loss Force: 3.7870480600620295, time: 0.07023930549621582
Test Loss Energy: 12.568174568053385, Test Loss Force: 11.575754948958568, time: 9.80389952659607


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.125026069451896, Training Loss Force: 3.5353741963918286, time: 1.0187122821807861
Validation Loss Energy: 2.556949660465909, Validation Loss Force: 3.8858927866271156, time: 0.07079434394836426
Test Loss Energy: 9.827688053558154, Test Loss Force: 11.5315922148089, time: 9.723109006881714

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ƒâ–ˆâ–â–ƒâ–ƒâ–‡â–â–‚â–ƒâ–‡â–â–ƒâ–ƒâ–ˆâ–â–ƒâ–ƒâ–ˆâ–
wandb:   test_error_force â–…â–ˆâ–†â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–‚
wandb:          test_loss â–ˆâ–†â–‡â–ƒâ–‚â–„â–†â–â–â–ƒâ–…â–â–â–ƒâ–…â–â–â–‚â–…â–
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–â–‡â–‚â–‡â–â–‡â–‚â–ˆâ–â–†â–‚â–ˆâ–‚â–‡â–‚â–ˆâ–â–‡â–‚
wandb:  valid_error_force â–ˆâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–‚â–…â–â–ƒ
wandb:         valid_loss â–ˆâ–â–…â–â–„â–â–„â–â–„â–â–„â–‚â–…â–â–„â–‚â–„â–‚â–„â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1967
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.82769
wandb:   test_error_force 11.53159
wandb:          test_loss 9.3116
wandb: train_error_energy 4.12503
wandb:  train_error_force 3.53537
wandb:         train_loss 1.45884
wandb: valid_error_energy 2.55695
wandb:  valid_error_force 3.88589
wandb:         valid_loss 1.16741
wandb: 
wandb: ğŸš€ View run al_71_12 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/tnfgbr68
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_113956-tnfgbr68/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.833862543106079, Uncertainty Bias: -0.17209532856941223
3.4332275e-05 0.005525589
2.752835 4.5837984
(48745, 22, 3)
Found uncertainty sample 0 after 128 steps.
Found uncertainty sample 1 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 12 steps.
Found uncertainty sample 4 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 16 steps.
Found uncertainty sample 9 after 25 steps.
Found uncertainty sample 10 after 155 steps.
Found uncertainty sample 11 after 216 steps.
Found uncertainty sample 12 after 126 steps.
Found uncertainty sample 13 after 4 steps.
Found uncertainty sample 14 after 14 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 4 steps.
Found uncertainty sample 17 after 59 steps.
Found uncertainty sample 18 after 294 steps.
Found uncertainty sample 19 after 453 steps.
Found uncertainty sample 20 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 17 steps.
Found uncertainty sample 23 after 11 steps.
Found uncertainty sample 24 after 108 steps.
Found uncertainty sample 25 after 25 steps.
Found uncertainty sample 26 after 50 steps.
Found uncertainty sample 27 after 141 steps.
Found uncertainty sample 28 after 58 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 31 steps.
Found uncertainty sample 31 after 58 steps.
Found uncertainty sample 32 after 181 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 12 steps.
Found uncertainty sample 38 after 34 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 258 steps.
Found uncertainty sample 41 after 95 steps.
Found uncertainty sample 42 after 221 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 14 steps.
Found uncertainty sample 47 after 168 steps.
Found uncertainty sample 48 after 22 steps.
Found uncertainty sample 49 after 821 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 33 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 9 steps.
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 44 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 377 steps.
Found uncertainty sample 59 after 6 steps.
Found uncertainty sample 60 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 119 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 607 steps.
Found uncertainty sample 65 after 1963 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 342 steps.
Found uncertainty sample 68 after 300 steps.
Found uncertainty sample 69 after 115 steps.
Found uncertainty sample 70 after 200 steps.
Found uncertainty sample 71 after 32 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 18 steps.
Found uncertainty sample 74 after 34 steps.
Found uncertainty sample 75 after 32 steps.
Found uncertainty sample 76 after 172 steps.
Found uncertainty sample 77 after 35 steps.
Found uncertainty sample 78 after 15 steps.
Found uncertainty sample 79 after 42 steps.
Found uncertainty sample 80 after 74 steps.
Found uncertainty sample 81 after 7 steps.
Found uncertainty sample 82 after 31 steps.
Found uncertainty sample 83 after 107 steps.
Found uncertainty sample 84 after 28 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 44 steps.
Found uncertainty sample 87 after 21 steps.
Found uncertainty sample 88 after 16 steps.
Found uncertainty sample 89 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 64 steps.
Found uncertainty sample 92 after 34 steps.
Found uncertainty sample 93 after 7 steps.
Found uncertainty sample 94 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 13 steps.
Found uncertainty sample 97 after 82 steps.
Found uncertainty sample 98 after 17 steps.
Found uncertainty sample 99 after 31 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_114719-30sv1d0d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_13
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/30sv1d0d
Training model 13. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.193094340741875, Training Loss Force: 4.057015683301337, time: 1.1240477561950684
Validation Loss Energy: 1.9400063255780138, Validation Loss Force: 3.9086315371401152, time: 0.07430124282836914
Test Loss Energy: 10.818298652552716, Test Loss Force: 11.956176100807685, time: 9.742001295089722


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6846252091233485, Training Loss Force: 3.587579763074428, time: 1.090057373046875
Validation Loss Energy: 1.7211354142741755, Validation Loss Force: 3.8598640994830324, time: 0.07524442672729492
Test Loss Energy: 10.54378338590009, Test Loss Force: 11.972414454622314, time: 9.77852988243103


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7123910804404994, Training Loss Force: 3.5351064085648773, time: 1.0397720336914062
Validation Loss Energy: 2.1030312584102977, Validation Loss Force: 3.7990166379902215, time: 0.07520413398742676
Test Loss Energy: 10.082280824936088, Test Loss Force: 11.972553358200324, time: 9.840260982513428


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6422688709364368, Training Loss Force: 3.5346512659792424, time: 1.05210280418396
Validation Loss Energy: 2.109040207982305, Validation Loss Force: 3.790523146087952, time: 0.0714726448059082
Test Loss Energy: 9.97255169418272, Test Loss Force: 11.97585604759469, time: 9.654989957809448


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5493865196506365, Training Loss Force: 3.804877283059682, time: 1.0096797943115234
Validation Loss Energy: 1.8865670109029335, Validation Loss Force: 3.8595093875335698, time: 0.06961607933044434
Test Loss Energy: 9.893869655866878, Test Loss Force: 11.78240820969884, time: 10.32502293586731


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.596010228946534, Training Loss Force: 3.4961315933980277, time: 1.1255764961242676
Validation Loss Energy: 3.7932788994379543, Validation Loss Force: 3.7699440636213057, time: 0.08320116996765137
Test Loss Energy: 10.088217958462405, Test Loss Force: 11.671190142827967, time: 11.120211124420166


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.567534250307463, Training Loss Force: 3.481032309545006, time: 1.1106677055358887
Validation Loss Energy: 2.8176518098638925, Validation Loss Force: 3.7670671782559184, time: 0.08469367027282715
Test Loss Energy: 9.847387842381094, Test Loss Force: 11.737550020350282, time: 11.172595500946045


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6196877731328407, Training Loss Force: 3.492198571145109, time: 1.2415084838867188
Validation Loss Energy: 1.5973442282815027, Validation Loss Force: 3.8066624284539436, time: 0.08301711082458496
Test Loss Energy: 10.426798682451878, Test Loss Force: 11.86332470374889, time: 12.213091611862183


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6419065963286172, Training Loss Force: 3.499067640242648, time: 1.2174103260040283
Validation Loss Energy: 3.272803019645933, Validation Loss Force: 3.7226649465356045, time: 0.07733821868896484
Test Loss Energy: 11.424373052637483, Test Loss Force: 11.906567816657493, time: 11.5523362159729


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.650429499399385, Training Loss Force: 3.492150731535526, time: 1.2075257301330566
Validation Loss Energy: 1.9360790541839987, Validation Loss Force: 3.787004305574855, time: 0.08621430397033691
Test Loss Energy: 10.546230372922148, Test Loss Force: 11.959522396048825, time: 11.764089345932007


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6069251210606392, Training Loss Force: 3.5041593436633165, time: 1.2078320980072021
Validation Loss Energy: 2.1516994969628698, Validation Loss Force: 3.7020454496189723, time: 0.08565735816955566
Test Loss Energy: 9.799553753343053, Test Loss Force: 11.865215535524456, time: 11.701627731323242


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.58635173378561, Training Loss Force: 3.493892446169294, time: 1.2067124843597412
Validation Loss Energy: 3.8903448335307953, Validation Loss Force: 3.822560226131177, time: 0.0932011604309082
Test Loss Energy: 10.004042828534667, Test Loss Force: 11.976555275345184, time: 11.584598779678345


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6545357291295626, Training Loss Force: 3.492367861358239, time: 1.243640661239624
Validation Loss Energy: 2.564694761368715, Validation Loss Force: 3.771934948455046, time: 0.08188009262084961
Test Loss Energy: 9.874168258230215, Test Loss Force: 11.897128626742132, time: 11.519548177719116


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6826736292043716, Training Loss Force: 3.489001672534553, time: 1.1916029453277588
Validation Loss Energy: 1.7754206490346351, Validation Loss Force: 3.8546369412823775, time: 0.08164358139038086
Test Loss Energy: 10.457517623521888, Test Loss Force: 11.957406875949527, time: 11.524021863937378


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6022037678148364, Training Loss Force: 3.4919054460557306, time: 1.230398416519165
Validation Loss Energy: 3.2939531371373105, Validation Loss Force: 3.7274790534705633, time: 0.07934713363647461
Test Loss Energy: 11.638194264014967, Test Loss Force: 12.072937533553535, time: 11.778026342391968


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.582475279772795, Training Loss Force: 3.4820466737007614, time: 1.2373554706573486
Validation Loss Energy: 2.0677122578252427, Validation Loss Force: 3.754220281881615, time: 0.08844399452209473
Test Loss Energy: 10.94814480048137, Test Loss Force: 12.028899032465995, time: 11.578403949737549


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.581210875294559, Training Loss Force: 3.497430607512899, time: 1.134474754333496
Validation Loss Energy: 2.1515117868728524, Validation Loss Force: 3.806073468562965, time: 0.07886648178100586
Test Loss Energy: 9.960103220527289, Test Loss Force: 11.986764275632874, time: 11.504885196685791


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6098964215695792, Training Loss Force: 3.4973630702481, time: 1.4342637062072754
Validation Loss Energy: 3.5691471512251396, Validation Loss Force: 3.8023751567182504, time: 0.07495832443237305
Test Loss Energy: 10.116524653560345, Test Loss Force: 12.161073073232041, time: 11.522804737091064


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.591320064796718, Training Loss Force: 3.5082850055585744, time: 1.1853125095367432
Validation Loss Energy: 3.3190919270396932, Validation Loss Force: 3.7600179184341367, time: 0.0854499340057373
Test Loss Energy: 10.11553278026175, Test Loss Force: 12.033316345314525, time: 11.61029314994812


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.658445040544119, Training Loss Force: 3.534969511573961, time: 1.2330684661865234
Validation Loss Energy: 1.706298022737137, Validation Loss Force: 3.7970698416473745, time: 0.08414101600646973
Test Loss Energy: 10.698691088431373, Test Loss Force: 12.213043473651293, time: 12.082472085952759

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–„â–‚â–‚â–â–‚â–â–ƒâ–‡â–„â–â–‚â–â–„â–ˆâ–…â–‚â–‚â–‚â–„
wandb:   test_error_force â–…â–…â–…â–…â–‚â–â–‚â–ƒâ–„â–…â–„â–…â–„â–…â–†â–†â–…â–‡â–†â–ˆ
wandb:          test_loss â–‡â–ˆâ–†â–‡â–ƒâ–â–‚â–„â–†â–„â–â–‚â–â–„â–†â–…â–‚â–â–â–„
wandb: train_error_energy â–ˆâ–â–â–â–…â–…â–…â–…â–†â–†â–…â–…â–†â–†â–…â–…â–…â–…â–…â–†
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: valid_error_energy â–‚â–â–ƒâ–ƒâ–‚â–ˆâ–…â–â–†â–‚â–ƒâ–ˆâ–„â–‚â–†â–‚â–ƒâ–‡â–†â–
wandb:  valid_error_force â–ˆâ–†â–„â–„â–†â–ƒâ–ƒâ–…â–‚â–„â–â–…â–ƒâ–†â–‚â–ƒâ–…â–„â–ƒâ–„
wandb:         valid_loss â–‚â–â–‚â–‚â–â–‡â–„â–â–…â–‚â–‚â–ˆâ–ƒâ–â–†â–‚â–‚â–‡â–†â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 2057
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.69869
wandb:   test_error_force 12.21304
wandb:          test_loss 15.23193
wandb: train_error_energy 2.65845
wandb:  train_error_force 3.53497
wandb:         train_loss 1.03508
wandb: valid_error_energy 1.7063
wandb:  valid_error_force 3.79707
wandb:         valid_loss 0.80634
wandb: 
wandb: ğŸš€ View run al_71_13 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/30sv1d0d
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_114719-30sv1d0d/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.5468862056732178, Uncertainty Bias: -0.022738605737686157
0.0002822876 0.0020138472
2.80055 4.7780333
(48745, 22, 3)
Found uncertainty sample 0 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 24 steps.
Found uncertainty sample 5 after 32 steps.
Found uncertainty sample 6 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 9 steps.
Found uncertainty sample 9 after 19 steps.
Found uncertainty sample 10 after 14 steps.
Found uncertainty sample 11 after 25 steps.
Found uncertainty sample 12 after 10 steps.
Found uncertainty sample 13 after 406 steps.
Found uncertainty sample 14 after 32 steps.
Found uncertainty sample 15 after 187 steps.
Found uncertainty sample 16 after 102 steps.
Found uncertainty sample 17 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 79 steps.
Found uncertainty sample 20 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 29 steps.
Found uncertainty sample 23 after 18 steps.
Found uncertainty sample 24 after 72 steps.
Found uncertainty sample 25 after 58 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 148 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 15 steps.
Found uncertainty sample 30 after 377 steps.
Found uncertainty sample 31 after 235 steps.
Found uncertainty sample 32 after 403 steps.
Found uncertainty sample 33 after 19 steps.
Found uncertainty sample 34 after 56 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 7 steps.
Found uncertainty sample 41 after 70 steps.
Found uncertainty sample 42 after 75 steps.
Found uncertainty sample 43 after 107 steps.
Found uncertainty sample 44 after 143 steps.
Found uncertainty sample 45 after 20 steps.
Found uncertainty sample 46 after 18 steps.
Found uncertainty sample 47 after 10 steps.
Found uncertainty sample 48 after 30 steps.
Found uncertainty sample 49 after 44 steps.
Found uncertainty sample 50 after 8 steps.
Found uncertainty sample 51 after 26 steps.
Found uncertainty sample 52 after 20 steps.
Found uncertainty sample 53 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 24 steps.
Found uncertainty sample 57 after 3 steps.
Found uncertainty sample 58 after 239 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 7 steps.
Found uncertainty sample 63 after 113 steps.
Found uncertainty sample 64 after 22 steps.
Found uncertainty sample 65 after 6 steps.
Found uncertainty sample 66 after 88 steps.
Found uncertainty sample 67 after 15 steps.
Found uncertainty sample 68 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 95 steps.
Found uncertainty sample 71 after 9 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 583 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 17 steps.
Found uncertainty sample 76 after 32 steps.
Found uncertainty sample 77 after 276 steps.
Found uncertainty sample 78 after 10 steps.
Found uncertainty sample 79 after 34 steps.
Found uncertainty sample 80 after 243 steps.
Found uncertainty sample 81 after 68 steps.
Found uncertainty sample 82 after 59 steps.
Found uncertainty sample 83 after 31 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 162 steps.
Found uncertainty sample 90 after 20 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 9 steps.
Found uncertainty sample 93 after 130 steps.
Found uncertainty sample 94 after 164 steps.
Found uncertainty sample 95 after 35 steps.
Found uncertainty sample 96 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 35 steps.
Found uncertainty sample 99 after 93 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_115456-64ctf7m4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_14
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/64ctf7m4
Training model 14. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 7.189526168537199, Training Loss Force: 4.204494133593069, time: 1.131800651550293
Validation Loss Energy: 1.7931453913869195, Validation Loss Force: 3.903611015906881, time: 0.07799458503723145
Test Loss Energy: 10.386340967034863, Test Loss Force: 11.779589805878144, time: 9.469409704208374


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 5.985260765925899, Training Loss Force: 3.6643882068816227, time: 1.0855250358581543
Validation Loss Energy: 8.597381309670192, Validation Loss Force: 3.9053705474980025, time: 0.07361960411071777
Test Loss Energy: 14.912356322642735, Test Loss Force: 11.556207269975857, time: 9.55361819267273


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 5.9417529044774815, Training Loss Force: 3.677623376030045, time: 1.0984082221984863
Validation Loss Energy: 4.606792385326727, Validation Loss Force: 3.855047320518323, time: 0.07022571563720703
Test Loss Energy: 11.834057192452075, Test Loss Force: 11.457402851772319, time: 9.652089834213257


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.7243945683651765, Training Loss Force: 3.9530412577477345, time: 1.1117208003997803
Validation Loss Energy: 4.993390333817407, Validation Loss Force: 3.7969916847771543, time: 0.07529306411743164
Test Loss Energy: 10.145983702291714, Test Loss Force: 11.345789511027409, time: 9.408450603485107


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.030738529027554, Training Loss Force: 3.5503172789797923, time: 1.0672862529754639
Validation Loss Energy: 4.786898992017857, Validation Loss Force: 3.8103012876366957, time: 0.07189130783081055
Test Loss Energy: 12.270011250536475, Test Loss Force: 11.399426497905168, time: 9.453063726425171


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.090364800884079, Training Loss Force: 3.557935658216741, time: 1.0780866146087646
Validation Loss Energy: 5.467006939198374, Validation Loss Force: 3.7454399921193904, time: 0.06992459297180176
Test Loss Energy: 10.30519443593835, Test Loss Force: 11.427982111460507, time: 9.729071378707886


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.05964019991515, Training Loss Force: 3.5433150612896336, time: 1.079206943511963
Validation Loss Energy: 4.812481672862646, Validation Loss Force: 3.763625381267313, time: 0.06986451148986816
Test Loss Energy: 12.519170591643942, Test Loss Force: 11.523030597806049, time: 9.434308290481567


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.077253294975302, Training Loss Force: 3.5362515604661517, time: 1.1179656982421875
Validation Loss Energy: 4.995962830460311, Validation Loss Force: 3.804004204189858, time: 0.06966018676757812
Test Loss Energy: 10.348278378879243, Test Loss Force: 11.610541125010654, time: 9.589584112167358


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.1467011688549835, Training Loss Force: 3.5421608623208094, time: 1.113792896270752
Validation Loss Energy: 4.736660824651921, Validation Loss Force: 3.751435024869777, time: 0.0702054500579834
Test Loss Energy: 12.09382207611934, Test Loss Force: 11.572494587686455, time: 9.59621810913086


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.0392105464554255, Training Loss Force: 3.533555754234597, time: 1.0928664207458496
Validation Loss Energy: 5.155202443479109, Validation Loss Force: 3.753996165386239, time: 0.07757234573364258
Test Loss Energy: 10.27808759877937, Test Loss Force: 11.468204991369252, time: 9.545963525772095


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.13429790309666, Training Loss Force: 3.5322387415509984, time: 1.0969069004058838
Validation Loss Energy: 4.588318194296537, Validation Loss Force: 3.796015948579699, time: 0.0762641429901123
Test Loss Energy: 12.05945619682374, Test Loss Force: 11.637912301922642, time: 9.41430115699768


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.058350401985548, Training Loss Force: 3.5289125425945915, time: 1.2986361980438232
Validation Loss Energy: 5.208217517763909, Validation Loss Force: 3.8095615807339236, time: 0.07265663146972656
Test Loss Energy: 10.335826729565419, Test Loss Force: 11.679701942564105, time: 9.526192665100098


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.1297499103501805, Training Loss Force: 3.5584800646312575, time: 1.0931177139282227
Validation Loss Energy: 4.70415357870707, Validation Loss Force: 3.854092960558876, time: 0.07572793960571289
Test Loss Energy: 12.309928712315113, Test Loss Force: 11.802379864725077, time: 9.466533660888672


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.127779696089731, Training Loss Force: 3.556069068610076, time: 1.0877089500427246
Validation Loss Energy: 5.101189366082259, Validation Loss Force: 3.8008478690463314, time: 0.06999635696411133
Test Loss Energy: 10.330784525486921, Test Loss Force: 11.617854960078468, time: 9.79241132736206


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.024728266362494, Training Loss Force: 3.5395464482111847, time: 1.0723295211791992
Validation Loss Energy: 4.800782413266454, Validation Loss Force: 3.869667999309087, time: 0.07155609130859375
Test Loss Energy: 12.361892010637838, Test Loss Force: 11.827262071571639, time: 9.6152184009552


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.115920664832243, Training Loss Force: 3.5320417644950552, time: 1.106391429901123
Validation Loss Energy: 5.11546245833623, Validation Loss Force: 3.8267128943681574, time: 0.07535696029663086
Test Loss Energy: 10.248481131415328, Test Loss Force: 11.644357167733954, time: 9.51973032951355


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.138148733482695, Training Loss Force: 3.539836317595535, time: 1.076355218887329
Validation Loss Energy: 4.440642903371499, Validation Loss Force: 3.8004933797660145, time: 0.07028317451477051
Test Loss Energy: 11.968000984273813, Test Loss Force: 11.748200245962568, time: 10.205289125442505


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.109025040172058, Training Loss Force: 3.522521489493626, time: 1.0663630962371826
Validation Loss Energy: 5.188191545455749, Validation Loss Force: 3.7543327298003084, time: 0.07697868347167969
Test Loss Energy: 10.30621991712249, Test Loss Force: 11.685422198673894, time: 9.537743330001831


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.190045132713814, Training Loss Force: 3.556996813057492, time: 1.0995306968688965
Validation Loss Energy: 4.4489357703732875, Validation Loss Force: 3.8723600692833537, time: 0.0853874683380127
Test Loss Energy: 11.975626551482696, Test Loss Force: 11.773425335314487, time: 9.556969404220581


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.096046755903473, Training Loss Force: 3.547637066137546, time: 1.107468843460083
Validation Loss Energy: 5.466449375656971, Validation Loss Force: 3.7437619574356784, time: 0.07372593879699707
Test Loss Energy: 10.328134543478656, Test Loss Force: 11.722667155052127, time: 9.66695499420166

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–ˆâ–ƒâ–â–„â–â–„â–â–„â–â–„â–â–„â–â–„â–â–„â–â–„â–
wandb:   test_error_force â–‡â–„â–ƒâ–â–‚â–‚â–„â–…â–„â–ƒâ–…â–†â–ˆâ–…â–ˆâ–…â–‡â–†â–‡â–†
wandb:          test_loss â–ƒâ–‡â–ƒâ–â–†â–‚â–ˆâ–ƒâ–‡â–‚â–†â–‚â–‡â–‚â–ˆâ–‚â–†â–‚â–†â–ƒ
wandb: train_error_energy â–ˆâ–…â–…â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–ƒâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–ˆâ–„â–„â–„â–…â–„â–„â–„â–„â–„â–…â–„â–„â–„â–„â–„â–„â–„â–…
wandb:  valid_error_force â–ˆâ–ˆâ–†â–ƒâ–„â–â–‚â–„â–â–â–ƒâ–„â–†â–ƒâ–†â–…â–ƒâ–â–‡â–
wandb:         valid_loss â–â–ˆâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 2147
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.32813
wandb:   test_error_force 11.72267
wandb:          test_loss 9.24401
wandb: train_error_energy 4.09605
wandb:  train_error_force 3.54764
wandb:         train_loss 1.45568
wandb: valid_error_energy 5.46645
wandb:  valid_error_force 3.74376
wandb:         valid_loss 1.89635
wandb: 
wandb: ğŸš€ View run al_71_14 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/64ctf7m4
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_115456-64ctf7m4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.451704978942871, Uncertainty Bias: -0.12403371930122375
2.4795532e-05 0.15358448
2.831097 4.4096174
(48745, 22, 3)
Found uncertainty sample 0 after 14 steps.
Found uncertainty sample 1 after 29 steps.
Found uncertainty sample 2 after 64 steps.
Found uncertainty sample 3 after 43 steps.
Found uncertainty sample 4 after 4 steps.
Found uncertainty sample 5 after 17 steps.
Found uncertainty sample 6 after 14 steps.
Found uncertainty sample 7 after 27 steps.
Found uncertainty sample 8 after 275 steps.
Found uncertainty sample 9 after 134 steps.
Found uncertainty sample 10 after 223 steps.
Found uncertainty sample 11 after 12 steps.
Found uncertainty sample 12 after 127 steps.
Found uncertainty sample 13 after 141 steps.
Found uncertainty sample 14 after 22 steps.
Found uncertainty sample 15 after 230 steps.
Found uncertainty sample 16 after 8 steps.
Found uncertainty sample 17 after 8 steps.
Found uncertainty sample 18 after 27 steps.
Found uncertainty sample 19 after 45 steps.
Found uncertainty sample 20 after 21 steps.
Found uncertainty sample 21 after 87 steps.
Found uncertainty sample 22 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 13 steps.
Found uncertainty sample 25 after 20 steps.
Found uncertainty sample 26 after 484 steps.
Found uncertainty sample 27 after 21 steps.
Found uncertainty sample 28 after 245 steps.
Found uncertainty sample 29 after 25 steps.
Found uncertainty sample 30 after 246 steps.
Found uncertainty sample 31 after 7 steps.
Found uncertainty sample 32 after 49 steps.
Found uncertainty sample 33 after 227 steps.
Found uncertainty sample 34 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 103 steps.
Found uncertainty sample 37 after 186 steps.
Found uncertainty sample 38 after 210 steps.
Found uncertainty sample 39 after 308 steps.
Found uncertainty sample 40 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 25 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 149 steps.
Found uncertainty sample 47 after 124 steps.
Found uncertainty sample 48 after 459 steps.
Found uncertainty sample 49 after 48 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 13 steps.
Found uncertainty sample 52 after 133 steps.
Found uncertainty sample 53 after 20 steps.
Found uncertainty sample 54 after 11 steps.
Found uncertainty sample 55 after 172 steps.
Found uncertainty sample 56 after 109 steps.
Found uncertainty sample 57 after 1834 steps.
Found uncertainty sample 58 after 6 steps.
Found uncertainty sample 59 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 450 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 35 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 70 steps.
Found uncertainty sample 67 after 11 steps.
Found uncertainty sample 68 after 41 steps.
Found uncertainty sample 69 after 278 steps.
Found uncertainty sample 70 after 367 steps.
Found uncertainty sample 71 after 10 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 327 steps.
Found uncertainty sample 74 after 38 steps.
Found uncertainty sample 75 after 1826 steps.
Found uncertainty sample 76 after 220 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 740 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 24 steps.
Found uncertainty sample 81 after 6 steps.
Found uncertainty sample 82 after 81 steps.
Found uncertainty sample 83 after 923 steps.
Found uncertainty sample 84 after 31 steps.
Found uncertainty sample 85 after 56 steps.
Found uncertainty sample 86 after 540 steps.
Found uncertainty sample 87 after 68 steps.
Found uncertainty sample 88 after 10 steps.
Found uncertainty sample 89 after 48 steps.
Found uncertainty sample 90 after 200 steps.
Found uncertainty sample 91 after 47 steps.
Found uncertainty sample 92 after 59 steps.
Found uncertainty sample 93 after 141 steps.
Found uncertainty sample 94 after 31 steps.
Found uncertainty sample 95 after 11 steps.
Found uncertainty sample 96 after 34 steps.
Found uncertainty sample 97 after 161 steps.
Found uncertainty sample 98 after 1190 steps.
Found uncertainty sample 99 after 108 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_120256-meusemq0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_15
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/meusemq0
Training model 15. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.173052026878551, Training Loss Force: 3.765619475717742, time: 1.1540307998657227
Validation Loss Energy: 5.690582125880942, Validation Loss Force: 3.833553290531059, time: 0.07866525650024414
Test Loss Energy: 10.403977739044661, Test Loss Force: 11.413922539098756, time: 9.789275407791138


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.092147994159384, Training Loss Force: 3.5461306972963023, time: 1.1470856666564941
Validation Loss Energy: 3.861499827029018, Validation Loss Force: 3.7190760765761133, time: 0.07426023483276367
Test Loss Energy: 9.958432283276878, Test Loss Force: 11.435043101632791, time: 9.783606052398682


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.075571777671302, Training Loss Force: 3.571860759667793, time: 1.184436559677124
Validation Loss Energy: 4.8495785752433616, Validation Loss Force: 3.7911962749160915, time: 0.08096551895141602
Test Loss Energy: 12.306550613857736, Test Loss Force: 11.719771099155661, time: 9.933637142181396


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.066038078901362, Training Loss Force: 3.5556040702231293, time: 1.1903321743011475
Validation Loss Energy: 2.9798923349623596, Validation Loss Force: 3.7512893992483405, time: 0.0724642276763916
Test Loss Energy: 11.296783953783606, Test Loss Force: 11.7211263498475, time: 9.706160306930542


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.0736882623092345, Training Loss Force: 3.5662379883765567, time: 1.1268939971923828
Validation Loss Energy: 5.4624829701605595, Validation Loss Force: 3.765989682258774, time: 0.07366371154785156
Test Loss Energy: 10.397390985189306, Test Loss Force: 11.6094315984649, time: 9.635151624679565


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.078263830150135, Training Loss Force: 3.5577060765036514, time: 1.1020503044128418
Validation Loss Energy: 3.588078036673628, Validation Loss Force: 3.723776510912076, time: 0.07503175735473633
Test Loss Energy: 10.054895788909931, Test Loss Force: 11.64459460066032, time: 9.893237829208374


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.130305184983746, Training Loss Force: 3.5523898729115397, time: 1.1497230529785156
Validation Loss Energy: 4.6361047743658075, Validation Loss Force: 3.7344685266266637, time: 0.07114052772521973
Test Loss Energy: 12.511894747896166, Test Loss Force: 11.761681349530948, time: 9.70564317703247


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.029735448497627, Training Loss Force: 3.524706906700627, time: 1.14473295211792
Validation Loss Energy: 3.0058581180727177, Validation Loss Force: 3.714399913177057, time: 0.07652974128723145
Test Loss Energy: 11.48708976914904, Test Loss Force: 11.86763975200328, time: 9.724665880203247


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.056175362610904, Training Loss Force: 3.546353980372875, time: 1.256870985031128
Validation Loss Energy: 5.331852779687146, Validation Loss Force: 3.722466325657437, time: 0.07783317565917969
Test Loss Energy: 10.38987339502909, Test Loss Force: 11.643110374403287, time: 10.252314329147339


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.074322605353141, Training Loss Force: 3.53121974064493, time: 1.1365339756011963
Validation Loss Energy: 3.926775732496041, Validation Loss Force: 3.7831173929635398, time: 0.07536602020263672
Test Loss Energy: 10.113168197144983, Test Loss Force: 11.77747038732008, time: 9.701052188873291


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.085488764794598, Training Loss Force: 3.5593418454024777, time: 1.1413938999176025
Validation Loss Energy: 4.8795910360772385, Validation Loss Force: 3.8104384518305876, time: 0.07894730567932129
Test Loss Energy: 12.441789663940684, Test Loss Force: 11.896006145748471, time: 9.923025608062744


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.140481901351322, Training Loss Force: 3.5472993190509237, time: 1.1782453060150146
Validation Loss Energy: 3.3187466785289326, Validation Loss Force: 3.7304562512861907, time: 0.07573795318603516
Test Loss Energy: 11.536413992754227, Test Loss Force: 11.88677070159906, time: 9.727067708969116


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.153379690349422, Training Loss Force: 3.5789480545606867, time: 1.120844841003418
Validation Loss Energy: 5.445404914675335, Validation Loss Force: 3.8237860197795097, time: 0.07579779624938965
Test Loss Energy: 10.614493920087225, Test Loss Force: 11.764683787652043, time: 9.792211294174194


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.020703587134334, Training Loss Force: 3.5689217718776076, time: 1.1803038120269775
Validation Loss Energy: 3.974815487384231, Validation Loss Force: 3.727637259898745, time: 0.07315731048583984
Test Loss Energy: 10.193473314000356, Test Loss Force: 11.891166269814551, time: 9.89890193939209


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.12690913059969, Training Loss Force: 3.5520452160645095, time: 1.135157823562622
Validation Loss Energy: 4.738698915344598, Validation Loss Force: 3.7917331302269233, time: 0.07613301277160645
Test Loss Energy: 12.399479546411595, Test Loss Force: 12.096436291167407, time: 9.764513492584229


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.041251278022887, Training Loss Force: 3.567680757202657, time: 1.177762508392334
Validation Loss Energy: 3.0850406023795145, Validation Loss Force: 3.7340276417177267, time: 0.07451081275939941
Test Loss Energy: 11.695482460635336, Test Loss Force: 12.04569787832933, time: 9.759421348571777


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.06859514775344, Training Loss Force: 3.5524905384514254, time: 1.1691217422485352
Validation Loss Energy: 5.31103967194813, Validation Loss Force: 3.8211993836458396, time: 0.0783073902130127
Test Loss Energy: 10.474294724752186, Test Loss Force: 11.860861801129031, time: 9.94912314414978


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.092550953741289, Training Loss Force: 3.5481079356923853, time: 1.1183958053588867
Validation Loss Energy: 3.979062901342804, Validation Loss Force: 3.751364772754237, time: 0.07365965843200684
Test Loss Energy: 10.255016030173243, Test Loss Force: 11.998310596500028, time: 10.160380125045776


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.177208018251224, Training Loss Force: 3.5449637391983, time: 1.1204833984375
Validation Loss Energy: 4.948152870130718, Validation Loss Force: 3.8323156954179924, time: 0.07329821586608887
Test Loss Energy: 12.510640097026007, Test Loss Force: 12.213658413552304, time: 9.85388970375061


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.109608779734605, Training Loss Force: 3.553045697166053, time: 1.1932101249694824
Validation Loss Energy: 3.2390479358073017, Validation Loss Force: 3.7372808379397315, time: 0.07343626022338867
Test Loss Energy: 11.66351891328868, Test Loss Force: 12.175419667288695, time: 9.698978662490845

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‡â–…â–‚â–â–ˆâ–…â–‚â–â–ˆâ–…â–ƒâ–‚â–ˆâ–†â–‚â–‚â–ˆâ–†
wandb:   test_error_force â–â–â–„â–„â–ƒâ–ƒâ–„â–…â–ƒâ–„â–…â–…â–„â–…â–‡â–‡â–…â–†â–ˆâ–ˆ
wandb:          test_loss â–â–‚â–ˆâ–†â–‚â–‚â–ˆâ–‡â–ƒâ–‚â–ˆâ–†â–‚â–ƒâ–ˆâ–‡â–‚â–‚â–ˆâ–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–â–â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–ƒâ–†â–â–‡â–ƒâ–…â–â–‡â–ƒâ–†â–‚â–‡â–„â–†â–â–‡â–„â–†â–‚
wandb:  valid_error_force â–ˆâ–â–†â–ƒâ–„â–‚â–‚â–â–â–…â–‡â–‚â–‡â–‚â–†â–‚â–‡â–ƒâ–ˆâ–‚
wandb:         valid_loss â–ˆâ–ƒâ–†â–â–ˆâ–‚â–…â–â–‡â–ƒâ–†â–‚â–ˆâ–ƒâ–†â–â–‡â–ƒâ–‡â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 2237
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.66352
wandb:   test_error_force 12.17542
wandb:          test_loss 11.29378
wandb: train_error_energy 4.10961
wandb:  train_error_force 3.55305
wandb:         train_loss 1.45734
wandb: valid_error_energy 3.23905
wandb:  valid_error_force 3.73728
wandb:         valid_loss 1.26602
wandb: 
wandb: ğŸš€ View run al_71_15 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/meusemq0
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_120256-meusemq0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.527524471282959, Uncertainty Bias: -0.13203416764736176
3.33786e-06 0.06410122
2.791064 4.4510465
(48745, 22, 3)
Found uncertainty sample 0 after 13 steps.
Found uncertainty sample 1 after 8 steps.
Found uncertainty sample 2 after 866 steps.
Found uncertainty sample 3 after 108 steps.
Found uncertainty sample 4 after 287 steps.
Found uncertainty sample 5 after 38 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 178 steps.
Found uncertainty sample 8 after 37 steps.
Found uncertainty sample 9 after 89 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 42 steps.
Found uncertainty sample 13 after 202 steps.
Found uncertainty sample 14 after 211 steps.
Found uncertainty sample 15 after 346 steps.
Found uncertainty sample 16 after 95 steps.
Found uncertainty sample 17 after 25 steps.
Found uncertainty sample 18 after 551 steps.
Found uncertainty sample 19 after 129 steps.
Found uncertainty sample 20 after 15 steps.
Found uncertainty sample 21 after 17 steps.
Found uncertainty sample 22 after 43 steps.
Found uncertainty sample 23 after 2979 steps.
Found uncertainty sample 24 after 548 steps.
Found uncertainty sample 25 after 174 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 24 steps.
Found uncertainty sample 30 after 619 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 11 steps.
Found uncertainty sample 33 after 698 steps.
Found uncertainty sample 34 after 69 steps.
Found uncertainty sample 35 after 45 steps.
Found uncertainty sample 36 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 93 steps.
Found uncertainty sample 39 after 64 steps.
Found uncertainty sample 40 after 77 steps.
Found uncertainty sample 41 after 73 steps.
Found uncertainty sample 42 after 4 steps.
Found uncertainty sample 43 after 347 steps.
Found uncertainty sample 44 after 19 steps.
Found uncertainty sample 45 after 37 steps.
Found uncertainty sample 46 after 14 steps.
Found uncertainty sample 47 after 11 steps.
Found uncertainty sample 48 after 71 steps.
Found uncertainty sample 49 after 519 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 114 steps.
Found uncertainty sample 52 after 9 steps.
Found uncertainty sample 53 after 98 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 124 steps.
Found uncertainty sample 56 after 85 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 18 steps.
Found uncertainty sample 59 after 13 steps.
Found uncertainty sample 60 after 64 steps.
Found uncertainty sample 61 after 59 steps.
Found uncertainty sample 62 after 38 steps.
Found uncertainty sample 63 after 1051 steps.
Found uncertainty sample 64 after 62 steps.
Found uncertainty sample 65 after 629 steps.
Found uncertainty sample 66 after 41 steps.
Found uncertainty sample 67 after 46 steps.
Found uncertainty sample 68 after 146 steps.
Found uncertainty sample 69 after 714 steps.
Found uncertainty sample 70 after 50 steps.
Found uncertainty sample 71 after 45 steps.
Found uncertainty sample 72 after 11 steps.
Found uncertainty sample 73 after 582 steps.
Found uncertainty sample 74 after 394 steps.
Found uncertainty sample 75 after 20 steps.
Found uncertainty sample 76 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 24 steps.
Found uncertainty sample 79 after 173 steps.
Found uncertainty sample 80 after 62 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 454 steps.
Found uncertainty sample 83 after 125 steps.
Found uncertainty sample 84 after 18 steps.
Found uncertainty sample 85 after 400 steps.
Found uncertainty sample 86 after 3 steps.
Found uncertainty sample 87 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 160 steps.
Found uncertainty sample 91 after 30 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 90 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 30 steps.
Found uncertainty sample 96 after 181 steps.
Found uncertainty sample 97 after 146 steps.
Found uncertainty sample 98 after 101 steps.
Found uncertainty sample 99 after 615 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_121027-m39o4qri
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_16
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/m39o4qri
Training model 16. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.056950367622353, Training Loss Force: 3.7416340456698496, time: 1.1561200618743896
Validation Loss Energy: 5.113707380128028, Validation Loss Force: 3.812498141430419, time: 0.08153748512268066
Test Loss Energy: 10.460011326763922, Test Loss Force: 11.825561718677827, time: 9.73462176322937


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.104337752444064, Training Loss Force: 3.5378647121687754, time: 1.1861605644226074
Validation Loss Energy: 5.275792704831783, Validation Loss Force: 3.7182339418662145, time: 0.07483983039855957
Test Loss Energy: 10.599339748173152, Test Loss Force: 11.940631999079072, time: 9.768567323684692


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.10514355548003, Training Loss Force: 3.558915880677149, time: 1.1506659984588623
Validation Loss Energy: 2.3100845057995967, Validation Loss Force: 3.70218884854895, time: 0.0778050422668457
Test Loss Energy: 10.339319193694083, Test Loss Force: 12.141841747649535, time: 10.120272397994995


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.072941352538919, Training Loss Force: 3.541417575307265, time: 1.1847169399261475
Validation Loss Energy: 2.9839445794145263, Validation Loss Force: 3.7988378346660414, time: 0.07893013954162598
Test Loss Energy: 11.46955766795913, Test Loss Force: 12.329559443391833, time: 9.859224319458008


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.105605310283438, Training Loss Force: 3.562856993398793, time: 1.2014293670654297
Validation Loss Energy: 5.1864121963106395, Validation Loss Force: 3.7504609586909163, time: 0.08083987236022949
Test Loss Energy: 12.925807106656304, Test Loss Force: 12.58120426647004, time: 9.807337999343872


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.070492200876678, Training Loss Force: 3.5407719415432535, time: 1.2110810279846191
Validation Loss Energy: 4.798139553173437, Validation Loss Force: 3.714128702157317, time: 0.07861661911010742
Test Loss Energy: 13.087344987023824, Test Loss Force: 12.599078807158634, time: 10.159701585769653


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.102873646379736, Training Loss Force: 3.5362914997480743, time: 1.1780216693878174
Validation Loss Energy: 1.9288490983502964, Validation Loss Force: 3.816835291054801, time: 0.07645535469055176
Test Loss Energy: 11.13916267680127, Test Loss Force: 12.62938033334717, time: 9.921492338180542


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.02269291493991, Training Loss Force: 3.539222045327368, time: 1.2002980709075928
Validation Loss Energy: 3.925023994325027, Validation Loss Force: 3.7834015187937435, time: 0.07990503311157227
Test Loss Energy: 10.357490035901678, Test Loss Force: 12.302786218240577, time: 10.043789148330688


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.1071872556050275, Training Loss Force: 3.5688313377453134, time: 1.183387279510498
Validation Loss Energy: 6.118278118635513, Validation Loss Force: 3.7763462469050215, time: 0.08925795555114746
Test Loss Energy: 10.858663961439975, Test Loss Force: 12.008503811760306, time: 9.758172988891602


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.092188371744773, Training Loss Force: 3.575948716024175, time: 1.1925876140594482
Validation Loss Energy: 5.560139200159256, Validation Loss Force: 3.7417666192190198, time: 0.07667684555053711
Test Loss Energy: 10.794809687718034, Test Loss Force: 12.437956760838231, time: 10.23258113861084


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.085770128830189, Training Loss Force: 3.5527160578931465, time: 1.1951994895935059
Validation Loss Energy: 2.3568224255261203, Validation Loss Force: 3.7813605334021694, time: 0.08090472221374512
Test Loss Energy: 10.368194494857775, Test Loss Force: 12.385668524112212, time: 9.934643983840942


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.134948680836425, Training Loss Force: 3.5390678881223536, time: 1.1907172203063965
Validation Loss Energy: 2.662898089458955, Validation Loss Force: 3.795761883084195, time: 0.07628250122070312
Test Loss Energy: 11.522280848641374, Test Loss Force: 12.797514060865007, time: 9.869919061660767


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.011140313478934, Training Loss Force: 3.57385884102889, time: 1.2442777156829834
Validation Loss Energy: 5.28922945177365, Validation Loss Force: 3.923698444899319, time: 0.07475447654724121
Test Loss Energy: 13.208963894726772, Test Loss Force: 13.158562969039039, time: 9.744621515274048


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.130309474415068, Training Loss Force: 3.589214694032689, time: 1.212360143661499
Validation Loss Energy: 4.777588749688324, Validation Loss Force: 3.7334384591576013, time: 0.0732872486114502
Test Loss Energy: 12.925728428670336, Test Loss Force: 13.173327388341246, time: 9.971790790557861


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.067270047173826, Training Loss Force: 3.563831948729107, time: 1.1637511253356934
Validation Loss Energy: 1.8865871067933535, Validation Loss Force: 3.740945081405119, time: 0.07922720909118652
Test Loss Energy: 11.284678670406596, Test Loss Force: 12.940716491733161, time: 9.881467580795288


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.072402235458611, Training Loss Force: 3.5458732581235743, time: 1.1429481506347656
Validation Loss Energy: 3.7604225082722875, Validation Loss Force: 3.729594855695169, time: 0.07710647583007812
Test Loss Energy: 10.600465274254816, Test Loss Force: 12.672213021553349, time: 9.910958766937256


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.097731299196451, Training Loss Force: 3.5518305606547758, time: 1.3974146842956543
Validation Loss Energy: 5.933965959785246, Validation Loss Force: 3.851614207626408, time: 0.07603740692138672
Test Loss Energy: 11.080546085020194, Test Loss Force: 12.690873572668997, time: 9.772536516189575


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.092114570649077, Training Loss Force: 3.6112141006241267, time: 1.2233848571777344
Validation Loss Energy: 5.207597606861647, Validation Loss Force: 3.793766990205828, time: 0.07872867584228516
Test Loss Energy: 10.901112609488818, Test Loss Force: 12.89181925195369, time: 9.870157718658447


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.067031968102895, Training Loss Force: 3.6019036691783333, time: 1.1977684497833252
Validation Loss Energy: 2.3822631547903814, Validation Loss Force: 3.8372662985394053, time: 0.07409405708312988
Test Loss Energy: 10.737971968266981, Test Loss Force: 13.164876112643752, time: 10.015013694763184


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.055940357354628, Training Loss Force: 3.5552350651368316, time: 1.2055318355560303
Validation Loss Energy: 3.2484743000964356, Validation Loss Force: 3.7292314596490153, time: 0.07891416549682617
Test Loss Energy: 12.097081677285257, Test Loss Force: 13.304685895782585, time: 9.850061178207397

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–„â–‡â–ˆâ–ƒâ–â–‚â–‚â–â–„â–ˆâ–‡â–ƒâ–‚â–ƒâ–‚â–‚â–…
wandb:   test_error_force â–â–‚â–‚â–ƒâ–…â–…â–…â–ƒâ–‚â–„â–„â–†â–‡â–‡â–†â–…â–…â–†â–‡â–ˆ
wandb:          test_loss â–â–â–‚â–„â–‡â–‡â–„â–â–â–‚â–‚â–„â–ˆâ–‡â–„â–‚â–‚â–‚â–‚â–…
wandb: train_error_energy â–ˆâ–‚â–‚â–â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–‚â–â–â–‚â–‚â–â–
wandb:  train_error_force â–ˆâ–â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–‚â–„â–ƒâ–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–‚â–‚â–
wandb: valid_error_energy â–†â–‡â–‚â–ƒâ–†â–†â–â–„â–ˆâ–‡â–‚â–‚â–‡â–†â–â–„â–ˆâ–†â–‚â–ƒ
wandb:  valid_error_force â–„â–‚â–â–„â–ƒâ–â–…â–„â–ƒâ–‚â–„â–„â–ˆâ–‚â–‚â–‚â–†â–„â–…â–‚
wandb:         valid_loss â–†â–†â–â–‚â–†â–…â–â–„â–ˆâ–‡â–‚â–‚â–‡â–…â–â–ƒâ–ˆâ–†â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2327
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.09708
wandb:   test_error_force 13.30469
wandb:          test_loss 11.31709
wandb: train_error_energy 4.05594
wandb:  train_error_force 3.55524
wandb:         train_loss 1.44598
wandb: valid_error_energy 3.24847
wandb:  valid_error_force 3.72923
wandb:         valid_loss 1.26471
wandb: 
wandb: ğŸš€ View run al_71_16 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/m39o4qri
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_121027-m39o4qri/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.7622134685516357, Uncertainty Bias: -0.17253144085407257
7.009506e-05 0.0568223
2.7537487 4.4460034
(48745, 22, 3)
Found uncertainty sample 0 after 36 steps.
Found uncertainty sample 1 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 140 steps.
Found uncertainty sample 4 after 196 steps.
Found uncertainty sample 5 after 47 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 47 steps.
Found uncertainty sample 8 after 41 steps.
Found uncertainty sample 9 after 160 steps.
Found uncertainty sample 10 after 245 steps.
Found uncertainty sample 11 after 54 steps.
Found uncertainty sample 12 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 273 steps.
Found uncertainty sample 15 after 664 steps.
Found uncertainty sample 16 after 9 steps.
Found uncertainty sample 17 after 49 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 13 steps.
Found uncertainty sample 20 after 23 steps.
Found uncertainty sample 21 after 589 steps.
Found uncertainty sample 22 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 539 steps.
Found uncertainty sample 25 after 73 steps.
Found uncertainty sample 26 after 175 steps.
Found uncertainty sample 27 after 89 steps.
Found uncertainty sample 28 after 840 steps.
Found uncertainty sample 29 after 11 steps.
Found uncertainty sample 30 after 3 steps.
Found uncertainty sample 31 after 143 steps.
Found uncertainty sample 32 after 15 steps.
Found uncertainty sample 33 after 66 steps.
Found uncertainty sample 34 after 33 steps.
Found uncertainty sample 35 after 1101 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 143 steps.
Found uncertainty sample 38 after 68 steps.
Found uncertainty sample 39 after 145 steps.
Found uncertainty sample 40 after 133 steps.
Found uncertainty sample 41 after 377 steps.
Found uncertainty sample 42 after 4 steps.
Found uncertainty sample 43 after 773 steps.
Found uncertainty sample 44 after 24 steps.
Found uncertainty sample 45 after 15 steps.
Found uncertainty sample 46 after 15 steps.
Found uncertainty sample 47 after 459 steps.
Found uncertainty sample 48 after 19 steps.
Found uncertainty sample 49 after 31 steps.
Found uncertainty sample 50 after 8 steps.
Found uncertainty sample 51 after 476 steps.
Found uncertainty sample 52 after 56 steps.
Found uncertainty sample 53 after 389 steps.
Found uncertainty sample 54 after 27 steps.
Found uncertainty sample 55 after 575 steps.
Found uncertainty sample 56 after 18 steps.
Found uncertainty sample 57 after 304 steps.
Found uncertainty sample 58 after 132 steps.
Found uncertainty sample 59 after 458 steps.
Found uncertainty sample 60 after 3 steps.
Found uncertainty sample 61 after 115 steps.
Found uncertainty sample 62 after 26 steps.
Found uncertainty sample 63 after 98 steps.
Found uncertainty sample 64 after 16 steps.
Found uncertainty sample 65 after 844 steps.
Found uncertainty sample 66 after 48 steps.
Found uncertainty sample 67 after 433 steps.
Found uncertainty sample 68 after 207 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 65 steps.
Found uncertainty sample 71 after 12 steps.
Found uncertainty sample 72 after 441 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 335 steps.
Found uncertainty sample 75 after 6 steps.
Found uncertainty sample 76 after 269 steps.
Found uncertainty sample 77 after 53 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1046 steps.
Found uncertainty sample 82 after 25 steps.
Found uncertainty sample 83 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 8 steps.
Found uncertainty sample 86 after 1377 steps.
Found uncertainty sample 87 after 22 steps.
Found uncertainty sample 88 after 50 steps.
Found uncertainty sample 89 after 16 steps.
Found uncertainty sample 90 after 85 steps.
Found uncertainty sample 91 after 1090 steps.
Found uncertainty sample 92 after 211 steps.
Found uncertainty sample 93 after 12 steps.
Found uncertainty sample 94 after 179 steps.
Found uncertainty sample 95 after 721 steps.
Found uncertainty sample 96 after 76 steps.
Found uncertainty sample 97 after 129 steps.
Found uncertainty sample 98 after 34 steps.
Found uncertainty sample 99 after 27 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_121854-6jj1rnii
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_17
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/6jj1rnii
Training model 17. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.7161864111736915, Training Loss Force: 4.015388571557454, time: 1.307342767715454
Validation Loss Energy: 1.4534558715212582, Validation Loss Force: 3.996723681982921, time: 0.09077811241149902
Test Loss Energy: 12.319143139295088, Test Loss Force: 16.536394946243156, time: 10.783199787139893


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.152851764539713, Training Loss Force: 3.7952858603980597, time: 1.2870385646820068
Validation Loss Energy: 4.043062021927077, Validation Loss Force: 4.905802979928421, time: 0.08447837829589844
Test Loss Energy: 13.580833082455383, Test Loss Force: 18.947358460608893, time: 10.920579195022583


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.56274395583522, Training Loss Force: 4.218341064849205, time: 1.295309066772461
Validation Loss Energy: 5.388177608157019, Validation Loss Force: 4.502694559605487, time: 0.09242630004882812
Test Loss Energy: 11.04022095450699, Test Loss Force: 14.015032638240264, time: 11.110020399093628


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 5.926440731479685, Training Loss Force: 3.755075149750833, time: 1.3343486785888672
Validation Loss Energy: 2.1703521573135216, Validation Loss Force: 3.774448660443425, time: 0.08339214324951172
Test Loss Energy: 10.931311069007966, Test Loss Force: 12.472037427771456, time: 10.794203281402588


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 5.799261304822788, Training Loss Force: 3.6236707327973154, time: 1.2659070491790771
Validation Loss Energy: 4.417882776620489, Validation Loss Force: 3.783899184334892, time: 0.08948493003845215
Test Loss Energy: 12.11994646436389, Test Loss Force: 12.223602803619757, time: 10.975605964660645


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 5.848648276895486, Training Loss Force: 3.6526440494645755, time: 1.33335280418396
Validation Loss Energy: 8.263129982688238, Validation Loss Force: 3.854677021728645, time: 0.08605790138244629
Test Loss Energy: 11.457399363677633, Test Loss Force: 11.694471696599352, time: 10.698711156845093


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 5.637602880819635, Training Loss Force: 3.7590054887053186, time: 1.2776529788970947
Validation Loss Energy: 1.37383873704161, Validation Loss Force: 4.497645091383282, time: 0.10218119621276855
Test Loss Energy: 10.383901483146957, Test Loss Force: 12.235858256934073, time: 11.383326053619385


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6248092619549186, Training Loss Force: 3.6271400938823013, time: 1.2886779308319092
Validation Loss Energy: 1.8791532416892678, Validation Loss Force: 3.753507740614646, time: 0.0867152214050293
Test Loss Energy: 11.437208723455733, Test Loss Force: 13.606236255828538, time: 11.097402811050415


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.588175369299659, Training Loss Force: 3.551951831136013, time: 1.2781519889831543
Validation Loss Energy: 3.617597619352994, Validation Loss Force: 3.740945192739692, time: 0.09393167495727539
Test Loss Energy: 11.07614303232346, Test Loss Force: 13.750580389516934, time: 10.860391855239868


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.644007978832886, Training Loss Force: 3.530683678488293, time: 1.3171768188476562
Validation Loss Energy: 1.849547708719574, Validation Loss Force: 3.7059520684497027, time: 0.09218382835388184
Test Loss Energy: 11.64474465868015, Test Loss Force: 14.478657126579092, time: 10.836348295211792


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6091489687554605, Training Loss Force: 3.5282866129039516, time: 1.3626680374145508
Validation Loss Energy: 2.3222562693234474, Validation Loss Force: 3.757077015689296, time: 0.09225249290466309
Test Loss Energy: 11.872758829398437, Test Loss Force: 14.359551820112854, time: 10.980812788009644


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.648789438369252, Training Loss Force: 3.521161378955551, time: 1.3874773979187012
Validation Loss Energy: 3.6038358867926457, Validation Loss Force: 3.7017609897850496, time: 0.08898711204528809
Test Loss Energy: 11.07571650549411, Test Loss Force: 13.659634476096668, time: 9.949121236801147


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5921109790672183, Training Loss Force: 3.521052215406759, time: 1.3044822216033936
Validation Loss Energy: 1.6276264527856712, Validation Loss Force: 3.750483336940082, time: 0.08198118209838867
Test Loss Energy: 11.81307954853552, Test Loss Force: 14.909914418539401, time: 11.539116621017456


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.615990116426729, Training Loss Force: 3.516546206393477, time: 1.2197444438934326
Validation Loss Energy: 2.1968622551511, Validation Loss Force: 3.723561189847964, time: 0.07297515869140625
Test Loss Energy: 11.941813833641092, Test Loss Force: 14.725171624555752, time: 9.337419748306274


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6697177568822856, Training Loss Force: 3.523968439964547, time: 1.2780046463012695
Validation Loss Energy: 3.714865625695765, Validation Loss Force: 3.7410586426695422, time: 0.08360052108764648
Test Loss Energy: 10.938876881610863, Test Loss Force: 13.866963433938004, time: 9.455841779708862


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6106296438645216, Training Loss Force: 3.531157779188117, time: 1.2216424942016602
Validation Loss Energy: 1.77475219625856, Validation Loss Force: 3.73726012970916, time: 0.07501697540283203
Test Loss Energy: 11.73136789014148, Test Loss Force: 14.566081822405685, time: 9.30264139175415


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.618182554925678, Training Loss Force: 3.526363952724581, time: 1.2379119396209717
Validation Loss Energy: 2.167346791850934, Validation Loss Force: 3.6845885225819526, time: 0.07962727546691895
Test Loss Energy: 11.934957512553934, Test Loss Force: 14.511489657171932, time: 9.203547954559326


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6415598927155557, Training Loss Force: 3.5173676123638056, time: 1.2340149879455566
Validation Loss Energy: 3.846778056446002, Validation Loss Force: 3.7348359307174444, time: 0.07764339447021484
Test Loss Energy: 10.961575937559727, Test Loss Force: 13.842479149679098, time: 9.323235034942627


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6594801766661074, Training Loss Force: 3.5245233051074827, time: 1.2109043598175049
Validation Loss Energy: 1.749635172265192, Validation Loss Force: 3.6962233194603984, time: 0.07756853103637695
Test Loss Energy: 11.429628207694366, Test Loss Force: 14.35433430197067, time: 9.776980638504028


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6085914975045585, Training Loss Force: 3.515920605708565, time: 1.2143158912658691
Validation Loss Energy: 2.2339895518702346, Validation Loss Force: 3.7737253197228786, time: 0.07491922378540039
Test Loss Energy: 11.680364745839228, Test Loss Force: 14.438993670839794, time: 9.294151306152344

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–ˆâ–‚â–‚â–…â–ƒâ–â–ƒâ–ƒâ–„â–„â–ƒâ–„â–„â–‚â–„â–„â–‚â–ƒâ–„
wandb:   test_error_force â–†â–ˆâ–ƒâ–‚â–‚â–â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–„â–„â–ƒâ–„â–„â–ƒâ–„â–„
wandb:          test_loss â–†â–ˆâ–ƒâ–‚â–‚â–â–â–„â–„â–…â–…â–„â–†â–†â–„â–…â–†â–„â–…â–…
wandb: train_error_energy â–ƒâ–‚â–…â–ˆâ–ˆâ–ˆâ–‡â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–†â–„â–ˆâ–ƒâ–‚â–‚â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–„â–ƒâ–ˆâ–†â–…â–…â–…â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–„â–…â–‚â–„â–ˆâ–â–‚â–ƒâ–â–‚â–ƒâ–â–‚â–ƒâ–â–‚â–„â–â–‚
wandb:  valid_error_force â–ƒâ–ˆâ–†â–‚â–‚â–‚â–†â–â–â–â–â–â–â–â–â–â–â–â–â–‚
wandb:         valid_loss â–â–ˆâ–†â–‚â–„â–‡â–ƒâ–â–ƒâ–â–‚â–ƒâ–â–‚â–ƒâ–â–â–„â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 2417
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.68036
wandb:   test_error_force 14.43899
wandb:          test_loss 14.99008
wandb: train_error_energy 2.60859
wandb:  train_error_force 3.51592
wandb:         train_loss 1.0104
wandb: valid_error_energy 2.23399
wandb:  valid_error_force 3.77373
wandb:         valid_loss 0.99121
wandb: 
wandb: ğŸš€ View run al_71_17 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/6jj1rnii
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_121854-6jj1rnii/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.33518385887146, Uncertainty Bias: -0.018787890672683716
7.6293945e-06 0.016522408
2.8246956 4.5812583
(48745, 22, 3)
Found uncertainty sample 0 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 217 steps.
Found uncertainty sample 3 after 28 steps.
Found uncertainty sample 4 after 90 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 127 steps.
Found uncertainty sample 7 after 15 steps.
Found uncertainty sample 8 after 33 steps.
Found uncertainty sample 9 after 65 steps.
Found uncertainty sample 10 after 325 steps.
Found uncertainty sample 11 after 20 steps.
Found uncertainty sample 12 after 203 steps.
Found uncertainty sample 13 after 57 steps.
Found uncertainty sample 14 after 47 steps.
Found uncertainty sample 15 after 812 steps.
Found uncertainty sample 16 after 257 steps.
Found uncertainty sample 17 after 69 steps.
Found uncertainty sample 18 after 11 steps.
Found uncertainty sample 19 after 48 steps.
Found uncertainty sample 20 after 64 steps.
Found uncertainty sample 21 after 175 steps.
Found uncertainty sample 22 after 98 steps.
Found uncertainty sample 23 after 189 steps.
Found uncertainty sample 24 after 14 steps.
Found uncertainty sample 25 after 96 steps.
Found uncertainty sample 26 after 656 steps.
Found uncertainty sample 27 after 5 steps.
Found uncertainty sample 28 after 70 steps.
Found uncertainty sample 29 after 83 steps.
Found uncertainty sample 30 after 82 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 37 steps.
Found uncertainty sample 33 after 44 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 62 steps.
Found uncertainty sample 39 after 1293 steps.
Found uncertainty sample 40 after 33 steps.
Found uncertainty sample 41 after 125 steps.
Found uncertainty sample 42 after 12 steps.
Found uncertainty sample 43 after 129 steps.
Found uncertainty sample 44 after 120 steps.
Found uncertainty sample 45 after 163 steps.
Found uncertainty sample 46 after 541 steps.
Found uncertainty sample 47 after 203 steps.
Found uncertainty sample 48 after 16 steps.
Found uncertainty sample 49 after 83 steps.
Found uncertainty sample 50 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 49 steps.
Found uncertainty sample 54 after 14 steps.
Found uncertainty sample 55 after 128 steps.
Found uncertainty sample 56 after 1078 steps.
Found uncertainty sample 57 after 214 steps.
Found uncertainty sample 58 after 41 steps.
Found uncertainty sample 59 after 24 steps.
Found uncertainty sample 60 after 93 steps.
Found uncertainty sample 61 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 44 steps.
Found uncertainty sample 64 after 59 steps.
Found uncertainty sample 65 after 98 steps.
Found uncertainty sample 66 after 5 steps.
Found uncertainty sample 67 after 158 steps.
Found uncertainty sample 68 after 29 steps.
Found uncertainty sample 69 after 563 steps.
Found uncertainty sample 70 after 47 steps.
Found uncertainty sample 71 after 51 steps.
Found uncertainty sample 72 after 12 steps.
Found uncertainty sample 73 after 108 steps.
Found uncertainty sample 74 after 110 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 27 steps.
Found uncertainty sample 77 after 51 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 34 steps.
Found uncertainty sample 80 after 41 steps.
Found uncertainty sample 81 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 206 steps.
Found uncertainty sample 85 after 68 steps.
Found uncertainty sample 86 after 39 steps.
Found uncertainty sample 87 after 181 steps.
Found uncertainty sample 88 after 13 steps.
Found uncertainty sample 89 after 19 steps.
Found uncertainty sample 90 after 18 steps.
Found uncertainty sample 91 after 241 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 24 steps.
Found uncertainty sample 94 after 347 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 65 steps.
Found uncertainty sample 97 after 767 steps.
Found uncertainty sample 98 after 513 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_122656-qyqmggwx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_18
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/qyqmggwx
Training model 18. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.068633992104089, Training Loss Force: 3.9639393736114057, time: 1.3305397033691406
Validation Loss Energy: 1.5802790876067432, Validation Loss Force: 5.841034769063138, time: 0.08022093772888184
Test Loss Energy: 11.336431228121093, Test Loss Force: 15.317858815171682, time: 9.929489612579346


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.9072258333751293, Training Loss Force: 4.259476115323268, time: 1.2428224086761475
Validation Loss Energy: 2.2992640099878088, Validation Loss Force: 4.315886056960224, time: 0.08769345283508301
Test Loss Energy: 11.176420223895605, Test Loss Force: 15.79924492412856, time: 10.018890619277954


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.657462635053437, Training Loss Force: 3.614251059931118, time: 1.2705049514770508
Validation Loss Energy: 1.4226480364981273, Validation Loss Force: 3.7750257075350966, time: 0.08204793930053711
Test Loss Energy: 11.631221921810516, Test Loss Force: 15.303101017653502, time: 10.133594989776611


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6826712711934233, Training Loss Force: 3.5438961445290005, time: 1.2811775207519531
Validation Loss Energy: 1.6096190319526316, Validation Loss Force: 3.7734192609818007, time: 0.0780332088470459
Test Loss Energy: 11.705196557178771, Test Loss Force: 14.94193156503084, time: 9.946863889694214


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6967948199764273, Training Loss Force: 3.560851726346769, time: 1.2828094959259033
Validation Loss Energy: 1.8903443702944527, Validation Loss Force: 3.755679321460307, time: 0.08771848678588867
Test Loss Energy: 10.854108136724609, Test Loss Force: 14.273826288056883, time: 10.02848505973816


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7057805516210367, Training Loss Force: 3.5524968558529286, time: 1.4853835105895996
Validation Loss Energy: 2.3701133241774044, Validation Loss Force: 3.7313696401227867, time: 0.08102679252624512
Test Loss Energy: 10.818847509847094, Test Loss Force: 14.368726805508821, time: 9.954159021377563


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6877341714386012, Training Loss Force: 3.5515989298382897, time: 1.2741291522979736
Validation Loss Energy: 1.5483011818890482, Validation Loss Force: 3.717647208666695, time: 0.08075284957885742
Test Loss Energy: 11.409898168510177, Test Loss Force: 14.594713712384479, time: 10.02102780342102


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7021130653600496, Training Loss Force: 3.550811433627046, time: 1.2906286716461182
Validation Loss Energy: 1.5158365770856412, Validation Loss Force: 3.7476695782775655, time: 0.07899761199951172
Test Loss Energy: 11.319492947071735, Test Loss Force: 14.595542964926512, time: 10.305951356887817


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6933413497692413, Training Loss Force: 3.561245005156027, time: 1.2743732929229736
Validation Loss Energy: 2.187818993127535, Validation Loss Force: 3.7456827014884517, time: 0.08002161979675293
Test Loss Energy: 10.69659502332007, Test Loss Force: 13.96629041349218, time: 9.973519086837769


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6756261490717193, Training Loss Force: 3.553179390338536, time: 1.285524845123291
Validation Loss Energy: 2.0474481408655114, Validation Loss Force: 3.782660030537804, time: 0.0795755386352539
Test Loss Energy: 10.63776034634637, Test Loss Force: 14.084012550519422, time: 10.02292275428772


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6843162536019487, Training Loss Force: 3.5484341868205576, time: 1.306562900543213
Validation Loss Energy: 1.4609987331722754, Validation Loss Force: 3.688229163117872, time: 0.08073759078979492
Test Loss Energy: 11.11492397212837, Test Loss Force: 14.161404254943015, time: 10.610235929489136


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6985842441957406, Training Loss Force: 3.5757182516048154, time: 1.2885634899139404
Validation Loss Energy: 1.6407885373360358, Validation Loss Force: 3.7167118869192253, time: 0.08815503120422363
Test Loss Energy: 11.31812092274811, Test Loss Force: 14.212246079529372, time: 9.999971866607666


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.8175533085734996, Training Loss Force: 3.6022652420557004, time: 1.2515652179718018
Validation Loss Energy: 4.95955464374795, Validation Loss Force: 4.187210233580619, time: 0.08554625511169434
Test Loss Energy: 13.079728802230054, Test Loss Force: 14.67781846823032, time: 10.06119990348816


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.9873838551683085, Training Loss Force: 3.8556523379551786, time: 1.4278552532196045
Validation Loss Energy: 3.0091977099233427, Validation Loss Force: 3.7311535397166633, time: 0.07823348045349121
Test Loss Energy: 11.643720800299418, Test Loss Force: 13.598216622521711, time: 9.99300742149353


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6762104164922893, Training Loss Force: 3.5528833249323557, time: 1.2514770030975342
Validation Loss Energy: 1.6241243124585187, Validation Loss Force: 3.7659876782364607, time: 0.08677339553833008
Test Loss Energy: 10.920183212248629, Test Loss Force: 13.268824621798448, time: 9.926495552062988


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6503558002483856, Training Loss Force: 3.5356053632888313, time: 1.2831764221191406
Validation Loss Energy: 2.741238995728718, Validation Loss Force: 3.71218791788141, time: 0.07860541343688965
Test Loss Energy: 10.332338798094552, Test Loss Force: 13.037663909881632, time: 10.12218689918518


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.654721705197156, Training Loss Force: 3.53811231910072, time: 1.252906084060669
Validation Loss Energy: 3.8155475953839004, Validation Loss Force: 3.7428432245395804, time: 0.08771872520446777
Test Loss Energy: 10.56769790761266, Test Loss Force: 13.009054734487338, time: 9.951460599899292


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.624010913051997, Training Loss Force: 3.5499845249769737, time: 1.283510446548462
Validation Loss Energy: 2.095459728106742, Validation Loss Force: 3.7517393022576875, time: 0.08866262435913086
Test Loss Energy: 10.426696720967568, Test Loss Force: 13.173178650014995, time: 9.887711524963379


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.651986999718638, Training Loss Force: 3.532115689615253, time: 1.278838872909546
Validation Loss Energy: 2.2341402988820276, Validation Loss Force: 3.751296747330156, time: 0.08231616020202637
Test Loss Energy: 11.464324140954579, Test Loss Force: 13.569978849936373, time: 10.153183460235596


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5922764967500234, Training Loss Force: 3.547828845245069, time: 1.26289701461792
Validation Loss Energy: 3.228523478625025, Validation Loss Force: 3.7607099741396803, time: 0.08352398872375488
Test Loss Energy: 11.817381329905714, Test Loss Force: 13.777108562026964, time: 9.943959712982178

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–ƒâ–„â–„â–‚â–‚â–„â–„â–‚â–‚â–ƒâ–„â–ˆâ–„â–‚â–â–‚â–â–„â–…
wandb:   test_error_force â–‡â–ˆâ–‡â–†â–„â–„â–…â–…â–ƒâ–„â–„â–„â–…â–‚â–‚â–â–â–â–‚â–ƒ
wandb:          test_loss â–ƒâ–ƒâ–…â–…â–„â–„â–…â–…â–„â–„â–…â–…â–ˆâ–ƒâ–‚â–â–â–â–‚â–ƒ
wandb: train_error_energy â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–…â–„â–„â–„â–„â–„â–„
wandb:  train_error_force â–…â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–‚â–„â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–‚â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: valid_error_energy â–â–ƒâ–â–â–‚â–ƒâ–â–â–ƒâ–‚â–â–â–ˆâ–„â–â–„â–†â–‚â–ƒâ–…
wandb:  valid_error_force â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–â–
wandb:         valid_loss â–ƒâ–‚â–â–â–â–‚â–â–â–‚â–‚â–â–â–ˆâ–‚â–â–‚â–ƒâ–‚â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2507
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.81738
wandb:   test_error_force 13.77711
wandb:          test_loss 16.1003
wandb: train_error_energy 2.59228
wandb:  train_error_force 3.54783
wandb:         train_loss 1.00801
wandb: valid_error_energy 3.22852
wandb:  valid_error_force 3.76071
wandb:         valid_loss 1.38201
wandb: 
wandb: ğŸš€ View run al_71_18 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/qyqmggwx
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_122656-qyqmggwx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.4413962364196777, Uncertainty Bias: -0.011341184377670288
6.1035156e-05 0.118478775
2.7960603 4.6948423
(48745, 22, 3)
Found uncertainty sample 0 after 51 steps.
Found uncertainty sample 1 after 13 steps.
Found uncertainty sample 2 after 50 steps.
Found uncertainty sample 3 after 1909 steps.
Found uncertainty sample 4 after 342 steps.
Found uncertainty sample 5 after 26 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 12 steps.
Found uncertainty sample 8 after 75 steps.
Found uncertainty sample 9 after 40 steps.
Found uncertainty sample 10 after 118 steps.
Found uncertainty sample 11 after 4 steps.
Found uncertainty sample 12 after 254 steps.
Found uncertainty sample 13 after 376 steps.
Found uncertainty sample 14 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 201 steps.
Found uncertainty sample 18 after 577 steps.
Found uncertainty sample 19 after 576 steps.
Found uncertainty sample 20 after 284 steps.
Found uncertainty sample 21 after 23 steps.
Found uncertainty sample 22 after 72 steps.
Found uncertainty sample 23 after 16 steps.
Found uncertainty sample 24 after 23 steps.
Found uncertainty sample 25 after 58 steps.
Found uncertainty sample 26 after 10 steps.
Found uncertainty sample 27 after 8 steps.
Found uncertainty sample 28 after 30 steps.
Found uncertainty sample 29 after 267 steps.
Found uncertainty sample 30 after 33 steps.
Found uncertainty sample 31 after 43 steps.
Found uncertainty sample 32 after 4 steps.
Found uncertainty sample 33 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 56 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 134 steps.
Found uncertainty sample 38 after 887 steps.
Found uncertainty sample 39 after 17 steps.
Found uncertainty sample 40 after 32 steps.
Found uncertainty sample 41 after 374 steps.
Found uncertainty sample 42 after 1160 steps.
Found uncertainty sample 43 after 893 steps.
Found uncertainty sample 44 after 129 steps.
Found uncertainty sample 45 after 8 steps.
Found uncertainty sample 46 after 94 steps.
Found uncertainty sample 47 after 29 steps.
Found uncertainty sample 48 after 47 steps.
Found uncertainty sample 49 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 560 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 16 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 9 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 154 steps.
Found uncertainty sample 59 after 77 steps.
Found uncertainty sample 60 after 88 steps.
Found uncertainty sample 61 after 600 steps.
Found uncertainty sample 62 after 341 steps.
Found uncertainty sample 63 after 23 steps.
Found uncertainty sample 64 after 194 steps.
Found uncertainty sample 65 after 37 steps.
Found uncertainty sample 66 after 48 steps.
Found uncertainty sample 67 after 10 steps.
Found uncertainty sample 68 after 45 steps.
Found uncertainty sample 69 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 5 steps.
Found uncertainty sample 73 after 9 steps.
Found uncertainty sample 74 after 11 steps.
Found uncertainty sample 75 after 14 steps.
Found uncertainty sample 76 after 98 steps.
Found uncertainty sample 77 after 24 steps.
Found uncertainty sample 78 after 55 steps.
Found uncertainty sample 79 after 79 steps.
Found uncertainty sample 80 after 48 steps.
Found uncertainty sample 81 after 244 steps.
Found uncertainty sample 82 after 11 steps.
Found uncertainty sample 83 after 1056 steps.
Found uncertainty sample 84 after 15 steps.
Found uncertainty sample 85 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 13 steps.
Found uncertainty sample 88 after 6 steps.
Found uncertainty sample 89 after 169 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 383 steps.
Found uncertainty sample 92 after 49 steps.
Found uncertainty sample 93 after 66 steps.
Found uncertainty sample 94 after 32 steps.
Found uncertainty sample 95 after 47 steps.
Found uncertainty sample 96 after 416 steps.
Found uncertainty sample 97 after 43 steps.
Found uncertainty sample 98 after 27 steps.
Found uncertainty sample 99 after 338 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_123508-c27z77kh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_19
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/c27z77kh
Training model 19. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.2128495808838458, Training Loss Force: 3.8504114303644275, time: 1.2989001274108887
Validation Loss Energy: 1.5781679614342017, Validation Loss Force: 3.772054633120982, time: 0.0838475227355957
Test Loss Energy: 10.965258652071965, Test Loss Force: 14.374533093515439, time: 9.860052585601807


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6800499481015514, Training Loss Force: 3.5636524125980142, time: 1.3129127025604248
Validation Loss Energy: 1.4806921491109937, Validation Loss Force: 3.7585478567327213, time: 0.07991290092468262
Test Loss Energy: 11.305528332025563, Test Loss Force: 14.760626097165773, time: 10.452434301376343


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7332886215832857, Training Loss Force: 3.5626682394123104, time: 1.355159044265747
Validation Loss Energy: 1.8257760517059016, Validation Loss Force: 3.739512650789366, time: 0.0836644172668457
Test Loss Energy: 10.80047887681595, Test Loss Force: 14.273841196397393, time: 10.080734729766846


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.734299752670773, Training Loss Force: 3.5697208094010295, time: 1.3416965007781982
Validation Loss Energy: 1.4833429696259777, Validation Loss Force: 3.8233991321384604, time: 0.08260917663574219
Test Loss Energy: 11.392417950397075, Test Loss Force: 14.822613969764618, time: 9.827123641967773


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7021287941612973, Training Loss Force: 3.5691905988633694, time: 1.3471894264221191
Validation Loss Energy: 1.8070057663789134, Validation Loss Force: 3.740732989043623, time: 0.08121895790100098
Test Loss Energy: 10.792112504514058, Test Loss Force: 14.356039980271074, time: 9.968343257904053


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.730592140316, Training Loss Force: 3.5695371616651865, time: 1.5484461784362793
Validation Loss Energy: 1.5284013516520547, Validation Loss Force: 3.806005109135165, time: 0.08340311050415039
Test Loss Energy: 11.297552747572194, Test Loss Force: 14.724917809708765, time: 9.956604480743408


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6837946307959053, Training Loss Force: 3.588562002631347, time: 1.411320686340332
Validation Loss Energy: 1.8487174868089822, Validation Loss Force: 3.7570494047152003, time: 0.08949661254882812
Test Loss Energy: 10.815682847343835, Test Loss Force: 14.308841134829576, time: 9.941065073013306


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.689171087301166, Training Loss Force: 3.5691163712889318, time: 1.3109710216522217
Validation Loss Energy: 1.694206642774966, Validation Loss Force: 3.7824761058232674, time: 0.09599709510803223
Test Loss Energy: 11.285965651437774, Test Loss Force: 14.299253369800953, time: 10.054762601852417


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.7169760876866365, Training Loss Force: 3.5806032262437264, time: 1.3277053833007812
Validation Loss Energy: 2.0335593755574113, Validation Loss Force: 3.821450777111592, time: 0.08886122703552246
Test Loss Energy: 10.540672980056783, Test Loss Force: 14.090286280519496, time: 9.893914937973022


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.692394699754753, Training Loss Force: 3.588816684540218, time: 1.3813245296478271
Validation Loss Energy: 1.5989516768459555, Validation Loss Force: 3.778918632213352, time: 0.07977914810180664
Test Loss Energy: 11.100095605978037, Test Loss Force: 14.448356957389954, time: 10.00035810470581


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.7124235531844245, Training Loss Force: 3.583923692987098, time: 1.3143749237060547
Validation Loss Energy: 2.0794373470423624, Validation Loss Force: 3.7828517486724476, time: 0.08709216117858887
Test Loss Energy: 10.497325131043103, Test Loss Force: 14.011115848325952, time: 10.076972961425781


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6960245136823202, Training Loss Force: 3.583270134604504, time: 1.3107247352600098
Validation Loss Energy: 1.5232079834942942, Validation Loss Force: 3.869545753947973, time: 0.08647394180297852
Test Loss Energy: 11.187485476816533, Test Loss Force: 14.414886707026403, time: 10.537737131118774


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.7432115801577384, Training Loss Force: 3.613351837396333, time: 1.3354506492614746
Validation Loss Energy: 2.0305688175930285, Validation Loss Force: 3.7678160145936737, time: 0.08001923561096191
Test Loss Energy: 10.627248040370944, Test Loss Force: 13.930132390319137, time: 9.827934980392456


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6986784556385635, Training Loss Force: 3.596492478313617, time: 1.5231308937072754
Validation Loss Energy: 1.5962005438807312, Validation Loss Force: 3.7400624209106614, time: 0.08344197273254395
Test Loss Energy: 11.026387954413167, Test Loss Force: 14.208060016166629, time: 9.86374568939209


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.7311001056628865, Training Loss Force: 3.5909325139195984, time: 1.3169047832489014
Validation Loss Energy: 2.0627616547038303, Validation Loss Force: 3.801150253746502, time: 0.08013224601745605
Test Loss Energy: 10.514540179348584, Test Loss Force: 13.863769318485518, time: 9.942949056625366


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7146357684337392, Training Loss Force: 3.5721943051692717, time: 1.3879244327545166
Validation Loss Energy: 1.5850872938093348, Validation Loss Force: 3.7529392662850922, time: 0.0847930908203125
Test Loss Energy: 11.162613552388121, Test Loss Force: 14.335574079015752, time: 10.018889427185059


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6915317291868455, Training Loss Force: 3.5650166005514956, time: 1.3270931243896484
Validation Loss Energy: 1.8954610289314409, Validation Loss Force: 3.763324555251164, time: 0.08109784126281738
Test Loss Energy: 10.53929230544487, Test Loss Force: 13.98932420062353, time: 9.947437763214111


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6817611577725287, Training Loss Force: 3.5624271158820444, time: 1.330176830291748
Validation Loss Energy: 1.9223154289359101, Validation Loss Force: 3.978754949913986, time: 0.08280467987060547
Test Loss Energy: 11.217053183427883, Test Loss Force: 14.400329698787496, time: 10.007493019104004


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 3.6160720845217305, Training Loss Force: 4.851463800851881, time: 1.368454933166504
Validation Loss Energy: 2.5990558392879883, Validation Loss Force: 3.850375499167612, time: 0.08243012428283691
Test Loss Energy: 10.070469642034375, Test Loss Force: 13.063325621951924, time: 10.122946500778198


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6449597978915556, Training Loss Force: 3.5895884883857336, time: 1.3429996967315674
Validation Loss Energy: 2.9214534856210492, Validation Loss Force: 3.7829040759217407, time: 0.09279894828796387
Test Loss Energy: 10.369667221594101, Test Loss Force: 13.020711471271504, time: 9.942087888717651

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–ˆâ–…â–ˆâ–…â–‡â–…â–‡â–ƒâ–†â–ƒâ–‡â–„â–†â–ƒâ–‡â–ƒâ–‡â–â–ƒ
wandb:   test_error_force â–†â–ˆâ–†â–ˆâ–†â–ˆâ–†â–†â–…â–‡â–…â–†â–…â–†â–„â–†â–…â–†â–â–
wandb:          test_loss â–…â–†â–…â–‡â–…â–†â–…â–‡â–…â–‡â–†â–‡â–†â–‡â–…â–‡â–†â–ˆâ–â–
wandb: train_error_energy â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–„
wandb:  train_error_force â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–
wandb:         train_loss â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–‚
wandb: valid_error_energy â–â–â–ƒâ–â–ƒâ–â–ƒâ–‚â–„â–‚â–„â–â–„â–‚â–„â–‚â–ƒâ–ƒâ–†â–ˆ
wandb:  valid_error_force â–‚â–‚â–â–ƒâ–â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–…â–‚â–â–ƒâ–â–‚â–ˆâ–„â–‚
wandb:         valid_loss â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–‚â–„â–‚â–„â–‚â–„â–‚â–ƒâ–†â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2597
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.36967
wandb:   test_error_force 13.02071
wandb:          test_loss 13.01906
wandb: train_error_energy 2.64496
wandb:  train_error_force 3.58959
wandb:         train_loss 1.0424
wandb: valid_error_energy 2.92145
wandb:  valid_error_force 3.7829
wandb:         valid_loss 1.19721
wandb: 
wandb: ğŸš€ View run al_71_19 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/c27z77kh
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_123508-c27z77kh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.5731303691864014, Uncertainty Bias: -0.041720807552337646
7.6293945e-05 0.016218185
2.6755064 4.7078834
(48745, 22, 3)
Found uncertainty sample 0 after 36 steps.
Found uncertainty sample 1 after 245 steps.
Found uncertainty sample 2 after 17 steps.
Found uncertainty sample 3 after 12 steps.
Found uncertainty sample 4 after 140 steps.
Found uncertainty sample 5 after 11 steps.
Found uncertainty sample 6 after 43 steps.
Found uncertainty sample 7 after 356 steps.
Found uncertainty sample 8 after 156 steps.
Found uncertainty sample 9 after 7 steps.
Found uncertainty sample 10 after 10 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 172 steps.
Found uncertainty sample 13 after 55 steps.
Found uncertainty sample 14 after 50 steps.
Found uncertainty sample 15 after 19 steps.
Found uncertainty sample 16 after 144 steps.
Found uncertainty sample 17 after 22 steps.
Found uncertainty sample 18 after 11 steps.
Found uncertainty sample 19 after 176 steps.
Found uncertainty sample 20 after 18 steps.
Found uncertainty sample 21 after 36 steps.
Found uncertainty sample 22 after 27 steps.
Found uncertainty sample 23 after 6 steps.
Found uncertainty sample 24 after 230 steps.
Found uncertainty sample 25 after 26 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 7 steps.
Found uncertainty sample 28 after 109 steps.
Found uncertainty sample 29 after 11 steps.
Found uncertainty sample 30 after 6 steps.
Found uncertainty sample 31 after 90 steps.
Found uncertainty sample 32 after 501 steps.
Found uncertainty sample 33 after 13 steps.
Found uncertainty sample 34 after 25 steps.
Found uncertainty sample 35 after 11 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 85 steps.
Found uncertainty sample 38 after 36 steps.
Found uncertainty sample 39 after 46 steps.
Found uncertainty sample 40 after 13 steps.
Found uncertainty sample 41 after 533 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 361 steps.
Found uncertainty sample 44 after 54 steps.
Found uncertainty sample 45 after 40 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 48 steps.
Found uncertainty sample 48 after 463 steps.
Found uncertainty sample 49 after 50 steps.
Found uncertainty sample 50 after 339 steps.
Found uncertainty sample 51 after 17 steps.
Found uncertainty sample 52 after 8 steps.
Found uncertainty sample 53 after 41 steps.
Found uncertainty sample 54 after 42 steps.
Found uncertainty sample 55 after 57 steps.
Found uncertainty sample 56 after 42 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 282 steps.
Found uncertainty sample 59 after 65 steps.
Found uncertainty sample 60 after 60 steps.
Found uncertainty sample 61 after 43 steps.
Found uncertainty sample 62 after 27 steps.
Found uncertainty sample 63 after 14 steps.
Found uncertainty sample 64 after 57 steps.
Found uncertainty sample 65 after 49 steps.
Found uncertainty sample 66 after 15 steps.
Found uncertainty sample 67 after 118 steps.
Found uncertainty sample 68 after 19 steps.
Found uncertainty sample 69 after 41 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 135 steps.
Found uncertainty sample 72 after 11 steps.
Found uncertainty sample 73 after 234 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 445 steps.
Found uncertainty sample 76 after 8 steps.
Found uncertainty sample 77 after 33 steps.
Found uncertainty sample 78 after 157 steps.
Found uncertainty sample 79 after 61 steps.
Found uncertainty sample 80 after 43 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 14 steps.
Found uncertainty sample 83 after 6 steps.
Found uncertainty sample 84 after 111 steps.
Found uncertainty sample 85 after 47 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 53 steps.
Found uncertainty sample 89 after 102 steps.
Found uncertainty sample 90 after 64 steps.
Found uncertainty sample 91 after 200 steps.
Found uncertainty sample 92 after 121 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 14 steps.
Found uncertainty sample 95 after 90 steps.
Found uncertainty sample 96 after 23 steps.
Found uncertainty sample 97 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_124240-qlhhwmeg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_20
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/qlhhwmeg
Training model 20. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.454156458906156, Training Loss Force: 3.8185236268373353, time: 1.3731510639190674
Validation Loss Energy: 1.9147150629779572, Validation Loss Force: 3.751231616604972, time: 0.08638429641723633
Test Loss Energy: 11.008932227574503, Test Loss Force: 12.969989748127103, time: 9.882401466369629


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6528850224547678, Training Loss Force: 3.576132568621796, time: 1.4016740322113037
Validation Loss Energy: 1.745410915897796, Validation Loss Force: 3.7657106778189267, time: 0.08587026596069336
Test Loss Energy: 10.78542225963555, Test Loss Force: 12.866889330956319, time: 9.85172414779663


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.605744172305396, Training Loss Force: 3.560888861560726, time: 1.3660714626312256
Validation Loss Energy: 1.6348461658386608, Validation Loss Force: 3.7161124615783874, time: 0.08718514442443848
Test Loss Energy: 10.976532845595033, Test Loss Force: 13.344455654471627, time: 10.228882074356079


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.669549096235365, Training Loss Force: 3.567984372945104, time: 1.359827995300293
Validation Loss Energy: 2.2508253433246885, Validation Loss Force: 3.787590148097791, time: 0.08327412605285645
Test Loss Energy: 11.362335996970948, Test Loss Force: 13.351390154579343, time: 10.509191751480103


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.661885647313123, Training Loss Force: 3.5654651127838397, time: 1.4043734073638916
Validation Loss Energy: 1.7258652966046872, Validation Loss Force: 3.762958041836967, time: 0.08754205703735352
Test Loss Energy: 11.02354776865361, Test Loss Force: 13.217784768563908, time: 10.11442756652832


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6903796821769244, Training Loss Force: 3.557744017735623, time: 1.3220951557159424
Validation Loss Energy: 1.7538298136361796, Validation Loss Force: 3.777295485474112, time: 0.09423565864562988
Test Loss Energy: 11.090171305097613, Test Loss Force: 13.397052578250713, time: 10.017359972000122


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6693188934499994, Training Loss Force: 3.551734153629623, time: 1.4136877059936523
Validation Loss Energy: 1.8765677189013275, Validation Loss Force: 3.739142463334261, time: 0.08481121063232422
Test Loss Energy: 11.144280117403335, Test Loss Force: 13.571869868241068, time: 9.975106477737427


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6928643557229335, Training Loss Force: 3.568947737750846, time: 1.396587610244751
Validation Loss Energy: 1.8741979068485044, Validation Loss Force: 3.7704530852888967, time: 0.08403468132019043
Test Loss Energy: 11.08399424381785, Test Loss Force: 13.18671586620815, time: 10.10491681098938


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6654786141198166, Training Loss Force: 3.556498820061718, time: 1.34201979637146
Validation Loss Energy: 1.8728331119858266, Validation Loss Force: 3.7301156788274943, time: 0.08762145042419434
Test Loss Energy: 11.004737779727481, Test Loss Force: 13.514040643886446, time: 10.038366556167603


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6602125299063837, Training Loss Force: 3.5536153633057017, time: 1.3604328632354736
Validation Loss Energy: 1.7130241896281775, Validation Loss Force: 3.7101411430920423, time: 0.08759856224060059
Test Loss Energy: 11.191952387760056, Test Loss Force: 13.537238045041969, time: 9.961254119873047


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.644148410305847, Training Loss Force: 3.5615982642530666, time: 1.421865701675415
Validation Loss Energy: 1.974576322823454, Validation Loss Force: 3.752175733783706, time: 0.08727073669433594
Test Loss Energy: 11.091111882886155, Test Loss Force: 13.470810242937883, time: 10.243175506591797


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6296931839683877, Training Loss Force: 3.5592562169220825, time: 1.3818914890289307
Validation Loss Energy: 1.7679752608022477, Validation Loss Force: 3.743576696706267, time: 0.09151387214660645
Test Loss Energy: 11.107907814231872, Test Loss Force: 13.839994301907385, time: 9.904466390609741


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6504220896349535, Training Loss Force: 3.555366210800133, time: 1.368577003479004
Validation Loss Energy: 1.897892074999394, Validation Loss Force: 3.717213894508719, time: 0.09047746658325195
Test Loss Energy: 11.32510654116321, Test Loss Force: 13.9371113356547, time: 10.091257333755493


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.643336540044032, Training Loss Force: 3.562190603943582, time: 1.441544532775879
Validation Loss Energy: 1.7833882520815099, Validation Loss Force: 3.7486564479323174, time: 0.08825874328613281
Test Loss Energy: 11.175873479043597, Test Loss Force: 13.925109995633978, time: 10.060216903686523


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.699093727868236, Training Loss Force: 3.5541862578368066, time: 1.373523235321045
Validation Loss Energy: 1.801194769677636, Validation Loss Force: 3.781752653768148, time: 0.08321905136108398
Test Loss Energy: 11.093351207030588, Test Loss Force: 13.652486600656806, time: 10.002366065979004


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.639585111004721, Training Loss Force: 3.596094449327455, time: 1.352036952972412
Validation Loss Energy: 1.4923792175714463, Validation Loss Force: 3.790345678778648, time: 0.09336996078491211
Test Loss Energy: 11.103200569834044, Test Loss Force: 14.23750895035606, time: 10.135017395019531


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6464578074478977, Training Loss Force: 3.560062806744151, time: 1.3140172958374023
Validation Loss Energy: 1.8418722475921567, Validation Loss Force: 3.7911192313916002, time: 0.0871269702911377
Test Loss Energy: 11.206798129098617, Test Loss Force: 13.969785372447923, time: 9.944417953491211


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6572966787658707, Training Loss Force: 3.5620456542820524, time: 1.3927373886108398
Validation Loss Energy: 1.8814568653384638, Validation Loss Force: 3.7658758983252048, time: 0.08350539207458496
Test Loss Energy: 11.349760981955438, Test Loss Force: 13.962243426246888, time: 10.070777416229248


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6768097592046196, Training Loss Force: 3.5665866707636473, time: 1.3910388946533203
Validation Loss Energy: 1.921137843157671, Validation Loss Force: 3.8951927861759352, time: 0.08579206466674805
Test Loss Energy: 11.443301170460044, Test Loss Force: 13.919406317300387, time: 10.06575894355774


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.664441203870586, Training Loss Force: 3.5991348268624157, time: 1.4489822387695312
Validation Loss Energy: 1.9042554025171454, Validation Loss Force: 3.7986345377358095, time: 0.08336973190307617
Test Loss Energy: 11.40076356200708, Test Loss Force: 14.418338498478821, time: 10.478875875473022

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–â–ƒâ–‡â–„â–„â–…â–„â–ƒâ–…â–„â–„â–‡â–…â–„â–„â–…â–‡â–ˆâ–ˆ
wandb:   test_error_force â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–„â–„â–„â–…â–†â–†â–…â–‡â–†â–†â–†â–ˆ
wandb:          test_loss â–ˆâ–‚â–…â–ˆâ–ƒâ–„â–ƒâ–ƒâ–â–…â–ƒâ–„â–„â–„â–â–‚â–ƒâ–ƒâ–„â–„
wandb: train_error_energy â–ˆâ–â–â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–‚â–
wandb:  train_error_force â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–ƒâ–‚â–ˆâ–ƒâ–ƒâ–…â–…â–…â–ƒâ–…â–„â–…â–„â–„â–â–„â–…â–…â–…
wandb:  valid_error_force â–ƒâ–ƒâ–â–„â–ƒâ–„â–‚â–ƒâ–‚â–â–ƒâ–‚â–â–‚â–„â–„â–„â–ƒâ–ˆâ–„
wandb:         valid_loss â–„â–ƒâ–â–ˆâ–ƒâ–ƒâ–„â–„â–„â–‚â–…â–ƒâ–„â–ƒâ–„â–â–„â–„â–†â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 2687
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.40076
wandb:   test_error_force 14.41834
wandb:          test_loss 14.78272
wandb: train_error_energy 2.66444
wandb:  train_error_force 3.59913
wandb:         train_loss 1.05222
wandb: valid_error_energy 1.90426
wandb:  valid_error_force 3.79863
wandb:         valid_loss 0.88535
wandb: 
wandb: ğŸš€ View run al_71_20 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/qlhhwmeg
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_124240-qlhhwmeg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8363029956817627, Uncertainty Bias: -0.06291013956069946
9.918213e-05 0.0065641403
2.7322333 4.728093
(48745, 22, 3)
Found uncertainty sample 0 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 14 steps.
Found uncertainty sample 3 after 16 steps.
Found uncertainty sample 4 after 4 steps.
Found uncertainty sample 5 after 29 steps.
Found uncertainty sample 6 after 32 steps.
Found uncertainty sample 7 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 52 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 70 steps.
Found uncertainty sample 13 after 11 steps.
Found uncertainty sample 14 after 14 steps.
Found uncertainty sample 15 after 6 steps.
Found uncertainty sample 16 after 394 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 227 steps.
Found uncertainty sample 19 after 11 steps.
Found uncertainty sample 20 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 27 steps.
Found uncertainty sample 23 after 34 steps.
Found uncertainty sample 24 after 29 steps.
Found uncertainty sample 25 after 20 steps.
Found uncertainty sample 26 after 25 steps.
Found uncertainty sample 27 after 34 steps.
Found uncertainty sample 28 after 294 steps.
Found uncertainty sample 29 after 44 steps.
Found uncertainty sample 30 after 22 steps.
Found uncertainty sample 31 after 7 steps.
Found uncertainty sample 32 after 37 steps.
Found uncertainty sample 33 after 18 steps.
Found uncertainty sample 34 after 92 steps.
Found uncertainty sample 35 after 10 steps.
Found uncertainty sample 36 after 43 steps.
Found uncertainty sample 37 after 23 steps.
Found uncertainty sample 38 after 48 steps.
Found uncertainty sample 39 after 35 steps.
Found uncertainty sample 40 after 65 steps.
Found uncertainty sample 41 after 120 steps.
Found uncertainty sample 42 after 9 steps.
Found uncertainty sample 43 after 23 steps.
Found uncertainty sample 44 after 66 steps.
Found uncertainty sample 45 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 47 steps.
Found uncertainty sample 48 after 113 steps.
Found uncertainty sample 49 after 15 steps.
Found uncertainty sample 50 after 18 steps.
Found uncertainty sample 51 after 31 steps.
Found uncertainty sample 52 after 163 steps.
Found uncertainty sample 53 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 20 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 241 steps.
Found uncertainty sample 61 after 56 steps.
Found uncertainty sample 62 after 6 steps.
Found uncertainty sample 63 after 35 steps.
Found uncertainty sample 64 after 278 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 5 steps.
Found uncertainty sample 67 after 14 steps.
Found uncertainty sample 68 after 67 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 108 steps.
Found uncertainty sample 71 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 62 steps.
Found uncertainty sample 74 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 421 steps.
Found uncertainty sample 77 after 154 steps.
Found uncertainty sample 78 after 14 steps.
Found uncertainty sample 79 after 141 steps.
Found uncertainty sample 80 after 89 steps.
Found uncertainty sample 81 after 14 steps.
Found uncertainty sample 82 after 53 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 80 steps.
Found uncertainty sample 85 after 54 steps.
Found uncertainty sample 86 after 191 steps.
Found uncertainty sample 87 after 11 steps.
Found uncertainty sample 88 after 7 steps.
Found uncertainty sample 89 after 46 steps.
Found uncertainty sample 90 after 50 steps.
Found uncertainty sample 91 after 102 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 128 steps.
Found uncertainty sample 96 after 278 steps.
Found uncertainty sample 97 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_124942-ermyw3hk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_21
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ermyw3hk
Training model 21. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.358296967466459, Training Loss Force: 3.7624590863504213, time: 1.433523416519165
Validation Loss Energy: 5.639294741499395, Validation Loss Force: 3.7596868094141422, time: 0.08808517456054688
Test Loss Energy: 13.003092186367407, Test Loss Force: 13.345956104099526, time: 9.973463296890259


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.110721357456786, Training Loss Force: 3.5877993483215915, time: 1.4445483684539795
Validation Loss Energy: 3.3155946854337515, Validation Loss Force: 3.800392844646628, time: 0.09220051765441895
Test Loss Energy: 11.568484211445663, Test Loss Force: 12.694768142300996, time: 9.916642665863037


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.121924279848458, Training Loss Force: 3.5790117591064456, time: 1.422835111618042
Validation Loss Energy: 2.296729812438005, Validation Loss Force: 3.7544335989234137, time: 0.08672547340393066
Test Loss Energy: 10.113736355540828, Test Loss Force: 12.215268094637254, time: 10.629883527755737


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.100755962353672, Training Loss Force: 3.579014758741377, time: 1.422203779220581
Validation Loss Energy: 5.008802378752799, Validation Loss Force: 3.7938961383104663, time: 0.09008932113647461
Test Loss Energy: 10.443183340387645, Test Loss Force: 12.078525367972503, time: 10.780091047286987


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.050778677486345, Training Loss Force: 3.579207966455823, time: 1.5777394771575928
Validation Loss Energy: 5.76942169970301, Validation Loss Force: 3.7659052906524675, time: 0.09250926971435547
Test Loss Energy: 10.712168619966963, Test Loss Force: 12.153074471084341, time: 11.246663331985474


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.140292724789366, Training Loss Force: 3.576476942178097, time: 1.4597351551055908
Validation Loss Energy: 3.65491540952647, Validation Loss Force: 3.746372864506255, time: 0.09589338302612305
Test Loss Energy: 10.294291934344377, Test Loss Force: 12.159435272881122, time: 10.972820520401001


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.136206473383844, Training Loss Force: 3.5735713785623644, time: 1.5387508869171143
Validation Loss Energy: 2.0330668314066194, Validation Loss Force: 3.7278919934016863, time: 0.10204815864562988
Test Loss Energy: 11.115498985469648, Test Loss Force: 12.481826802807513, time: 10.906269788742065


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.064777179012447, Training Loss Force: 3.604346035882752, time: 1.4738106727600098
Validation Loss Energy: 5.193102068130894, Validation Loss Force: 3.8438637626640997, time: 0.09536361694335938
Test Loss Energy: 12.521237362355775, Test Loss Force: 12.627222391861052, time: 11.1183500289917


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.048317075196945, Training Loss Force: 3.583980754187428, time: 1.509310245513916
Validation Loss Energy: 5.60644970646017, Validation Loss Force: 3.7599203893482205, time: 0.09450554847717285
Test Loss Energy: 13.013631963420085, Test Loss Force: 12.922375469676114, time: 11.143552780151367


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.140251577066848, Training Loss Force: 3.579904268231634, time: 1.5903618335723877
Validation Loss Energy: 2.796766111983304, Validation Loss Force: 3.7732377856278694, time: 0.10666990280151367
Test Loss Energy: 11.495542122052244, Test Loss Force: 12.685436416479206, time: 11.365599870681763


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.107085029535935, Training Loss Force: 3.572012167301283, time: 1.4977033138275146
Validation Loss Energy: 2.669685379030553, Validation Loss Force: 3.7440401825335154, time: 0.09780073165893555
Test Loss Energy: 10.318073358198037, Test Loss Force: 12.351862124944807, time: 11.085736751556396


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.086396506463774, Training Loss Force: 3.5810847551073075, time: 1.5387160778045654
Validation Loss Energy: 5.310936798802713, Validation Loss Force: 3.752238081144566, time: 0.09308743476867676
Test Loss Energy: 10.789050596243898, Test Loss Force: 12.489702570861544, time: 11.21140170097351


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.100069673011183, Training Loss Force: 3.5738728265164728, time: 1.4995272159576416
Validation Loss Energy: 5.978150198411697, Validation Loss Force: 3.7553594572321813, time: 0.1015481948852539
Test Loss Energy: 10.91326474898265, Test Loss Force: 12.136670824483392, time: 11.812864542007446


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.1420770737525965, Training Loss Force: 3.5992852012964427, time: 1.4681634902954102
Validation Loss Energy: 3.8026753147269607, Validation Loss Force: 3.9196681333580257, time: 0.09464430809020996
Test Loss Energy: 10.522970778292295, Test Loss Force: 12.678072111292726, time: 11.081019163131714


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.134574147301018, Training Loss Force: 3.576663664333742, time: 1.5190789699554443
Validation Loss Energy: 2.0424245022642347, Validation Loss Force: 3.8127682391044297, time: 0.09583187103271484
Test Loss Energy: 11.36771983847288, Test Loss Force: 13.009955110585677, time: 11.288339853286743


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.066191957339701, Training Loss Force: 3.610040192708389, time: 1.4537220001220703
Validation Loss Energy: 4.879019221710497, Validation Loss Force: 3.7763066117970463, time: 0.09302115440368652
Test Loss Energy: 12.876679354181094, Test Loss Force: 13.089493047129412, time: 11.137521266937256


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.077831853853471, Training Loss Force: 3.5565545059039603, time: 1.5146143436431885
Validation Loss Energy: 5.775462936301176, Validation Loss Force: 3.738257353479198, time: 0.09862875938415527
Test Loss Energy: 13.318882296585791, Test Loss Force: 13.2338152916247, time: 11.533257007598877


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.119770986026334, Training Loss Force: 3.611792688466382, time: 1.534815788269043
Validation Loss Energy: 3.914797913798293, Validation Loss Force: 3.793964052399974, time: 0.10059475898742676
Test Loss Energy: 12.173559897276082, Test Loss Force: 13.118426321545927, time: 11.1826171875


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.152946876703423, Training Loss Force: 3.6157714994495787, time: 1.5308599472045898
Validation Loss Energy: 2.082097352659616, Validation Loss Force: 3.7172965047618582, time: 0.1032712459564209
Test Loss Energy: 10.458015381161792, Test Loss Force: 12.67799956545182, time: 11.102419376373291


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.1128919874796015, Training Loss Force: 3.5771345814238096, time: 1.7299549579620361
Validation Loss Energy: 5.135202300791875, Validation Loss Force: 3.765946150440745, time: 0.10685920715332031
Test Loss Energy: 10.8064909763986, Test Loss Force: 12.557621216343454, time: 11.257721662521362

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–„â–â–‚â–‚â–â–ƒâ–†â–‡â–„â–â–‚â–ƒâ–‚â–„â–‡â–ˆâ–…â–‚â–ƒ
wandb:   test_error_force â–ˆâ–„â–‚â–â–â–â–ƒâ–„â–†â–„â–ƒâ–ƒâ–â–„â–†â–‡â–‡â–‡â–„â–„
wandb:          test_loss â–ˆâ–„â–‚â–â–â–â–ƒâ–„â–…â–ƒâ–â–‚â–â–‚â–ƒâ–…â–…â–„â–‚â–
wandb: train_error_energy â–ˆâ–‚â–ƒâ–‚â–â–ƒâ–ƒâ–â–â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–â–‚â–ƒâ–ƒâ–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–‚
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–ƒâ–â–†â–ˆâ–„â–â–‡â–‡â–‚â–‚â–‡â–ˆâ–„â–â–†â–ˆâ–„â–â–‡
wandb:  valid_error_force â–‚â–„â–‚â–„â–ƒâ–‚â–â–…â–‚â–ƒâ–‚â–‚â–‚â–ˆâ–„â–ƒâ–‚â–„â–â–ƒ
wandb:         valid_loss â–ˆâ–ƒâ–â–…â–‡â–ƒâ–â–†â–†â–‚â–‚â–†â–‡â–ƒâ–â–…â–‡â–ƒâ–â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 2777
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.80649
wandb:   test_error_force 12.55762
wandb:          test_loss 9.53698
wandb: train_error_energy 4.11289
wandb:  train_error_force 3.57713
wandb:         train_loss 1.46666
wandb: valid_error_energy 5.1352
wandb:  valid_error_force 3.76595
wandb:         valid_loss 1.78825
wandb: 
wandb: ğŸš€ View run al_71_21 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ermyw3hk
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_124942-ermyw3hk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.5531280040740967, Uncertainty Bias: -0.14875325560569763
3.0517578e-05 0.10920334
2.8350267 4.379827
(48745, 22, 3)
Found uncertainty sample 0 after 126 steps.
Found uncertainty sample 1 after 26 steps.
Found uncertainty sample 2 after 224 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 6 steps.
Found uncertainty sample 5 after 447 steps.
Found uncertainty sample 6 after 24 steps.
Found uncertainty sample 7 after 328 steps.
Found uncertainty sample 8 after 89 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 4 steps.
Found uncertainty sample 11 after 801 steps.
Found uncertainty sample 12 after 94 steps.
Found uncertainty sample 13 after 9 steps.
Found uncertainty sample 14 after 233 steps.
Found uncertainty sample 15 after 904 steps.
Found uncertainty sample 16 after 11 steps.
Found uncertainty sample 17 after 8 steps.
Found uncertainty sample 18 after 185 steps.
Found uncertainty sample 19 after 244 steps.
Found uncertainty sample 20 after 22 steps.
Found uncertainty sample 21 after 47 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 43 steps.
Found uncertainty sample 24 after 23 steps.
Found uncertainty sample 25 after 13 steps.
Found uncertainty sample 26 after 68 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 955 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 557 steps.
Found uncertainty sample 31 after 109 steps.
Found uncertainty sample 32 after 205 steps.
Found uncertainty sample 33 after 94 steps.
Found uncertainty sample 34 after 33 steps.
Found uncertainty sample 35 after 34 steps.
Found uncertainty sample 36 after 167 steps.
Found uncertainty sample 37 after 234 steps.
Found uncertainty sample 38 after 71 steps.
Found uncertainty sample 39 after 910 steps.
Found uncertainty sample 40 after 58 steps.
Found uncertainty sample 41 after 50 steps.
Found uncertainty sample 42 after 40 steps.
Found uncertainty sample 43 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 189 steps.
Found uncertainty sample 46 after 3502 steps.
Found uncertainty sample 47 after 1304 steps.
Found uncertainty sample 48 after 25 steps.
Found uncertainty sample 49 after 569 steps.
Found uncertainty sample 50 after 8 steps.
Found uncertainty sample 51 after 44 steps.
Found uncertainty sample 52 after 360 steps.
Found uncertainty sample 53 after 193 steps.
Found uncertainty sample 54 after 561 steps.
Found uncertainty sample 55 after 20 steps.
Found uncertainty sample 56 after 217 steps.
Found uncertainty sample 57 after 243 steps.
Found uncertainty sample 58 after 141 steps.
Found uncertainty sample 59 after 488 steps.
Found uncertainty sample 60 after 93 steps.
Found uncertainty sample 61 after 55 steps.
Found uncertainty sample 62 after 1627 steps.
Found uncertainty sample 63 after 186 steps.
Found uncertainty sample 64 after 130 steps.
Found uncertainty sample 65 after 93 steps.
Found uncertainty sample 66 after 40 steps.
Found uncertainty sample 67 after 13 steps.
Found uncertainty sample 68 after 42 steps.
Found uncertainty sample 69 after 47 steps.
Found uncertainty sample 70 after 17 steps.
Found uncertainty sample 71 after 295 steps.
Found uncertainty sample 72 after 264 steps.
Found uncertainty sample 73 after 260 steps.
Found uncertainty sample 74 after 190 steps.
Found uncertainty sample 75 after 4 steps.
Found uncertainty sample 76 after 281 steps.
Found uncertainty sample 77 after 800 steps.
Found uncertainty sample 78 after 420 steps.
Found uncertainty sample 79 after 210 steps.
Found uncertainty sample 80 after 49 steps.
Found uncertainty sample 81 after 37 steps.
Found uncertainty sample 82 after 11 steps.
Found uncertainty sample 83 after 20 steps.
Found uncertainty sample 84 after 269 steps.
Found uncertainty sample 85 after 113 steps.
Found uncertainty sample 86 after 9 steps.
Found uncertainty sample 87 after 99 steps.
Found uncertainty sample 88 after 783 steps.
Found uncertainty sample 89 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 615 steps.
Found uncertainty sample 93 after 17 steps.
Found uncertainty sample 94 after 14 steps.
Found uncertainty sample 95 after 72 steps.
Found uncertainty sample 96 after 10 steps.
Found uncertainty sample 97 after 21 steps.
Found uncertainty sample 98 after 11 steps.
Found uncertainty sample 99 after 39 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_125909-1eozlu7e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_22
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/1eozlu7e
Training model 22. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.8293806365011815, Training Loss Force: 3.829284200968182, time: 1.481266975402832
Validation Loss Energy: 2.485416417994596, Validation Loss Force: 3.738288193155909, time: 0.0914449691772461
Test Loss Energy: 11.722602839808971, Test Loss Force: 13.554225635066173, time: 9.962083339691162


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.618104472180844, Training Loss Force: 3.5562382365454925, time: 1.4742681980133057
Validation Loss Energy: 2.4389807716336698, Validation Loss Force: 3.688853082064762, time: 0.11151266098022461
Test Loss Energy: 11.88942440486342, Test Loss Force: 14.336751208380532, time: 9.971587181091309


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6422287356834997, Training Loss Force: 3.54307234187863, time: 1.4724972248077393
Validation Loss Energy: 2.261845462025801, Validation Loss Force: 3.694201594949947, time: 0.09281420707702637
Test Loss Energy: 11.877694334152789, Test Loss Force: 14.6292607021364, time: 10.200006008148193


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.661775906238325, Training Loss Force: 3.54266859323503, time: 1.452538013458252
Validation Loss Energy: 2.1745209690635816, Validation Loss Force: 3.7090143258790746, time: 0.09592270851135254
Test Loss Energy: 11.754490544412393, Test Loss Force: 14.308805055433092, time: 9.962943315505981


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.661945316635699, Training Loss Force: 3.5455045223221773, time: 1.4974889755249023
Validation Loss Energy: 2.1901298535239735, Validation Loss Force: 3.740212388580444, time: 0.09032773971557617
Test Loss Energy: 11.613276574665129, Test Loss Force: 14.4182128104895, time: 10.197158098220825


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.69574449886163, Training Loss Force: 3.545411080452139, time: 1.4923529624938965
Validation Loss Energy: 2.2490150431682956, Validation Loss Force: 3.7179079542365763, time: 0.08861517906188965
Test Loss Energy: 11.63075793526914, Test Loss Force: 14.479967614943563, time: 9.989724159240723


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6462932516797086, Training Loss Force: 3.550171707206449, time: 1.4589285850524902
Validation Loss Energy: 2.1917792752224337, Validation Loss Force: 3.7060875626249996, time: 0.09492158889770508
Test Loss Energy: 11.5518819859356, Test Loss Force: 14.477272702731714, time: 10.617804050445557


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6661330490779087, Training Loss Force: 3.550281839366026, time: 1.4672918319702148
Validation Loss Energy: 2.113077088263403, Validation Loss Force: 3.7330967733531755, time: 0.08820629119873047
Test Loss Energy: 11.728554639187932, Test Loss Force: 14.729224464152415, time: 10.221336126327515


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.672628310538724, Training Loss Force: 3.550613462999157, time: 1.4536266326904297
Validation Loss Energy: 2.4315769111969976, Validation Loss Force: 3.7353707820059627, time: 0.09816861152648926
Test Loss Energy: 11.714215958629511, Test Loss Force: 14.562443114777851, time: 10.00985312461853


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6763187201028598, Training Loss Force: 3.547735147603033, time: 1.4544997215270996
Validation Loss Energy: 2.426771265689584, Validation Loss Force: 3.7171858381963325, time: 0.08986043930053711
Test Loss Energy: 11.807666827661425, Test Loss Force: 14.30654517577881, time: 10.094966173171997


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6653367248336326, Training Loss Force: 3.5532760169410795, time: 1.607304573059082
Validation Loss Energy: 2.4602593692222476, Validation Loss Force: 3.7347871661744865, time: 0.1346724033355713
Test Loss Energy: 11.69521808823973, Test Loss Force: 14.278344960833286, time: 10.016539573669434


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6596740690803213, Training Loss Force: 3.558981381873631, time: 1.4210000038146973
Validation Loss Energy: 2.061267433400082, Validation Loss Force: 3.7440889470764738, time: 0.08821535110473633
Test Loss Energy: 11.739171629972931, Test Loss Force: 15.026610962465027, time: 10.106448650360107


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6578792840151966, Training Loss Force: 3.556502873877223, time: 1.502763271331787
Validation Loss Energy: 2.217762426526174, Validation Loss Force: 3.750801308480467, time: 0.09243059158325195
Test Loss Energy: 11.581193956449948, Test Loss Force: 14.436147385577772, time: 10.11464810371399


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6600792984737414, Training Loss Force: 3.552196865769951, time: 1.4690284729003906
Validation Loss Energy: 2.497334895364954, Validation Loss Force: 3.7155383091856127, time: 0.08949995040893555
Test Loss Energy: 11.812025850157616, Test Loss Force: 14.817885484362398, time: 9.943100929260254


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6899274690635497, Training Loss Force: 3.539676874916987, time: 1.4342741966247559
Validation Loss Energy: 2.332088559465608, Validation Loss Force: 3.715063467231922, time: 0.09977865219116211
Test Loss Energy: 11.844424947680166, Test Loss Force: 14.890681125074604, time: 10.013562202453613


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6811497350482996, Training Loss Force: 3.5425793993974772, time: 1.4906914234161377
Validation Loss Energy: 2.131410441058794, Validation Loss Force: 3.7359179914320806, time: 0.0968935489654541
Test Loss Energy: 11.587116526714635, Test Loss Force: 14.955143259917174, time: 10.269915342330933


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.670551962812675, Training Loss Force: 3.537405120170776, time: 1.432765007019043
Validation Loss Energy: 2.343920752541919, Validation Loss Force: 3.747995343238058, time: 0.09458184242248535
Test Loss Energy: 11.980696702287716, Test Loss Force: 14.922136692985168, time: 9.958808660507202


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6551442747702385, Training Loss Force: 3.561572463967916, time: 1.449974536895752
Validation Loss Energy: 2.237684635012333, Validation Loss Force: 3.7404220315813346, time: 0.09774613380432129
Test Loss Energy: 11.865371408589837, Test Loss Force: 14.95463443643678, time: 10.089491367340088


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6378244859490434, Training Loss Force: 3.5501915519755545, time: 1.5409722328186035
Validation Loss Energy: 1.968462273414569, Validation Loss Force: 3.782107588386757, time: 0.0899207592010498
Test Loss Energy: 11.518612585473292, Test Loss Force: 14.666415546293146, time: 9.890307903289795


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.638258398839611, Training Loss Force: 3.544703034627816, time: 1.4705874919891357
Validation Loss Energy: 2.2815763977156083, Validation Loss Force: 3.7243305117471444, time: 0.09101605415344238
Test Loss Energy: 11.899480325359416, Test Loss Force: 15.097351080171784, time: 10.535801410675049

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–‡â–†â–…â–‚â–ƒâ–‚â–„â–„â–…â–„â–„â–‚â–…â–†â–‚â–ˆâ–†â–â–‡
wandb:   test_error_force â–â–…â–†â–„â–…â–…â–…â–†â–†â–„â–„â–ˆâ–…â–‡â–‡â–‡â–‡â–‡â–†â–ˆ
wandb:          test_loss â–â–…â–‡â–†â–†â–…â–†â–ˆâ–†â–ˆâ–†â–‡â–„â–‡â–‡â–„â–‡â–†â–„â–ˆ
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‡â–…â–„â–„â–…â–„â–ƒâ–‡â–‡â–ˆâ–‚â–„â–ˆâ–†â–ƒâ–†â–…â–â–…
wandb:  valid_error_force â–…â–â–â–ƒâ–…â–ƒâ–‚â–„â–„â–ƒâ–„â–…â–†â–ƒâ–ƒâ–…â–…â–…â–ˆâ–„
wandb:         valid_loss â–ˆâ–†â–„â–ƒâ–ƒâ–„â–ƒâ–ƒâ–‡â–‡â–‡â–â–„â–‡â–…â–‚â–†â–„â–â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 2867
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.89948
wandb:   test_error_force 15.09735
wandb:          test_loss 15.23047
wandb: train_error_energy 2.63826
wandb:  train_error_force 3.5447
wandb:         train_loss 1.02924
wandb: valid_error_energy 2.28158
wandb:  valid_error_force 3.72433
wandb:         valid_loss 0.98934
wandb: 
wandb: ğŸš€ View run al_71_22 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/1eozlu7e
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_125909-1eozlu7e/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.839113235473633, Uncertainty Bias: -0.07205411791801453
0.00035476685 0.018219948
2.6426816 4.806415
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 7 steps.
Found uncertainty sample 3 after 8 steps.
Found uncertainty sample 4 after 28 steps.
Found uncertainty sample 5 after 133 steps.
Found uncertainty sample 6 after 623 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 47 steps.
Found uncertainty sample 9 after 1327 steps.
Found uncertainty sample 10 after 5 steps.
Found uncertainty sample 11 after 63 steps.
Found uncertainty sample 12 after 62 steps.
Found uncertainty sample 13 after 6 steps.
Found uncertainty sample 14 after 992 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 52 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 115 steps.
Found uncertainty sample 19 after 51 steps.
Found uncertainty sample 20 after 80 steps.
Found uncertainty sample 21 after 1161 steps.
Found uncertainty sample 22 after 136 steps.
Found uncertainty sample 23 after 7 steps.
Found uncertainty sample 24 after 87 steps.
Found uncertainty sample 25 after 130 steps.
Found uncertainty sample 26 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 24 steps.
Found uncertainty sample 33 after 376 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 10 steps.
Found uncertainty sample 36 after 54 steps.
Found uncertainty sample 37 after 12 steps.
Found uncertainty sample 38 after 587 steps.
Found uncertainty sample 39 after 143 steps.
Found uncertainty sample 40 after 130 steps.
Found uncertainty sample 41 after 16 steps.
Found uncertainty sample 42 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 323 steps.
Found uncertainty sample 45 after 616 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 537 steps.
Found uncertainty sample 48 after 326 steps.
Found uncertainty sample 49 after 18 steps.
Found uncertainty sample 50 after 4 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 717 steps.
Found uncertainty sample 53 after 226 steps.
Found uncertainty sample 54 after 64 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 30 steps.
Found uncertainty sample 59 after 50 steps.
Found uncertainty sample 60 after 251 steps.
Found uncertainty sample 61 after 367 steps.
Found uncertainty sample 62 after 96 steps.
Found uncertainty sample 63 after 15 steps.
Found uncertainty sample 64 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 16 steps.
Found uncertainty sample 67 after 7 steps.
Found uncertainty sample 68 after 428 steps.
Found uncertainty sample 69 after 76 steps.
Found uncertainty sample 70 after 351 steps.
Found uncertainty sample 71 after 62 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 417 steps.
Found uncertainty sample 74 after 6 steps.
Found uncertainty sample 75 after 173 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 100 steps.
Found uncertainty sample 78 after 13 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 232 steps.
Found uncertainty sample 81 after 512 steps.
Found uncertainty sample 82 after 20 steps.
Found uncertainty sample 83 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 5 steps.
Found uncertainty sample 86 after 14 steps.
Found uncertainty sample 87 after 133 steps.
Found uncertainty sample 88 after 24 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 102 steps.
Found uncertainty sample 91 after 767 steps.
Found uncertainty sample 92 after 167 steps.
Found uncertainty sample 93 after 14 steps.
Found uncertainty sample 94 after 27 steps.
Found uncertainty sample 95 after 134 steps.
Found uncertainty sample 96 after 644 steps.
Found uncertainty sample 97 after 393 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 60 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_130728-pukhyq1v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_23
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/pukhyq1v
Training model 23. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.4616839172009524, Training Loss Force: 3.7794962802722893, time: 1.5959713459014893
Validation Loss Energy: 4.055038504609949, Validation Loss Force: 3.7581978208353233, time: 0.09378886222839355
Test Loss Energy: 13.103463892248447, Test Loss Force: 15.64728599528839, time: 9.983562469482422


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.604896108601934, Training Loss Force: 3.554763300225404, time: 1.5054523944854736
Validation Loss Energy: 3.8999218334971495, Validation Loss Force: 3.7362358516378924, time: 0.10290694236755371
Test Loss Energy: 11.145772224516005, Test Loss Force: 14.29320426283941, time: 9.835446119308472


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6906949001106133, Training Loss Force: 3.5411708836076152, time: 1.521399736404419
Validation Loss Energy: 3.096898625076753, Validation Loss Force: 3.7189983650441842, time: 0.09339237213134766
Test Loss Energy: 12.132377301690317, Test Loss Force: 14.771368719981355, time: 10.655567407608032


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6288299986441768, Training Loss Force: 3.5318507384868183, time: 1.5816583633422852
Validation Loss Energy: 3.7057548949209833, Validation Loss Force: 3.718576073008933, time: 0.09798884391784668
Test Loss Energy: 10.995755796019715, Test Loss Force: 14.171349647604528, time: 10.00156545639038


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6625843426466136, Training Loss Force: 3.540305224979881, time: 1.5137012004852295
Validation Loss Energy: 3.469516179905783, Validation Loss Force: 3.7159652772729292, time: 0.09629940986633301
Test Loss Energy: 12.532374802049036, Test Loss Force: 15.156416516992312, time: 9.984929323196411


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.622766195844931, Training Loss Force: 3.546129802104768, time: 1.4979832172393799
Validation Loss Energy: 3.689734517878917, Validation Loss Force: 3.772073894002105, time: 0.09356546401977539
Test Loss Energy: 10.940172433655452, Test Loss Force: 13.759582089353474, time: 10.40938115119934


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6441033769675153, Training Loss Force: 3.5493416201743013, time: 1.7148196697235107
Validation Loss Energy: 3.291143497853994, Validation Loss Force: 3.713006560996414, time: 0.10726475715637207
Test Loss Energy: 12.388730404403006, Test Loss Force: 14.864411360691323, time: 11.748932123184204


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6618586782657863, Training Loss Force: 3.5528819374856666, time: 1.5956871509552002
Validation Loss Energy: 3.7549898264499415, Validation Loss Force: 3.740976923092987, time: 0.09980988502502441
Test Loss Energy: 10.97843335823232, Test Loss Force: 13.861243527570327, time: 11.904011726379395


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.661840638473528, Training Loss Force: 3.5447901502998174, time: 1.760310173034668
Validation Loss Energy: 3.0689349430545576, Validation Loss Force: 3.7289345303429204, time: 0.11223697662353516
Test Loss Energy: 11.945464101411604, Test Loss Force: 14.753326172507638, time: 11.813012599945068


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.648988271142252, Training Loss Force: 3.57859507850705, time: 1.6259524822235107
Validation Loss Energy: 3.7269061257580907, Validation Loss Force: 3.7212104716744046, time: 0.11009740829467773
Test Loss Energy: 11.0599230376382, Test Loss Force: 13.926251157519753, time: 11.888087272644043


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.628479493465568, Training Loss Force: 3.5479473090797553, time: 1.7726187705993652
Validation Loss Energy: 3.182543524671239, Validation Loss Force: 3.695586263033899, time: 0.11797070503234863
Test Loss Energy: 12.423610870929034, Test Loss Force: 15.057147594629068, time: 11.761359214782715


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.615565847824761, Training Loss Force: 3.5462668106696875, time: 1.7301838397979736
Validation Loss Energy: 3.8123901468946446, Validation Loss Force: 3.71379113573209, time: 0.10028743743896484
Test Loss Energy: 11.026011347441559, Test Loss Force: 14.053611165962488, time: 11.945685625076294


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.673564617936356, Training Loss Force: 3.5549508427149896, time: 1.6645798683166504
Validation Loss Energy: 3.6349243962742186, Validation Loss Force: 3.6848905732784036, time: 0.11052656173706055
Test Loss Energy: 12.254802383618994, Test Loss Force: 14.858586447751328, time: 11.897225618362427


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6369124840060256, Training Loss Force: 3.545500053057122, time: 1.668097734451294
Validation Loss Energy: 3.770779184253108, Validation Loss Force: 3.738275055676299, time: 0.10567593574523926
Test Loss Energy: 10.940101244813949, Test Loss Force: 13.905752441127682, time: 11.951177597045898


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.673419485539049, Training Loss Force: 3.538582211128692, time: 1.567033290863037
Validation Loss Energy: 3.1708152067515036, Validation Loss Force: 3.7004046006826306, time: 0.11660122871398926
Test Loss Energy: 12.09748264962414, Test Loss Force: 14.86483678699882, time: 11.789458990097046


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6796758049692846, Training Loss Force: 3.5390398680523307, time: 1.637047290802002
Validation Loss Energy: 3.877371238412459, Validation Loss Force: 3.728871180970904, time: 0.1130673885345459
Test Loss Energy: 11.167532017706678, Test Loss Force: 14.337129307585164, time: 11.778037786483765


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6304976043909813, Training Loss Force: 3.5376000342860476, time: 1.6412088871002197
Validation Loss Energy: 2.8484899278908697, Validation Loss Force: 3.7451106644525636, time: 0.654292106628418
Test Loss Energy: 12.44153195529816, Test Loss Force: 15.929407131674038, time: 11.99154806137085


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6825779225797723, Training Loss Force: 3.542257604811419, time: 1.6917145252227783
Validation Loss Energy: 3.8859052560992873, Validation Loss Force: 3.7177460737674863, time: 0.11256575584411621
Test Loss Energy: 11.028206361591764, Test Loss Force: 13.93180650322794, time: 11.77099347114563


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.686094678861777, Training Loss Force: 3.542259084483807, time: 1.694540023803711
Validation Loss Energy: 3.492589045472248, Validation Loss Force: 3.6953524604306747, time: 0.11376357078552246
Test Loss Energy: 12.365229748855256, Test Loss Force: 15.311689700032753, time: 11.855973482131958


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.683647584453847, Training Loss Force: 3.539380178336065, time: 1.627687931060791
Validation Loss Energy: 3.8022261910596247, Validation Loss Force: 3.680822741986022, time: 0.11036443710327148
Test Loss Energy: 10.946113521599267, Test Loss Force: 14.084587703757546, time: 10.682099103927612

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‚â–…â–â–†â–â–†â–â–„â–â–†â–â–…â–â–…â–‚â–†â–â–†â–
wandb:   test_error_force â–‡â–ƒâ–„â–‚â–†â–â–…â–â–„â–‚â–…â–‚â–…â–â–…â–ƒâ–ˆâ–‚â–†â–‚
wandb:          test_loss â–ˆâ–‚â–…â–‚â–†â–â–†â–â–„â–â–†â–â–…â–â–…â–â–†â–â–…â–
wandb: train_error_energy â–ˆâ–â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–‚â–‚â–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–â–â–â–â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‡â–‚â–†â–…â–†â–„â–†â–‚â–†â–ƒâ–‡â–†â–†â–ƒâ–‡â–â–‡â–…â–‡
wandb:  valid_error_force â–‡â–…â–„â–„â–„â–ˆâ–ƒâ–†â–…â–„â–‚â–„â–â–…â–ƒâ–…â–†â–„â–‚â–
wandb:         valid_loss â–ˆâ–†â–‚â–„â–ƒâ–„â–ƒâ–„â–â–„â–‚â–…â–„â–…â–‚â–…â–â–…â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 2957
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.94611
wandb:   test_error_force 14.08459
wandb:          test_loss 12.57168
wandb: train_error_energy 2.68365
wandb:  train_error_force 3.53938
wandb:         train_loss 1.04012
wandb: valid_error_energy 3.80223
wandb:  valid_error_force 3.68082
wandb:         valid_loss 1.52506
wandb: 
wandb: ğŸš€ View run al_71_23 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/pukhyq1v
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_130728-pukhyq1v/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.77302622795105, Uncertainty Bias: -0.08801768720149994
1.5258789e-05 0.03093338
2.6223252 4.615478
(48745, 22, 3)
Found uncertainty sample 0 after 124 steps.
Found uncertainty sample 1 after 333 steps.
Found uncertainty sample 2 after 378 steps.
Found uncertainty sample 3 after 8 steps.
Found uncertainty sample 4 after 446 steps.
Found uncertainty sample 5 after 2 steps.
Found uncertainty sample 6 after 22 steps.
Found uncertainty sample 7 after 1801 steps.
Found uncertainty sample 8 after 33 steps.
Found uncertainty sample 9 after 76 steps.
Found uncertainty sample 10 after 22 steps.
Found uncertainty sample 11 after 23 steps.
Found uncertainty sample 12 after 42 steps.
Found uncertainty sample 13 after 13 steps.
Found uncertainty sample 14 after 5 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 168 steps.
Found uncertainty sample 17 after 23 steps.
Found uncertainty sample 18 after 11 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 38 steps.
Found uncertainty sample 21 after 101 steps.
Found uncertainty sample 22 after 538 steps.
Found uncertainty sample 23 after 315 steps.
Found uncertainty sample 24 after 14 steps.
Found uncertainty sample 25 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 10 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1253 steps.
Found uncertainty sample 30 after 647 steps.
Found uncertainty sample 31 after 1017 steps.
Found uncertainty sample 32 after 101 steps.
Found uncertainty sample 33 after 1866 steps.
Found uncertainty sample 34 after 31 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 11 steps.
Found uncertainty sample 37 after 147 steps.
Found uncertainty sample 38 after 10 steps.
Found uncertainty sample 39 after 17 steps.
Found uncertainty sample 40 after 15 steps.
Found uncertainty sample 41 after 188 steps.
Found uncertainty sample 42 after 7 steps.
Found uncertainty sample 43 after 37 steps.
Found uncertainty sample 44 after 174 steps.
Found uncertainty sample 45 after 264 steps.
Found uncertainty sample 46 after 24 steps.
Found uncertainty sample 47 after 8 steps.
Found uncertainty sample 48 after 112 steps.
Found uncertainty sample 49 after 92 steps.
Found uncertainty sample 50 after 52 steps.
Found uncertainty sample 51 after 21 steps.
Found uncertainty sample 52 after 2645 steps.
Found uncertainty sample 53 after 852 steps.
Found uncertainty sample 54 after 354 steps.
Found uncertainty sample 55 after 112 steps.
Found uncertainty sample 56 after 168 steps.
Found uncertainty sample 57 after 19 steps.
Found uncertainty sample 58 after 3 steps.
Found uncertainty sample 59 after 154 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 11 steps.
Found uncertainty sample 62 after 15 steps.
Found uncertainty sample 63 after 227 steps.
Found uncertainty sample 64 after 965 steps.
Found uncertainty sample 65 after 19 steps.
Found uncertainty sample 66 after 36 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 153 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 157 steps.
Found uncertainty sample 71 after 1266 steps.
Found uncertainty sample 72 after 13 steps.
Found uncertainty sample 73 after 296 steps.
Found uncertainty sample 74 after 30 steps.
Found uncertainty sample 75 after 19 steps.
Found uncertainty sample 76 after 38 steps.
Found uncertainty sample 77 after 349 steps.
Found uncertainty sample 78 after 181 steps.
Found uncertainty sample 79 after 274 steps.
Found uncertainty sample 80 after 1371 steps.
Found uncertainty sample 81 after 4 steps.
Found uncertainty sample 82 after 1112 steps.
Found uncertainty sample 83 after 1768 steps.
Found uncertainty sample 84 after 80 steps.
Found uncertainty sample 85 after 882 steps.
Found uncertainty sample 86 after 551 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 200 steps.
Found uncertainty sample 89 after 20 steps.
Found uncertainty sample 90 after 16 steps.
Found uncertainty sample 91 after 96 steps.
Found uncertainty sample 92 after 224 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 21 steps.
Found uncertainty sample 95 after 24 steps.
Found uncertainty sample 96 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 78 steps.
Found uncertainty sample 99 after 135 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_131720-j5ui19ts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_24
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j5ui19ts
Training model 24. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.905690436409421, Training Loss Force: 4.192764599452274, time: 1.5861563682556152
Validation Loss Energy: 1.3431494176713992, Validation Loss Force: 4.111356430336297, time: 0.09612584114074707
Test Loss Energy: 11.409739441549151, Test Loss Force: 15.468832413222398, time: 10.031567335128784


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.662429098369021, Training Loss Force: 3.5849211541351185, time: 1.5503873825073242
Validation Loss Energy: 1.9862048849658138, Validation Loss Force: 3.67010200195161, time: 0.1024024486541748
Test Loss Energy: 11.394233209896644, Test Loss Force: 14.531611076434626, time: 10.626441240310669


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6016338562701353, Training Loss Force: 3.5203670423019715, time: 1.528010368347168
Validation Loss Energy: 1.7012024063344389, Validation Loss Force: 3.670915857679976, time: 0.09712600708007812
Test Loss Energy: 11.35018693520772, Test Loss Force: 14.541435453661455, time: 10.236325740814209


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.614876225804693, Training Loss Force: 3.5270485849044864, time: 1.5100622177124023
Validation Loss Energy: 1.931537883954661, Validation Loss Force: 3.721199004213389, time: 0.09336543083190918
Test Loss Energy: 11.421745836509226, Test Loss Force: 14.460414358739943, time: 9.98532772064209


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6224698606599226, Training Loss Force: 3.524014534553069, time: 1.5692925453186035
Validation Loss Energy: 1.8402468741614568, Validation Loss Force: 3.680860484406257, time: 0.09484577178955078
Test Loss Energy: 11.453626266476679, Test Loss Force: 14.583831188060056, time: 10.130979299545288


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6319092916848232, Training Loss Force: 3.5335001891412468, time: 1.528214454650879
Validation Loss Energy: 1.845119710083654, Validation Loss Force: 3.7101276716087135, time: 0.09808158874511719
Test Loss Energy: 11.44180246021626, Test Loss Force: 14.86249312778693, time: 10.01357388496399


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6178581182281473, Training Loss Force: 3.5383480837996295, time: 1.5356764793395996
Validation Loss Energy: 1.6543706872301929, Validation Loss Force: 3.7174566038777805, time: 0.09420132637023926
Test Loss Energy: 11.188158511214302, Test Loss Force: 14.3143144123849, time: 9.967211961746216


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6479930296962575, Training Loss Force: 3.544820530796237, time: 1.5855977535247803
Validation Loss Energy: 1.7265528432600257, Validation Loss Force: 3.7750200294718756, time: 0.09651446342468262
Test Loss Energy: 11.384098639394045, Test Loss Force: 14.83314128667158, time: 10.207701921463013


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.59385780551578, Training Loss Force: 3.5448822565390645, time: 1.5268957614898682
Validation Loss Energy: 1.6915387324030324, Validation Loss Force: 3.6769195745270538, time: 0.09526729583740234
Test Loss Energy: 11.37265557172541, Test Loss Force: 14.749279147956525, time: 10.100074768066406


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6580213092775646, Training Loss Force: 3.5241621317242355, time: 1.5824956893920898
Validation Loss Energy: 1.7138462841149429, Validation Loss Force: 3.7607336884037217, time: 0.09548687934875488
Test Loss Energy: 11.59672565865481, Test Loss Force: 15.222781906450342, time: 9.995296955108643


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.687709571953116, Training Loss Force: 3.5540622119040557, time: 1.7254250049591064
Validation Loss Energy: 1.7633228116672326, Validation Loss Force: 3.7155209409922305, time: 0.09492039680480957
Test Loss Energy: 11.769818169762111, Test Loss Force: 15.293989191020097, time: 9.991225481033325


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.650002630335607, Training Loss Force: 3.535399705402108, time: 1.5684316158294678
Validation Loss Energy: 1.9069750277982207, Validation Loss Force: 3.714166555912125, time: 0.09934139251708984
Test Loss Energy: 11.713117992226431, Test Loss Force: 14.955118302319914, time: 10.003624200820923


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6091719386526613, Training Loss Force: 3.5406829762275365, time: 1.5747666358947754
Validation Loss Energy: 1.7516478786752332, Validation Loss Force: 3.7249880537350695, time: 0.0976107120513916
Test Loss Energy: 11.821384436173764, Test Loss Force: 15.317149664071607, time: 10.18716287612915


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.690425134682014, Training Loss Force: 3.5524654468196672, time: 1.5378072261810303
Validation Loss Energy: 1.730714529597416, Validation Loss Force: 3.672114263023359, time: 0.09297394752502441
Test Loss Energy: 11.815655678998983, Test Loss Force: 15.305135089809287, time: 10.00688886642456


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.705705642458389, Training Loss Force: 3.54437930492517, time: 1.5512511730194092
Validation Loss Energy: 1.663701303786578, Validation Loss Force: 3.6952712975269844, time: 0.09767794609069824
Test Loss Energy: 11.63542615311027, Test Loss Force: 15.249432185590447, time: 10.58033537864685


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6670464260852067, Training Loss Force: 3.545581238182835, time: 1.5711355209350586
Validation Loss Energy: 1.8310363876092999, Validation Loss Force: 3.7451507448988304, time: 0.09440851211547852
Test Loss Energy: 11.735459481164378, Test Loss Force: 15.24794129558034, time: 10.178148031234741


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6861703007380466, Training Loss Force: 3.54846730401155, time: 1.5458006858825684
Validation Loss Energy: 1.6655261887720292, Validation Loss Force: 3.699268542700107, time: 0.09283208847045898
Test Loss Energy: 11.696913409510334, Test Loss Force: 15.35512150114348, time: 10.003615140914917


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.681826887384201, Training Loss Force: 3.528578382243237, time: 1.534982442855835
Validation Loss Energy: 1.6664834434303728, Validation Loss Force: 3.7223781486756495, time: 0.09723949432373047
Test Loss Energy: 11.52104585794125, Test Loss Force: 15.012038155209023, time: 10.135172843933105


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.656646209008476, Training Loss Force: 3.5586287177071547, time: 1.5797882080078125
Validation Loss Energy: 1.6609938139739555, Validation Loss Force: 3.7011430829051, time: 0.10449767112731934
Test Loss Energy: 11.487493721290738, Test Loss Force: 14.974483234053206, time: 9.898258447647095


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6699262884906854, Training Loss Force: 3.558633783430224, time: 1.5759532451629639
Validation Loss Energy: 1.620862821810828, Validation Loss Force: 3.7037169155628793, time: 0.09409666061401367
Test Loss Energy: 11.391070398694975, Test Loss Force: 14.950385424156742, time: 9.908708095550537

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–ƒâ–ƒâ–„â–„â–„â–â–ƒâ–ƒâ–†â–‡â–‡â–ˆâ–ˆâ–†â–‡â–‡â–…â–„â–ƒ
wandb:   test_error_force â–ˆâ–‚â–‚â–‚â–ƒâ–„â–â–„â–„â–‡â–‡â–…â–‡â–‡â–‡â–‡â–‡â–…â–…â–…
wandb:          test_loss â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–â–â–
wandb: train_error_energy â–ˆâ–ƒâ–â–â–‚â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–â–ƒâ–„â–ƒâ–ƒâ–ƒâ–‚â–ƒ
wandb:  train_error_force â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–ˆâ–…â–‡â–†â–†â–„â–…â–…â–…â–†â–‡â–…â–…â–„â–†â–…â–…â–„â–„
wandb:  valid_error_force â–ˆâ–â–â–‚â–â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–‚
wandb:         valid_loss â–â–ˆâ–ƒâ–ˆâ–†â–‡â–„â–†â–„â–†â–†â–ˆâ–†â–…â–„â–ˆâ–„â–…â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3047
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.39107
wandb:   test_error_force 14.95039
wandb:          test_loss 13.91725
wandb: train_error_energy 2.66993
wandb:  train_error_force 3.55863
wandb:         train_loss 1.03809
wandb: valid_error_energy 1.62086
wandb:  valid_error_force 3.70372
wandb:         valid_loss 0.77668
wandb: 
wandb: ğŸš€ View run al_71_24 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j5ui19ts
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_131720-j5ui19ts/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.921373128890991, Uncertainty Bias: -0.09439486265182495
4.863739e-05 0.0041422546
2.6442802 4.7250214
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 307 steps.
Found uncertainty sample 2 after 19 steps.
Found uncertainty sample 3 after 3 steps.
Found uncertainty sample 4 after 88 steps.
Found uncertainty sample 5 after 246 steps.
Found uncertainty sample 6 after 12 steps.
Found uncertainty sample 7 after 86 steps.
Found uncertainty sample 8 after 1878 steps.
Found uncertainty sample 9 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 18 steps.
Found uncertainty sample 12 after 52 steps.
Found uncertainty sample 13 after 84 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 212 steps.
Found uncertainty sample 16 after 605 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 84 steps.
Found uncertainty sample 19 after 460 steps.
Found uncertainty sample 20 after 792 steps.
Found uncertainty sample 21 after 492 steps.
Found uncertainty sample 22 after 15 steps.
Found uncertainty sample 23 after 132 steps.
Found uncertainty sample 24 after 863 steps.
Found uncertainty sample 25 after 545 steps.
Found uncertainty sample 26 after 20 steps.
Found uncertainty sample 27 after 163 steps.
Found uncertainty sample 28 after 57 steps.
Found uncertainty sample 29 after 930 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 429 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 328 steps.
Found uncertainty sample 34 after 545 steps.
Found uncertainty sample 35 after 174 steps.
Found uncertainty sample 36 after 11 steps.
Found uncertainty sample 37 after 15 steps.
Found uncertainty sample 38 after 399 steps.
Found uncertainty sample 39 after 37 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 368 steps.
Found uncertainty sample 42 after 173 steps.
Found uncertainty sample 43 after 111 steps.
Found uncertainty sample 44 after 705 steps.
Found uncertainty sample 45 after 50 steps.
Found uncertainty sample 46 after 25 steps.
Found uncertainty sample 47 after 115 steps.
Found uncertainty sample 48 after 47 steps.
Found uncertainty sample 49 after 75 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 18 steps.
Found uncertainty sample 52 after 31 steps.
Found uncertainty sample 53 after 22 steps.
Found uncertainty sample 54 after 19 steps.
Found uncertainty sample 55 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 120 steps.
Found uncertainty sample 58 after 420 steps.
Found uncertainty sample 59 after 9 steps.
Found uncertainty sample 60 after 212 steps.
Found uncertainty sample 61 after 93 steps.
Found uncertainty sample 62 after 78 steps.
Found uncertainty sample 63 after 52 steps.
Found uncertainty sample 64 after 539 steps.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 58 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 27 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 11 steps.
Found uncertainty sample 71 after 774 steps.
Found uncertainty sample 72 after 82 steps.
Found uncertainty sample 73 after 19 steps.
Found uncertainty sample 74 after 127 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 14 steps.
Found uncertainty sample 77 after 93 steps.
Found uncertainty sample 78 after 57 steps.
Found uncertainty sample 79 after 28 steps.
Found uncertainty sample 80 after 92 steps.
Found uncertainty sample 81 after 10 steps.
Found uncertainty sample 82 after 41 steps.
Found uncertainty sample 83 after 62 steps.
Found uncertainty sample 84 after 34 steps.
Found uncertainty sample 85 after 33 steps.
Found uncertainty sample 86 after 713 steps.
Found uncertainty sample 87 after 511 steps.
Found uncertainty sample 88 after 298 steps.
Found uncertainty sample 89 after 10 steps.
Found uncertainty sample 90 after 22 steps.
Found uncertainty sample 91 after 42 steps.
Found uncertainty sample 92 after 178 steps.
Found uncertainty sample 93 after 15 steps.
Found uncertainty sample 94 after 9 steps.
Found uncertainty sample 95 after 2565 steps.
Found uncertainty sample 96 after 49 steps.
Found uncertainty sample 97 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 204 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_132607-sd99fpcg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_25
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sd99fpcg
Training model 25. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.295612441747157, Training Loss Force: 3.893532906899758, time: 1.6257860660552979
Validation Loss Energy: 3.5850386028317143, Validation Loss Force: 3.7790445516153746, time: 0.1032106876373291
Test Loss Energy: 12.449474295597783, Test Loss Force: 15.047550307288484, time: 10.051108837127686


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.636888615963146, Training Loss Force: 3.5663760235827433, time: 1.5966060161590576
Validation Loss Energy: 3.8836990502014346, Validation Loss Force: 3.7859942783289267, time: 0.09676837921142578
Test Loss Energy: 10.976297944801573, Test Loss Force: 14.090122526598485, time: 9.98517107963562


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.655836627817813, Training Loss Force: 3.615051112023031, time: 1.587418794631958
Validation Loss Energy: 3.9644337611244946, Validation Loss Force: 3.8695798223272995, time: 0.0957803726196289
Test Loss Energy: 12.31423880822117, Test Loss Force: 14.668238099081597, time: 10.103768110275269


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6908206800269943, Training Loss Force: 3.7466768652446194, time: 1.5616869926452637
Validation Loss Energy: 3.934765769459368, Validation Loss Force: 3.716240050999004, time: 0.09820270538330078
Test Loss Energy: 10.977084817925842, Test Loss Force: 13.634654252856846, time: 10.135644435882568


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.613740842471726, Training Loss Force: 3.5444654358530316, time: 1.5927579402923584
Validation Loss Energy: 3.1606356640762687, Validation Loss Force: 3.719302197093803, time: 0.09705662727355957
Test Loss Energy: 11.998140696444262, Test Loss Force: 14.783753535908803, time: 10.203866004943848


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6590855877030872, Training Loss Force: 3.5554966745163408, time: 1.6035048961639404
Validation Loss Energy: 3.616820949371998, Validation Loss Force: 3.719509390734089, time: 0.09886670112609863
Test Loss Energy: 10.80692382190214, Test Loss Force: 13.524108698102186, time: 10.04364800453186


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6465069907807526, Training Loss Force: 3.5754502276208586, time: 1.5988881587982178
Validation Loss Energy: 3.3012895288227666, Validation Loss Force: 3.808000447351824, time: 0.09927630424499512
Test Loss Energy: 12.368369878115027, Test Loss Force: 15.126738221924505, time: 10.082225799560547


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.672043762420773, Training Loss Force: 3.6056648135604727, time: 1.6497955322265625
Validation Loss Energy: 3.750436465084864, Validation Loss Force: 3.752017638690097, time: 0.09939885139465332
Test Loss Energy: 10.797107488743936, Test Loss Force: 13.83193137052685, time: 10.221113920211792


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6492110747675746, Training Loss Force: 3.5531343303143434, time: 1.649064540863037
Validation Loss Energy: 3.4494099353705145, Validation Loss Force: 3.702860530027641, time: 0.09772729873657227
Test Loss Energy: 12.455948721612572, Test Loss Force: 15.11925703758675, time: 10.85030722618103


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.662768275596063, Training Loss Force: 3.564433242032074, time: 1.7555358409881592
Validation Loss Energy: 3.8794563122949266, Validation Loss Force: 3.764028635090588, time: 0.10881471633911133
Test Loss Energy: 10.914765767524385, Test Loss Force: 13.745071107176773, time: 11.76319694519043


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.624551675792092, Training Loss Force: 3.548667168898939, time: 1.694870948791504
Validation Loss Energy: 3.1024253846132535, Validation Loss Force: 3.773465019491289, time: 0.11275482177734375
Test Loss Energy: 12.305065329047904, Test Loss Force: 15.226371552254422, time: 11.701143741607666


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.670188971169677, Training Loss Force: 3.5619622015491146, time: 1.7917230129241943
Validation Loss Energy: 3.713769759494081, Validation Loss Force: 3.708072212722654, time: 0.10760712623596191
Test Loss Energy: 10.890216100143062, Test Loss Force: 13.79188952179955, time: 11.56656789779663


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6663881516718284, Training Loss Force: 3.5447071488338944, time: 1.9332275390625
Validation Loss Energy: 3.4012143120801746, Validation Loss Force: 3.7065620705749716, time: 0.11031794548034668
Test Loss Energy: 12.226511995830023, Test Loss Force: 15.038115897182022, time: 11.580333948135376


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6489865106847232, Training Loss Force: 3.551616453225319, time: 1.8101537227630615
Validation Loss Energy: 3.5512787316064456, Validation Loss Force: 3.7195876589388828, time: 0.10731077194213867
Test Loss Energy: 10.967571916362534, Test Loss Force: 14.071184457446975, time: 11.394424676895142


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6833691425274853, Training Loss Force: 3.5459027157079626, time: 1.6868975162506104
Validation Loss Energy: 3.7913145122325793, Validation Loss Force: 3.8732066573761714, time: 0.11252212524414062
Test Loss Energy: 12.602060204794146, Test Loss Force: 15.036809176705923, time: 12.1635422706604


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.7252619143404355, Training Loss Force: 3.7944139700290127, time: 1.7751884460449219
Validation Loss Energy: 4.25692905651634, Validation Loss Force: 3.7465006765960176, time: 0.11653542518615723
Test Loss Energy: 11.051818570884201, Test Loss Force: 13.753962386433077, time: 11.409902095794678


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.632706798310699, Training Loss Force: 3.576423019637385, time: 1.747154951095581
Validation Loss Energy: 3.5079971936975856, Validation Loss Force: 3.7494181990805364, time: 0.10965228080749512
Test Loss Energy: 12.289637318385008, Test Loss Force: 15.017478197937063, time: 11.504399061203003


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.668857838146446, Training Loss Force: 3.5825313831109082, time: 1.7242650985717773
Validation Loss Energy: 3.47902303909117, Validation Loss Force: 3.7460694777949284, time: 0.10335612297058105
Test Loss Energy: 10.890217270613341, Test Loss Force: 14.108727249546279, time: 11.532002925872803


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.7172087956579034, Training Loss Force: 3.584462689328422, time: 1.7275092601776123
Validation Loss Energy: 3.1013501153075684, Validation Loss Force: 3.7514182133492597, time: 0.10468864440917969
Test Loss Energy: 12.512840865599792, Test Loss Force: 15.259326250099308, time: 11.558409214019775


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6690732806664554, Training Loss Force: 3.572833055450224, time: 1.7548229694366455
Validation Loss Energy: 8.826897977173552, Validation Loss Force: 3.7636377394049116, time: 0.11396169662475586
Test Loss Energy: 12.385197519412417, Test Loss Force: 13.398692519023363, time: 11.47669792175293

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–‚â–‡â–‚â–†â–â–‡â–â–‡â–â–‡â–â–‡â–‚â–ˆâ–‚â–‡â–â–ˆâ–‡
wandb:   test_error_force â–‡â–„â–†â–‚â–†â–â–ˆâ–ƒâ–‡â–‚â–ˆâ–‚â–‡â–„â–‡â–‚â–‡â–„â–ˆâ–
wandb:          test_loss â–ˆâ–â–ˆâ–â–†â–â–ˆâ–â–ˆâ–â–‡â–â–‡â–â–ˆâ–â–‡â–â–‡â–‚
wandb: train_error_energy â–ˆâ–â–â–‚â–â–â–â–‚â–â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–‚â–…â–â–â–‚â–‚â–â–â–â–â–â–â–â–†â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–ƒâ–â–â–‚â–
wandb: valid_error_energy â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–â–ˆ
wandb:  valid_error_force â–„â–„â–ˆâ–‚â–‚â–‚â–…â–ƒâ–â–„â–„â–â–â–‚â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:         valid_loss â–â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3137
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.3852
wandb:   test_error_force 13.39869
wandb:          test_loss 13.05261
wandb: train_error_energy 2.66907
wandb:  train_error_force 3.57283
wandb:         train_loss 1.04308
wandb: valid_error_energy 8.8269
wandb:  valid_error_force 3.76364
wandb:         valid_loss 4.72036
wandb: 
wandb: ğŸš€ View run al_71_25 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sd99fpcg
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_132607-sd99fpcg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.690732717514038, Uncertainty Bias: -0.0928502082824707
7.6293945e-05 1.4339046
2.749671 4.55768
(48745, 22, 3)
Found uncertainty sample 0 after 93 steps.
Found uncertainty sample 1 after 142 steps.
Found uncertainty sample 2 after 17 steps.
Found uncertainty sample 3 after 49 steps.
Found uncertainty sample 4 after 55 steps.
Found uncertainty sample 5 after 32 steps.
Found uncertainty sample 6 after 28 steps.
Found uncertainty sample 7 after 142 steps.
Found uncertainty sample 8 after 664 steps.
Found uncertainty sample 9 after 87 steps.
Found uncertainty sample 10 after 59 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 6 steps.
Found uncertainty sample 14 after 162 steps.
Found uncertainty sample 15 after 33 steps.
Found uncertainty sample 16 after 13 steps.
Found uncertainty sample 17 after 589 steps.
Found uncertainty sample 18 after 14 steps.
Found uncertainty sample 19 after 11 steps.
Found uncertainty sample 20 after 310 steps.
Found uncertainty sample 21 after 747 steps.
Found uncertainty sample 22 after 56 steps.
Found uncertainty sample 23 after 28 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 4 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 17 steps.
Found uncertainty sample 28 after 11 steps.
Found uncertainty sample 29 after 97 steps.
Found uncertainty sample 30 after 58 steps.
Found uncertainty sample 31 after 52 steps.
Found uncertainty sample 32 after 446 steps.
Found uncertainty sample 33 after 130 steps.
Found uncertainty sample 34 after 73 steps.
Found uncertainty sample 35 after 15 steps.
Found uncertainty sample 36 after 560 steps.
Found uncertainty sample 37 after 37 steps.
Found uncertainty sample 38 after 112 steps.
Found uncertainty sample 39 after 12 steps.
Found uncertainty sample 40 after 14 steps.
Found uncertainty sample 41 after 17 steps.
Found uncertainty sample 42 after 9 steps.
Found uncertainty sample 43 after 461 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 77 steps.
Found uncertainty sample 46 after 206 steps.
Found uncertainty sample 47 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1398 steps.
Found uncertainty sample 50 after 50 steps.
Found uncertainty sample 51 after 133 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 85 steps.
Found uncertainty sample 54 after 318 steps.
Found uncertainty sample 55 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 626 steps.
Found uncertainty sample 58 after 235 steps.
Found uncertainty sample 59 after 9 steps.
Found uncertainty sample 60 after 66 steps.
Found uncertainty sample 61 after 4 steps.
Found uncertainty sample 62 after 194 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 31 steps.
Found uncertainty sample 65 after 147 steps.
Found uncertainty sample 66 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 52 steps.
Found uncertainty sample 69 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 449 steps.
Found uncertainty sample 72 after 136 steps.
Found uncertainty sample 73 after 11 steps.
Found uncertainty sample 74 after 174 steps.
Found uncertainty sample 75 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 6 steps.
Found uncertainty sample 78 after 32 steps.
Found uncertainty sample 79 after 16 steps.
Found uncertainty sample 80 after 28 steps.
Found uncertainty sample 81 after 163 steps.
Found uncertainty sample 82 after 776 steps.
Found uncertainty sample 83 after 94 steps.
Found uncertainty sample 84 after 1129 steps.
Found uncertainty sample 85 after 9 steps.
Found uncertainty sample 86 after 9 steps.
Found uncertainty sample 87 after 651 steps.
Found uncertainty sample 88 after 213 steps.
Found uncertainty sample 89 after 8 steps.
Found uncertainty sample 90 after 240 steps.
Found uncertainty sample 91 after 239 steps.
Found uncertainty sample 92 after 44 steps.
Found uncertainty sample 93 after 87 steps.
Found uncertainty sample 94 after 23 steps.
Found uncertainty sample 95 after 1662 steps.
Found uncertainty sample 96 after 213 steps.
Found uncertainty sample 97 after 16 steps.
Found uncertainty sample 98 after 220 steps.
Found uncertainty sample 99 after 185 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_133453-npqedl4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_26
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/npqedl4s
Training model 26. Added 100 samples to the dataset.
Epoch 0, Batch 100/101, Loss: 0.796272873878479, Variance: 0.1536690890789032

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.960882718594516, Training Loss Force: 3.7982642213089717, time: 1.6513588428497314
Validation Loss Energy: 6.2014434859555205, Validation Loss Force: 3.79518583800368, time: 0.10019087791442871
Test Loss Energy: 10.883738974119154, Test Loss Force: 12.08456128732473, time: 9.904537200927734

Epoch 1, Batch 100/101, Loss: 1.2037169933319092, Variance: 0.15372943878173828

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.143182310899255, Training Loss Force: 3.63465338367253, time: 1.6575579643249512
Validation Loss Energy: 5.0593090851087235, Validation Loss Force: 3.788301798688176, time: 0.09958505630493164
Test Loss Energy: 12.527884728851104, Test Loss Force: 12.756320618610482, time: 9.872887372970581

Epoch 2, Batch 100/101, Loss: 1.9009685516357422, Variance: 0.15750879049301147

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.1260872179174, Training Loss Force: 3.5875607993964103, time: 1.6655867099761963
Validation Loss Energy: 2.6683412367311603, Validation Loss Force: 3.798327476983575, time: 0.10552978515625
Test Loss Energy: 10.179699345712148, Test Loss Force: 12.279553614375471, time: 10.042369365692139

Epoch 3, Batch 100/101, Loss: 1.6007390022277832, Variance: 0.15620392560958862

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.136100510517137, Training Loss Force: 3.6183073122186116, time: 1.6703333854675293
Validation Loss Energy: 3.9482049657618394, Validation Loss Force: 3.7191026855390517, time: 0.09942865371704102
Test Loss Energy: 10.418024972504625, Test Loss Force: 12.119998216870306, time: 9.97157335281372

Epoch 4, Batch 100/101, Loss: 1.0997509956359863, Variance: 0.16197341680526733

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.189626548761782, Training Loss Force: 3.5983049075655984, time: 1.613555669784546
Validation Loss Energy: 5.487542155843272, Validation Loss Force: 3.7809158631177517, time: 0.1004030704498291
Test Loss Energy: 13.042769599148665, Test Loss Force: 12.758623968877927, time: 10.2747323513031

Epoch 5, Batch 100/101, Loss: 1.3184921741485596, Variance: 0.16199404001235962

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.1017354962042, Training Loss Force: 3.584360029632724, time: 1.6356263160705566
Validation Loss Energy: 5.269940959676801, Validation Loss Force: 3.7906478408096715, time: 0.10019946098327637
Test Loss Energy: 10.792885294689203, Test Loss Force: 12.3244206498679, time: 9.965629816055298

Epoch 6, Batch 100/101, Loss: 1.4913840293884277, Variance: 0.15887349843978882

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.154538928977408, Training Loss Force: 3.588732177996628, time: 1.611253023147583
Validation Loss Energy: 1.8520908687025017, Validation Loss Force: 3.741168418558485, time: 0.10500526428222656
Test Loss Energy: 10.813748684851952, Test Loss Force: 12.677777605116578, time: 10.017383813858032

Epoch 7, Batch 100/101, Loss: 1.5153284072875977, Variance: 0.16387741267681122

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.129064934926927, Training Loss Force: 3.606081014595813, time: 1.6594915390014648
Validation Loss Energy: 3.52623335141078, Validation Loss Force: 3.750237398868405, time: 0.1047818660736084
Test Loss Energy: 11.670976828117784, Test Loss Force: 12.829225873164447, time: 10.240507364273071

Epoch 8, Batch 100/101, Loss: 1.024072289466858, Variance: 0.1597222536802292

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.096247610070823, Training Loss Force: 3.5976680385339237, time: 1.658308506011963
Validation Loss Energy: 5.737295886005132, Validation Loss Force: 3.7972586650831213, time: 0.10188460350036621
Test Loss Energy: 10.97466098849496, Test Loss Force: 12.372663853241237, time: 10.136657238006592

Epoch 9, Batch 100/101, Loss: 1.1841633319854736, Variance: 0.15879155695438385

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.119757435175787, Training Loss Force: 3.5824355839809807, time: 1.6285138130187988
Validation Loss Energy: 4.812024978444349, Validation Loss Force: 3.738366906698994, time: 0.09986758232116699
Test Loss Energy: 12.737302052814007, Test Loss Force: 13.274825175228104, time: 10.053251028060913

Epoch 10, Batch 100/101, Loss: 1.7511076927185059, Variance: 0.16353733837604523

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.141792084905234, Training Loss Force: 3.577237732126433, time: 1.8544847965240479
Validation Loss Energy: 2.3524695777269513, Validation Loss Force: 3.685086188123101, time: 0.10255169868469238
Test Loss Energy: 10.404940401881895, Test Loss Force: 12.665452259338199, time: 10.066360473632812

Epoch 11, Batch 100/101, Loss: 1.8651565313339233, Variance: 0.15733888745307922

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.061360912814007, Training Loss Force: 3.6309928418061124, time: 1.6321680545806885
Validation Loss Energy: 4.019715829983986, Validation Loss Force: 3.802853450043703, time: 0.09935355186462402
Test Loss Energy: 10.759053173155117, Test Loss Force: 12.743695329331889, time: 10.01238465309143

Epoch 12, Batch 100/101, Loss: 1.037513017654419, Variance: 0.16356298327445984

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.146300155145566, Training Loss Force: 3.5809172670096037, time: 1.6323165893554688
Validation Loss Energy: 5.412433626230371, Validation Loss Force: 3.7446803563280575, time: 0.10424590110778809
Test Loss Energy: 12.889936478236427, Test Loss Force: 13.430011804090162, time: 10.139289379119873

Epoch 13, Batch 100/101, Loss: 1.1846522092819214, Variance: 0.16537262499332428

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.11641569930789, Training Loss Force: 3.561076603933045, time: 1.645505428314209
Validation Loss Energy: 5.364832974897949, Validation Loss Force: 3.7395914756670243, time: 0.10013079643249512
Test Loss Energy: 10.939783613689219, Test Loss Force: 12.709876208877986, time: 10.062353134155273

Epoch 14, Batch 100/101, Loss: 2.0320019721984863, Variance: 0.16032853722572327

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.185850214530193, Training Loss Force: 3.5969715294454616, time: 1.6758356094360352
Validation Loss Energy: 2.0729411973790652, Validation Loss Force: 3.6915058509347727, time: 0.10061764717102051
Test Loss Energy: 11.361700883944795, Test Loss Force: 13.298095080255477, time: 10.009442329406738

Epoch 15, Batch 100/101, Loss: 1.7865042686462402, Variance: 0.1624271422624588

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.061079476650714, Training Loss Force: 3.591021108789159, time: 1.629772424697876
Validation Loss Energy: 3.483051680613869, Validation Loss Force: 3.676055284237135, time: 0.10700798034667969
Test Loss Energy: 12.059179560138007, Test Loss Force: 13.638414502864146, time: 10.773298740386963

Epoch 16, Batch 100/101, Loss: 0.9730311632156372, Variance: 0.16192582249641418

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.084927990345213, Training Loss Force: 3.57658198692419, time: 1.727257490158081
Validation Loss Energy: 5.825593109131338, Validation Loss Force: 3.7409468627582867, time: 0.1007082462310791
Test Loss Energy: 11.21222597282172, Test Loss Force: 13.107255861533876, time: 10.02654767036438

Epoch 17, Batch 100/101, Loss: 1.1957510709762573, Variance: 0.15966856479644775

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.119668622153914, Training Loss Force: 3.567358817049296, time: 1.6765878200531006
Validation Loss Energy: 5.078403187041177, Validation Loss Force: 3.7317086538974618, time: 0.10333776473999023
Test Loss Energy: 13.118109665035893, Test Loss Force: 14.298027588976494, time: 10.177422285079956

Epoch 18, Batch 100/101, Loss: 1.8827364444732666, Variance: 0.16662529110908508

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.115771199999839, Training Loss Force: 3.573882524775207, time: 1.6072218418121338
Validation Loss Energy: 2.5218070138444286, Validation Loss Force: 3.7321494274718257, time: 0.11075091361999512
Test Loss Energy: 10.70135624924739, Test Loss Force: 13.5091986788263, time: 10.04283332824707

Epoch 19, Batch 100/101, Loss: 1.8117121458053589, Variance: 0.16300557553768158

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.09639451659101, Training Loss Force: 3.5747744998948296, time: 1.6138279438018799
Validation Loss Energy: 3.7632303662024373, Validation Loss Force: 3.697211970468318, time: 0.10210442543029785
Test Loss Energy: 10.689714803188453, Test Loss Force: 12.948470013260264, time: 10.042709350585938

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–‡â–â–‚â–ˆâ–‚â–ƒâ–…â–ƒâ–‡â–‚â–‚â–‡â–ƒâ–„â–…â–ƒâ–ˆâ–‚â–‚
wandb:   test_error_force â–â–ƒâ–‚â–â–ƒâ–‚â–ƒâ–ƒâ–‚â–…â–ƒâ–ƒâ–…â–ƒâ–…â–†â–„â–ˆâ–†â–„
wandb:          test_loss â–‚â–ˆâ–‚â–â–ˆâ–â–ƒâ–…â–‚â–‡â–‚â–‚â–‡â–â–„â–†â–‚â–ˆâ–‚â–
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–â–‚â–â–‚â–â–â–â–â–
wandb:  train_error_force â–ˆâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–â–‚â–‚â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–†â–‚â–„â–‡â–‡â–â–„â–‡â–†â–‚â–„â–‡â–‡â–â–„â–‡â–†â–‚â–„
wandb:  valid_error_force â–ˆâ–‡â–ˆâ–ƒâ–‡â–‡â–…â–…â–ˆâ–„â–â–ˆâ–…â–…â–‚â–â–…â–„â–„â–‚
wandb:         valid_loss â–ˆâ–†â–‚â–ƒâ–†â–†â–â–ƒâ–‡â–…â–â–„â–†â–†â–â–ƒâ–‡â–†â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3227
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.68971
wandb:   test_error_force 12.94847
wandb:          test_loss 9.09832
wandb: train_error_energy 4.09639
wandb:  train_error_force 3.57477
wandb:         train_loss 1.45669
wandb: valid_error_energy 3.76323
wandb:  valid_error_force 3.69721
wandb:         valid_loss 1.36469
wandb: 
wandb: ğŸš€ View run al_71_26 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/npqedl4s
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_133453-npqedl4s/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.063199281692505, Uncertainty Bias: -0.2599032521247864
0.0002708435 0.003616333
2.6228278 4.4443283
(48745, 22, 3)
Found uncertainty sample 0 after 41 steps.
Found uncertainty sample 1 after 933 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 1866 steps.
Found uncertainty sample 5 after 25 steps.
Found uncertainty sample 6 after 205 steps.
Found uncertainty sample 7 after 18 steps.
Found uncertainty sample 8 after 55 steps.
Found uncertainty sample 9 after 581 steps.
Found uncertainty sample 10 after 1714 steps.
Found uncertainty sample 11 after 185 steps.
Found uncertainty sample 12 after 20 steps.
Found uncertainty sample 13 after 156 steps.
Found uncertainty sample 14 after 51 steps.
Found uncertainty sample 15 after 3 steps.
Found uncertainty sample 16 after 462 steps.
Found uncertainty sample 17 after 60 steps.
Found uncertainty sample 18 after 1065 steps.
Found uncertainty sample 19 after 114 steps.
Found uncertainty sample 20 after 97 steps.
Found uncertainty sample 21 after 70 steps.
Found uncertainty sample 22 after 34 steps.
Found uncertainty sample 23 after 46 steps.
Found uncertainty sample 24 after 247 steps.
Found uncertainty sample 25 after 50 steps.
Found uncertainty sample 26 after 791 steps.
Found uncertainty sample 27 after 435 steps.
Found uncertainty sample 28 after 57 steps.
Found uncertainty sample 29 after 35 steps.
Found uncertainty sample 30 after 122 steps.
Found uncertainty sample 31 after 5 steps.
Found uncertainty sample 32 after 58 steps.
Found uncertainty sample 33 after 47 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 902 steps.
Found uncertainty sample 36 after 170 steps.
Found uncertainty sample 37 after 3 steps.
Found uncertainty sample 38 after 12 steps.
Found uncertainty sample 39 after 236 steps.
Found uncertainty sample 40 after 4 steps.
Found uncertainty sample 41 after 1287 steps.
Found uncertainty sample 42 after 1015 steps.
Found uncertainty sample 43 after 157 steps.
Found uncertainty sample 44 after 15 steps.
Found uncertainty sample 45 after 2575 steps.
Found uncertainty sample 46 after 10 steps.
Found uncertainty sample 47 after 27 steps.
Found uncertainty sample 48 after 129 steps.
Found uncertainty sample 49 after 172 steps.
Found uncertainty sample 50 after 27 steps.
Found uncertainty sample 51 after 1487 steps.
Found uncertainty sample 52 after 16 steps.
Found uncertainty sample 53 after 35 steps.
Found uncertainty sample 54 after 95 steps.
Found uncertainty sample 55 after 33 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 206 steps.
Found uncertainty sample 58 after 2214 steps.
Found uncertainty sample 59 after 168 steps.
Found uncertainty sample 60 after 32 steps.
Found uncertainty sample 61 after 1098 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 104 steps.
Found uncertainty sample 64 after 397 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1210 steps.
Found uncertainty sample 67 after 111 steps.
Found uncertainty sample 68 after 447 steps.
Found uncertainty sample 69 after 548 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 460 steps.
Found uncertainty sample 72 after 3382 steps.
Found uncertainty sample 73 after 167 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 51 steps.
Found uncertainty sample 76 after 490 steps.
Found uncertainty sample 77 after 193 steps.
Found uncertainty sample 78 after 1195 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 687 steps.
Found uncertainty sample 81 after 172 steps.
Found uncertainty sample 82 after 2462 steps.
Found uncertainty sample 83 after 44 steps.
Found uncertainty sample 84 after 1001 steps.
Found uncertainty sample 85 after 2925 steps.
Found uncertainty sample 86 after 456 steps.
Found uncertainty sample 87 after 84 steps.
Found uncertainty sample 88 after 14 steps.
Found uncertainty sample 89 after 852 steps.
Found uncertainty sample 90 after 211 steps.
Found uncertainty sample 91 after 648 steps.
Found uncertainty sample 92 after 366 steps.
Found uncertainty sample 93 after 106 steps.
Found uncertainty sample 94 after 472 steps.
Found uncertainty sample 95 after 137 steps.
Found uncertainty sample 96 after 2269 steps.
Found uncertainty sample 97 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 201 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_134602-95yl4xjg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_27
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/95yl4xjg
Training model 27. Added 100 samples to the dataset.
Epoch 0, Batch 100/104, Loss: 0.8175183534622192, Variance: 0.11325877159833908

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.517749599828292, Training Loss Force: 3.7651720561422737, time: 1.683361530303955
Validation Loss Energy: 1.8778165588062634, Validation Loss Force: 3.7401541605987836, time: 0.10761356353759766
Test Loss Energy: 11.780711284277741, Test Loss Force: 16.83258188040018, time: 10.034749746322632

Epoch 1, Batch 100/104, Loss: 0.664724588394165, Variance: 0.09741546213626862

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7201507445477644, Training Loss Force: 3.5823703434687153, time: 1.7130417823791504
Validation Loss Energy: 2.012817410610519, Validation Loss Force: 3.6719831108964076, time: 0.10287642478942871
Test Loss Energy: 11.330705212924489, Test Loss Force: 15.492344087978353, time: 10.68416976928711

Epoch 2, Batch 100/104, Loss: 0.7044376730918884, Variance: 0.09165836870670319

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6897385444233337, Training Loss Force: 3.56079124530351, time: 1.7513737678527832
Validation Loss Energy: 2.0788359176792115, Validation Loss Force: 3.7064025281319144, time: 0.10560965538024902
Test Loss Energy: 11.193123473581107, Test Loss Force: 15.35892818291384, time: 10.25910210609436

Epoch 3, Batch 100/104, Loss: 0.6900217533111572, Variance: 0.09038053452968597

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6690482365772472, Training Loss Force: 3.558284202684261, time: 1.6681897640228271
Validation Loss Energy: 1.9952188659966896, Validation Loss Force: 3.701355843274034, time: 0.11093854904174805
Test Loss Energy: 11.017882380086665, Test Loss Force: 14.63256345681596, time: 10.054311752319336

Epoch 4, Batch 100/104, Loss: 0.7512513399124146, Variance: 0.08858378231525421

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.712670034351579, Training Loss Force: 3.5634677648354023, time: 1.7004475593566895
Validation Loss Energy: 1.8320498662269902, Validation Loss Force: 3.713150405264683, time: 0.10387730598449707
Test Loss Energy: 10.926204582328912, Test Loss Force: 14.285715160340196, time: 10.268402099609375

Epoch 5, Batch 100/104, Loss: 0.6719918251037598, Variance: 0.08728250861167908

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6992008098301499, Training Loss Force: 3.5600562893895336, time: 1.7184624671936035
Validation Loss Energy: 2.058213414729109, Validation Loss Force: 3.6841572124462907, time: 0.10291576385498047
Test Loss Energy: 10.826536496959319, Test Loss Force: 14.428616052884093, time: 10.106882572174072

Epoch 6, Batch 100/104, Loss: 0.6468332409858704, Variance: 0.08517004549503326

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.7126268938103428, Training Loss Force: 3.5778751458538136, time: 1.7376222610473633
Validation Loss Energy: 1.934595409664682, Validation Loss Force: 3.720656804843055, time: 0.10378408432006836
Test Loss Energy: 10.72647688228792, Test Loss Force: 14.506501672447566, time: 10.066465854644775

Epoch 7, Batch 100/104, Loss: 0.8714168071746826, Variance: 0.08619686961174011

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6746227500813513, Training Loss Force: 3.560548546639695, time: 1.6447057723999023
Validation Loss Energy: 2.1517576136099565, Validation Loss Force: 3.6992548485476324, time: 0.11537790298461914
Test Loss Energy: 10.644610274147686, Test Loss Force: 14.169252654510814, time: 10.269309520721436

Epoch 8, Batch 100/104, Loss: 0.5464210510253906, Variance: 0.08599217236042023

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.7304914996846508, Training Loss Force: 3.5546989339699953, time: 1.696667194366455
Validation Loss Energy: 1.8990402684503707, Validation Loss Force: 3.6695176067781223, time: 0.11400604248046875
Test Loss Energy: 10.81909718841068, Test Loss Force: 14.263198576510282, time: 10.169840574264526

Epoch 9, Batch 100/104, Loss: 0.6154321432113647, Variance: 0.08365249633789062

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.7133951714126394, Training Loss Force: 3.5586502814654137, time: 1.7421104907989502
Validation Loss Energy: 2.1865364188469907, Validation Loss Force: 3.6722060027114813, time: 0.10779190063476562
Test Loss Energy: 10.719660571698606, Test Loss Force: 14.320389437568709, time: 10.337912321090698

Epoch 10, Batch 100/104, Loss: 0.5543253421783447, Variance: 0.08657372742891312

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.705360326565794, Training Loss Force: 3.5696170473545314, time: 1.6528418064117432
Validation Loss Energy: 2.050581986425299, Validation Loss Force: 3.6879094102243197, time: 0.10138082504272461
Test Loss Energy: 10.734510682481975, Test Loss Force: 14.178616808421417, time: 10.086158037185669

Epoch 11, Batch 100/104, Loss: 0.5737640857696533, Variance: 0.08345527946949005

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.7123630014329883, Training Loss Force: 3.5726391087209644, time: 1.6685869693756104
Validation Loss Energy: 2.0162465154578073, Validation Loss Force: 3.6861529960012414, time: 0.10930681228637695
Test Loss Energy: 10.90970949448645, Test Loss Force: 14.409249162594962, time: 10.083834171295166

Epoch 12, Batch 100/104, Loss: 0.47840601205825806, Variance: 0.08452460914850235

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.7015770600187592, Training Loss Force: 3.5652939023347994, time: 1.7090179920196533
Validation Loss Energy: 2.1365264873556313, Validation Loss Force: 3.6924671136377425, time: 0.10227370262145996
Test Loss Energy: 10.924982420169531, Test Loss Force: 14.563852034562894, time: 10.32612943649292

Epoch 13, Batch 100/104, Loss: 0.9137592315673828, Variance: 0.08203709125518799

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6919728280440678, Training Loss Force: 3.5742678221888537, time: 1.7029085159301758
Validation Loss Energy: 2.0265753577953864, Validation Loss Force: 3.702641980260913, time: 0.10970211029052734
Test Loss Energy: 11.033445004157635, Test Loss Force: 14.679736691808259, time: 10.252843856811523

Epoch 14, Batch 100/104, Loss: 0.964999794960022, Variance: 0.0902404710650444

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.0250390075962814, Training Loss Force: 4.066375592840289, time: 1.757093906402588
Validation Loss Energy: 5.669405622095772, Validation Loss Force: 4.26957109727639, time: 0.10394763946533203
Test Loss Energy: 11.087220041242647, Test Loss Force: 13.71691656388811, time: 10.347394466400146

Epoch 15, Batch 100/104, Loss: 1.3839986324310303, Variance: 0.10864853113889694

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 3.198163153988447, Training Loss Force: 4.082011433113636, time: 1.6911656856536865
Validation Loss Energy: 2.2387064637229956, Validation Loss Force: 3.6649840629670325, time: 0.10312867164611816
Test Loss Energy: 11.314973789319607, Test Loss Force: 13.897531491596798, time: 10.243571758270264

Epoch 16, Batch 100/104, Loss: 1.0433741807937622, Variance: 0.11629470437765121

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6243334580502333, Training Loss Force: 3.556517564949915, time: 1.6589534282684326
Validation Loss Energy: 1.7024968934142901, Validation Loss Force: 3.690474113447116, time: 0.10862874984741211
Test Loss Energy: 11.07115595226536, Test Loss Force: 13.71260764311556, time: 10.265065670013428

Epoch 17, Batch 100/104, Loss: 0.783570408821106, Variance: 0.11564403772354126

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.617027259444546, Training Loss Force: 3.537452949550901, time: 1.6889488697052002
Validation Loss Energy: 3.798120617347008, Validation Loss Force: 3.6834229609375937, time: 0.11554574966430664
Test Loss Energy: 10.73614552634622, Test Loss Force: 13.24296859767283, time: 10.742541551589966

Epoch 18, Batch 100/104, Loss: 1.1729841232299805, Variance: 0.10972929000854492

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6671794336991637, Training Loss Force: 3.528505985509801, time: 1.678917407989502
Validation Loss Energy: 2.310611006995143, Validation Loss Force: 3.6646670934378043, time: 0.10551071166992188
Test Loss Energy: 11.654051869911356, Test Loss Force: 14.017926455787192, time: 10.109387636184692

Epoch 19, Batch 100/104, Loss: 1.0158746242523193, Variance: 0.11465464532375336

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6573678806917425, Training Loss Force: 3.542253747722292, time: 1.7984495162963867
Validation Loss Energy: 1.569082113936478, Validation Loss Force: 3.674877809793469, time: 0.10372614860534668
Test Loss Energy: 11.228770826635396, Test Loss Force: 14.09616187342343, time: 10.219913244247437

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–„â–‚â–‡â–…
wandb:   test_error_force â–ˆâ–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–‚â–‚â–‚â–â–ƒâ–ƒ
wandb:          test_loss â–ƒâ–„â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–„â–…â–„â–â–…â–„
wandb: train_error_energy â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–ˆâ–…â–…â–†â–†
wandb:  train_error_force â–„â–‚â–â–â–â–â–‚â–â–â–â–‚â–‚â–â–‚â–ˆâ–ˆâ–â–â–â–
wandb:         train_loss â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–„â–ˆâ–„â–ƒâ–„â–„
wandb: valid_error_energy â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–â–…â–‚â–
wandb:  valid_error_force â–‚â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–
wandb:         valid_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–ƒâ–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 3317
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.22877
wandb:   test_error_force 14.09616
wandb:          test_loss 14.90589
wandb: train_error_energy 2.65737
wandb:  train_error_force 3.54225
wandb:         train_loss 1.03434
wandb: valid_error_energy 1.56908
wandb:  valid_error_force 3.67488
wandb:         valid_loss 0.7447
wandb: 
wandb: ğŸš€ View run al_71_27 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/95yl4xjg
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_134602-95yl4xjg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8117198944091797, Uncertainty Bias: -0.0716690868139267
7.6293945e-06 0.0017886162
2.5445082 4.6191235
(48745, 22, 3)
Found uncertainty sample 0 after 1604 steps.
Found uncertainty sample 1 after 35 steps.
Found uncertainty sample 2 after 964 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 734 steps.
Found uncertainty sample 5 after 2922 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 8 steps.
Found uncertainty sample 8 after 411 steps.
Found uncertainty sample 9 after 1546 steps.
Found uncertainty sample 10 after 109 steps.
Found uncertainty sample 11 after 89 steps.
Found uncertainty sample 12 after 194 steps.
Found uncertainty sample 13 after 414 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 298 steps.
Found uncertainty sample 17 after 332 steps.
Found uncertainty sample 18 after 2877 steps.
Found uncertainty sample 19 after 113 steps.
Found uncertainty sample 20 after 767 steps.
Found uncertainty sample 21 after 33 steps.
Found uncertainty sample 22 after 116 steps.
Found uncertainty sample 23 after 139 steps.
Found uncertainty sample 24 after 18 steps.
Found uncertainty sample 25 after 141 steps.
Found uncertainty sample 26 after 19 steps.
Found uncertainty sample 27 after 16 steps.
Found uncertainty sample 28 after 166 steps.
Found uncertainty sample 29 after 85 steps.
Found uncertainty sample 30 after 61 steps.
Found uncertainty sample 31 after 623 steps.
Found uncertainty sample 32 after 31 steps.
Found uncertainty sample 33 after 24 steps.
Found uncertainty sample 34 after 139 steps.
Found uncertainty sample 35 after 58 steps.
Found uncertainty sample 36 after 17 steps.
Found uncertainty sample 37 after 93 steps.
Found uncertainty sample 38 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 62 steps.
Found uncertainty sample 41 after 234 steps.
Found uncertainty sample 42 after 9 steps.
Found uncertainty sample 43 after 232 steps.
Found uncertainty sample 44 after 593 steps.
Found uncertainty sample 45 after 572 steps.
Found uncertainty sample 46 after 16 steps.
Found uncertainty sample 47 after 8 steps.
Found uncertainty sample 48 after 6 steps.
Found uncertainty sample 49 after 37 steps.
Found uncertainty sample 50 after 324 steps.
Found uncertainty sample 51 after 15 steps.
Found uncertainty sample 52 after 78 steps.
Found uncertainty sample 53 after 102 steps.
Found uncertainty sample 54 after 605 steps.
Found uncertainty sample 55 after 138 steps.
Found uncertainty sample 56 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1586 steps.
Found uncertainty sample 60 after 63 steps.
Found uncertainty sample 61 after 162 steps.
Found uncertainty sample 62 after 81 steps.
Found uncertainty sample 63 after 12 steps.
Found uncertainty sample 64 after 820 steps.
Found uncertainty sample 65 after 26 steps.
Found uncertainty sample 66 after 159 steps.
Found uncertainty sample 67 after 204 steps.
Found uncertainty sample 68 after 370 steps.
Found uncertainty sample 69 after 800 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 337 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1633 steps.
Found uncertainty sample 75 after 133 steps.
Found uncertainty sample 76 after 16 steps.
Found uncertainty sample 77 after 19 steps.
Found uncertainty sample 78 after 19 steps.
Found uncertainty sample 79 after 369 steps.
Found uncertainty sample 80 after 1193 steps.
Found uncertainty sample 81 after 516 steps.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 84 steps.
Found uncertainty sample 84 after 6 steps.
Found uncertainty sample 85 after 19 steps.
Found uncertainty sample 86 after 29 steps.
Found uncertainty sample 87 after 391 steps.
Found uncertainty sample 88 after 644 steps.
Found uncertainty sample 89 after 11 steps.
Found uncertainty sample 90 after 403 steps.
Found uncertainty sample 91 after 1050 steps.
Found uncertainty sample 92 after 88 steps.
Found uncertainty sample 93 after 137 steps.
Found uncertainty sample 94 after 98 steps.
Found uncertainty sample 95 after 8 steps.
Found uncertainty sample 96 after 54 steps.
Found uncertainty sample 97 after 44 steps.
Found uncertainty sample 98 after 190 steps.
Found uncertainty sample 99 after 100 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_135601-9q4i0vo6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_28
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/9q4i0vo6
Training model 28. Added 100 samples to the dataset.
Epoch 0, Batch 100/107, Loss: 0.7125653028488159, Variance: 0.09517316520214081

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.426558613533693, Training Loss Force: 3.7458713438754536, time: 1.7754850387573242
Validation Loss Energy: 1.602908730238238, Validation Loss Force: 3.667124915470555, time: 0.10225367546081543
Test Loss Energy: 12.091872133263273, Test Loss Force: 16.723575335507295, time: 9.392661094665527

Epoch 1, Batch 100/107, Loss: 0.5414764881134033, Variance: 0.08907313644886017

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7102044308645863, Training Loss Force: 3.593445038381496, time: 1.7621378898620605
Validation Loss Energy: 1.584842246414241, Validation Loss Force: 3.7148650690229, time: 0.10778617858886719
Test Loss Energy: 11.988045236436466, Test Loss Force: 16.378602673346432, time: 9.356734275817871

Epoch 2, Batch 100/107, Loss: 0.6493338346481323, Variance: 0.08728627860546112

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7028119390404772, Training Loss Force: 3.5898723473124265, time: 1.8696870803833008
Validation Loss Energy: 1.9345924036312119, Validation Loss Force: 3.7224340386312775, time: 0.11879634857177734
Test Loss Energy: 11.37594145484539, Test Loss Force: 15.759441879316714, time: 11.950242519378662

Epoch 3, Batch 100/107, Loss: 0.5367681980133057, Variance: 0.08752366155385971

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7115420002621347, Training Loss Force: 3.591668708796487, time: 2.0244479179382324
Validation Loss Energy: 2.07205976156491, Validation Loss Force: 3.6900691896052464, time: 0.12275052070617676
Test Loss Energy: 11.059207120194024, Test Loss Force: 15.237592976687331, time: 11.460262775421143

Epoch 4, Batch 100/107, Loss: 0.5579927563667297, Variance: 0.08326533436775208

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6996076988152753, Training Loss Force: 3.589578871459113, time: 1.7194464206695557
Validation Loss Energy: 1.459402529399692, Validation Loss Force: 3.674538350680502, time: 0.10359644889831543
Test Loss Energy: 11.849715062958763, Test Loss Force: 15.98493980941114, time: 10.207960367202759

Epoch 5, Batch 100/107, Loss: 0.8566350936889648, Variance: 0.08441080152988434

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7148091320349406, Training Loss Force: 3.609399417953526, time: 1.7473728656768799
Validation Loss Energy: 1.6927220519117792, Validation Loss Force: 3.679734001197008, time: 0.10799145698547363
Test Loss Energy: 11.758498206654384, Test Loss Force: 15.980643479354695, time: 9.89791488647461

Epoch 6, Batch 100/107, Loss: 0.44150203466415405, Variance: 0.0833573192358017

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.694643316219566, Training Loss Force: 3.6087140714566406, time: 1.7301652431488037
Validation Loss Energy: 1.882036027787022, Validation Loss Force: 3.706262580573699, time: 0.1062009334564209
Test Loss Energy: 11.245346117339448, Test Loss Force: 15.653592307263809, time: 9.921105861663818

Epoch 7, Batch 100/107, Loss: 0.6191491484642029, Variance: 0.08657799661159515

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6962660852536666, Training Loss Force: 3.592468494748259, time: 1.7205541133880615
Validation Loss Energy: 1.9520845123932882, Validation Loss Force: 3.7387884193922356, time: 0.10634732246398926
Test Loss Energy: 11.126951225708126, Test Loss Force: 15.31613737049678, time: 10.220137119293213

Epoch 8, Batch 100/107, Loss: 0.5209736824035645, Variance: 0.08145830035209656

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.7234287420766472, Training Loss Force: 3.6043425933254256, time: 1.7239770889282227
Validation Loss Energy: 1.586947193517983, Validation Loss Force: 3.7489175275059177, time: 0.11020278930664062
Test Loss Energy: 11.942946531767708, Test Loss Force: 16.47135930894524, time: 9.963735342025757

Epoch 9, Batch 100/107, Loss: 0.4865525960922241, Variance: 0.08118678629398346

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6522205268442174, Training Loss Force: 3.61375056243778, time: 1.7976057529449463
Validation Loss Energy: 1.973201118178204, Validation Loss Force: 3.6969141504856395, time: 0.10492062568664551
Test Loss Energy: 12.14941196233984, Test Loss Force: 16.39688116151349, time: 10.132835626602173

Epoch 10, Batch 100/107, Loss: 0.6066986322402954, Variance: 0.08503451943397522

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.703121551123299, Training Loss Force: 3.6010164552694595, time: 1.7084178924560547
Validation Loss Energy: 2.1208172904449984, Validation Loss Force: 3.6845939779760277, time: 0.1088254451751709
Test Loss Energy: 11.270437040953201, Test Loss Force: 15.833390535840742, time: 9.977594137191772

Epoch 11, Batch 100/107, Loss: 0.7557139992713928, Variance: 0.08086025714874268

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.663794496546954, Training Loss Force: 3.6051190885007367, time: 1.7226762771606445
Validation Loss Energy: 2.1542444939662513, Validation Loss Force: 3.6772146111454083, time: 0.1042013168334961
Test Loss Energy: 11.39607088087088, Test Loss Force: 15.473294609642231, time: 9.976279973983765

Epoch 12, Batch 100/107, Loss: 0.6873382329940796, Variance: 0.08283087611198425

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.713076878454705, Training Loss Force: 3.604889142027676, time: 1.7319576740264893
Validation Loss Energy: 1.6033456071025485, Validation Loss Force: 3.6952030494337578, time: 0.1061701774597168
Test Loss Energy: 11.764928560499575, Test Loss Force: 15.707336245280041, time: 10.213040828704834

Epoch 13, Batch 100/107, Loss: 0.7169001698493958, Variance: 0.08287950605154037

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6847371852427802, Training Loss Force: 3.600635672300741, time: 1.674424171447754
Validation Loss Energy: 1.5803934838804634, Validation Loss Force: 3.733132177747378, time: 0.11132287979125977
Test Loss Energy: 11.785208744029122, Test Loss Force: 15.828899702536615, time: 10.109637022018433

Epoch 14, Batch 100/107, Loss: 0.7729674577713013, Variance: 0.08320479094982147

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.7064215182710494, Training Loss Force: 3.5993697836390024, time: 1.8361713886260986
Validation Loss Energy: 2.0720293672264907, Validation Loss Force: 3.7111518383454083, time: 0.10882997512817383
Test Loss Energy: 11.268279948174616, Test Loss Force: 15.301582353456308, time: 10.093451023101807

Epoch 15, Batch 100/107, Loss: 0.7963517308235168, Variance: 0.08280454576015472

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.7160257775191092, Training Loss Force: 3.5964410017954735, time: 1.801048994064331
Validation Loss Energy: 2.0112372390164426, Validation Loss Force: 3.692261367346905, time: 0.10750770568847656
Test Loss Energy: 11.07147213312612, Test Loss Force: 14.97311375585217, time: 10.040122985839844

Epoch 16, Batch 100/107, Loss: 0.5824648141860962, Variance: 0.08174798637628555

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6606317445780867, Training Loss Force: 3.616212290545174, time: 1.6896946430206299
Validation Loss Energy: 1.7339002570682047, Validation Loss Force: 3.7578600317409507, time: 0.10390281677246094
Test Loss Energy: 11.395262967261031, Test Loss Force: 15.567318482164072, time: 10.007603645324707

Epoch 17, Batch 100/107, Loss: 1.0122414827346802, Variance: 0.07942357659339905

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.683408386304255, Training Loss Force: 3.5945926980508283, time: 1.716569423675537
Validation Loss Energy: 1.697722143250874, Validation Loss Force: 3.771651713301426, time: 0.10765314102172852
Test Loss Energy: 11.343230900312236, Test Loss Force: 15.166284933746141, time: 10.166428327560425

Epoch 18, Batch 100/107, Loss: 0.7269998788833618, Variance: 0.08185939490795135

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6868419408926767, Training Loss Force: 3.5976526653176286, time: 1.6989924907684326
Validation Loss Energy: 1.8863366596714712, Validation Loss Force: 3.6887961900979773, time: 0.10543227195739746
Test Loss Energy: 10.94388499749131, Test Loss Force: 15.130396547753328, time: 9.919646978378296

Epoch 19, Batch 100/107, Loss: 0.7065879702568054, Variance: 0.08315921574831009

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.7123419963969313, Training Loss Force: 3.595548154707844, time: 1.7270493507385254
Validation Loss Energy: 2.0394249261994415, Validation Loss Force: 3.7764887551584154, time: 0.10544490814208984
Test Loss Energy: 11.06270061023289, Test Loss Force: 15.10509502342819, time: 10.757611274719238

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‡â–„â–‚â–†â–†â–ƒâ–‚â–‡â–ˆâ–ƒâ–„â–†â–†â–ƒâ–‚â–„â–ƒâ–â–‚
wandb:   test_error_force â–ˆâ–‡â–„â–‚â–…â–…â–„â–‚â–‡â–‡â–„â–ƒâ–„â–„â–‚â–â–ƒâ–‚â–‚â–‚
wandb:          test_loss â–ƒâ–…â–â–â–‡â–…â–ƒâ–‚â–‡â–ˆâ–ƒâ–„â–‡â–‡â–ƒâ–ƒâ–…â–†â–‚â–‚
wandb: train_error_energy â–ˆâ–‚â–â–‚â–â–‚â–â–â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–‚â–†â–‡â–â–ƒâ–…â–†â–‚â–†â–ˆâ–ˆâ–‚â–‚â–‡â–‡â–„â–ƒâ–…â–‡
wandb:  valid_error_force â–â–„â–…â–‚â–â–‚â–„â–†â–†â–ƒâ–‚â–‚â–ƒâ–…â–„â–ƒâ–‡â–ˆâ–‚â–ˆ
wandb:         valid_loss â–ƒâ–ƒâ–…â–†â–â–„â–„â–…â–ƒâ–ˆâ–‡â–ˆâ–ƒâ–ƒâ–‡â–†â–…â–„â–„â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 3407
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.0627
wandb:   test_error_force 15.1051
wandb:          test_loss 17.13956
wandb: train_error_energy 1.71234
wandb:  train_error_force 3.59555
wandb:         train_loss 0.64924
wandb: valid_error_energy 2.03942
wandb:  valid_error_force 3.77649
wandb:         valid_loss 0.85758
wandb: 
wandb: ğŸš€ View run al_71_28 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/9q4i0vo6
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_135601-9q4i0vo6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2803821563720703, Uncertainty Bias: -0.025931626558303833
0.0001449585 0.010378122
2.725019 4.905152
(48745, 22, 3)
Found uncertainty sample 0 after 64 steps.
Found uncertainty sample 1 after 1935 steps.
Found uncertainty sample 2 after 99 steps.
Found uncertainty sample 3 after 469 steps.
Found uncertainty sample 4 after 1049 steps.
Found uncertainty sample 5 after 73 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 369 steps.
Found uncertainty sample 8 after 69 steps.
Found uncertainty sample 9 after 43 steps.
Found uncertainty sample 10 after 6 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 4 steps.
Found uncertainty sample 13 after 59 steps.
Found uncertainty sample 14 after 126 steps.
Found uncertainty sample 15 after 8 steps.
Found uncertainty sample 16 after 57 steps.
Found uncertainty sample 17 after 298 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 81 steps.
Found uncertainty sample 20 after 50 steps.
Found uncertainty sample 21 after 237 steps.
Found uncertainty sample 22 after 166 steps.
Found uncertainty sample 23 after 50 steps.
Found uncertainty sample 24 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 13 steps.
Found uncertainty sample 27 after 15 steps.
Found uncertainty sample 28 after 182 steps.
Found uncertainty sample 29 after 346 steps.
Found uncertainty sample 30 after 173 steps.
Found uncertainty sample 31 after 252 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 26 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 24 steps.
Found uncertainty sample 36 after 10 steps.
Found uncertainty sample 37 after 42 steps.
Found uncertainty sample 38 after 575 steps.
Found uncertainty sample 39 after 2440 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 290 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 15 steps.
Found uncertainty sample 45 after 58 steps.
Found uncertainty sample 46 after 171 steps.
Found uncertainty sample 47 after 117 steps.
Found uncertainty sample 48 after 349 steps.
Found uncertainty sample 49 after 46 steps.
Found uncertainty sample 50 after 5 steps.
Found uncertainty sample 51 after 226 steps.
Found uncertainty sample 52 after 12 steps.
Found uncertainty sample 53 after 759 steps.
Found uncertainty sample 54 after 916 steps.
Found uncertainty sample 55 after 48 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 187 steps.
Found uncertainty sample 58 after 17 steps.
Found uncertainty sample 59 after 566 steps.
Found uncertainty sample 60 after 1989 steps.
Found uncertainty sample 61 after 156 steps.
Found uncertainty sample 62 after 133 steps.
Found uncertainty sample 63 after 50 steps.
Found uncertainty sample 64 after 1577 steps.
Found uncertainty sample 65 after 6 steps.
Found uncertainty sample 66 after 4 steps.
Found uncertainty sample 67 after 21 steps.
Found uncertainty sample 68 after 207 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 16 steps.
Found uncertainty sample 71 after 86 steps.
Found uncertainty sample 72 after 49 steps.
Found uncertainty sample 73 after 13 steps.
Found uncertainty sample 74 after 179 steps.
Found uncertainty sample 75 after 170 steps.
Found uncertainty sample 76 after 203 steps.
Found uncertainty sample 77 after 21 steps.
Found uncertainty sample 78 after 116 steps.
Found uncertainty sample 79 after 174 steps.
Found uncertainty sample 80 after 4 steps.
Found uncertainty sample 81 after 323 steps.
Found uncertainty sample 82 after 528 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 50 steps.
Found uncertainty sample 85 after 405 steps.
Found uncertainty sample 86 after 201 steps.
Found uncertainty sample 87 after 35 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 883 steps.
Found uncertainty sample 90 after 94 steps.
Found uncertainty sample 91 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 110 steps.
Found uncertainty sample 94 after 10 steps.
Found uncertainty sample 95 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 339 steps.
Found uncertainty sample 98 after 266 steps.
Found uncertainty sample 99 after 5 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_140505-s8mlj69s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_29
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/s8mlj69s
Training model 29. Added 100 samples to the dataset.
Epoch 0, Batch 100/110, Loss: 2.3534600734710693, Variance: 0.14485955238342285

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.963306668291159, Training Loss Force: 4.1775734003904885, time: 1.9318499565124512
Validation Loss Energy: 2.7794308736345337, Validation Loss Force: 3.8716317185070244, time: 0.12209296226501465
Test Loss Energy: 9.663582180060354, Test Loss Force: 12.218239093273908, time: 11.405960321426392

Epoch 1, Batch 100/110, Loss: 2.011122465133667, Variance: 0.16250702738761902

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 6.1114923391655775, Training Loss Force: 3.8569950176590777, time: 1.9584324359893799
Validation Loss Energy: 1.5437323450183287, Validation Loss Force: 4.075146196494666, time: 0.13306570053100586
Test Loss Energy: 9.523570614374645, Test Loss Force: 11.912500720635839, time: 11.414093017578125

Epoch 2, Batch 100/110, Loss: 4.539758682250977, Variance: 0.09631627798080444

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.532500262682278, Training Loss Force: 4.145742273113129, time: 2.0392885208129883
Validation Loss Energy: 1.3726498508042153, Validation Loss Force: 4.393221278032403, time: 0.11499953269958496
Test Loss Energy: 10.897539993794076, Test Loss Force: 14.478023184023215, time: 11.553574562072754

Epoch 3, Batch 100/110, Loss: 1.1947959661483765, Variance: 0.13929885625839233

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.018523139140593, Training Loss Force: 3.7884820230463605, time: 1.999194622039795
Validation Loss Energy: 2.1134637532353064, Validation Loss Force: 3.685355283785955, time: 0.12361454963684082
Test Loss Energy: 10.193085897665808, Test Loss Force: 12.628704818112007, time: 11.319715023040771

Epoch 4, Batch 100/110, Loss: 1.5563944578170776, Variance: 0.14393135905265808

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.100518297103887, Training Loss Force: 3.6039585005699837, time: 1.921706199645996
Validation Loss Energy: 5.933775800334624, Validation Loss Force: 3.6949290550496934, time: 0.11987638473510742
Test Loss Energy: 10.909447668213433, Test Loss Force: 12.509602262950398, time: 11.140835523605347

Epoch 5, Batch 100/110, Loss: 1.1886337995529175, Variance: 0.14550179243087769

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.133423714954824, Training Loss Force: 3.59722098564956, time: 2.0060739517211914
Validation Loss Energy: 2.099069640300524, Validation Loss Force: 3.7305586790933174, time: 0.12859153747558594
Test Loss Energy: 11.340489416440485, Test Loss Force: 13.21718888727649, time: 11.69235873222351

Epoch 6, Batch 100/110, Loss: 1.7562839984893799, Variance: 0.15430444478988647

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.120562609252613, Training Loss Force: 3.5888015373111064, time: 1.7978935241699219
Validation Loss Energy: 5.386614916090897, Validation Loss Force: 3.6938164886620655, time: 0.10190749168395996
Test Loss Energy: 12.959386802985403, Test Loss Force: 13.57014809714019, time: 9.776948690414429

Epoch 7, Batch 100/110, Loss: 1.4044544696807861, Variance: 0.153865784406662

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.113708714278805, Training Loss Force: 3.5899243323095926, time: 1.7976994514465332
Validation Loss Energy: 2.4931498287672413, Validation Loss Force: 3.680923499774554, time: 0.10733771324157715
Test Loss Energy: 10.534922963224702, Test Loss Force: 12.933265343248156, time: 9.544106006622314

Epoch 8, Batch 100/110, Loss: 1.7084683179855347, Variance: 0.14715009927749634

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.152650724051952, Training Loss Force: 3.5807575779776957, time: 1.8051338195800781
Validation Loss Energy: 5.787099180529071, Validation Loss Force: 3.79080593590328, time: 0.10814619064331055
Test Loss Energy: 11.100726060439229, Test Loss Force: 12.67831719657655, time: 9.55028223991394

Epoch 9, Batch 100/110, Loss: 1.2732348442077637, Variance: 0.14982366561889648

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.141880348028516, Training Loss Force: 3.598140637561952, time: 1.7972519397735596
Validation Loss Energy: 1.8904933916203648, Validation Loss Force: 3.692443844711993, time: 0.11424827575683594
Test Loss Energy: 11.046624825879313, Test Loss Force: 13.125889300640841, time: 9.90345048904419

Epoch 10, Batch 100/110, Loss: 2.2757842540740967, Variance: 0.1611652374267578

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.112098566357024, Training Loss Force: 3.608630127591258, time: 1.8500089645385742
Validation Loss Energy: 5.538715533622105, Validation Loss Force: 3.7091467026863283, time: 0.10384869575500488
Test Loss Energy: 13.060269570696367, Test Loss Force: 13.636663787105913, time: 9.63696837425232

Epoch 11, Batch 100/110, Loss: 1.4709452390670776, Variance: 0.15147928893566132

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.156873787115152, Training Loss Force: 3.5918945919076264, time: 1.7597291469573975
Validation Loss Energy: 2.434835117474786, Validation Loss Force: 3.772890867098514, time: 0.10661959648132324
Test Loss Energy: 10.58471055316191, Test Loss Force: 13.085052968400877, time: 9.687364339828491

Epoch 12, Batch 100/110, Loss: 1.6734685897827148, Variance: 0.14945295453071594

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.08565471259036, Training Loss Force: 3.5862346154939115, time: 1.7702760696411133
Validation Loss Energy: 5.86885972820742, Validation Loss Force: 3.6863096437454015, time: 0.1029057502746582
Test Loss Energy: 11.248764585363244, Test Loss Force: 12.811523410259179, time: 9.54041337966919

Epoch 13, Batch 100/110, Loss: 1.202956199645996, Variance: 0.15208016335964203

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.179338871175012, Training Loss Force: 3.598743929248868, time: 1.8207554817199707
Validation Loss Energy: 2.107504013544549, Validation Loss Force: 3.6881178285449083, time: 0.10199713706970215
Test Loss Energy: 11.37820181014895, Test Loss Force: 13.45096021422922, time: 9.530789375305176

Epoch 14, Batch 100/110, Loss: 1.9351470470428467, Variance: 0.15853968262672424

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.129136493825754, Training Loss Force: 3.589999634966216, time: 1.7866005897521973
Validation Loss Energy: 5.2547188515228855, Validation Loss Force: 3.7654423614980836, time: 0.10648679733276367
Test Loss Energy: 12.978045815235753, Test Loss Force: 13.826892416718398, time: 9.758558511734009

Epoch 15, Batch 100/110, Loss: 1.2788108587265015, Variance: 0.15732425451278687

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.161208204993966, Training Loss Force: 3.5883487679425716, time: 1.8093881607055664
Validation Loss Energy: 2.441233292713875, Validation Loss Force: 3.694603067420055, time: 0.10555291175842285
Test Loss Energy: 10.582222944759117, Test Loss Force: 12.973814523222828, time: 9.534499883651733

Epoch 16, Batch 100/110, Loss: 1.5501461029052734, Variance: 0.15020623803138733

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.151615753813149, Training Loss Force: 3.58813511487279, time: 1.765429973602295
Validation Loss Energy: 6.077935368114609, Validation Loss Force: 3.7180682760216444, time: 0.10663175582885742
Test Loss Energy: 11.535766997822654, Test Loss Force: 12.791831334415662, time: 11.34986138343811

Epoch 17, Batch 100/110, Loss: 1.1101016998291016, Variance: 0.15362736582756042

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.165179115492053, Training Loss Force: 3.597945484249665, time: 2.1818552017211914
Validation Loss Energy: 1.9485093365862716, Validation Loss Force: 3.6709099569476087, time: 0.12096524238586426
Test Loss Energy: 11.435171881342404, Test Loss Force: 13.790620372952954, time: 11.473932981491089

Epoch 18, Batch 100/110, Loss: 1.6070626974105835, Variance: 0.159677192568779

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.109552338600546, Training Loss Force: 3.5738905748761205, time: 1.9767146110534668
Validation Loss Energy: 5.689271495288011, Validation Loss Force: 3.707927700446947, time: 0.11525893211364746
Test Loss Energy: 13.709174425301015, Test Loss Force: 13.970295077278443, time: 10.137052297592163

Epoch 19, Batch 100/110, Loss: 1.2611593008041382, Variance: 0.1600652039051056

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.208056244734476, Training Loss Force: 3.5863925769860314, time: 1.7706570625305176
Validation Loss Energy: 2.6144264677622377, Validation Loss Force: 3.6405772978818876, time: 0.10966324806213379
Test Loss Energy: 10.676021732582832, Test Loss Force: 13.15262921635316, time: 10.233779430389404

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–ƒâ–‚â–ƒâ–„â–‡â–ƒâ–„â–„â–‡â–ƒâ–„â–„â–‡â–ƒâ–„â–„â–ˆâ–ƒ
wandb:   test_error_force â–‚â–â–ˆâ–ƒâ–ƒâ–…â–†â–„â–ƒâ–„â–†â–„â–ƒâ–…â–†â–„â–ƒâ–†â–‡â–„
wandb:          test_loss â–‚â–â–ˆâ–ƒâ–ƒâ–„â–…â–ƒâ–ƒâ–ƒâ–…â–ƒâ–ƒâ–„â–…â–‚â–ƒâ–ƒâ–…â–‚
wandb: train_error_energy â–ˆâ–ˆâ–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:  train_error_force â–ˆâ–„â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–â–â–‚â–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–‡â–ƒâ–ˆâ–‚â–‡â–ƒ
wandb:  valid_error_force â–ƒâ–…â–ˆâ–â–‚â–‚â–â–â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–
wandb:         valid_loss â–ƒâ–ƒâ–â–‚â–ˆâ–‚â–‡â–‚â–ˆâ–‚â–‡â–‚â–ˆâ–‚â–‡â–‚â–ˆâ–‚â–‡â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 3497
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.67602
wandb:   test_error_force 13.15263
wandb:          test_loss 9.64866
wandb: train_error_energy 4.20806
wandb:  train_error_force 3.58639
wandb:         train_loss 1.48319
wandb: valid_error_energy 2.61443
wandb:  valid_error_force 3.64058
wandb:         valid_loss 1.09962
wandb: 
wandb: ğŸš€ View run al_71_29 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/s8mlj69s
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_140505-s8mlj69s/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8315279483795166, Uncertainty Bias: -0.2000332474708557
0.00018119812 0.0066337585
2.5408149 4.304309
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 2451 steps.
Found uncertainty sample 2 after 55 steps.
Found uncertainty sample 3 after 12 steps.
Found uncertainty sample 4 after 930 steps.
Found uncertainty sample 5 after 1445 steps.
Found uncertainty sample 6 after 342 steps.
Found uncertainty sample 7 after 71 steps.
Found uncertainty sample 8 after 1890 steps.
Found uncertainty sample 9 after 15 steps.
Found uncertainty sample 10 after 999 steps.
Found uncertainty sample 11 after 955 steps.
Found uncertainty sample 12 after 730 steps.
Found uncertainty sample 13 after 32 steps.
Found uncertainty sample 14 after 472 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 92 steps.
Found uncertainty sample 17 after 73 steps.
Found uncertainty sample 18 after 881 steps.
Found uncertainty sample 19 after 39 steps.
Found uncertainty sample 20 after 24 steps.
Found uncertainty sample 21 after 711 steps.
Found uncertainty sample 22 after 3641 steps.
Found uncertainty sample 23 after 2355 steps.
Found uncertainty sample 24 after 303 steps.
Found uncertainty sample 25 after 22 steps.
Found uncertainty sample 26 after 757 steps.
Found uncertainty sample 27 after 1941 steps.
Found uncertainty sample 28 after 549 steps.
Found uncertainty sample 29 after 110 steps.
Found uncertainty sample 30 after 2107 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1051 steps.
Found uncertainty sample 33 after 730 steps.
Found uncertainty sample 34 after 506 steps.
Found uncertainty sample 35 after 1349 steps.
Found uncertainty sample 36 after 36 steps.
Found uncertainty sample 37 after 1063 steps.
Found uncertainty sample 38 after 1008 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 361 steps.
Found uncertainty sample 41 after 1047 steps.
Found uncertainty sample 42 after 6 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1171 steps.
Found uncertainty sample 45 after 671 steps.
Found uncertainty sample 46 after 872 steps.
Found uncertainty sample 47 after 139 steps.
Found uncertainty sample 48 after 511 steps.
Found uncertainty sample 49 after 170 steps.
Found uncertainty sample 50 after 404 steps.
Found uncertainty sample 51 after 233 steps.
Found uncertainty sample 52 after 195 steps.
Found uncertainty sample 53 after 280 steps.
Found uncertainty sample 54 after 47 steps.
Found uncertainty sample 55 after 899 steps.
Found uncertainty sample 56 after 3541 steps.
Found uncertainty sample 57 after 24 steps.
Found uncertainty sample 58 after 9 steps.
Found uncertainty sample 59 after 660 steps.
Found uncertainty sample 60 after 1861 steps.
Found uncertainty sample 61 after 810 steps.
Found uncertainty sample 62 after 568 steps.
Found uncertainty sample 63 after 34 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 1327 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 15 steps.
Found uncertainty sample 69 after 251 steps.
Found uncertainty sample 70 after 1154 steps.
Found uncertainty sample 71 after 2627 steps.
Found uncertainty sample 72 after 3647 steps.
Found uncertainty sample 73 after 629 steps.
Found uncertainty sample 74 after 1702 steps.
Found uncertainty sample 75 after 14 steps.
Found uncertainty sample 76 after 406 steps.
Found uncertainty sample 77 after 46 steps.
Found uncertainty sample 78 after 58 steps.
Found uncertainty sample 79 after 127 steps.
Found uncertainty sample 80 after 1361 steps.
Found uncertainty sample 81 after 363 steps.
Found uncertainty sample 82 after 574 steps.
Found uncertainty sample 83 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1448 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 10 steps.
Found uncertainty sample 88 after 324 steps.
Found uncertainty sample 89 after 372 steps.
Found uncertainty sample 90 after 363 steps.
Found uncertainty sample 91 after 303 steps.
Found uncertainty sample 92 after 102 steps.
Found uncertainty sample 93 after 527 steps.
Found uncertainty sample 94 after 806 steps.
Found uncertainty sample 95 after 13 steps.
Found uncertainty sample 96 after 276 steps.
Found uncertainty sample 97 after 544 steps.
Found uncertainty sample 98 after 524 steps.
Found uncertainty sample 99 after 19 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_141910-909v4us7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_30
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/909v4us7
Training model 30. Added 98 samples to the dataset.
Epoch 0, Batch 100/113, Loss: 1.6546592712402344, Variance: 0.12136875092983246

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.8945142029389825, Training Loss Force: 3.8861122733777274, time: 1.7988989353179932
Validation Loss Energy: 4.528124934754462, Validation Loss Force: 3.6830018935826443, time: 0.11482620239257812
Test Loss Energy: 11.48949606017871, Test Loss Force: 13.764619327093948, time: 10.465404033660889

Epoch 1, Batch 100/113, Loss: 2.287987232208252, Variance: 0.14652296900749207

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.052375806272449, Training Loss Force: 3.6172298605679987, time: 1.8098793029785156
Validation Loss Energy: 2.3593892441058033, Validation Loss Force: 3.648344109026422, time: 0.11621999740600586
Test Loss Energy: 10.271330012222613, Test Loss Force: 12.501203919556058, time: 10.43181300163269

Epoch 2, Batch 100/113, Loss: 1.6103333234786987, Variance: 0.1499902606010437

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.139780071934089, Training Loss Force: 3.5945900855051014, time: 1.8498203754425049
Validation Loss Energy: 3.2228456380730117, Validation Loss Force: 3.6858379191597535, time: 0.1168372631072998
Test Loss Energy: 11.724537706448313, Test Loss Force: 13.445534902639022, time: 10.703246593475342

Epoch 3, Batch 100/113, Loss: 0.9295656085014343, Variance: 0.15125791728496552

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.064222516194759, Training Loss Force: 3.6104379786915812, time: 1.7588379383087158
Validation Loss Energy: 5.13833191563789, Validation Loss Force: 3.6920415928998747, time: 0.1244804859161377
Test Loss Energy: 13.03499390397153, Test Loss Force: 13.772834442926369, time: 11.113683223724365

Epoch 4, Batch 100/113, Loss: 1.3000456094741821, Variance: 0.15760639309883118

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.13342268850022, Training Loss Force: 3.609699666919387, time: 1.8077456951141357
Validation Loss Energy: 5.044387802970956, Validation Loss Force: 3.6542200137837373, time: 0.11110138893127441
Test Loss Energy: 12.86016962380212, Test Loss Force: 13.54689045173972, time: 10.677183151245117

Epoch 5, Batch 100/113, Loss: 1.9042644500732422, Variance: 0.15772846341133118

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.136898432680963, Training Loss Force: 3.604511028510455, time: 1.8654227256774902
Validation Loss Energy: 2.2753644852170876, Validation Loss Force: 3.6888008661500415, time: 0.1251845359802246
Test Loss Energy: 11.059932690520549, Test Loss Force: 12.515272156195776, time: 10.582258462905884

Epoch 6, Batch 100/113, Loss: 1.7391245365142822, Variance: 0.15713217854499817

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.113681087352428, Training Loss Force: 3.605634461349456, time: 1.8474886417388916
Validation Loss Energy: 3.3622950854091864, Validation Loss Force: 3.909396628325523, time: 0.11921882629394531
Test Loss Energy: 10.414944103548656, Test Loss Force: 12.378237497456833, time: 10.747014045715332

Epoch 7, Batch 100/113, Loss: 0.9763145446777344, Variance: 0.15399351716041565

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.0560552227681335, Training Loss Force: 3.7568526942594844, time: 1.8101587295532227
Validation Loss Energy: 6.541371094805465, Validation Loss Force: 3.805295239897947, time: 0.11447906494140625
Test Loss Energy: 11.347529715869307, Test Loss Force: 12.307577335460573, time: 10.602096796035767

Epoch 8, Batch 100/113, Loss: 1.0089036226272583, Variance: 0.10159744322299957

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.4163024304717506, Training Loss Force: 4.3355992599098165, time: 1.7850947380065918
Validation Loss Energy: 4.09303677169338, Validation Loss Force: 4.298562397407041, time: 0.11220955848693848
Test Loss Energy: 13.117122767401769, Test Loss Force: 15.711654389917946, time: 10.570228576660156

Epoch 9, Batch 100/113, Loss: 0.8197138905525208, Variance: 0.09469670802354813

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 3.3722425014113613, Training Loss Force: 4.158819170646452, time: 1.791827917098999
Validation Loss Energy: 2.565079867656597, Validation Loss Force: 3.8412111051286617, time: 0.11447811126708984
Test Loss Energy: 11.656717128655165, Test Loss Force: 15.243599676251739, time: 10.69117021560669

Epoch 10, Batch 100/113, Loss: 1.350207805633545, Variance: 0.12914079427719116

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 3.6422365111111983, Training Loss Force: 4.213404523512523, time: 1.8178112506866455
Validation Loss Energy: 5.235963429361388, Validation Loss Force: 3.7735862628412464, time: 0.1132199764251709
Test Loss Energy: 10.846411310725799, Test Loss Force: 12.377780566379707, time: 10.485010623931885

Epoch 11, Batch 100/113, Loss: 2.186441659927368, Variance: 0.14245302975177765

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.112369277401233, Training Loss Force: 3.6179339857294615, time: 1.9392235279083252
Validation Loss Energy: 2.4470555342070215, Validation Loss Force: 3.6929386155542447, time: 0.11171317100524902
Test Loss Energy: 10.110446177587686, Test Loss Force: 12.141840078213843, time: 10.693520784378052

Epoch 12, Batch 100/113, Loss: 1.6347606182098389, Variance: 0.14465318620204926

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.12507625478991, Training Loss Force: 3.5998740657189603, time: 1.8210370540618896
Validation Loss Energy: 3.1506259066248044, Validation Loss Force: 3.6894158783310944, time: 0.11246991157531738
Test Loss Energy: 11.438024839683964, Test Loss Force: 12.454812157669792, time: 10.488661766052246

Epoch 13, Batch 100/113, Loss: 0.7705771923065186, Variance: 0.1496838480234146

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.101702825286911, Training Loss Force: 3.582984882932568, time: 1.814836025238037
Validation Loss Energy: 5.902399268312793, Validation Loss Force: 3.6996648937798584, time: 0.11553049087524414
Test Loss Energy: 12.917699716058232, Test Loss Force: 12.634083446124821, time: 10.510536193847656

Epoch 14, Batch 100/113, Loss: 1.0921441316604614, Variance: 0.15661028027534485

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.134377628778813, Training Loss Force: 3.577529248453481, time: 1.7874324321746826
Validation Loss Energy: 5.118780228610555, Validation Loss Force: 3.650686365772436, time: 0.11269307136535645
Test Loss Energy: 12.389860080757757, Test Loss Force: 12.492126955216577, time: 10.593337059020996

Epoch 15, Batch 100/113, Loss: 1.82631254196167, Variance: 0.15623703598976135

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.11991481058937, Training Loss Force: 3.5761974229106954, time: 1.821622371673584
Validation Loss Energy: 2.075198394511335, Validation Loss Force: 3.6461681861324156, time: 0.12371659278869629
Test Loss Energy: 10.971375450453863, Test Loss Force: 12.59206047153457, time: 10.496747255325317

Epoch 16, Batch 100/113, Loss: 1.5276106595993042, Variance: 0.15509340167045593

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.13502847394291, Training Loss Force: 3.5742634054121973, time: 1.865478515625
Validation Loss Energy: 3.4249275741103276, Validation Loss Force: 3.6987433775194365, time: 0.11326026916503906
Test Loss Energy: 10.543922615904068, Test Loss Force: 12.458842282681955, time: 10.672018051147461

Epoch 17, Batch 100/113, Loss: 0.9582740068435669, Variance: 0.15576738119125366

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.10705316287678, Training Loss Force: 3.58062530922678, time: 1.7893242835998535
Validation Loss Energy: 5.897350913436318, Validation Loss Force: 3.6864003814223674, time: 0.11430764198303223
Test Loss Energy: 11.196752738171865, Test Loss Force: 12.261868018366558, time: 10.576987028121948

Epoch 18, Batch 100/113, Loss: 1.4380306005477905, Variance: 0.15204571187496185

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.107363739042787, Training Loss Force: 3.6067173250523843, time: 1.7609941959381104
Validation Loss Energy: 4.658336284564384, Validation Loss Force: 3.7539492935310212, time: 0.11571574211120605
Test Loss Energy: 10.500774520933005, Test Loss Force: 11.609652392993095, time: 10.484615087509155

Epoch 19, Batch 100/113, Loss: 2.2555480003356934, Variance: 0.15029685199260712

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.136651036421958, Training Loss Force: 3.599735949762259, time: 1.7941365242004395
Validation Loss Energy: 2.144960415261365, Validation Loss Force: 3.656349287491672, time: 0.12028884887695312
Test Loss Energy: 10.185406707996005, Test Loss Force: 11.917891337808324, time: 11.305732011795044

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.039 MB of 0.048 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–â–…â–ˆâ–‡â–ƒâ–‚â–„â–ˆâ–…â–ƒâ–â–„â–ˆâ–†â–ƒâ–‚â–„â–‚â–
wandb:   test_error_force â–…â–ƒâ–„â–…â–„â–ƒâ–‚â–‚â–ˆâ–‡â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚
wandb:          test_loss â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–â–‚â–ˆâ–†â–‚â–â–‚â–ƒâ–ƒâ–‚â–â–â–â–
wandb: train_error_energy â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  train_error_force â–„â–â–â–â–â–â–â–ƒâ–ˆâ–†â–‡â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–†â–‡â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: valid_error_energy â–…â–â–ƒâ–†â–†â–â–ƒâ–ˆâ–„â–‚â–†â–‚â–ƒâ–‡â–†â–â–ƒâ–‡â–…â–
wandb:  valid_error_force â–â–â–â–â–â–â–„â–ƒâ–ˆâ–ƒâ–‚â–‚â–â–‚â–â–â–‚â–â–‚â–
wandb:         valid_loss â–„â–â–‚â–…â–„â–â–ƒâ–‡â–ˆâ–‚â–…â–â–‚â–†â–„â–â–‚â–†â–„â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 3585
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.18541
wandb:   test_error_force 11.91789
wandb:          test_loss 9.12417
wandb: train_error_energy 4.13665
wandb:  train_error_force 3.59974
wandb:         train_loss 1.48931
wandb: valid_error_energy 2.14496
wandb:  valid_error_force 3.65635
wandb:         valid_loss 1.02467
wandb: 
wandb: ğŸš€ View run al_71_30 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/909v4us7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_141910-909v4us7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8418514728546143, Uncertainty Bias: -0.1960413157939911
2.670288e-05 0.0670414
2.5504866 4.4460626
(48745, 22, 3)
Found uncertainty sample 0 after 195 steps.
Found uncertainty sample 1 after 1674 steps.
Found uncertainty sample 2 after 117 steps.
Found uncertainty sample 3 after 217 steps.
Found uncertainty sample 4 after 169 steps.
Found uncertainty sample 5 after 95 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 19 steps.
Found uncertainty sample 8 after 38 steps.
Found uncertainty sample 9 after 18 steps.
Found uncertainty sample 10 after 142 steps.
Found uncertainty sample 11 after 615 steps.
Found uncertainty sample 12 after 860 steps.
Found uncertainty sample 13 after 20 steps.
Found uncertainty sample 14 after 1824 steps.
Found uncertainty sample 15 after 159 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 70 steps.
Found uncertainty sample 18 after 22 steps.
Found uncertainty sample 19 after 727 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 84 steps.
Found uncertainty sample 22 after 149 steps.
Found uncertainty sample 23 after 870 steps.
Found uncertainty sample 24 after 571 steps.
Found uncertainty sample 25 after 3142 steps.
Found uncertainty sample 26 after 832 steps.
Found uncertainty sample 27 after 86 steps.
Found uncertainty sample 28 after 1462 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 16 steps.
Found uncertainty sample 31 after 806 steps.
Found uncertainty sample 32 after 168 steps.
Found uncertainty sample 33 after 46 steps.
Found uncertainty sample 34 after 1150 steps.
Found uncertainty sample 35 after 795 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1900 steps.
Found uncertainty sample 38 after 928 steps.
Found uncertainty sample 39 after 36 steps.
Found uncertainty sample 40 after 222 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 417 steps.
Found uncertainty sample 43 after 817 steps.
Found uncertainty sample 44 after 948 steps.
Found uncertainty sample 45 after 5 steps.
Found uncertainty sample 46 after 110 steps.
Found uncertainty sample 47 after 2253 steps.
Found uncertainty sample 48 after 13 steps.
Found uncertainty sample 49 after 1025 steps.
Found uncertainty sample 50 after 1019 steps.
Found uncertainty sample 51 after 4 steps.
Found uncertainty sample 52 after 2118 steps.
Found uncertainty sample 53 after 127 steps.
Found uncertainty sample 54 after 130 steps.
Found uncertainty sample 55 after 14 steps.
Found uncertainty sample 56 after 1525 steps.
Found uncertainty sample 57 after 745 steps.
Found uncertainty sample 58 after 448 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 184 steps.
Found uncertainty sample 61 after 1150 steps.
Found uncertainty sample 62 after 8 steps.
Found uncertainty sample 63 after 1950 steps.
Found uncertainty sample 64 after 7 steps.
Found uncertainty sample 65 after 86 steps.
Found uncertainty sample 66 after 2337 steps.
Found uncertainty sample 67 after 203 steps.
Found uncertainty sample 68 after 23 steps.
Found uncertainty sample 69 after 1147 steps.
Found uncertainty sample 70 after 292 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 54 steps.
Found uncertainty sample 73 after 229 steps.
Found uncertainty sample 74 after 759 steps.
Found uncertainty sample 75 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 57 steps.
Found uncertainty sample 78 after 385 steps.
Found uncertainty sample 79 after 378 steps.
Found uncertainty sample 80 after 1544 steps.
Found uncertainty sample 81 after 1352 steps.
Found uncertainty sample 82 after 1474 steps.
Found uncertainty sample 83 after 46 steps.
Found uncertainty sample 84 after 3121 steps.
Found uncertainty sample 85 after 33 steps.
Found uncertainty sample 86 after 15 steps.
Found uncertainty sample 87 after 143 steps.
Found uncertainty sample 88 after 212 steps.
Found uncertainty sample 89 after 207 steps.
Found uncertainty sample 90 after 62 steps.
Found uncertainty sample 91 after 6 steps.
Found uncertainty sample 92 after 160 steps.
Found uncertainty sample 93 after 51 steps.
Found uncertainty sample 94 after 2517 steps.
Found uncertainty sample 95 after 701 steps.
Found uncertainty sample 96 after 92 steps.
Found uncertainty sample 97 after 925 steps.
Found uncertainty sample 98 after 39 steps.
Found uncertainty sample 99 after 533 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_143227-t6cnp1v7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_31
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t6cnp1v7
Training model 31. Added 98 samples to the dataset.
Epoch 0, Batch 100/115, Loss: 0.8512174487113953, Variance: 0.12842462956905365

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.982820933300635, Training Loss Force: 3.825131782723034, time: 2.194180488586426
Validation Loss Energy: 2.5500001564215022, Validation Loss Force: 3.604223293995036, time: 0.1337449550628662
Test Loss Energy: 10.548071476032328, Test Loss Force: 12.61177801793045, time: 11.517532587051392

Epoch 1, Batch 100/115, Loss: 1.5545320510864258, Variance: 0.11910057067871094

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6088137211465936, Training Loss Force: 3.558269787649735, time: 1.9630322456359863
Validation Loss Energy: 3.2107991259437463, Validation Loss Force: 3.8109456179219623, time: 0.14007806777954102
Test Loss Energy: 10.451642683469903, Test Loss Force: 11.9759930998711, time: 11.685012340545654

Epoch 2, Batch 100/115, Loss: 1.2176015377044678, Variance: 0.11904855072498322

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6368048553701016, Training Loss Force: 3.564556835494978, time: 2.160078763961792
Validation Loss Energy: 2.4479687003744717, Validation Loss Force: 3.7295148432487326, time: 0.13061285018920898
Test Loss Energy: 10.163566889146413, Test Loss Force: 11.966419590331048, time: 11.695754766464233

Epoch 3, Batch 100/115, Loss: 0.9781758785247803, Variance: 0.11718221753835678

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6207194231167357, Training Loss Force: 3.558731324637894, time: 2.070662498474121
Validation Loss Energy: 2.170769068177747, Validation Loss Force: 3.623668619393667, time: 0.1378343105316162
Test Loss Energy: 10.77040605865384, Test Loss Force: 12.485005697483203, time: 11.529651403427124

Epoch 4, Batch 100/115, Loss: 1.0363752841949463, Variance: 0.1205032616853714

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6704032701657847, Training Loss Force: 3.5732670251375582, time: 2.330188751220703
Validation Loss Energy: 3.8258994839780835, Validation Loss Force: 3.686734978925648, time: 0.12935519218444824
Test Loss Energy: 11.493533963455494, Test Loss Force: 12.808538846291729, time: 12.128798961639404

Epoch 5, Batch 100/115, Loss: 1.60004723072052, Variance: 0.12190857529640198

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6699042792591543, Training Loss Force: 3.5644776620917007, time: 2.1092045307159424
Validation Loss Energy: 1.6901333004212225, Validation Loss Force: 3.6470127330913957, time: 0.13967037200927734
Test Loss Energy: 10.705277613788548, Test Loss Force: 12.776262662137736, time: 12.937076807022095

Epoch 6, Batch 100/115, Loss: 0.8386593461036682, Variance: 0.11838074028491974

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6656625636944646, Training Loss Force: 3.567839259462704, time: 2.182987928390503
Validation Loss Energy: 2.8951746668030416, Validation Loss Force: 3.7150195271871254, time: 0.13946080207824707
Test Loss Energy: 10.386043247894495, Test Loss Force: 12.585889826856794, time: 12.286661863327026

Epoch 7, Batch 100/115, Loss: 0.6636098027229309, Variance: 0.11637672781944275

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6886862910210563, Training Loss Force: 3.566746006235041, time: 2.150681734085083
Validation Loss Energy: 3.7650353965223764, Validation Loss Force: 3.6726924234607607, time: 0.1369023323059082
Test Loss Energy: 10.75882334595392, Test Loss Force: 12.730270865333063, time: 12.419069290161133

Epoch 8, Batch 100/115, Loss: 1.1035120487213135, Variance: 0.11232249438762665

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.723960525426513, Training Loss Force: 3.5727907067906175, time: 2.0849180221557617
Validation Loss Energy: 2.732849527429121, Validation Loss Force: 3.7141008685140755, time: 0.132979154586792
Test Loss Energy: 10.415300364855684, Test Loss Force: 12.542491213941075, time: 11.579272747039795

Epoch 9, Batch 100/115, Loss: 0.7276478409767151, Variance: 0.11572214215993881

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6391136081546676, Training Loss Force: 3.559007293032343, time: 2.044987678527832
Validation Loss Energy: 2.2328062509172497, Validation Loss Force: 3.758453445014849, time: 0.12874674797058105
Test Loss Energy: 10.993633975899137, Test Loss Force: 12.941903413877137, time: 11.329836130142212

Epoch 10, Batch 100/115, Loss: 1.1274625062942505, Variance: 0.12407021224498749

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.695522790473904, Training Loss Force: 3.559006508849699, time: 1.9903297424316406
Validation Loss Energy: 3.0428517390812466, Validation Loss Force: 3.6177825831902113, time: 0.13083696365356445
Test Loss Energy: 11.305456835410999, Test Loss Force: 12.737097784766503, time: 11.626006603240967

Epoch 11, Batch 100/115, Loss: 1.6890842914581299, Variance: 0.11599893122911453

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.686727913946368, Training Loss Force: 3.5763255272033776, time: 2.2213997840881348
Validation Loss Energy: 1.8284842466374862, Validation Loss Force: 3.743797139160735, time: 0.14329099655151367
Test Loss Energy: 10.665142202050909, Test Loss Force: 12.651872477198152, time: 11.29378867149353

Epoch 12, Batch 100/115, Loss: 0.9216996431350708, Variance: 0.12158700078725815

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.677916114563742, Training Loss Force: 3.58365613820418, time: 2.205590009689331
Validation Loss Energy: 3.4187795305451636, Validation Loss Force: 3.3909339373933585, time: 0.1283426284790039
Test Loss Energy: 10.366123461955379, Test Loss Force: 12.346210119842759, time: 11.908783674240112

Epoch 13, Batch 100/115, Loss: 1.117586612701416, Variance: 0.11524967849254608

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.714810930172627, Training Loss Force: 3.568845582655796, time: 2.067030668258667
Validation Loss Energy: 4.063010653818552, Validation Loss Force: 3.754075324267616, time: 0.13400626182556152
Test Loss Energy: 10.784969329608204, Test Loss Force: 12.621322988513999, time: 11.296356916427612

Epoch 14, Batch 100/115, Loss: 1.363820195198059, Variance: 0.11938798427581787

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.7146749625833624, Training Loss Force: 3.562459629048438, time: 2.007737398147583
Validation Loss Energy: 2.428483739867866, Validation Loss Force: 3.473132252612602, time: 0.13533473014831543
Test Loss Energy: 10.460082454983459, Test Loss Force: 12.744522977723424, time: 11.524389266967773

Epoch 15, Batch 100/115, Loss: 0.4102104902267456, Variance: 0.11555682122707367

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.657317431403493, Training Loss Force: 3.5578446735250906, time: 2.1394131183624268
Validation Loss Energy: 2.393174796505902, Validation Loss Force: 3.7641268321839423, time: 0.12856292724609375
Test Loss Energy: 10.98863636627179, Test Loss Force: 13.022055600164114, time: 11.524404525756836

Epoch 16, Batch 100/115, Loss: 0.9265960454940796, Variance: 0.11931873112916946

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.634774958901995, Training Loss Force: 3.57551080403347, time: 1.9528007507324219
Validation Loss Energy: 3.1790527404705293, Validation Loss Force: 3.5676302176685066, time: 0.1378328800201416
Test Loss Energy: 11.810274155121464, Test Loss Force: 13.183148240167567, time: 11.809895038604736

Epoch 17, Batch 100/115, Loss: 1.5193226337432861, Variance: 0.11846211552619934

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.695770957173294, Training Loss Force: 3.5755802342094234, time: 2.235743999481201
Validation Loss Energy: 1.7886543773827022, Validation Loss Force: 3.7065356842811794, time: 0.17303800582885742
Test Loss Energy: 10.72081731809298, Test Loss Force: 12.651497362459564, time: 12.391213417053223

Epoch 18, Batch 100/115, Loss: 0.7880918979644775, Variance: 0.1204659715294838

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6765803843174307, Training Loss Force: 3.56531008455433, time: 2.0936226844787598
Validation Loss Energy: 2.954846509428402, Validation Loss Force: 3.686251861102033, time: 0.13086295127868652
Test Loss Energy: 10.418062105790677, Test Loss Force: 12.583601184411055, time: 11.862835884094238

Epoch 19, Batch 100/115, Loss: 0.8338844776153564, Variance: 0.11977798491716385

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6730891925353713, Training Loss Force: 3.564839083159343, time: 2.0795509815216064
Validation Loss Energy: 3.905059478701142, Validation Loss Force: 3.66513510687102, time: 0.13451075553894043
Test Loss Energy: 10.647488404595, Test Loss Force: 12.201703794414787, time: 10.747169494628906

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–‚â–â–„â–‡â–ƒâ–‚â–„â–‚â–…â–†â–ƒâ–‚â–„â–‚â–…â–ˆâ–ƒâ–‚â–ƒ
wandb:   test_error_force â–…â–â–â–„â–†â–†â–…â–…â–„â–‡â–…â–…â–ƒâ–…â–…â–‡â–ˆâ–…â–…â–‚
wandb:          test_loss â–ƒâ–ƒâ–ƒâ–†â–ˆâ–…â–ƒâ–ƒâ–‚â–†â–‡â–…â–â–‚â–‚â–…â–ˆâ–„â–â–‚
wandb: train_error_energy â–ˆâ–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–â–ƒâ–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–‚â–â–â–‚â–â–‚â–‚â–â–â–‚â–â–
wandb: valid_error_energy â–„â–…â–ƒâ–‚â–‡â–â–…â–‡â–„â–ƒâ–…â–â–†â–ˆâ–ƒâ–ƒâ–…â–â–…â–ˆ
wandb:  valid_error_force â–…â–ˆâ–‡â–…â–†â–…â–†â–†â–†â–‡â–…â–‡â–â–‡â–‚â–‡â–„â–†â–†â–†
wandb:         valid_loss â–ƒâ–…â–ƒâ–‚â–‡â–â–„â–‡â–ƒâ–ƒâ–„â–â–„â–ˆâ–‚â–ƒâ–…â–â–„â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 3673
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.64749
wandb:   test_error_force 12.2017
wandb:          test_loss 11.77781
wandb: train_error_energy 2.67309
wandb:  train_error_force 3.56484
wandb:         train_loss 1.04096
wandb: valid_error_energy 3.90506
wandb:  valid_error_force 3.66514
wandb:         valid_loss 1.60117
wandb: 
wandb: ğŸš€ View run al_71_31 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/t6cnp1v7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_143227-t6cnp1v7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.7755126953125, Uncertainty Bias: -0.08293907344341278
0.00012588501 0.00075531006
2.4157517 4.6000967
(48745, 22, 3)
Found uncertainty sample 0 after 153 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1571 steps.
Found uncertainty sample 5 after 15 steps.
Found uncertainty sample 6 after 324 steps.
Found uncertainty sample 7 after 1804 steps.
Found uncertainty sample 8 after 69 steps.
Found uncertainty sample 9 after 122 steps.
Found uncertainty sample 10 after 62 steps.
Found uncertainty sample 11 after 453 steps.
Found uncertainty sample 12 after 227 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 85 steps.
Found uncertainty sample 15 after 820 steps.
Found uncertainty sample 16 after 1097 steps.
Found uncertainty sample 17 after 37 steps.
Found uncertainty sample 18 after 37 steps.
Found uncertainty sample 19 after 7 steps.
Found uncertainty sample 20 after 1757 steps.
Found uncertainty sample 21 after 615 steps.
Found uncertainty sample 22 after 409 steps.
Found uncertainty sample 23 after 696 steps.
Found uncertainty sample 24 after 7 steps.
Found uncertainty sample 25 after 560 steps.
Found uncertainty sample 26 after 5 steps.
Found uncertainty sample 27 after 41 steps.
Found uncertainty sample 28 after 491 steps.
Found uncertainty sample 29 after 286 steps.
Found uncertainty sample 30 after 2119 steps.
Found uncertainty sample 31 after 157 steps.
Found uncertainty sample 32 after 182 steps.
Found uncertainty sample 33 after 23 steps.
Found uncertainty sample 34 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 502 steps.
Found uncertainty sample 37 after 34 steps.
Found uncertainty sample 38 after 561 steps.
Found uncertainty sample 39 after 26 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 29 steps.
Found uncertainty sample 42 after 12 steps.
Found uncertainty sample 43 after 683 steps.
Found uncertainty sample 44 after 480 steps.
Found uncertainty sample 45 after 167 steps.
Found uncertainty sample 46 after 607 steps.
Found uncertainty sample 47 after 10 steps.
Found uncertainty sample 48 after 11 steps.
Found uncertainty sample 49 after 334 steps.
Found uncertainty sample 50 after 96 steps.
Found uncertainty sample 51 after 22 steps.
Found uncertainty sample 52 after 119 steps.
Found uncertainty sample 53 after 3 steps.
Found uncertainty sample 54 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 172 steps.
Found uncertainty sample 57 after 13 steps.
Found uncertainty sample 58 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 206 steps.
Found uncertainty sample 61 after 619 steps.
Found uncertainty sample 62 after 28 steps.
Found uncertainty sample 63 after 154 steps.
Found uncertainty sample 64 after 478 steps.
Found uncertainty sample 65 after 165 steps.
Found uncertainty sample 66 after 25 steps.
Found uncertainty sample 67 after 37 steps.
Found uncertainty sample 68 after 16 steps.
Found uncertainty sample 69 after 67 steps.
Found uncertainty sample 70 after 1294 steps.
Found uncertainty sample 71 after 590 steps.
Found uncertainty sample 72 after 903 steps.
Found uncertainty sample 73 after 28 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 233 steps.
Found uncertainty sample 76 after 183 steps.
Found uncertainty sample 77 after 12 steps.
Found uncertainty sample 78 after 130 steps.
Found uncertainty sample 79 after 15 steps.
Found uncertainty sample 80 after 1189 steps.
Found uncertainty sample 81 after 12 steps.
Found uncertainty sample 82 after 590 steps.
Found uncertainty sample 83 after 392 steps.
Found uncertainty sample 84 after 512 steps.
Found uncertainty sample 85 after 137 steps.
Found uncertainty sample 86 after 339 steps.
Found uncertainty sample 87 after 174 steps.
Found uncertainty sample 88 after 97 steps.
Found uncertainty sample 89 after 228 steps.
Found uncertainty sample 90 after 200 steps.
Found uncertainty sample 91 after 128 steps.
Found uncertainty sample 92 after 389 steps.
Found uncertainty sample 93 after 123 steps.
Found uncertainty sample 94 after 141 steps.
Found uncertainty sample 95 after 391 steps.
Found uncertainty sample 96 after 736 steps.
Found uncertainty sample 97 after 889 steps.
Found uncertainty sample 98 after 409 steps.
Found uncertainty sample 99 after 85 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_144305-ci54jh38
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_32
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ci54jh38
Training model 32. Added 100 samples to the dataset.
Epoch 0, Batch 100/118, Loss: 0.7032887935638428, Variance: 0.1009148582816124

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.317982171378276, Training Loss Force: 3.997519214682414, time: 1.900298833847046
Validation Loss Energy: 5.0020765087539525, Validation Loss Force: 4.21835489279849, time: 0.11736750602722168
Test Loss Energy: 11.290685253390297, Test Loss Force: 12.39129612654116, time: 10.310544729232788

Epoch 1, Batch 100/118, Loss: 2.072402000427246, Variance: 0.13117103278636932

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.954387943537063, Training Loss Force: 3.763779119216362, time: 1.9074583053588867
Validation Loss Energy: 6.123042421904834, Validation Loss Force: 3.7964380550573296, time: 0.11854219436645508
Test Loss Energy: 11.00061407771529, Test Loss Force: 11.737965285483563, time: 10.364414930343628

Epoch 2, Batch 100/118, Loss: 1.399445652961731, Variance: 0.14134708046913147

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.08553311207772, Training Loss Force: 3.5858231926636037, time: 1.887364387512207
Validation Loss Energy: 1.8091040148525155, Validation Loss Force: 3.784077616543829, time: 0.13167858123779297
Test Loss Energy: 10.078288583989712, Test Loss Force: 11.627865665913962, time: 10.519679546356201

Epoch 3, Batch 100/118, Loss: 1.991313099861145, Variance: 0.15253040194511414

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.064892424795205, Training Loss Force: 3.571097021534416, time: 1.9085454940795898
Validation Loss Energy: 5.236342857586049, Validation Loss Force: 3.7092920314155706, time: 0.11745119094848633
Test Loss Energy: 11.636085085906002, Test Loss Force: 11.633728043229214, time: 10.394421100616455

Epoch 4, Batch 100/118, Loss: 1.2849421501159668, Variance: 0.151015505194664

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.137741325841984, Training Loss Force: 3.5760598242831887, time: 1.9042935371398926
Validation Loss Energy: 2.6500641821181645, Validation Loss Force: 3.674147120991107, time: 0.1229715347290039
Test Loss Energy: 10.19713341651911, Test Loss Force: 11.481996628629057, time: 11.261446237564087

Epoch 5, Batch 100/118, Loss: 1.8418052196502686, Variance: 0.1499631404876709

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.084339655461752, Training Loss Force: 3.5925985279213086, time: 1.8917865753173828
Validation Loss Energy: 5.881550756179574, Validation Loss Force: 3.6057963772879673, time: 0.11952090263366699
Test Loss Energy: 10.902899586509431, Test Loss Force: 11.488115101109878, time: 10.322573900222778

Epoch 6, Batch 100/118, Loss: 1.456051230430603, Variance: 0.15432094037532806

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.18950116603201, Training Loss Force: 3.5849470178797778, time: 1.903008222579956
Validation Loss Energy: 2.2551827187298255, Validation Loss Force: 3.659732633829472, time: 0.1185302734375
Test Loss Energy: 10.36533882718064, Test Loss Force: 11.584013516751236, time: 10.335196018218994

Epoch 7, Batch 100/118, Loss: 1.8415149450302124, Variance: 0.15917590260505676

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.162968924880086, Training Loss Force: 3.5832494920976905, time: 2.1370561122894287
Validation Loss Energy: 5.47250961335559, Validation Loss Force: 3.7197345463521465, time: 0.11750006675720215
Test Loss Energy: 11.940433825870812, Test Loss Force: 11.698532384409058, time: 10.358431100845337

Epoch 8, Batch 100/118, Loss: 1.2814549207687378, Variance: 0.15847733616828918

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.186300793347311, Training Loss Force: 3.5884748387527514, time: 1.9465312957763672
Validation Loss Energy: 2.693252273208783, Validation Loss Force: 3.7255776816334856, time: 0.12138700485229492
Test Loss Energy: 9.99221626630639, Test Loss Force: 11.519895005867303, time: 10.32112455368042

Epoch 9, Batch 100/118, Loss: 1.6685186624526978, Variance: 0.15361182391643524

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.135450015985261, Training Loss Force: 3.5883556164083474, time: 1.9407474994659424
Validation Loss Energy: 6.110077733552333, Validation Loss Force: 3.7165940949410294, time: 0.11851954460144043
Test Loss Energy: 11.219902439166262, Test Loss Force: 11.460668193372134, time: 10.513148307800293

Epoch 10, Batch 100/118, Loss: 1.1944692134857178, Variance: 0.15440872311592102

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.110512320852998, Training Loss Force: 3.5878166664979223, time: 1.9043614864349365
Validation Loss Energy: 2.0326482876353236, Validation Loss Force: 3.612693628306129, time: 0.12091898918151855
Test Loss Energy: 10.317441486115472, Test Loss Force: 11.580375741769032, time: 10.373783111572266

Epoch 11, Batch 100/118, Loss: 1.8407788276672363, Variance: 0.16155096888542175

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.1224364932698165, Training Loss Force: 3.5862444600433783, time: 1.913341760635376
Validation Loss Energy: 5.760996789897686, Validation Loss Force: 3.746959412148153, time: 0.12171053886413574
Test Loss Energy: 11.739032064216525, Test Loss Force: 11.585062332729223, time: 10.579681158065796

Epoch 12, Batch 100/118, Loss: 1.1871052980422974, Variance: 0.15795260667800903

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.1675088760902, Training Loss Force: 3.587365627498526, time: 1.886641502380371
Validation Loss Energy: 2.43997947986468, Validation Loss Force: 3.6995292511584643, time: 0.12110137939453125
Test Loss Energy: 10.09441576178114, Test Loss Force: 11.485034633191535, time: 10.340362310409546

Epoch 13, Batch 100/118, Loss: 1.7661798000335693, Variance: 0.15703192353248596

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.117730052527994, Training Loss Force: 3.583114824440071, time: 1.8742592334747314
Validation Loss Energy: 5.778212974476435, Validation Loss Force: 3.7603349992979376, time: 0.11832690238952637
Test Loss Energy: 10.819171622859, Test Loss Force: 11.305614130760974, time: 10.394960641860962

Epoch 14, Batch 100/118, Loss: 1.3036315441131592, Variance: 0.1555282175540924

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.177309215096947, Training Loss Force: 3.582883484406638, time: 1.8984451293945312
Validation Loss Energy: 2.2787322818261972, Validation Loss Force: 3.657019187617196, time: 0.11947846412658691
Test Loss Energy: 10.309388324197986, Test Loss Force: 11.484464819349684, time: 10.600292205810547

Epoch 15, Batch 100/118, Loss: 1.655600666999817, Variance: 0.16019439697265625

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.127813204937902, Training Loss Force: 3.603043803577888, time: 1.9471347332000732
Validation Loss Energy: 5.489985281712393, Validation Loss Force: 3.668942155481996, time: 0.11847186088562012
Test Loss Energy: 11.821355211722716, Test Loss Force: 11.438833802520156, time: 10.344565391540527

Epoch 16, Batch 100/118, Loss: 1.307078242301941, Variance: 0.16293226182460785

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.209526458067061, Training Loss Force: 3.5887590060975416, time: 1.9461524486541748
Validation Loss Energy: 2.625471042957878, Validation Loss Force: 3.72619039290003, time: 0.11781549453735352
Test Loss Energy: 9.934448918326583, Test Loss Force: 11.403829126248139, time: 11.18753433227539

Epoch 17, Batch 100/118, Loss: 1.4176701307296753, Variance: 0.15502382814884186

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.1661028874355255, Training Loss Force: 3.577313674243907, time: 1.8873910903930664
Validation Loss Energy: 6.126867729385775, Validation Loss Force: 3.7747626239391834, time: 0.1203317642211914
Test Loss Energy: 10.890535708476543, Test Loss Force: 11.49189188692433, time: 10.342433452606201

Epoch 18, Batch 100/118, Loss: 1.3497627973556519, Variance: 0.157220259308815

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.212956391319053, Training Loss Force: 3.598569193496429, time: 1.8448891639709473
Validation Loss Energy: 1.9907237697247968, Validation Loss Force: 3.766791291184114, time: 0.12044811248779297
Test Loss Energy: 10.15869361050899, Test Loss Force: 11.464482233716575, time: 10.362551212310791

Epoch 19, Batch 100/118, Loss: 1.6403942108154297, Variance: 0.15987429022789001

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.1000125148435, Training Loss Force: 3.62450793740177, time: 1.9206767082214355
Validation Loss Energy: 5.790000485277463, Validation Loss Force: 3.6298845028252895, time: 0.12207531929016113
Test Loss Energy: 12.029753289237567, Test Loss Force: 11.531737418408186, time: 10.637705326080322

wandb: - 0.039 MB of 0.040 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–…â–â–‡â–‚â–„â–‚â–ˆâ–â–…â–‚â–‡â–‚â–„â–‚â–‡â–â–„â–‚â–ˆ
wandb:   test_error_force â–ˆâ–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–„â–‚â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:          test_loss â–ˆâ–ƒâ–‚â–ƒâ–â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–â–‚â–ƒâ–â–â–â–ƒ
wandb: train_error_energy â–â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡
wandb:  train_error_force â–ˆâ–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–‚
wandb:         train_loss â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–ˆâ–â–‡â–‚â–ˆâ–‚â–‡â–‚â–ˆâ–â–‡â–‚â–‡â–‚â–‡â–‚â–ˆâ–â–‡
wandb:  valid_error_force â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–
wandb:         valid_loss â–ˆâ–…â–â–„â–â–„â–â–„â–â–„â–â–„â–â–„â–â–„â–â–„â–â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 3763
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.02975
wandb:   test_error_force 11.53174
wandb:          test_loss 10.38676
wandb: train_error_energy 4.10001
wandb:  train_error_force 3.62451
wandb:         train_loss 1.47751
wandb: valid_error_energy 5.79
wandb:  valid_error_force 3.62988
wandb:         valid_loss 1.9994
wandb: 
wandb: ğŸš€ View run al_71_32 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ci54jh38
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_144305-ci54jh38/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.849863052368164, Uncertainty Bias: -0.19084404408931732
0.00018119812 0.74403787
2.370397 4.5703545
(48745, 22, 3)
Found uncertainty sample 0 after 1262 steps.
Found uncertainty sample 1 after 704 steps.
Found uncertainty sample 2 after 14 steps.
Found uncertainty sample 3 after 1104 steps.
Found uncertainty sample 4 after 1262 steps.
Found uncertainty sample 5 after 568 steps.
Found uncertainty sample 6 after 27 steps.
Found uncertainty sample 7 after 1399 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 438 steps.
Found uncertainty sample 10 after 447 steps.
Found uncertainty sample 11 after 1063 steps.
Found uncertainty sample 12 after 36 steps.
Found uncertainty sample 13 after 2692 steps.
Found uncertainty sample 14 after 598 steps.
Found uncertainty sample 15 after 118 steps.
Found uncertainty sample 16 after 522 steps.
Found uncertainty sample 17 after 44 steps.
Found uncertainty sample 18 after 8 steps.
Found uncertainty sample 19 after 3062 steps.
Found uncertainty sample 20 after 202 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 269 steps.
Found uncertainty sample 23 after 106 steps.
Found uncertainty sample 24 after 542 steps.
Found uncertainty sample 25 after 5 steps.
Found uncertainty sample 26 after 22 steps.
Found uncertainty sample 27 after 3632 steps.
Found uncertainty sample 28 after 581 steps.
Found uncertainty sample 29 after 147 steps.
Found uncertainty sample 30 after 578 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 133 steps.
Found uncertainty sample 33 after 446 steps.
Found uncertainty sample 34 after 12 steps.
Found uncertainty sample 35 after 583 steps.
Found uncertainty sample 36 after 758 steps.
Found uncertainty sample 37 after 230 steps.
Found uncertainty sample 38 after 209 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 53 steps.
Found uncertainty sample 41 after 159 steps.
Found uncertainty sample 42 after 11 steps.
Found uncertainty sample 43 after 486 steps.
Found uncertainty sample 44 after 1494 steps.
Found uncertainty sample 45 after 290 steps.
Found uncertainty sample 46 after 1 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 3260 steps.
Found uncertainty sample 49 after 98 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 179 steps.
Found uncertainty sample 52 after 117 steps.
Found uncertainty sample 53 after 181 steps.
Found uncertainty sample 54 after 19 steps.
Found uncertainty sample 55 after 1832 steps.
Found uncertainty sample 56 after 229 steps.
Found uncertainty sample 57 after 8 steps.
Found uncertainty sample 58 after 1468 steps.
Found uncertainty sample 59 after 270 steps.
Found uncertainty sample 60 after 559 steps.
Found uncertainty sample 61 after 49 steps.
Found uncertainty sample 62 after 818 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 14 steps.
Found uncertainty sample 65 after 1643 steps.
Found uncertainty sample 66 after 3052 steps.
Found uncertainty sample 67 after 54 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 15 steps.
Found uncertainty sample 70 after 64 steps.
Found uncertainty sample 71 after 256 steps.
Found uncertainty sample 72 after 138 steps.
Found uncertainty sample 73 after 103 steps.
Found uncertainty sample 74 after 83 steps.
Found uncertainty sample 75 after 1479 steps.
Found uncertainty sample 76 after 2004 steps.
Found uncertainty sample 77 after 145 steps.
Found uncertainty sample 78 after 1846 steps.
Found uncertainty sample 79 after 15 steps.
Found uncertainty sample 80 after 3397 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1520 steps.
Found uncertainty sample 83 after 1749 steps.
Found uncertainty sample 84 after 1058 steps.
Found uncertainty sample 85 after 4 steps.
Found uncertainty sample 86 after 137 steps.
Found uncertainty sample 87 after 41 steps.
Found uncertainty sample 88 after 3119 steps.
Found uncertainty sample 89 after 1769 steps.
Found uncertainty sample 90 after 451 steps.
Found uncertainty sample 91 after 1267 steps.
Found uncertainty sample 92 after 1887 steps.
Found uncertainty sample 93 after 170 steps.
Found uncertainty sample 94 after 31 steps.
Found uncertainty sample 95 after 51 steps.
Found uncertainty sample 96 after 2500 steps.
Found uncertainty sample 97 after 844 steps.
Found uncertainty sample 98 after 1933 steps.
Found uncertainty sample 99 after 2108 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_145753-8ghzn2d5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_33
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8ghzn2d5
Training model 33. Added 98 samples to the dataset.
Epoch 0, Batch 100/121, Loss: 1.070783257484436, Variance: 0.12539666891098022

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1998608289525583, Training Loss Force: 3.7847553508814658, time: 1.9751291275024414
Validation Loss Energy: 3.4868688606709846, Validation Loss Force: 3.671378452830641, time: 0.12969541549682617
Test Loss Energy: 10.330637414627306, Test Loss Force: 11.77130765147731, time: 10.3275785446167

Epoch 1, Batch 100/121, Loss: 1.4392809867858887, Variance: 0.12075045704841614

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6558775061107354, Training Loss Force: 3.558631888806397, time: 2.043421745300293
Validation Loss Energy: 2.048752388276259, Validation Loss Force: 3.7058489726351382, time: 0.11992859840393066
Test Loss Energy: 10.241827595469104, Test Loss Force: 11.621074905151882, time: 10.241577863693237

Epoch 2, Batch 100/121, Loss: 0.6924179792404175, Variance: 0.11741688847541809

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6391531301998437, Training Loss Force: 3.5548625271017227, time: 2.0349838733673096
Validation Loss Energy: 2.0988629662215783, Validation Loss Force: 3.6911168108253123, time: 0.11815834045410156
Test Loss Energy: 10.506632011185115, Test Loss Force: 11.797124844901814, time: 11.034803628921509

Epoch 3, Batch 100/121, Loss: 0.982687771320343, Variance: 0.12462389469146729

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.664449885458853, Training Loss Force: 3.5659277206628373, time: 1.942533254623413
Validation Loss Energy: 3.539524101615966, Validation Loss Force: 3.7751883673461974, time: 0.12086653709411621
Test Loss Energy: 10.783660729851148, Test Loss Force: 11.711611662267792, time: 10.322356462478638

Epoch 4, Batch 100/121, Loss: 1.4176039695739746, Variance: 0.12294824421405792

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.655495112337904, Training Loss Force: 3.555547488678691, time: 1.999453067779541
Validation Loss Energy: 1.7347559745960053, Validation Loss Force: 3.5970635160538253, time: 0.11850833892822266
Test Loss Energy: 10.081549957182345, Test Loss Force: 11.700908732850538, time: 10.498800277709961

Epoch 5, Batch 100/121, Loss: 0.911545991897583, Variance: 0.12717175483703613

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6697556559893454, Training Loss Force: 3.553830332314208, time: 2.005516529083252
Validation Loss Energy: 2.845566875789227, Validation Loss Force: 3.6265157413165476, time: 0.12199950218200684
Test Loss Energy: 10.058245264472697, Test Loss Force: 11.566845926119017, time: 10.371394157409668

Epoch 6, Batch 100/121, Loss: 1.016668438911438, Variance: 0.11938981711864471

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.642314993426961, Training Loss Force: 3.555808968504682, time: 1.971874713897705
Validation Loss Energy: 3.6138405970768033, Validation Loss Force: 3.588122978729581, time: 0.12100362777709961
Test Loss Energy: 10.45233701949403, Test Loss Force: 11.494665486448605, time: 10.455916166305542

Epoch 7, Batch 100/121, Loss: 1.8563886880874634, Variance: 0.1199585497379303

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.695134153827921, Training Loss Force: 3.549682296883632, time: 2.1229465007781982
Validation Loss Energy: 2.4858403429369185, Validation Loss Force: 3.5841397246011275, time: 0.1197197437286377
Test Loss Energy: 9.826764650439026, Test Loss Force: 11.566308391142671, time: 10.294397830963135

Epoch 8, Batch 100/121, Loss: 1.0878465175628662, Variance: 0.11972194910049438

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6932027989462606, Training Loss Force: 3.559214021403728, time: 1.9505407810211182
Validation Loss Energy: 2.397393857259893, Validation Loss Force: 3.6214995992431844, time: 0.12405776977539062
Test Loss Energy: 10.376693074341999, Test Loss Force: 11.729022258384067, time: 10.278950691223145

Epoch 9, Batch 100/121, Loss: 1.1073181629180908, Variance: 0.12317802757024765

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6726247148102895, Training Loss Force: 3.543618940590143, time: 1.9714634418487549
Validation Loss Energy: 3.390797070091662, Validation Loss Force: 3.613800368184439, time: 0.12485337257385254
Test Loss Energy: 10.915530033318403, Test Loss Force: 11.817897996935917, time: 10.47704815864563

Epoch 10, Batch 100/121, Loss: 1.4218623638153076, Variance: 0.12143589556217194

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6671696979844377, Training Loss Force: 3.564452278097103, time: 1.9398396015167236
Validation Loss Energy: 1.7119633752612156, Validation Loss Force: 3.6548633049463226, time: 0.12214374542236328
Test Loss Energy: 10.263374843382246, Test Loss Force: 11.800507196223297, time: 10.296996593475342

Epoch 11, Batch 100/121, Loss: 0.8511543869972229, Variance: 0.12316630035638809

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6846349263480462, Training Loss Force: 3.5598987106250473, time: 2.05033278465271
Validation Loss Energy: 3.066881970640628, Validation Loss Force: 3.706394808934856, time: 0.1236882209777832
Test Loss Energy: 10.228157910952845, Test Loss Force: 11.542409206787609, time: 10.443795442581177

Epoch 12, Batch 100/121, Loss: 1.082329273223877, Variance: 0.1186780259013176

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6805230130056144, Training Loss Force: 3.5587896673785755, time: 1.9396638870239258
Validation Loss Energy: 3.6489384500884277, Validation Loss Force: 3.6499203467989205, time: 0.14357972145080566
Test Loss Energy: 10.316426063514532, Test Loss Force: 11.625306728772818, time: 10.330990552902222

Epoch 13, Batch 100/121, Loss: 1.3284651041030884, Variance: 0.11845109611749649

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.69079723607389, Training Loss Force: 3.5477827468850216, time: 2.0232646465301514
Validation Loss Energy: 2.4834610488896316, Validation Loss Force: 3.683461222919169, time: 0.12942957878112793
Test Loss Energy: 10.191251024628485, Test Loss Force: 11.811928108894818, time: 10.341943740844727

Epoch 14, Batch 100/121, Loss: 0.743548572063446, Variance: 0.12052817642688751

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.674099723079178, Training Loss Force: 3.5962147123109944, time: 1.9834423065185547
Validation Loss Energy: 2.362773555343235, Validation Loss Force: 3.534295162062144, time: 0.12250685691833496
Test Loss Energy: 10.532777253386115, Test Loss Force: 11.787988289169334, time: 10.457327127456665

Epoch 15, Batch 100/121, Loss: 1.0703585147857666, Variance: 0.12700089812278748

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6784616901920053, Training Loss Force: 3.563399354836368, time: 1.9240782260894775
Validation Loss Energy: 2.8579640547118053, Validation Loss Force: 3.6589348845026612, time: 0.11798882484436035
Test Loss Energy: 10.771723481238627, Test Loss Force: 11.796624685699392, time: 10.334191083908081

Epoch 16, Batch 100/121, Loss: 1.602067232131958, Variance: 0.12213663011789322

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.69594883490432, Training Loss Force: 3.5585207769025793, time: 1.9496636390686035
Validation Loss Energy: 1.8185422177179078, Validation Loss Force: 3.6194787282977923, time: 0.12226390838623047
Test Loss Energy: 10.193708998216149, Test Loss Force: 11.780633953964717, time: 11.204328298568726

Epoch 17, Batch 100/121, Loss: 1.00925612449646, Variance: 0.12065944075584412

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.662358748902821, Training Loss Force: 3.5534093900956387, time: 1.9432337284088135
Validation Loss Energy: 2.9919653076084667, Validation Loss Force: 3.7115617722430607, time: 0.13935065269470215
Test Loss Energy: 9.900932912368061, Test Loss Force: 11.815158392345126, time: 10.40558910369873

Epoch 18, Batch 100/121, Loss: 0.9539476633071899, Variance: 0.12114232778549194

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6808141115084956, Training Loss Force: 3.5661266359795456, time: 1.9428861141204834
Validation Loss Energy: 3.4838543657734142, Validation Loss Force: 3.6037371330264265, time: 0.11947321891784668
Test Loss Energy: 10.392307726049852, Test Loss Force: 11.748028214811521, time: 10.335112810134888

Epoch 19, Batch 100/121, Loss: 1.3359445333480835, Variance: 0.11790221929550171

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6914492702208745, Training Loss Force: 3.549990044045713, time: 1.9704954624176025
Validation Loss Energy: 2.3654984319050025, Validation Loss Force: 3.6213332653911756, time: 0.15108156204223633
Test Loss Energy: 9.936888313613155, Test Loss Force: 11.73673287354862, time: 10.441511869430542

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.039 MB of 0.056 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–„â–…â–‡â–ƒâ–‚â–…â–â–…â–ˆâ–„â–„â–„â–ƒâ–†â–‡â–ƒâ–â–…â–‚
wandb:   test_error_force â–‡â–„â–ˆâ–†â–…â–ƒâ–â–ƒâ–†â–ˆâ–ˆâ–‚â–„â–ˆâ–‡â–ˆâ–‡â–ˆâ–†â–†
wandb:          test_loss â–‚â–â–†â–†â–„â–â–‚â–â–„â–ˆâ–…â–‚â–‚â–ƒâ–…â–ˆâ–…â–â–‚â–
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–‚â–‚â–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–ƒâ–‚â–â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–
wandb: valid_error_energy â–‡â–‚â–‚â–ˆâ–â–…â–ˆâ–„â–ƒâ–‡â–â–†â–ˆâ–„â–ƒâ–…â–â–†â–‡â–ƒ
wandb:  valid_error_force â–…â–†â–†â–ˆâ–ƒâ–„â–ƒâ–‚â–„â–ƒâ–…â–†â–„â–…â–â–…â–ƒâ–†â–ƒâ–„
wandb:         valid_loss â–‡â–‚â–ƒâ–ˆâ–â–„â–‡â–ƒâ–ƒâ–‡â–â–…â–ˆâ–ƒâ–ƒâ–…â–â–…â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3851
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.93689
wandb:   test_error_force 11.73673
wandb:          test_loss 11.2966
wandb: train_error_energy 2.69145
wandb:  train_error_force 3.54999
wandb:         train_loss 1.04462
wandb: valid_error_energy 2.3655
wandb:  valid_error_force 3.62133
wandb:         valid_loss 0.9375
wandb: 
wandb: ğŸš€ View run al_71_33 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/8ghzn2d5
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_145753-8ghzn2d5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.824899911880493, Uncertainty Bias: -0.09519636631011963
0.00045776367 0.032078743
2.3743317 4.6399603
(48745, 22, 3)
Found uncertainty sample 0 after 376 steps.
Found uncertainty sample 1 after 98 steps.
Found uncertainty sample 2 after 11 steps.
Found uncertainty sample 3 after 1363 steps.
Found uncertainty sample 4 after 13 steps.
Found uncertainty sample 5 after 489 steps.
Found uncertainty sample 6 after 2734 steps.
Found uncertainty sample 7 after 710 steps.
Found uncertainty sample 8 after 293 steps.
Found uncertainty sample 9 after 362 steps.
Found uncertainty sample 10 after 343 steps.
Found uncertainty sample 11 after 629 steps.
Found uncertainty sample 12 after 486 steps.
Found uncertainty sample 13 after 574 steps.
Found uncertainty sample 14 after 2486 steps.
Found uncertainty sample 15 after 1313 steps.
Found uncertainty sample 16 after 93 steps.
Found uncertainty sample 17 after 72 steps.
Found uncertainty sample 18 after 35 steps.
Found uncertainty sample 19 after 222 steps.
Found uncertainty sample 20 after 321 steps.
Found uncertainty sample 21 after 1661 steps.
Found uncertainty sample 22 after 788 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 40 steps.
Found uncertainty sample 25 after 102 steps.
Found uncertainty sample 26 after 38 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 381 steps.
Found uncertainty sample 30 after 287 steps.
Found uncertainty sample 31 after 2886 steps.
Found uncertainty sample 32 after 121 steps.
Found uncertainty sample 33 after 673 steps.
Found uncertainty sample 34 after 26 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 407 steps.
Found uncertainty sample 37 after 21 steps.
Found uncertainty sample 38 after 117 steps.
Found uncertainty sample 39 after 188 steps.
Found uncertainty sample 40 after 3393 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 89 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 725 steps.
Found uncertainty sample 45 after 465 steps.
Found uncertainty sample 46 after 50 steps.
Found uncertainty sample 47 after 182 steps.
Found uncertainty sample 48 after 376 steps.
Found uncertainty sample 49 after 810 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 59 steps.
Found uncertainty sample 52 after 26 steps.
Found uncertainty sample 53 after 3 steps.
Found uncertainty sample 54 after 1732 steps.
Found uncertainty sample 55 after 244 steps.
Found uncertainty sample 56 after 1432 steps.
Found uncertainty sample 57 after 23 steps.
Found uncertainty sample 58 after 660 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 11 steps.
Found uncertainty sample 61 after 260 steps.
Found uncertainty sample 62 after 434 steps.
Found uncertainty sample 63 after 19 steps.
Found uncertainty sample 64 after 1601 steps.
Found uncertainty sample 65 after 52 steps.
Found uncertainty sample 66 after 41 steps.
Found uncertainty sample 67 after 1411 steps.
Found uncertainty sample 68 after 363 steps.
Found uncertainty sample 69 after 141 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 510 steps.
Found uncertainty sample 72 after 55 steps.
Found uncertainty sample 73 after 934 steps.
Found uncertainty sample 74 after 302 steps.
Found uncertainty sample 75 after 21 steps.
Found uncertainty sample 76 after 44 steps.
Found uncertainty sample 77 after 771 steps.
Found uncertainty sample 78 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Did not find any uncertainty samples for sample 80.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 210 steps.
Found uncertainty sample 83 after 17 steps.
Found uncertainty sample 84 after 368 steps.
Found uncertainty sample 85 after 50 steps.
Found uncertainty sample 86 after 2221 steps.
Found uncertainty sample 87 after 49 steps.
Found uncertainty sample 88 after 1435 steps.
Found uncertainty sample 89 after 1105 steps.
Found uncertainty sample 90 after 370 steps.
Found uncertainty sample 91 after 818 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1442 steps.
Found uncertainty sample 94 after 710 steps.
Found uncertainty sample 95 after 24 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 18 steps.
Found uncertainty sample 98 after 1431 steps.
Found uncertainty sample 99 after 141 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_151032-1x5k9fsc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_34
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/1x5k9fsc
Training model 34. Added 98 samples to the dataset.
Epoch 0, Batch 100/124, Loss: 0.8937876224517822, Variance: 0.11087599396705627

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.983282642404548, Training Loss Force: 3.8013654975947007, time: 1.9702534675598145
Validation Loss Energy: 2.231907632467334, Validation Loss Force: 3.6575875877237007, time: 0.12183642387390137
Test Loss Energy: 10.329697690209526, Test Loss Force: 11.810720388753131, time: 9.688263416290283

Epoch 1, Batch 100/124, Loss: 1.0469311475753784, Variance: 0.1208048090338707

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6830332935532275, Training Loss Force: 3.5451468471172594, time: 2.01953125
Validation Loss Energy: 3.5466794261742582, Validation Loss Force: 3.6557064787789026, time: 0.11479496955871582
Test Loss Energy: 10.22966378391512, Test Loss Force: 11.769847240326628, time: 9.767187356948853

Epoch 2, Batch 100/124, Loss: 1.4062694311141968, Variance: 0.11747853457927704

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.652451457836759, Training Loss Force: 3.5513719383969238, time: 2.0115201473236084
Validation Loss Energy: 1.737380427372959, Validation Loss Force: 3.6255149919517002, time: 0.11534595489501953
Test Loss Energy: 10.231396476296412, Test Loss Force: 11.902642740186693, time: 9.873663663864136

Epoch 3, Batch 100/124, Loss: 0.8425644636154175, Variance: 0.12159410119056702

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.640917744669829, Training Loss Force: 3.547492052433738, time: 2.0152039527893066
Validation Loss Energy: 2.0519131768027044, Validation Loss Force: 3.6786352402964275, time: 0.11594700813293457
Test Loss Energy: 10.353530190401516, Test Loss Force: 11.91844850497533, time: 10.414463758468628

Epoch 4, Batch 100/124, Loss: 0.8810847997665405, Variance: 0.12243279814720154

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6696315469688057, Training Loss Force: 3.561243623020725, time: 1.9669337272644043
Validation Loss Energy: 3.812323346150866, Validation Loss Force: 3.6692472122119173, time: 0.12506890296936035
Test Loss Energy: 10.256103681515574, Test Loss Force: 11.755207478440314, time: 10.230121374130249

Epoch 5, Batch 100/124, Loss: 1.2759617567062378, Variance: 0.11918755620718002

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.7065097550274206, Training Loss Force: 3.5567988954673573, time: 2.1124002933502197
Validation Loss Energy: 1.7216696346667806, Validation Loss Force: 3.6197247777040418, time: 0.1307690143585205
Test Loss Energy: 9.997981480599137, Test Loss Force: 11.809961224530749, time: 11.734227418899536

Epoch 6, Batch 100/124, Loss: 0.9822506904602051, Variance: 0.12246044725179672

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.701443317261434, Training Loss Force: 3.5555593412249507, time: 2.167642116546631
Validation Loss Energy: 2.116445441545377, Validation Loss Force: 3.570436442691586, time: 0.13218164443969727
Test Loss Energy: 10.599422618401261, Test Loss Force: 12.019261114418217, time: 10.689598560333252

Epoch 7, Batch 100/124, Loss: 1.0834038257598877, Variance: 0.12273894250392914

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.710565112358333, Training Loss Force: 3.5548432713703066, time: 2.173489570617676
Validation Loss Energy: 3.686821968338657, Validation Loss Force: 3.62997030466952, time: 0.12221264839172363
Test Loss Energy: 9.993562684844044, Test Loss Force: 11.59910975738154, time: 10.408025741577148

Epoch 8, Batch 100/124, Loss: 1.4676589965820312, Variance: 0.11622114479541779

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.7386204469762836, Training Loss Force: 3.573799738630512, time: 1.9582884311676025
Validation Loss Energy: 1.9494190142704353, Validation Loss Force: 3.650574994087948, time: 0.1217353343963623
Test Loss Energy: 10.120353937997727, Test Loss Force: 11.787727097095352, time: 10.289896249771118

Epoch 9, Batch 100/124, Loss: 1.0668635368347168, Variance: 0.12097811698913574

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6828339992804646, Training Loss Force: 3.562874112454624, time: 2.0338151454925537
Validation Loss Energy: 1.9948553215044378, Validation Loss Force: 3.6859986862831136, time: 0.12374377250671387
Test Loss Energy: 10.143894216995381, Test Loss Force: 11.798268119229961, time: 10.55504322052002

Epoch 10, Batch 100/124, Loss: 1.10318922996521, Variance: 0.12262161076068878

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6851131903780523, Training Loss Force: 3.564887174417011, time: 2.0851545333862305
Validation Loss Energy: 3.8321997590160124, Validation Loss Force: 3.732591091722773, time: 0.11954665184020996
Test Loss Energy: 10.471651732922973, Test Loss Force: 11.754694793806982, time: 10.32794451713562

Epoch 11, Batch 100/124, Loss: 1.3894681930541992, Variance: 0.12058378756046295

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.7351101361280508, Training Loss Force: 3.566557817921653, time: 1.948287010192871
Validation Loss Energy: 1.8007578185902773, Validation Loss Force: 3.6839211089285584, time: 0.12302851676940918
Test Loss Energy: 10.089576711238609, Test Loss Force: 11.837200128975216, time: 10.446447849273682

Epoch 12, Batch 100/124, Loss: 0.7166122198104858, Variance: 0.12658673524856567

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.664000038658477, Training Loss Force: 3.54540657990167, time: 2.009971857070923
Validation Loss Energy: 2.4292400727335344, Validation Loss Force: 3.621449424462302, time: 0.12486767768859863
Test Loss Energy: 10.454218021184579, Test Loss Force: 11.858273919330532, time: 10.352551698684692

Epoch 13, Batch 100/124, Loss: 1.059234857559204, Variance: 0.12528610229492188

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.720989761687387, Training Loss Force: 3.539899769006638, time: 2.0727272033691406
Validation Loss Energy: 3.7232989598171096, Validation Loss Force: 3.72081590194782, time: 0.12207865715026855
Test Loss Energy: 10.454215638274974, Test Loss Force: 11.741954201548237, time: 10.249703645706177

Epoch 14, Batch 100/124, Loss: 1.045048713684082, Variance: 0.1193251758813858

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.7341046430723916, Training Loss Force: 3.562449227401254, time: 2.098459005355835
Validation Loss Energy: 1.973381071959638, Validation Loss Force: 3.729640354423987, time: 0.11989927291870117
Test Loss Energy: 10.241256807012565, Test Loss Force: 11.77928191157387, time: 10.415990352630615

Epoch 15, Batch 100/124, Loss: 0.8212576508522034, Variance: 0.12443137913942337

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.694056094160544, Training Loss Force: 3.5534647715808405, time: 2.0495145320892334
Validation Loss Energy: 2.018419506541392, Validation Loss Force: 3.6904096136178457, time: 0.12234711647033691
Test Loss Energy: 10.250206614445844, Test Loss Force: 11.82512504466394, time: 10.240938186645508

Epoch 16, Batch 100/124, Loss: 1.040347695350647, Variance: 0.11977886408567429

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6743230520924453, Training Loss Force: 3.5506564126451123, time: 2.0230188369750977
Validation Loss Energy: 3.5622497888724047, Validation Loss Force: 3.688458289669032, time: 0.12217473983764648
Test Loss Energy: 10.48942090797068, Test Loss Force: 11.807188212573228, time: 10.504021644592285

Epoch 17, Batch 100/124, Loss: 1.5548267364501953, Variance: 0.11850875616073608

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6851137721909817, Training Loss Force: 3.5556474338079487, time: 1.9941067695617676
Validation Loss Energy: 1.7905428715208398, Validation Loss Force: 3.676113512218795, time: 0.13045978546142578
Test Loss Energy: 10.346007783126584, Test Loss Force: 11.966003611723854, time: 10.339842081069946

Epoch 18, Batch 100/124, Loss: 0.870574951171875, Variance: 0.11928631365299225

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6798276614360828, Training Loss Force: 3.567111132792095, time: 1.9862444400787354
Validation Loss Energy: 1.743739672623035, Validation Loss Force: 3.662386627379785, time: 0.13730478286743164
Test Loss Energy: 10.109027129271022, Test Loss Force: 11.75627924682744, time: 10.448293924331665

Epoch 19, Batch 100/124, Loss: 1.034487009048462, Variance: 0.12604308128356934

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.7046316898261904, Training Loss Force: 3.5416495390697174, time: 2.0169966220855713
Validation Loss Energy: 4.220002050693967, Validation Loss Force: 3.697993650505097, time: 0.12272882461547852
Test Loss Energy: 10.267745785089685, Test Loss Force: 11.683727096575018, time: 11.12882685661316

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–„â–„â–…â–„â–â–ˆâ–â–‚â–ƒâ–‡â–‚â–†â–†â–„â–„â–‡â–…â–‚â–„
wandb:   test_error_force â–…â–„â–†â–†â–„â–…â–ˆâ–â–„â–„â–„â–…â–…â–ƒâ–„â–…â–„â–‡â–„â–‚
wandb:          test_loss â–ˆâ–ƒâ–…â–†â–ƒâ–ƒâ–ˆâ–â–…â–…â–„â–„â–†â–â–…â–…â–ƒâ–…â–„â–
wandb: train_error_energy â–ˆâ–‚â–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–†â–â–‚â–‡â–â–‚â–‡â–‚â–‚â–‡â–â–ƒâ–‡â–‚â–‚â–†â–â–â–ˆ
wandb:  valid_error_force â–…â–…â–ƒâ–†â–…â–ƒâ–â–„â–„â–†â–ˆâ–†â–ƒâ–‡â–ˆâ–†â–†â–†â–…â–‡
wandb:         valid_loss â–‚â–†â–â–‚â–‡â–â–‚â–†â–‚â–‚â–‡â–â–ƒâ–†â–‚â–‚â–†â–â–â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3939
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.26775
wandb:   test_error_force 11.68373
wandb:          test_loss 11.14469
wandb: train_error_energy 2.70463
wandb:  train_error_force 3.54165
wandb:         train_loss 1.04832
wandb: valid_error_energy 4.22
wandb:  valid_error_force 3.69799
wandb:         valid_loss 1.73126
wandb: 
wandb: ğŸš€ View run al_71_34 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/1x5k9fsc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_151032-1x5k9fsc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8410110473632812, Uncertainty Bias: -0.10960942506790161
0.00017929077 0.019831657
2.3662724 4.592888
(48745, 22, 3)
Found uncertainty sample 0 after 1144 steps.
Found uncertainty sample 1 after 534 steps.
Found uncertainty sample 2 after 599 steps.
Found uncertainty sample 3 after 1585 steps.
Found uncertainty sample 4 after 531 steps.
Found uncertainty sample 5 after 2089 steps.
Found uncertainty sample 6 after 75 steps.
Found uncertainty sample 7 after 70 steps.
Found uncertainty sample 8 after 1107 steps.
Found uncertainty sample 9 after 492 steps.
Found uncertainty sample 10 after 106 steps.
Found uncertainty sample 11 after 143 steps.
Found uncertainty sample 12 after 821 steps.
Found uncertainty sample 13 after 251 steps.
Found uncertainty sample 14 after 513 steps.
Found uncertainty sample 15 after 146 steps.
Found uncertainty sample 16 after 566 steps.
Found uncertainty sample 17 after 402 steps.
Found uncertainty sample 18 after 587 steps.
Found uncertainty sample 19 after 537 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 13 steps.
Found uncertainty sample 22 after 2697 steps.
Found uncertainty sample 23 after 1557 steps.
Found uncertainty sample 24 after 3604 steps.
Found uncertainty sample 25 after 702 steps.
Found uncertainty sample 26 after 260 steps.
Found uncertainty sample 27 after 1147 steps.
Found uncertainty sample 28 after 153 steps.
Found uncertainty sample 29 after 275 steps.
Found uncertainty sample 30 after 2173 steps.
Found uncertainty sample 31 after 476 steps.
Found uncertainty sample 32 after 897 steps.
Found uncertainty sample 33 after 205 steps.
Found uncertainty sample 34 after 2300 steps.
Found uncertainty sample 35 after 102 steps.
Found uncertainty sample 36 after 2472 steps.
Found uncertainty sample 37 after 360 steps.
Found uncertainty sample 38 after 28 steps.
Found uncertainty sample 39 after 1317 steps.
Found uncertainty sample 40 after 158 steps.
Found uncertainty sample 41 after 1814 steps.
Found uncertainty sample 42 after 431 steps.
Found uncertainty sample 43 after 414 steps.
Found uncertainty sample 44 after 486 steps.
Found uncertainty sample 45 after 316 steps.
Found uncertainty sample 46 after 1925 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 3572 steps.
Found uncertainty sample 49 after 174 steps.
Found uncertainty sample 50 after 1766 steps.
Found uncertainty sample 51 after 88 steps.
Found uncertainty sample 52 after 28 steps.
Found uncertainty sample 53 after 1263 steps.
Found uncertainty sample 54 after 347 steps.
Found uncertainty sample 55 after 49 steps.
Found uncertainty sample 56 after 844 steps.
Found uncertainty sample 57 after 898 steps.
Found uncertainty sample 58 after 172 steps.
Found uncertainty sample 59 after 553 steps.
Found uncertainty sample 60 after 129 steps.
Found uncertainty sample 61 after 2529 steps.
Found uncertainty sample 62 after 699 steps.
Found uncertainty sample 63 after 37 steps.
Found uncertainty sample 64 after 2668 steps.
Found uncertainty sample 65 after 23 steps.
Found uncertainty sample 66 after 785 steps.
Found uncertainty sample 67 after 1048 steps.
Found uncertainty sample 68 after 77 steps.
Found uncertainty sample 69 after 18 steps.
Found uncertainty sample 70 after 5 steps.
Found uncertainty sample 71 after 4 steps.
Found uncertainty sample 72 after 95 steps.
Found uncertainty sample 73 after 544 steps.
Found uncertainty sample 74 after 2764 steps.
Found uncertainty sample 75 after 60 steps.
Found uncertainty sample 76 after 45 steps.
Found uncertainty sample 77 after 298 steps.
Found uncertainty sample 78 after 9 steps.
Found uncertainty sample 79 after 3229 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 37 steps.
Found uncertainty sample 82 after 89 steps.
Found uncertainty sample 83 after 69 steps.
Found uncertainty sample 84 after 1200 steps.
Found uncertainty sample 85 after 440 steps.
Found uncertainty sample 86 after 1762 steps.
Found uncertainty sample 87 after 133 steps.
Found uncertainty sample 88 after 34 steps.
Found uncertainty sample 89 after 29 steps.
Found uncertainty sample 90 after 39 steps.
Found uncertainty sample 91 after 605 steps.
Found uncertainty sample 92 after 127 steps.
Found uncertainty sample 93 after 987 steps.
Found uncertainty sample 94 after 820 steps.
Found uncertainty sample 95 after 396 steps.
Found uncertainty sample 96 after 133 steps.
Found uncertainty sample 97 after 41 steps.
Found uncertainty sample 98 after 381 steps.
Found uncertainty sample 99 after 15 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_152441-ym9fkzdv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_35
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ym9fkzdv
Training model 35. Added 100 samples to the dataset.
Epoch 0, Batch 100/126, Loss: 0.9788642525672913, Variance: 0.11290493607521057

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.556666763846995, Training Loss Force: 3.74760145388942, time: 2.0434396266937256
Validation Loss Energy: 2.8035374064879193, Validation Loss Force: 3.677698471199509, time: 0.1335461139678955
Test Loss Energy: 10.92585636042728, Test Loss Force: 12.337748165352714, time: 10.186473608016968

Epoch 1, Batch 100/126, Loss: 0.9577070474624634, Variance: 0.12558989226818085

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6667954887267142, Training Loss Force: 3.534019598390765, time: 2.0033016204833984
Validation Loss Energy: 2.3882824584776743, Validation Loss Force: 3.6388452288187643, time: 0.12350797653198242
Test Loss Energy: 10.552090344011653, Test Loss Force: 11.88378284072928, time: 10.165043592453003

Epoch 2, Batch 100/126, Loss: 1.2132526636123657, Variance: 0.12281372398138046

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6392042535445692, Training Loss Force: 3.5324610857891687, time: 2.028229236602783
Validation Loss Energy: 2.4291440281085905, Validation Loss Force: 3.652781719547141, time: 0.17920255661010742
Test Loss Energy: 10.274890279689114, Test Loss Force: 11.861206870688408, time: 10.35147762298584

Epoch 3, Batch 100/126, Loss: 1.0570464134216309, Variance: 0.12380512058734894

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.678601304345311, Training Loss Force: 3.55156616919839, time: 1.9914638996124268
Validation Loss Energy: 2.1399185549016466, Validation Loss Force: 3.683980264698327, time: 0.12333083152770996
Test Loss Energy: 10.295769748662652, Test Loss Force: 11.828312542995635, time: 10.252370357513428

Epoch 4, Batch 100/126, Loss: 1.0728693008422852, Variance: 0.12265296280384064

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6784390262527458, Training Loss Force: 3.554990062527335, time: 2.081817388534546
Validation Loss Energy: 2.10182105160218, Validation Loss Force: 3.645804827197786, time: 0.12041330337524414
Test Loss Energy: 10.50416603083465, Test Loss Force: 11.94867427235874, time: 10.34882402420044

Epoch 5, Batch 100/126, Loss: 1.0497022867202759, Variance: 0.12164318561553955

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6586131955116414, Training Loss Force: 3.55002489578345, time: 2.054283618927002
Validation Loss Energy: 2.418399202694894, Validation Loss Force: 3.7362764887570243, time: 0.12340021133422852
Test Loss Energy: 10.46021594455354, Test Loss Force: 11.880546056738877, time: 10.145902633666992

Epoch 6, Batch 100/126, Loss: 1.0278372764587402, Variance: 0.12246078252792358

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6752890318782327, Training Loss Force: 3.541690926884866, time: 2.0125904083251953
Validation Loss Energy: 2.2694360676528578, Validation Loss Force: 3.6467138368745116, time: 0.12341928482055664
Test Loss Energy: 10.359934131722257, Test Loss Force: 11.907309810873686, time: 10.882179498672485

Epoch 7, Batch 100/126, Loss: 1.081400752067566, Variance: 0.12322767823934555

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6678297321258713, Training Loss Force: 3.539557774139034, time: 2.2560875415802
Validation Loss Energy: 2.38573185763412, Validation Loss Force: 3.6373044325519923, time: 0.12638282775878906
Test Loss Energy: 10.497659526473164, Test Loss Force: 11.929902582525715, time: 10.137284994125366

Epoch 8, Batch 100/126, Loss: 1.1014519929885864, Variance: 0.12573948502540588

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.7326403120943676, Training Loss Force: 3.5721401832911437, time: 2.0613203048706055
Validation Loss Energy: 2.1940581825964314, Validation Loss Force: 3.5945037115522394, time: 0.13356471061706543
Test Loss Energy: 10.41401706777477, Test Loss Force: 11.805305690670291, time: 10.113049030303955

Epoch 9, Batch 100/126, Loss: 0.9376363754272461, Variance: 0.124759241938591

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6910370995761603, Training Loss Force: 3.5429989490678033, time: 2.0836455821990967
Validation Loss Energy: 2.3554317824867526, Validation Loss Force: 3.7023909207988797, time: 0.12429189682006836
Test Loss Energy: 10.354822114454574, Test Loss Force: 11.825162061369289, time: 10.398248195648193

Epoch 10, Batch 100/126, Loss: 1.0475636720657349, Variance: 0.1248571053147316

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6971566087971595, Training Loss Force: 3.556984124814443, time: 2.0807366371154785
Validation Loss Energy: 2.2026310931607487, Validation Loss Force: 3.66979007958969, time: 0.12270545959472656
Test Loss Energy: 10.369654672847052, Test Loss Force: 11.926349137362052, time: 10.145947933197021

Epoch 11, Batch 100/126, Loss: 0.9317728281021118, Variance: 0.12680764496326447

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.686110407763311, Training Loss Force: 3.5430334062345277, time: 2.033095598220825
Validation Loss Energy: 2.1883162875534405, Validation Loss Force: 3.6531916534447944, time: 0.12541484832763672
Test Loss Energy: 10.50090270374941, Test Loss Force: 11.995850184044018, time: 10.297825813293457

Epoch 12, Batch 100/126, Loss: 1.069953441619873, Variance: 0.12297400087118149

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.675949821124535, Training Loss Force: 3.5441024832172974, time: 2.000211000442505
Validation Loss Energy: 2.287791130689977, Validation Loss Force: 3.653780539112725, time: 0.1290264129638672
Test Loss Energy: 10.431907486755337, Test Loss Force: 11.956027866110077, time: 10.13032341003418

Epoch 13, Batch 100/126, Loss: 0.9061756134033203, Variance: 0.12458747625350952

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.671186292779188, Training Loss Force: 3.5482959847343247, time: 2.080319404602051
Validation Loss Energy: 2.410188945946244, Validation Loss Force: 3.7114613484582475, time: 0.12338662147521973
Test Loss Energy: 10.407357492948044, Test Loss Force: 11.995370682942463, time: 10.163605213165283

Epoch 14, Batch 100/126, Loss: 1.0246299505233765, Variance: 0.1232215017080307

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6603632565951987, Training Loss Force: 3.5648004391979917, time: 2.066417694091797
Validation Loss Energy: 2.265219678928618, Validation Loss Force: 3.6553542161900445, time: 0.12215495109558105
Test Loss Energy: 10.554568975700185, Test Loss Force: 12.108119995400177, time: 10.44089150428772

Epoch 15, Batch 100/126, Loss: 1.03290593624115, Variance: 0.12280852347612381

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.691248486818695, Training Loss Force: 3.5569176898842625, time: 1.9985287189483643
Validation Loss Energy: 2.235009933231449, Validation Loss Force: 3.570800432522129, time: 0.1259596347808838
Test Loss Energy: 10.441193951284347, Test Loss Force: 12.002210174439096, time: 10.411725282669067

Epoch 16, Batch 100/126, Loss: 1.2375290393829346, Variance: 0.11996106803417206

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.679952253074986, Training Loss Force: 3.5506662306385386, time: 2.064357280731201
Validation Loss Energy: 2.2501924062773893, Validation Loss Force: 3.6773097650937676, time: 0.1330702304840088
Test Loss Energy: 10.64215975706189, Test Loss Force: 12.298992761385355, time: 10.518603563308716

Epoch 17, Batch 100/126, Loss: 0.9929132461547852, Variance: 0.12335728853940964

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.702333644349788, Training Loss Force: 3.5670917117784495, time: 2.128993511199951
Validation Loss Energy: 2.223347859382627, Validation Loss Force: 3.58901818291926, time: 0.12593817710876465
Test Loss Energy: 10.564449824620361, Test Loss Force: 11.977472910051045, time: 10.369955778121948

Epoch 18, Batch 100/126, Loss: 0.9659373760223389, Variance: 0.12626206874847412

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.677032727451762, Training Loss Force: 3.546874671670929, time: 2.0866096019744873
Validation Loss Energy: 2.5852506120211363, Validation Loss Force: 3.704142658969893, time: 0.12148666381835938
Test Loss Energy: 10.681064020599436, Test Loss Force: 12.21584877380078, time: 10.297994375228882

Epoch 19, Batch 100/126, Loss: 0.9245187044143677, Variance: 0.12054058164358139

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.7104049838268254, Training Loss Force: 3.561691638615475, time: 2.0022265911102295
Validation Loss Energy: 2.1582877945410566, Validation Loss Force: 3.6103771269579936, time: 0.12244439125061035
Test Loss Energy: 10.477734527044936, Test Loss Force: 11.986943068464791, time: 10.458621978759766

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–„â–â–â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–„â–ƒâ–…â–„â–…â–ƒ
wandb:   test_error_force â–ˆâ–‚â–‚â–â–ƒâ–‚â–‚â–ƒâ–â–â–ƒâ–„â–ƒâ–ƒâ–…â–„â–‡â–ƒâ–†â–ƒ
wandb:          test_loss â–ˆâ–ƒâ–â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚
wandb:  train_error_force â–ˆâ–â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–„â–„â–â–â–„â–ƒâ–„â–‚â–„â–‚â–‚â–ƒâ–„â–ƒâ–‚â–‚â–‚â–†â–‚
wandb:  valid_error_force â–†â–„â–„â–†â–„â–ˆâ–„â–„â–‚â–‡â–…â–„â–…â–‡â–…â–â–†â–‚â–‡â–ƒ
wandb:         valid_loss â–ˆâ–ƒâ–„â–ƒâ–â–„â–‚â–ƒâ–‚â–„â–‚â–‚â–ƒâ–…â–ƒâ–‚â–ƒâ–‚â–†â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 4029
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.47773
wandb:   test_error_force 11.98694
wandb:          test_loss 12.56874
wandb: train_error_energy 2.7104
wandb:  train_error_force 3.56169
wandb:         train_loss 1.05422
wandb: valid_error_energy 2.15829
wandb:  valid_error_force 3.61038
wandb:         valid_loss 0.86839
wandb: 
wandb: ğŸš€ View run al_71_35 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ym9fkzdv
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_152441-ym9fkzdv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.241276502609253, Uncertainty Bias: -0.13205145299434662
0.0001449585 0.0028733015
2.3807044 4.782193
(48745, 22, 3)
Found uncertainty sample 0 after 70 steps.
Found uncertainty sample 1 after 11 steps.
Found uncertainty sample 2 after 18 steps.
Found uncertainty sample 3 after 184 steps.
Found uncertainty sample 4 after 598 steps.
Found uncertainty sample 5 after 69 steps.
Found uncertainty sample 6 after 413 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 289 steps.
Found uncertainty sample 10 after 226 steps.
Found uncertainty sample 11 after 212 steps.
Found uncertainty sample 12 after 48 steps.
Found uncertainty sample 13 after 1223 steps.
Found uncertainty sample 14 after 49 steps.
Found uncertainty sample 15 after 143 steps.
Found uncertainty sample 16 after 538 steps.
Found uncertainty sample 17 after 44 steps.
Found uncertainty sample 18 after 828 steps.
Found uncertainty sample 19 after 544 steps.
Found uncertainty sample 20 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 14 steps.
Found uncertainty sample 23 after 94 steps.
Found uncertainty sample 24 after 73 steps.
Found uncertainty sample 25 after 279 steps.
Found uncertainty sample 26 after 363 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 9 steps.
Found uncertainty sample 29 after 6 steps.
Found uncertainty sample 30 after 129 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 394 steps.
Found uncertainty sample 33 after 225 steps.
Found uncertainty sample 34 after 1649 steps.
Found uncertainty sample 35 after 383 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 356 steps.
Found uncertainty sample 38 after 10 steps.
Found uncertainty sample 39 after 411 steps.
Found uncertainty sample 40 after 1056 steps.
Found uncertainty sample 41 after 165 steps.
Found uncertainty sample 42 after 339 steps.
Found uncertainty sample 43 after 1208 steps.
Found uncertainty sample 44 after 136 steps.
Found uncertainty sample 45 after 71 steps.
Found uncertainty sample 46 after 334 steps.
Found uncertainty sample 47 after 31 steps.
Found uncertainty sample 48 after 7 steps.
Found uncertainty sample 49 after 1650 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 49 steps.
Found uncertainty sample 53 after 235 steps.
Found uncertainty sample 54 after 21 steps.
Found uncertainty sample 55 after 20 steps.
Found uncertainty sample 56 after 122 steps.
Found uncertainty sample 57 after 1774 steps.
Found uncertainty sample 58 after 7 steps.
Found uncertainty sample 59 after 409 steps.
Found uncertainty sample 60 after 4 steps.
Found uncertainty sample 61 after 412 steps.
Found uncertainty sample 62 after 963 steps.
Found uncertainty sample 63 after 8 steps.
Found uncertainty sample 64 after 42 steps.
Found uncertainty sample 65 after 111 steps.
Found uncertainty sample 66 after 12 steps.
Found uncertainty sample 67 after 126 steps.
Found uncertainty sample 68 after 642 steps.
Found uncertainty sample 69 after 203 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1648 steps.
Found uncertainty sample 72 after 54 steps.
Found uncertainty sample 73 after 254 steps.
Found uncertainty sample 74 after 268 steps.
Found uncertainty sample 75 after 16 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 508 steps.
Found uncertainty sample 78 after 99 steps.
Found uncertainty sample 79 after 82 steps.
Found uncertainty sample 80 after 8 steps.
Found uncertainty sample 81 after 23 steps.
Found uncertainty sample 82 after 942 steps.
Found uncertainty sample 83 after 125 steps.
Found uncertainty sample 84 after 53 steps.
Found uncertainty sample 85 after 1106 steps.
Found uncertainty sample 86 after 12 steps.
Found uncertainty sample 87 after 34 steps.
Found uncertainty sample 88 after 40 steps.
Found uncertainty sample 89 after 456 steps.
Found uncertainty sample 90 after 1569 steps.
Found uncertainty sample 91 after 14 steps.
Found uncertainty sample 92 after 1202 steps.
Found uncertainty sample 93 after 2016 steps.
Found uncertainty sample 94 after 403 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1234 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 49 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_153458-qfco8tf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_36
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/qfco8tf1
Training model 36. Added 100 samples to the dataset.
Epoch 0, Batch 100/129, Loss: 1.8258625268936157, Variance: 0.12072158604860306

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.173686581487058, Training Loss Force: 3.6774228482236686, time: 2.2779462337493896
Validation Loss Energy: 3.2861549667547245, Validation Loss Force: 3.6957045374619106, time: 0.13593530654907227
Test Loss Energy: 10.946335085560111, Test Loss Force: 12.095864528096177, time: 11.301283597946167

Epoch 1, Batch 100/129, Loss: 1.4168747663497925, Variance: 0.12282275408506393

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6646973485053334, Training Loss Force: 3.5422470880791685, time: 2.2165586948394775
Validation Loss Energy: 3.817947746107856, Validation Loss Force: 3.6965893504247798, time: 0.1503140926361084
Test Loss Energy: 10.702887951830593, Test Loss Force: 12.026223528609862, time: 11.192175149917603

Epoch 2, Batch 100/129, Loss: 1.1359217166900635, Variance: 0.11942145973443985

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6621486714196334, Training Loss Force: 3.5527168066261305, time: 2.4749011993408203
Validation Loss Energy: 3.091728136173737, Validation Loss Force: 3.73296636345671, time: 0.13700342178344727
Test Loss Energy: 10.715291327406227, Test Loss Force: 12.093909530300756, time: 11.370562076568604

Epoch 3, Batch 100/129, Loss: 1.2933440208435059, Variance: 0.12538720667362213

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.670960997057934, Training Loss Force: 3.545157777727919, time: 2.137258529663086
Validation Loss Energy: 3.811831915345804, Validation Loss Force: 3.6524207728615923, time: 0.13711857795715332
Test Loss Energy: 10.473866038848184, Test Loss Force: 11.900976298288255, time: 11.354913473129272

Epoch 4, Batch 100/129, Loss: 1.4122463464736938, Variance: 0.1195342093706131

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.669201890954754, Training Loss Force: 3.5559485696482525, time: 2.243553876876831
Validation Loss Energy: 3.2776553143394183, Validation Loss Force: 3.677267309509944, time: 0.1473398208618164
Test Loss Energy: 10.893893610492642, Test Loss Force: 12.240263729846232, time: 11.444043636322021

Epoch 5, Batch 100/129, Loss: 1.5552027225494385, Variance: 0.12349176406860352

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.694510254631524, Training Loss Force: 3.5563412527285068, time: 2.290149211883545
Validation Loss Energy: 3.898967918875995, Validation Loss Force: 3.669719864585673, time: 0.14401960372924805
Test Loss Energy: 10.578671761261122, Test Loss Force: 11.908136465997618, time: 11.273719549179077

Epoch 6, Batch 100/129, Loss: 1.2471472024917603, Variance: 0.12165205180644989

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.697515955823947, Training Loss Force: 3.5559179362425715, time: 2.3050923347473145
Validation Loss Energy: 2.998638256573631, Validation Loss Force: 3.6732917745785496, time: 0.13121700286865234
Test Loss Energy: 10.84359397944759, Test Loss Force: 12.129687934056783, time: 11.559109926223755

Epoch 7, Batch 100/129, Loss: 1.3642189502716064, Variance: 0.125812828540802

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6925380058244035, Training Loss Force: 3.554881573623314, time: 2.1991190910339355
Validation Loss Energy: 3.6251242076005963, Validation Loss Force: 3.6961188505194342, time: 0.13378429412841797
Test Loss Energy: 10.367652212715143, Test Loss Force: 11.97375328142448, time: 11.379099607467651

Epoch 8, Batch 100/129, Loss: 1.1050164699554443, Variance: 0.11941853165626526

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6653733237010826, Training Loss Force: 3.5523484393618348, time: 2.1892731189727783
Validation Loss Energy: 3.418260711435152, Validation Loss Force: 3.715657845405451, time: 0.13997626304626465
Test Loss Energy: 11.165766715370676, Test Loss Force: 12.388349768402245, time: 11.571571588516235

Epoch 9, Batch 100/129, Loss: 1.6053489446640015, Variance: 0.12525877356529236

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6557205385498297, Training Loss Force: 3.5437467457897016, time: 2.3679065704345703
Validation Loss Energy: 3.6757633278752997, Validation Loss Force: 3.663129080535357, time: 0.1311049461364746
Test Loss Energy: 10.793143259809277, Test Loss Force: 12.22028048669795, time: 11.476508855819702

Epoch 10, Batch 100/129, Loss: 1.4786040782928467, Variance: 0.11880339682102203

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6521721564003142, Training Loss Force: 3.551395403334442, time: 2.2333614826202393
Validation Loss Energy: 3.5248840506095056, Validation Loss Force: 3.731053561270142, time: 0.1352834701538086
Test Loss Energy: 10.961329066259509, Test Loss Force: 12.107503283472449, time: 11.737545728683472

Epoch 11, Batch 100/129, Loss: 1.4206799268722534, Variance: 0.1254863142967224

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.691261282218822, Training Loss Force: 3.5535771156262426, time: 2.2267580032348633
Validation Loss Energy: 3.9306734816724576, Validation Loss Force: 3.6779480090890453, time: 0.13015437126159668
Test Loss Energy: 10.455656039513507, Test Loss Force: 11.850146108456226, time: 11.544498443603516

Epoch 12, Batch 100/129, Loss: 1.3167599439620972, Variance: 0.12193143367767334

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6780161973127834, Training Loss Force: 3.549406988574716, time: 2.040919303894043
Validation Loss Energy: 3.2933019041085205, Validation Loss Force: 3.694301647619518, time: 0.12616372108459473
Test Loss Energy: 10.992982028619226, Test Loss Force: 12.460825758811914, time: 10.928169250488281

Epoch 13, Batch 100/129, Loss: 1.2036268711090088, Variance: 0.1273612529039383

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.68950361742368, Training Loss Force: 3.5611784792800587, time: 2.4340736865997314
Validation Loss Energy: 3.713107764123238, Validation Loss Force: 3.71081271323616, time: 0.1431114673614502
Test Loss Energy: 10.504223024809553, Test Loss Force: 12.005404765540863, time: 11.438288688659668

Epoch 14, Batch 100/129, Loss: 1.2911100387573242, Variance: 0.12413414567708969

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.7177736205567995, Training Loss Force: 3.559989177479897, time: 2.073119878768921
Validation Loss Energy: 3.4076554253529037, Validation Loss Force: 3.625571772583912, time: 0.12432241439819336
Test Loss Energy: 10.833911364357643, Test Loss Force: 12.061322453386767, time: 9.925503015518188

Epoch 15, Batch 100/129, Loss: 1.298363208770752, Variance: 0.12664969265460968

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.680753341390986, Training Loss Force: 3.5771044774004825, time: 2.0856730937957764
Validation Loss Energy: 3.7841635668341573, Validation Loss Force: 3.6243331383481636, time: 0.12042832374572754
Test Loss Energy: 10.419645798704938, Test Loss Force: 11.944019801714308, time: 10.065643548965454

Epoch 16, Batch 100/129, Loss: 1.1513200998306274, Variance: 0.12063619494438171

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.648877929504408, Training Loss Force: 3.5563992122989094, time: 2.1018590927124023
Validation Loss Energy: 3.0860501471756265, Validation Loss Force: 3.640046603084093, time: 0.1262814998626709
Test Loss Energy: 10.866225185426334, Test Loss Force: 12.272461375911103, time: 9.824728727340698

Epoch 17, Batch 100/129, Loss: 1.8200345039367676, Variance: 0.12681396305561066

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.7101698429277845, Training Loss Force: 3.5588532644912365, time: 2.168437957763672
Validation Loss Energy: 3.6947809058446026, Validation Loss Force: 3.655192781059247, time: 0.12278890609741211
Test Loss Energy: 10.458857233746384, Test Loss Force: 11.87300614836856, time: 9.911956310272217

Epoch 18, Batch 100/129, Loss: 1.135667085647583, Variance: 0.11631181836128235

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6757441417618453, Training Loss Force: 3.549213742826164, time: 2.193624496459961
Validation Loss Energy: 3.3003369873280115, Validation Loss Force: 3.684210430372167, time: 0.12028288841247559
Test Loss Energy: 11.029110710760515, Test Loss Force: 12.34109937097314, time: 9.80153489112854

Epoch 19, Batch 100/129, Loss: 1.4669380187988281, Variance: 0.12448418885469437

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6574446251829893, Training Loss Force: 3.542750587897164, time: 2.1094229221343994
Validation Loss Energy: 3.854148034060969, Validation Loss Force: 3.660338293906395, time: 0.1207723617553711
Test Loss Energy: 10.638616347165286, Test Loss Force: 12.14812224865694, time: 9.604162454605103

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–„â–‚â–†â–ƒâ–…â–â–ˆâ–…â–†â–‚â–†â–‚â–…â–â–…â–‚â–‡â–ƒ
wandb:   test_error_force â–„â–ƒâ–„â–‚â–…â–‚â–„â–‚â–‡â–…â–„â–â–ˆâ–ƒâ–ƒâ–‚â–†â–â–‡â–„
wandb:          test_loss â–‡â–ƒâ–†â–‚â–†â–‚â–‡â–â–ˆâ–ƒâ–‡â–â–‡â–ƒâ–†â–‚â–‡â–‚â–‡â–ƒ
wandb: train_error_energy â–ˆâ–â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–‚â–â–â–‚â–â–
wandb:  train_error_force â–ˆâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–‡â–‚â–‡â–ƒâ–ˆâ–â–†â–„â–†â–…â–ˆâ–ƒâ–†â–„â–‡â–‚â–†â–ƒâ–‡
wandb:  valid_error_force â–†â–†â–ˆâ–ƒâ–„â–„â–„â–†â–‡â–ƒâ–ˆâ–„â–†â–‡â–â–â–‚â–ƒâ–…â–ƒ
wandb:         valid_loss â–ƒâ–ˆâ–‚â–‡â–ƒâ–ˆâ–â–…â–…â–†â–†â–ˆâ–ƒâ–‡â–„â–†â–â–†â–ƒâ–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 4119
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.63862
wandb:   test_error_force 12.14812
wandb:          test_loss 11.64745
wandb: train_error_energy 2.65744
wandb:  train_error_force 3.54275
wandb:         train_loss 1.01995
wandb: valid_error_energy 3.85415
wandb:  valid_error_force 3.66034
wandb:         valid_loss 1.56907
wandb: 
wandb: ğŸš€ View run al_71_36 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/qfco8tf1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_153458-qfco8tf1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.014472484588623, Uncertainty Bias: -0.1251561939716339
0.00021028519 0.028755188
2.446755 4.6626086
(48745, 22, 3)
Found uncertainty sample 0 after 53 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 560 steps.
Found uncertainty sample 3 after 13 steps.
Found uncertainty sample 4 after 94 steps.
Found uncertainty sample 5 after 255 steps.
Found uncertainty sample 6 after 2426 steps.
Found uncertainty sample 7 after 1797 steps.
Found uncertainty sample 8 after 62 steps.
Found uncertainty sample 9 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 900 steps.
Found uncertainty sample 12 after 142 steps.
Found uncertainty sample 13 after 553 steps.
Found uncertainty sample 14 after 541 steps.
Found uncertainty sample 15 after 53 steps.
Found uncertainty sample 16 after 378 steps.
Found uncertainty sample 17 after 342 steps.
Found uncertainty sample 18 after 17 steps.
Found uncertainty sample 19 after 614 steps.
Found uncertainty sample 20 after 143 steps.
Found uncertainty sample 21 after 349 steps.
Found uncertainty sample 22 after 260 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 77 steps.
Found uncertainty sample 25 after 3684 steps.
Found uncertainty sample 26 after 22 steps.
Found uncertainty sample 27 after 380 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 7 steps.
Found uncertainty sample 30 after 33 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 338 steps.
Found uncertainty sample 33 after 55 steps.
Found uncertainty sample 34 after 3119 steps.
Found uncertainty sample 35 after 39 steps.
Found uncertainty sample 36 after 35 steps.
Found uncertainty sample 37 after 17 steps.
Found uncertainty sample 38 after 8 steps.
Found uncertainty sample 39 after 838 steps.
Found uncertainty sample 40 after 718 steps.
Found uncertainty sample 41 after 974 steps.
Found uncertainty sample 42 after 3487 steps.
Found uncertainty sample 43 after 17 steps.
Found uncertainty sample 44 after 16 steps.
Found uncertainty sample 45 after 1878 steps.
Found uncertainty sample 46 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 116 steps.
Found uncertainty sample 49 after 829 steps.
Found uncertainty sample 50 after 46 steps.
Found uncertainty sample 51 after 65 steps.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 264 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 29 steps.
Found uncertainty sample 56 after 270 steps.
Found uncertainty sample 57 after 278 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 23 steps.
Found uncertainty sample 61 after 169 steps.
Found uncertainty sample 62 after 11 steps.
Found uncertainty sample 63 after 70 steps.
Found uncertainty sample 64 after 619 steps.
Found uncertainty sample 65 after 3967 steps.
Found uncertainty sample 66 after 7 steps.
Found uncertainty sample 67 after 3814 steps.
Found uncertainty sample 68 after 2541 steps.
Found uncertainty sample 69 after 646 steps.
Found uncertainty sample 70 after 3494 steps.
Found uncertainty sample 71 after 2037 steps.
Found uncertainty sample 72 after 89 steps.
Found uncertainty sample 73 after 379 steps.
Found uncertainty sample 74 after 61 steps.
Found uncertainty sample 75 after 675 steps.
Found uncertainty sample 76 after 19 steps.
Found uncertainty sample 77 after 1655 steps.
Found uncertainty sample 78 after 932 steps.
Found uncertainty sample 79 after 163 steps.
Found uncertainty sample 80 after 96 steps.
Found uncertainty sample 81 after 286 steps.
Found uncertainty sample 82 after 283 steps.
Found uncertainty sample 83 after 72 steps.
Found uncertainty sample 84 after 2153 steps.
Found uncertainty sample 85 after 24 steps.
Found uncertainty sample 86 after 26 steps.
Found uncertainty sample 87 after 37 steps.
Found uncertainty sample 88 after 12 steps.
Found uncertainty sample 89 after 1525 steps.
Found uncertainty sample 90 after 52 steps.
Found uncertainty sample 91 after 568 steps.
Found uncertainty sample 92 after 14 steps.
Found uncertainty sample 93 after 216 steps.
Found uncertainty sample 94 after 31 steps.
Found uncertainty sample 95 after 75 steps.
Found uncertainty sample 96 after 449 steps.
Found uncertainty sample 97 after 327 steps.
Found uncertainty sample 98 after 400 steps.
Found uncertainty sample 99 after 89 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_154918-dy55efsg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_37
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/dy55efsg
Training model 37. Added 96 samples to the dataset.
Epoch 0, Batch 100/132, Loss: 0.9180957078933716, Variance: 0.15088480710983276

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.781724544833394, Training Loss Force: 3.771327920312385, time: 2.1999659538269043
Validation Loss Energy: 3.6832384795501927, Validation Loss Force: 3.6587344080482773, time: 0.13540124893188477
Test Loss Energy: 10.34193251806557, Test Loss Force: 11.398636449122506, time: 11.153369903564453

Epoch 1, Batch 100/132, Loss: 0.902262270450592, Variance: 0.15935826301574707

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.205696107149978, Training Loss Force: 3.5940912321413956, time: 2.212822914123535
Validation Loss Energy: 3.340401549862604, Validation Loss Force: 3.750954245072195, time: 0.12672114372253418
Test Loss Energy: 10.599059917176106, Test Loss Force: 11.392177873420327, time: 10.475383043289185

Epoch 2, Batch 100/132, Loss: 1.1758068799972534, Variance: 0.16061335802078247

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.164131656213185, Training Loss Force: 3.612666847280669, time: 2.3730180263519287
Validation Loss Energy: 3.916388513730704, Validation Loss Force: 3.66858269325742, time: 0.1290302276611328
Test Loss Energy: 10.349016246138495, Test Loss Force: 11.395477583011884, time: 10.394187450408936

Epoch 3, Batch 100/132, Loss: 1.0418943166732788, Variance: 0.16507548093795776

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.163473405732523, Training Loss Force: 3.6014317853782383, time: 2.138806104660034
Validation Loss Energy: 3.3171568950500565, Validation Loss Force: 3.8357281000793364, time: 0.12704849243164062
Test Loss Energy: 10.671610626491018, Test Loss Force: 11.619468404388561, time: 10.431270360946655

Epoch 4, Batch 100/132, Loss: 1.0478116273880005, Variance: 0.16139352321624756

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.190805600587198, Training Loss Force: 3.592299968510092, time: 2.153301954269409
Validation Loss Energy: 3.7276768578951955, Validation Loss Force: 3.6441299841051653, time: 0.12531042098999023
Test Loss Energy: 10.205601223360528, Test Loss Force: 11.410146117016104, time: 10.542900085449219

Epoch 5, Batch 100/132, Loss: 1.0592734813690186, Variance: 0.169394850730896

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.229126726124012, Training Loss Force: 3.5942673330718495, time: 2.1222455501556396
Validation Loss Energy: 2.583751825999896, Validation Loss Force: 3.69437097194695, time: 0.13625645637512207
Test Loss Energy: 10.367298879915415, Test Loss Force: 11.43356433601597, time: 10.364110469818115

Epoch 6, Batch 100/132, Loss: 1.0744571685791016, Variance: 0.16329509019851685

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.161911519998611, Training Loss Force: 3.6256615048759455, time: 2.1511969566345215
Validation Loss Energy: 3.7705433405160456, Validation Loss Force: 3.7073500595708855, time: 0.12392830848693848
Test Loss Energy: 10.234378773934965, Test Loss Force: 11.494590888787302, time: 10.681171178817749

Epoch 7, Batch 100/132, Loss: 1.1999151706695557, Variance: 0.1645449995994568

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.1501861929303585, Training Loss Force: 3.571850773004795, time: 2.1338677406311035
Validation Loss Energy: 3.3327314142389266, Validation Loss Force: 3.651386623124855, time: 0.12552928924560547
Test Loss Energy: 11.057716565936342, Test Loss Force: 11.803066389841991, time: 10.376286506652832

Epoch 8, Batch 100/132, Loss: 0.921265721321106, Variance: 0.1616644263267517

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.106063059272146, Training Loss Force: 3.609213935390529, time: 2.128566265106201
Validation Loss Energy: 3.6131206335049697, Validation Loss Force: 3.7088935278674087, time: 0.13933300971984863
Test Loss Energy: 10.519796066532292, Test Loss Force: 11.555604738925048, time: 10.349322319030762

Epoch 9, Batch 100/132, Loss: 0.9875754714012146, Variance: 0.168614000082016

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.17466059629891, Training Loss Force: 3.607733350885081, time: 2.111616373062134
Validation Loss Energy: 3.689739602157749, Validation Loss Force: 3.649501728804576, time: 0.1260368824005127
Test Loss Energy: 11.067822513544328, Test Loss Force: 11.777340558384127, time: 10.611693859100342

Epoch 10, Batch 100/132, Loss: 0.8843228816986084, Variance: 0.16470083594322205

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.14921199854805, Training Loss Force: 3.5906187304270762, time: 2.1911542415618896
Validation Loss Energy: 3.645375595307486, Validation Loss Force: 3.7014216420066557, time: 0.13101744651794434
Test Loss Energy: 10.625215404489436, Test Loss Force: 11.509401683893824, time: 10.403801441192627

Epoch 11, Batch 100/132, Loss: 1.1212708950042725, Variance: 0.1683478206396103

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.179885590222942, Training Loss Force: 3.6222267776327226, time: 2.1693902015686035
Validation Loss Energy: 3.7014341114788274, Validation Loss Force: 3.6279363704675673, time: 0.12715697288513184
Test Loss Energy: 10.905686401622386, Test Loss Force: 11.776910440869072, time: 10.560566902160645

Epoch 12, Batch 100/132, Loss: 1.0314915180206299, Variance: 0.1636917144060135

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.1512807636024665, Training Loss Force: 3.5677876869268648, time: 2.1657304763793945
Validation Loss Energy: 3.5694129068508045, Validation Loss Force: 3.7324818353951716, time: 0.1318964958190918
Test Loss Energy: 10.424987666489958, Test Loss Force: 11.665998137207195, time: 10.49299168586731

Epoch 13, Batch 100/132, Loss: 1.0488771200180054, Variance: 0.1672019064426422

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.12685702589905, Training Loss Force: 3.597198030147503, time: 2.154924154281616
Validation Loss Energy: 3.3741949327943352, Validation Loss Force: 3.6109748451687125, time: 0.13070964813232422
Test Loss Energy: 11.070177910101567, Test Loss Force: 12.016750129252525, time: 10.459372520446777

Epoch 14, Batch 100/132, Loss: 1.0818612575531006, Variance: 0.1685909479856491

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.148894444512314, Training Loss Force: 3.5817906157182136, time: 2.2945330142974854
Validation Loss Energy: 4.1453900732764835, Validation Loss Force: 3.6276056325628154, time: 0.12991738319396973
Test Loss Energy: 10.608909018838785, Test Loss Force: 11.824779140386715, time: 10.488613843917847

Epoch 15, Batch 100/132, Loss: 0.9883954524993896, Variance: 0.16947698593139648

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.147414359325558, Training Loss Force: 3.599549746958444, time: 2.142118453979492
Validation Loss Energy: 3.4632597706896013, Validation Loss Force: 3.692025004048503, time: 0.12564921379089355
Test Loss Energy: 11.170415692732304, Test Loss Force: 12.014510133603231, time: 10.453526258468628

Epoch 16, Batch 100/132, Loss: 0.9679612517356873, Variance: 0.16323360800743103

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.107618442054863, Training Loss Force: 3.5878615061112007, time: 2.1211392879486084
Validation Loss Energy: 3.673271289017124, Validation Loss Force: 3.593934049653908, time: 0.12897181510925293
Test Loss Energy: 10.561885291605396, Test Loss Force: 11.869496038584439, time: 10.585129976272583

Epoch 17, Batch 100/132, Loss: 0.9642423987388611, Variance: 0.16906528174877167

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.1790996288657665, Training Loss Force: 3.5660085756328885, time: 2.1089930534362793
Validation Loss Energy: 3.3875410533938086, Validation Loss Force: 3.5786165648824864, time: 0.14070892333984375
Test Loss Energy: 10.989215119522074, Test Loss Force: 12.006596336923337, time: 11.097986459732056

Epoch 18, Batch 100/132, Loss: 0.9281794428825378, Variance: 0.1656825840473175

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.124742725004292, Training Loss Force: 3.585303005573629, time: 2.150254487991333
Validation Loss Energy: 3.659183828607808, Validation Loss Force: 3.6230594708334567, time: 0.1295032501220703
Test Loss Energy: 10.781216433511753, Test Loss Force: 12.039716523410123, time: 10.48429250717163

Epoch 19, Batch 100/132, Loss: 0.959187388420105, Variance: 0.1710081845521927

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.167637748074233, Training Loss Force: 3.5602100686464326, time: 2.1664376258850098
Validation Loss Energy: 3.0641385383167, Validation Loss Force: 3.6370544493241646, time: 0.1281578540802002
Test Loss Energy: 11.112968573295325, Test Loss Force: 12.337123507284154, time: 10.445191144943237

wandb: - 0.039 MB of 0.056 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–‚â–„â–â–‚â–â–‡â–ƒâ–‡â–„â–†â–ƒâ–‡â–„â–ˆâ–„â–‡â–…â–ˆ
wandb:   test_error_force â–â–â–â–ƒâ–â–â–‚â–„â–‚â–„â–‚â–„â–ƒâ–†â–„â–†â–…â–†â–†â–ˆ
wandb:          test_loss â–„â–†â–ƒâ–†â–â–„â–‚â–ˆâ–ƒâ–‡â–„â–†â–‚â–‡â–ƒâ–ˆâ–ƒâ–†â–„â–‡
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–‚
wandb:  train_error_force â–ˆâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚â–‚â–‚â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–„â–‡â–„â–†â–â–†â–„â–†â–†â–†â–†â–…â–…â–ˆâ–…â–†â–…â–†â–ƒ
wandb:  valid_error_force â–ƒâ–†â–ƒâ–ˆâ–ƒâ–„â–…â–ƒâ–…â–ƒâ–„â–‚â–…â–‚â–‚â–„â–â–â–‚â–ƒ
wandb:         valid_loss â–†â–…â–‡â–…â–†â–â–‡â–„â–†â–†â–†â–†â–†â–„â–ˆâ–…â–…â–„â–…â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 4205
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.11297
wandb:   test_error_force 12.33712
wandb:          test_loss 9.47942
wandb: train_error_energy 4.16764
wandb:  train_error_force 3.56021
wandb:         train_loss 1.47112
wandb: valid_error_energy 3.06414
wandb:  valid_error_force 3.63705
wandb:         valid_loss 1.18908
wandb: 
wandb: ğŸš€ View run al_71_37 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/dy55efsg
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_154918-dy55efsg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2908923625946045, Uncertainty Bias: -0.2960638403892517
0.00037384033 0.009871542
2.348382 4.6261387
(48745, 22, 3)
Found uncertainty sample 0 after 358 steps.
Found uncertainty sample 1 after 147 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 33 steps.
Found uncertainty sample 4 after 346 steps.
Found uncertainty sample 5 after 1320 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 2748 steps.
Found uncertainty sample 8 after 40 steps.
Found uncertainty sample 9 after 622 steps.
Found uncertainty sample 10 after 98 steps.
Found uncertainty sample 11 after 7 steps.
Found uncertainty sample 12 after 25 steps.
Found uncertainty sample 13 after 479 steps.
Found uncertainty sample 14 after 2169 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 201 steps.
Found uncertainty sample 17 after 451 steps.
Found uncertainty sample 18 after 241 steps.
Found uncertainty sample 19 after 432 steps.
Found uncertainty sample 20 after 49 steps.
Found uncertainty sample 21 after 83 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 195 steps.
Found uncertainty sample 24 after 11 steps.
Found uncertainty sample 25 after 4 steps.
Found uncertainty sample 26 after 167 steps.
Found uncertainty sample 27 after 874 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 139 steps.
Found uncertainty sample 30 after 202 steps.
Found uncertainty sample 31 after 562 steps.
Found uncertainty sample 32 after 460 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 30 steps.
Found uncertainty sample 35 after 1259 steps.
Found uncertainty sample 36 after 1016 steps.
Found uncertainty sample 37 after 476 steps.
Found uncertainty sample 38 after 1053 steps.
Found uncertainty sample 39 after 2189 steps.
Found uncertainty sample 40 after 476 steps.
Found uncertainty sample 41 after 26 steps.
Found uncertainty sample 42 after 216 steps.
Found uncertainty sample 43 after 568 steps.
Found uncertainty sample 44 after 34 steps.
Found uncertainty sample 45 after 12 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 371 steps.
Found uncertainty sample 48 after 98 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 183 steps.
Found uncertainty sample 51 after 133 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 367 steps.
Found uncertainty sample 54 after 1019 steps.
Found uncertainty sample 55 after 8 steps.
Found uncertainty sample 56 after 171 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1371 steps.
Found uncertainty sample 59 after 3187 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 119 steps.
Found uncertainty sample 62 after 214 steps.
Found uncertainty sample 63 after 2643 steps.
Found uncertainty sample 64 after 176 steps.
Found uncertainty sample 65 after 593 steps.
Found uncertainty sample 66 after 618 steps.
Found uncertainty sample 67 after 1604 steps.
Found uncertainty sample 68 after 411 steps.
Found uncertainty sample 69 after 14 steps.
Found uncertainty sample 70 after 55 steps.
Found uncertainty sample 71 after 9 steps.
Found uncertainty sample 72 after 13 steps.
Found uncertainty sample 73 after 1214 steps.
Found uncertainty sample 74 after 1593 steps.
Found uncertainty sample 75 after 4 steps.
Found uncertainty sample 76 after 1051 steps.
Found uncertainty sample 77 after 32 steps.
Found uncertainty sample 78 after 82 steps.
Found uncertainty sample 79 after 803 steps.
Found uncertainty sample 80 after 617 steps.
Found uncertainty sample 81 after 3190 steps.
Found uncertainty sample 82 after 898 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1921 steps.
Found uncertainty sample 86 after 27 steps.
Found uncertainty sample 87 after 1421 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 16 steps.
Found uncertainty sample 90 after 5 steps.
Found uncertainty sample 91 after 141 steps.
Found uncertainty sample 92 after 417 steps.
Found uncertainty sample 93 after 11 steps.
Found uncertainty sample 94 after 17 steps.
Found uncertainty sample 95 after 100 steps.
Found uncertainty sample 96 after 1590 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1266 steps.
Found uncertainty sample 99 after 14 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_160213-l0qghabp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_38
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/l0qghabp
Training model 38. Added 98 samples to the dataset.
Epoch 0, Batch 100/135, Loss: 1.0712791681289673, Variance: 0.17126250267028809

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.624670768977352, Training Loss Force: 3.711662613326348, time: 2.205310821533203
Validation Loss Energy: 2.035789369942354, Validation Loss Force: 3.712393664172248, time: 0.13359522819519043
Test Loss Energy: 10.54535553266136, Test Loss Force: 12.35424536563755, time: 10.02801775932312

Epoch 1, Batch 100/135, Loss: 1.635542631149292, Variance: 0.17005765438079834

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.082431065838556, Training Loss Force: 3.5737853915945434, time: 2.1995131969451904
Validation Loss Energy: 4.743363165944348, Validation Loss Force: 3.684716705786959, time: 0.13308429718017578
Test Loss Energy: 11.811135518652794, Test Loss Force: 12.256353135489345, time: 10.104634284973145

Epoch 2, Batch 100/135, Loss: 1.9777305126190186, Variance: 0.16717244684696198

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.0895586441718645, Training Loss Force: 3.577785550434675, time: 2.376845598220825
Validation Loss Energy: 5.693686579113468, Validation Loss Force: 3.6873253490545514, time: 0.16447234153747559
Test Loss Energy: 12.194295892707986, Test Loss Force: 12.392383885137736, time: 10.148453950881958

Epoch 3, Batch 100/135, Loss: 1.2732082605361938, Variance: 0.16846489906311035

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.156243531544174, Training Loss Force: 3.5838864178301213, time: 2.194944381713867
Validation Loss Energy: 3.294612312032305, Validation Loss Force: 3.833050429376506, time: 0.1294546127319336
Test Loss Energy: 11.09850415345207, Test Loss Force: 12.59045529979128, time: 10.11062741279602

Epoch 4, Batch 100/135, Loss: 1.0738589763641357, Variance: 0.16530343890190125

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.139241368301486, Training Loss Force: 3.612255766749373, time: 2.187333822250366
Validation Loss Energy: 2.2564729379843795, Validation Loss Force: 3.672830849446479, time: 0.13022112846374512
Test Loss Energy: 10.225779450068993, Test Loss Force: 12.096542006550408, time: 10.301013708114624

Epoch 5, Batch 100/135, Loss: 1.4654922485351562, Variance: 0.16308170557022095

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.120056754660015, Training Loss Force: 3.570427291814389, time: 2.196913242340088
Validation Loss Energy: 5.508616602936559, Validation Loss Force: 3.7011349925927983, time: 0.12505888938903809
Test Loss Energy: 11.409657531946044, Test Loss Force: 12.057530246276698, time: 10.774147510528564

Epoch 6, Batch 100/135, Loss: 1.719553828239441, Variance: 0.1615995615720749

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.1040738091260796, Training Loss Force: 3.593001039542053, time: 2.2403295040130615
Validation Loss Energy: 5.801544989817234, Validation Loss Force: 3.615523975820017, time: 0.13273859024047852
Test Loss Energy: 11.565628329260026, Test Loss Force: 11.941711704280573, time: 10.322550058364868

Epoch 7, Batch 100/135, Loss: 1.3779305219650269, Variance: 0.16270676255226135

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.122453894402298, Training Loss Force: 3.577050795033948, time: 2.216621160507202
Validation Loss Energy: 3.753583819239504, Validation Loss Force: 3.652999786864053, time: 0.1254899501800537
Test Loss Energy: 10.84888632841249, Test Loss Force: 11.969948185151702, time: 10.126039981842041

Epoch 8, Batch 100/135, Loss: 1.0017824172973633, Variance: 0.16884276270866394

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.160839057373784, Training Loss Force: 3.5798047587837774, time: 2.133394718170166
Validation Loss Energy: 2.184536701470462, Validation Loss Force: 3.6649896296956808, time: 0.12454438209533691
Test Loss Energy: 10.661938713509763, Test Loss Force: 12.403258332750939, time: 10.050805568695068

Epoch 9, Batch 100/135, Loss: 1.7611689567565918, Variance: 0.16752758622169495

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.101889847051205, Training Loss Force: 3.6104073703762136, time: 2.1171698570251465
Validation Loss Energy: 4.984040604825997, Validation Loss Force: 3.718706037567106, time: 0.12386178970336914
Test Loss Energy: 11.845418826166881, Test Loss Force: 12.568231902443944, time: 10.298352718353271

Epoch 10, Batch 100/135, Loss: 1.8403387069702148, Variance: 0.17177729308605194

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.101624708301545, Training Loss Force: 3.5749542330729516, time: 2.1559317111968994
Validation Loss Energy: 5.354695071353003, Validation Loss Force: 3.7236150015582297, time: 0.13148880004882812
Test Loss Energy: 12.381967991405075, Test Loss Force: 12.691604818144226, time: 10.075042963027954

Epoch 11, Batch 100/135, Loss: 1.2045128345489502, Variance: 0.1710592657327652

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.15024693103806, Training Loss Force: 3.5671608014196714, time: 2.2334914207458496
Validation Loss Energy: 3.1324572176628265, Validation Loss Force: 3.5678160721823082, time: 0.12589216232299805
Test Loss Energy: 11.287142281837141, Test Loss Force: 12.783241588820387, time: 10.293829202651978

Epoch 12, Batch 100/135, Loss: 1.1211692094802856, Variance: 0.16621844470500946

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.152551107236618, Training Loss Force: 3.5969709219759434, time: 2.2114195823669434
Validation Loss Energy: 2.020806742454884, Validation Loss Force: 3.81495381099476, time: 0.1300504207611084
Test Loss Energy: 10.538271520131515, Test Loss Force: 12.39184665327123, time: 10.144321918487549

Epoch 13, Batch 100/135, Loss: 1.7407032251358032, Variance: 0.16169998049736023

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.14796678578855, Training Loss Force: 3.5719515643811315, time: 2.1874661445617676
Validation Loss Energy: 4.9299618400310985, Validation Loss Force: 3.5962608680058046, time: 0.1243596076965332
Test Loss Energy: 11.22745762893535, Test Loss Force: 12.135920809514714, time: 10.06169867515564

Epoch 14, Batch 100/135, Loss: 1.8338193893432617, Variance: 0.16442230343818665

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.1366893654670145, Training Loss Force: 3.583967703614193, time: 2.2230122089385986
Validation Loss Energy: 5.618021970420135, Validation Loss Force: 3.8805402656973036, time: 0.12593364715576172
Test Loss Energy: 11.479432986290675, Test Loss Force: 12.037906827141777, time: 10.307212829589844

Epoch 15, Batch 100/135, Loss: 1.1789138317108154, Variance: 0.16309624910354614

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.100900928427961, Training Loss Force: 3.591845276164815, time: 2.2647175788879395
Validation Loss Energy: 3.8016768662766207, Validation Loss Force: 3.668444489940848, time: 0.12388944625854492
Test Loss Energy: 10.695389844637745, Test Loss Force: 12.074452518463687, time: 10.128020763397217

Epoch 16, Batch 100/135, Loss: 1.0542126893997192, Variance: 0.16807988286018372

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.153462624827687, Training Loss Force: 3.625534225617705, time: 2.187256336212158
Validation Loss Energy: 2.4593876937389894, Validation Loss Force: 3.6792524049458906, time: 0.12994980812072754
Test Loss Energy: 10.726892902864165, Test Loss Force: 12.30327989555582, time: 10.363958835601807

Epoch 17, Batch 100/135, Loss: 1.7496007680892944, Variance: 0.17176741361618042

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.162192984745088, Training Loss Force: 3.5568790536064556, time: 2.155383586883545
Validation Loss Energy: 4.884251723970653, Validation Loss Force: 3.6993528229718406, time: 0.12432336807250977
Test Loss Energy: 12.080029753364986, Test Loss Force: 12.68539358472831, time: 10.13956069946289

Epoch 18, Batch 100/135, Loss: 1.8955276012420654, Variance: 0.17146413028240204

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.115536182563727, Training Loss Force: 3.5659830482272232, time: 2.213379144668579
Validation Loss Energy: 5.411687536145415, Validation Loss Force: 3.7235001042789313, time: 0.13219213485717773
Test Loss Energy: 12.34478919224513, Test Loss Force: 12.573663187629448, time: 10.117941617965698

Epoch 19, Batch 100/135, Loss: 1.1574639081954956, Variance: 0.1692897081375122

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.1014481456887335, Training Loss Force: 3.5770894240072635, time: 2.2921791076660156
Validation Loss Energy: 3.706683313924929, Validation Loss Force: 3.6133842737737485, time: 0.18550825119018555
Test Loss Energy: 11.392117013629257, Test Loss Force: 12.641835792518425, time: 10.259408235549927

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–†â–‡â–„â–â–…â–…â–ƒâ–‚â–†â–ˆâ–„â–‚â–„â–…â–ƒâ–ƒâ–‡â–ˆâ–…
wandb:   test_error_force â–„â–„â–…â–†â–‚â–‚â–â–â–…â–†â–‡â–ˆâ–…â–ƒâ–‚â–‚â–„â–‡â–†â–‡
wandb:          test_loss â–‚â–†â–‡â–…â–â–ƒâ–ƒâ–‚â–ƒâ–†â–ˆâ–…â–â–‚â–ƒâ–â–ƒâ–†â–‡â–„
wandb: train_error_energy â–ˆâ–â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–„â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–â–ƒâ–‚â–‚â–ƒâ–„â–â–â–‚
wandb:         train_loss â–ˆâ–â–â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–â–â–
wandb: valid_error_energy â–â–†â–ˆâ–ƒâ–â–‡â–ˆâ–„â–â–†â–‡â–ƒâ–â–†â–ˆâ–„â–‚â–†â–‡â–„
wandb:  valid_error_force â–„â–„â–„â–‡â–ƒâ–„â–‚â–ƒâ–ƒâ–„â–„â–â–‡â–‚â–ˆâ–ƒâ–ƒâ–„â–„â–‚
wandb:         valid_loss â–â–…â–ˆâ–ƒâ–â–‡â–ˆâ–ƒâ–â–†â–‡â–‚â–â–…â–ˆâ–ƒâ–â–†â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 4293
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.39212
wandb:   test_error_force 12.64184
wandb:          test_loss 9.61685
wandb: train_error_energy 4.10145
wandb:  train_error_force 3.57709
wandb:         train_loss 1.46381
wandb: valid_error_energy 3.70668
wandb:  valid_error_force 3.61338
wandb:         valid_loss 1.33331
wandb: 
wandb: ğŸš€ View run al_71_38 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/l0qghabp
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_160213-l0qghabp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1631946563720703, Uncertainty Bias: -0.27999696135520935
4.1007996e-05 0.15162128
2.3012393 4.5388813
(48745, 22, 3)
Found uncertainty sample 0 after 52 steps.
Found uncertainty sample 1 after 7 steps.
Found uncertainty sample 2 after 1520 steps.
Found uncertainty sample 3 after 14 steps.
Found uncertainty sample 4 after 1195 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 8 steps.
Found uncertainty sample 7 after 15 steps.
Found uncertainty sample 8 after 1007 steps.
Found uncertainty sample 9 after 392 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 461 steps.
Found uncertainty sample 12 after 797 steps.
Found uncertainty sample 13 after 85 steps.
Found uncertainty sample 14 after 1977 steps.
Found uncertainty sample 15 after 560 steps.
Found uncertainty sample 16 after 74 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 1516 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 788 steps.
Found uncertainty sample 21 after 760 steps.
Found uncertainty sample 22 after 1499 steps.
Found uncertainty sample 23 after 253 steps.
Found uncertainty sample 24 after 3882 steps.
Found uncertainty sample 25 after 20 steps.
Found uncertainty sample 26 after 415 steps.
Found uncertainty sample 27 after 227 steps.
Found uncertainty sample 28 after 2760 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 776 steps.
Found uncertainty sample 31 after 2805 steps.
Found uncertainty sample 32 after 222 steps.
Found uncertainty sample 33 after 234 steps.
Found uncertainty sample 34 after 479 steps.
Found uncertainty sample 35 after 119 steps.
Found uncertainty sample 36 after 785 steps.
Found uncertainty sample 37 after 479 steps.
Found uncertainty sample 38 after 733 steps.
Found uncertainty sample 39 after 549 steps.
Found uncertainty sample 40 after 197 steps.
Found uncertainty sample 41 after 652 steps.
Found uncertainty sample 42 after 9 steps.
Found uncertainty sample 43 after 30 steps.
Found uncertainty sample 44 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 12 steps.
Found uncertainty sample 47 after 854 steps.
Found uncertainty sample 48 after 25 steps.
Found uncertainty sample 49 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 629 steps.
Found uncertainty sample 53 after 148 steps.
Found uncertainty sample 54 after 2730 steps.
Found uncertainty sample 55 after 809 steps.
Found uncertainty sample 56 after 1262 steps.
Found uncertainty sample 57 after 1026 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 14 steps.
Found uncertainty sample 60 after 16 steps.
Found uncertainty sample 61 after 402 steps.
Found uncertainty sample 62 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 399 steps.
Found uncertainty sample 65 after 42 steps.
Found uncertainty sample 66 after 15 steps.
Found uncertainty sample 67 after 194 steps.
Found uncertainty sample 68 after 3 steps.
Found uncertainty sample 69 after 232 steps.
Found uncertainty sample 70 after 63 steps.
Found uncertainty sample 71 after 1035 steps.
Found uncertainty sample 72 after 283 steps.
Found uncertainty sample 73 after 530 steps.
Found uncertainty sample 74 after 60 steps.
Found uncertainty sample 75 after 689 steps.
Found uncertainty sample 76 after 53 steps.
Found uncertainty sample 77 after 597 steps.
Found uncertainty sample 78 after 1418 steps.
Found uncertainty sample 79 after 128 steps.
Found uncertainty sample 80 after 1003 steps.
Found uncertainty sample 81 after 54 steps.
Found uncertainty sample 82 after 1180 steps.
Found uncertainty sample 83 after 514 steps.
Found uncertainty sample 84 after 1139 steps.
Found uncertainty sample 85 after 23 steps.
Found uncertainty sample 86 after 777 steps.
Found uncertainty sample 87 after 117 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 89 steps.
Found uncertainty sample 90 after 154 steps.
Found uncertainty sample 91 after 151 steps.
Found uncertainty sample 92 after 2081 steps.
Found uncertainty sample 93 after 1148 steps.
Found uncertainty sample 94 after 144 steps.
Found uncertainty sample 95 after 1251 steps.
Found uncertainty sample 96 after 30 steps.
Found uncertainty sample 97 after 42 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 30 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_161621-wkdyey09
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_39
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/wkdyey09
Training model 39. Added 95 samples to the dataset.
Epoch 0, Batch 100/137, Loss: 0.9199867844581604, Variance: 0.17168587446212769

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.621891747520281, Training Loss Force: 3.6674190183412865, time: 2.214343547821045
Validation Loss Energy: 5.8583545690177905, Validation Loss Force: 3.6123732074051396, time: 0.1243278980255127
Test Loss Energy: 11.560995393403358, Test Loss Force: 12.272260003728306, time: 9.646344900131226

Epoch 1, Batch 100/137, Loss: 1.2907978296279907, Variance: 0.1672210842370987

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.101526745585676, Training Loss Force: 3.5633105922684423, time: 2.244840145111084
Validation Loss Energy: 5.342714580625504, Validation Loss Force: 3.630753951617089, time: 0.1232919692993164
Test Loss Energy: 11.358523108443872, Test Loss Force: 12.326520375613331, time: 9.565536260604858

Epoch 2, Batch 100/137, Loss: 1.8546409606933594, Variance: 0.1652638465166092

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.173886931692636, Training Loss Force: 3.5814482066946467, time: 2.287626028060913
Validation Loss Energy: 2.6785069739193474, Validation Loss Force: 3.724726565934701, time: 0.1252748966217041
Test Loss Energy: 10.473380769333364, Test Loss Force: 12.453836857484168, time: 10.025890827178955

Epoch 3, Batch 100/137, Loss: 1.6757972240447998, Variance: 0.16343429684638977

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.112221719386728, Training Loss Force: 3.59465762306642, time: 2.3762295246124268
Validation Loss Energy: 3.404956378412064, Validation Loss Force: 3.640490160022781, time: 0.14520740509033203
Test Loss Energy: 11.437184395394281, Test Loss Force: 12.667789431999678, time: 12.037613868713379

Epoch 4, Batch 100/137, Loss: 1.153384804725647, Variance: 0.16828425228595734

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.10979098334899, Training Loss Force: 3.578895782781321, time: 2.3940701484680176
Validation Loss Energy: 5.4090535085951865, Validation Loss Force: 3.711659449775075, time: 0.1459953784942627
Test Loss Energy: 12.462115066793961, Test Loss Force: 12.82874408708108, time: 10.979210376739502

Epoch 5, Batch 100/137, Loss: 1.290623664855957, Variance: 0.1715797483921051

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.158043787913473, Training Loss Force: 3.5616276101038533, time: 2.2091867923736572
Validation Loss Energy: 4.868122758608423, Validation Loss Force: 3.6831246213935747, time: 0.14104032516479492
Test Loss Energy: 12.016195027895606, Test Loss Force: 12.766442980781688, time: 10.110320806503296

Epoch 6, Batch 100/137, Loss: 1.8521519899368286, Variance: 0.16735029220581055

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.106871614498415, Training Loss Force: 3.587696260671002, time: 2.210031747817993
Validation Loss Energy: 1.7589674959516959, Validation Loss Force: 3.6312960767643747, time: 0.12808752059936523
Test Loss Energy: 10.63243898277569, Test Loss Force: 12.410782213605977, time: 10.369778633117676

Epoch 7, Batch 100/137, Loss: 1.5520236492156982, Variance: 0.16943414509296417

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.068487326883346, Training Loss Force: 3.5884211218804976, time: 2.223602056503296
Validation Loss Energy: 3.8466711010329084, Validation Loss Force: 3.689531480729355, time: 0.12746787071228027
Test Loss Energy: 10.75749636841458, Test Loss Force: 12.30338568088724, time: 10.283438920974731

Epoch 8, Batch 100/137, Loss: 1.0540969371795654, Variance: 0.16769492626190186

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.136012835164505, Training Loss Force: 3.561911043546785, time: 2.3006441593170166
Validation Loss Energy: 5.8865968843645895, Validation Loss Force: 3.6636892418834623, time: 0.12746930122375488
Test Loss Energy: 11.532101360052037, Test Loss Force: 12.18150139819463, time: 10.16956377029419

Epoch 9, Batch 100/137, Loss: 1.1822359561920166, Variance: 0.16376152634620667

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.122675701784318, Training Loss Force: 3.573119332304106, time: 2.2877936363220215
Validation Loss Energy: 5.170158313557543, Validation Loss Force: 3.588609436590385, time: 0.13141465187072754
Test Loss Energy: 11.311399774656406, Test Loss Force: 12.06866333984129, time: 10.351661682128906

Epoch 10, Batch 100/137, Loss: 1.7034573554992676, Variance: 0.16386115550994873

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.173420612116074, Training Loss Force: 3.5820100773399663, time: 2.179729461669922
Validation Loss Energy: 2.2156003833422644, Validation Loss Force: 3.7105441742461713, time: 0.1304025650024414
Test Loss Energy: 10.612279539550363, Test Loss Force: 12.74008383164052, time: 10.872277975082397

Epoch 11, Batch 100/137, Loss: 1.623350977897644, Variance: 0.1656438261270523

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.1023270802583855, Training Loss Force: 3.585795215523693, time: 2.224945545196533
Validation Loss Energy: 3.2922285646021, Validation Loss Force: 3.615349032094367, time: 0.12801027297973633
Test Loss Energy: 11.672783358054184, Test Loss Force: 12.90382817438344, time: 10.353829860687256

Epoch 12, Batch 100/137, Loss: 0.91553795337677, Variance: 0.16623464226722717

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.059971380341766, Training Loss Force: 3.6181382730418843, time: 2.1944844722747803
Validation Loss Energy: 5.648441247783622, Validation Loss Force: 3.786297182590437, time: 0.1306760311126709
Test Loss Energy: 12.4473154168028, Test Loss Force: 12.806132088939242, time: 10.15138554573059

Epoch 13, Batch 100/137, Loss: 1.2437896728515625, Variance: 0.16820138692855835

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.127780632502404, Training Loss Force: 3.5882003836173824, time: 2.1700875759124756
Validation Loss Energy: 5.19498711089727, Validation Loss Force: 3.710359878416392, time: 0.1293952465057373
Test Loss Energy: 12.044963186728475, Test Loss Force: 12.665191468301838, time: 10.154197454452515

Epoch 14, Batch 100/137, Loss: 1.7720651626586914, Variance: 0.17207303643226624

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.123446446653115, Training Loss Force: 3.5579790753778475, time: 2.4544215202331543
Validation Loss Energy: 2.1056155565179355, Validation Loss Force: 3.6031435713064326, time: 0.12792706489562988
Test Loss Energy: 10.8401768357806, Test Loss Force: 12.843994498698478, time: 10.145774841308594

Epoch 15, Batch 100/137, Loss: 1.7969274520874023, Variance: 0.171959787607193

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.141020834102594, Training Loss Force: 3.5463407442362063, time: 2.172642230987549
Validation Loss Energy: 3.7536257552619876, Validation Loss Force: 3.5895119146388303, time: 0.1330103874206543
Test Loss Energy: 10.858379257369476, Test Loss Force: 12.56165308540296, time: 10.184819459915161

Epoch 16, Batch 100/137, Loss: 1.0229510068893433, Variance: 0.17043980956077576

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.127286988008459, Training Loss Force: 3.5605089690743785, time: 2.2444539070129395
Validation Loss Energy: 5.876844866449537, Validation Loss Force: 3.6920186208663197, time: 0.1317899227142334
Test Loss Energy: 11.594415038414622, Test Loss Force: 12.364916114117086, time: 10.354034900665283

Epoch 17, Batch 100/137, Loss: 1.1744472980499268, Variance: 0.1660965085029602

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.14922060752284, Training Loss Force: 3.5551574282138727, time: 2.284057378768921
Validation Loss Energy: 5.16907421170907, Validation Loss Force: 3.739813810809233, time: 0.12873077392578125
Test Loss Energy: 11.442068865779355, Test Loss Force: 12.469294433013774, time: 10.214806318283081

Epoch 18, Batch 100/137, Loss: 1.9217135906219482, Variance: 0.16568325459957123

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.108223994743546, Training Loss Force: 3.5581003666748106, time: 2.2269446849823
Validation Loss Energy: 2.704512948810546, Validation Loss Force: 3.7068782236506643, time: 0.12832951545715332
Test Loss Energy: 10.487685814026873, Test Loss Force: 12.431220848966085, time: 10.320961475372314

Epoch 19, Batch 100/137, Loss: 1.666076898574829, Variance: 0.1652720868587494

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.083882995071917, Training Loss Force: 3.598979857108669, time: 2.22168231010437
Validation Loss Energy: 3.766568622038186, Validation Loss Force: 3.6453488007869255, time: 0.12965631484985352
Test Loss Energy: 11.639084399664208, Test Loss Force: 12.927365044561292, time: 10.127764225006104

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–„â–â–„â–ˆâ–†â–‚â–‚â–…â–„â–â–…â–ˆâ–‡â–‚â–‚â–…â–„â–â–…
wandb:   test_error_force â–ƒâ–ƒâ–„â–†â–‡â–‡â–„â–ƒâ–‚â–â–†â–ˆâ–‡â–†â–‡â–…â–ƒâ–„â–„â–ˆ
wandb:          test_loss â–ƒâ–ƒâ–â–…â–‡â–†â–ƒâ–‚â–ƒâ–‚â–‚â–†â–ˆâ–‡â–ƒâ–‚â–ƒâ–ƒâ–â–…
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:  train_error_force â–ˆâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–…â–ƒâ–‚â–â–‚â–‚â–‚â–„
wandb:         train_loss â–ˆâ–â–‚â–â–â–‚â–‚â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‡â–ƒâ–„â–‡â–†â–â–…â–ˆâ–‡â–‚â–„â–ˆâ–‡â–‚â–„â–ˆâ–‡â–ƒâ–„
wandb:  valid_error_force â–‚â–‚â–†â–ƒâ–…â–„â–ƒâ–…â–„â–â–…â–‚â–ˆâ–…â–‚â–â–…â–†â–…â–ƒ
wandb:         valid_loss â–ˆâ–‡â–‚â–ƒâ–‡â–†â–â–„â–ˆâ–†â–‚â–ƒâ–ˆâ–‡â–â–ƒâ–ˆâ–†â–‚â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 4378
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.63908
wandb:   test_error_force 12.92737
wandb:          test_loss 9.86381
wandb: train_error_energy 4.08388
wandb:  train_error_force 3.59898
wandb:         train_loss 1.45961
wandb: valid_error_energy 3.76657
wandb:  valid_error_force 3.64535
wandb:         valid_loss 1.35214
wandb: 
wandb: ğŸš€ View run al_71_39 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/wkdyey09
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_161621-wkdyey09/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1692495346069336, Uncertainty Bias: -0.2750060558319092
0.00012588501 0.14856684
2.4290054 4.5595727
(48745, 22, 3)
Found uncertainty sample 0 after 453 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 939 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 201 steps.
Found uncertainty sample 5 after 529 steps.
Found uncertainty sample 6 after 6 steps.
Found uncertainty sample 7 after 682 steps.
Found uncertainty sample 8 after 432 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 435 steps.
Found uncertainty sample 11 after 91 steps.
Found uncertainty sample 12 after 500 steps.
Found uncertainty sample 13 after 4 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 320 steps.
Found uncertainty sample 16 after 609 steps.
Found uncertainty sample 17 after 49 steps.
Found uncertainty sample 18 after 217 steps.
Found uncertainty sample 19 after 243 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1670 steps.
Found uncertainty sample 22 after 884 steps.
Found uncertainty sample 23 after 147 steps.
Found uncertainty sample 24 after 3294 steps.
Found uncertainty sample 25 after 86 steps.
Found uncertainty sample 26 after 339 steps.
Found uncertainty sample 27 after 667 steps.
Found uncertainty sample 28 after 54 steps.
Found uncertainty sample 29 after 8 steps.
Found uncertainty sample 30 after 2575 steps.
Found uncertainty sample 31 after 774 steps.
Found uncertainty sample 32 after 206 steps.
Found uncertainty sample 33 after 20 steps.
Found uncertainty sample 34 after 79 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 12 steps.
Found uncertainty sample 38 after 473 steps.
Found uncertainty sample 39 after 21 steps.
Found uncertainty sample 40 after 357 steps.
Found uncertainty sample 41 after 140 steps.
Found uncertainty sample 42 after 18 steps.
Found uncertainty sample 43 after 1900 steps.
Found uncertainty sample 44 after 119 steps.
Found uncertainty sample 45 after 418 steps.
Found uncertainty sample 46 after 365 steps.
Found uncertainty sample 47 after 300 steps.
Found uncertainty sample 48 after 1197 steps.
Found uncertainty sample 49 after 164 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 11 steps.
Found uncertainty sample 52 after 352 steps.
Found uncertainty sample 53 after 24 steps.
Found uncertainty sample 54 after 166 steps.
Found uncertainty sample 55 after 1831 steps.
Found uncertainty sample 56 after 210 steps.
Found uncertainty sample 57 after 14 steps.
Found uncertainty sample 58 after 48 steps.
Found uncertainty sample 59 after 10 steps.
Found uncertainty sample 60 after 618 steps.
Found uncertainty sample 61 after 57 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 18 steps.
Found uncertainty sample 64 after 18 steps.
Found uncertainty sample 65 after 1784 steps.
Found uncertainty sample 66 after 2115 steps.
Found uncertainty sample 67 after 1991 steps.
Found uncertainty sample 68 after 424 steps.
Found uncertainty sample 69 after 628 steps.
Found uncertainty sample 70 after 1686 steps.
Found uncertainty sample 71 after 809 steps.
Found uncertainty sample 72 after 2113 steps.
Found uncertainty sample 73 after 649 steps.
Found uncertainty sample 74 after 1211 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 189 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 162 steps.
Found uncertainty sample 79 after 486 steps.
Found uncertainty sample 80 after 1146 steps.
Found uncertainty sample 81 after 822 steps.
Found uncertainty sample 82 after 1329 steps.
Found uncertainty sample 83 after 129 steps.
Found uncertainty sample 84 after 74 steps.
Found uncertainty sample 85 after 46 steps.
Found uncertainty sample 86 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 360 steps.
Found uncertainty sample 89 after 242 steps.
Found uncertainty sample 90 after 593 steps.
Found uncertainty sample 91 after 16 steps.
Found uncertainty sample 92 after 8 steps.
Found uncertainty sample 93 after 5 steps.
Found uncertainty sample 94 after 21 steps.
Found uncertainty sample 95 after 2717 steps.
Found uncertainty sample 96 after 304 steps.
Found uncertainty sample 97 after 57 steps.
Found uncertainty sample 98 after 1900 steps.
Found uncertainty sample 99 after 580 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_162825-81hnwapw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_40
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/81hnwapw
Training model 40. Added 100 samples to the dataset.
Epoch 0, Batch 100/140, Loss: 1.039814829826355, Variance: 0.17204315960407257

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.589766498041407, Training Loss Force: 3.6664564945043576, time: 2.315392017364502
Validation Loss Energy: 3.0743714473639088, Validation Loss Force: 3.6709125547543113, time: 0.1385939121246338
Test Loss Energy: 11.173144674489286, Test Loss Force: 12.597739649267266, time: 10.20295524597168

Epoch 1, Batch 100/140, Loss: 1.15329909324646, Variance: 0.16646122932434082

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.138192162713868, Training Loss Force: 3.5636327275593227, time: 2.265599012374878
Validation Loss Energy: 4.027515113712301, Validation Loss Force: 3.598407769687791, time: 0.1394960880279541
Test Loss Energy: 11.010593824378851, Test Loss Force: 12.071417097329629, time: 10.185098886489868

Epoch 2, Batch 100/140, Loss: 0.9678048491477966, Variance: 0.16740849614143372

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.11927761879495, Training Loss Force: 3.5800996039537916, time: 2.484992742538452
Validation Loss Energy: 3.1715343909813263, Validation Loss Force: 3.6383130495599967, time: 0.13541841506958008
Test Loss Energy: 11.444556809934578, Test Loss Force: 13.061962364497289, time: 10.249515056610107

Epoch 3, Batch 100/140, Loss: 0.8029087781906128, Variance: 0.16695991158485413

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.1106491424706775, Training Loss Force: 3.5735898651106175, time: 2.2814691066741943
Validation Loss Energy: 3.5904018489920553, Validation Loss Force: 3.6492227243647286, time: 0.13175535202026367
Test Loss Energy: 10.801985006255778, Test Loss Force: 12.330953641365488, time: 10.303853273391724

Epoch 4, Batch 100/140, Loss: 1.0121933221817017, Variance: 0.1683679074048996

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.126114975524009, Training Loss Force: 3.566387345296681, time: 2.2982678413391113
Validation Loss Energy: 3.590876505388124, Validation Loss Force: 3.6588309722345613, time: 0.13111352920532227
Test Loss Energy: 11.74524488779325, Test Loss Force: 13.070916117021612, time: 10.49627137184143

Epoch 5, Batch 100/140, Loss: 0.9278883337974548, Variance: 0.1695065051317215

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.162755285278037, Training Loss Force: 3.563450165898059, time: 2.3332159519195557
Validation Loss Energy: 3.3575455896380704, Validation Loss Force: 3.885719253139389, time: 0.13592743873596191
Test Loss Energy: 10.644738428984724, Test Loss Force: 12.545095449680824, time: 10.28568983078003

Epoch 6, Batch 100/140, Loss: 1.0232372283935547, Variance: 0.17043590545654297

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.143761289826864, Training Loss Force: 3.5704439143017614, time: 2.2478885650634766
Validation Loss Energy: 3.1285411353933945, Validation Loss Force: 3.6864466966047202, time: 0.13043689727783203
Test Loss Energy: 11.085032707429267, Test Loss Force: 12.588248678866119, time: 10.45151686668396

Epoch 7, Batch 100/140, Loss: 0.8959182500839233, Variance: 0.16706326603889465

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.069740610326392, Training Loss Force: 3.583268088880216, time: 2.399136781692505
Validation Loss Energy: 3.7615953066638887, Validation Loss Force: 3.726137694535494, time: 0.14895892143249512
Test Loss Energy: 11.055905978991413, Test Loss Force: 12.66634779500568, time: 10.299043655395508

Epoch 8, Batch 100/140, Loss: 1.0174540281295776, Variance: 0.17059406638145447

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.122660186585985, Training Loss Force: 3.57984423743428, time: 2.288113832473755
Validation Loss Energy: 3.5686447725204022, Validation Loss Force: 3.681168101831356, time: 0.13206863403320312
Test Loss Energy: 11.60931883694411, Test Loss Force: 12.860414783697596, time: 10.167060852050781

Epoch 9, Batch 100/140, Loss: 0.9958938360214233, Variance: 0.16666656732559204

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.1303749941467345, Training Loss Force: 3.585645977461062, time: 2.249260663986206
Validation Loss Energy: 3.5157045892916816, Validation Loss Force: 3.6221939558632354, time: 0.1336517333984375
Test Loss Energy: 10.960055352112521, Test Loss Force: 12.64481760051802, time: 10.369579792022705

Epoch 10, Batch 100/140, Loss: 0.9792112708091736, Variance: 0.16875720024108887

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.09872664989847, Training Loss Force: 3.578162229696806, time: 2.332448959350586
Validation Loss Energy: 3.4525139803762728, Validation Loss Force: 3.750282600705028, time: 0.1383528709411621
Test Loss Energy: 11.733059294300402, Test Loss Force: 13.142511904458082, time: 10.281719446182251

Epoch 11, Batch 100/140, Loss: 0.8495599031448364, Variance: 0.16497988998889923

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.077466785147041, Training Loss Force: 3.563227771907576, time: 2.284242868423462
Validation Loss Energy: 3.6111635943814107, Validation Loss Force: 3.6406731940607338, time: 0.15479707717895508
Test Loss Energy: 11.079263953746324, Test Loss Force: 12.619133705471235, time: 10.529640436172485

Epoch 12, Batch 100/140, Loss: 1.0632041692733765, Variance: 0.16940923035144806

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.1277582992688595, Training Loss Force: 3.556824709080733, time: 2.2324676513671875
Validation Loss Energy: 3.217631060679102, Validation Loss Force: 3.6245164692783107, time: 0.1407794952392578
Test Loss Energy: 11.371543009418108, Test Loss Force: 12.757569109356206, time: 10.209620714187622

Epoch 13, Batch 100/140, Loss: 0.9655469655990601, Variance: 0.16705812513828278

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.148004420585364, Training Loss Force: 3.562262033554772, time: 2.314829111099243
Validation Loss Energy: 3.8305500033138338, Validation Loss Force: 3.7530285566126094, time: 0.16888856887817383
Test Loss Energy: 10.819535783674578, Test Loss Force: 12.162651972347723, time: 10.94449496269226

Epoch 14, Batch 100/140, Loss: 1.0565532445907593, Variance: 0.1696358621120453

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.102022468971242, Training Loss Force: 3.5612932160057795, time: 2.485297918319702
Validation Loss Energy: 3.2273722420290536, Validation Loss Force: 3.6717653775832146, time: 0.13258576393127441
Test Loss Energy: 11.147463763908547, Test Loss Force: 12.410674945367989, time: 10.349778890609741

Epoch 15, Batch 100/140, Loss: 0.8249441385269165, Variance: 0.16798537969589233

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.11904867992207, Training Loss Force: 3.554008518553045, time: 2.2068862915039062
Validation Loss Energy: 3.610841763242496, Validation Loss Force: 3.7033726690632753, time: 0.1321885585784912
Test Loss Energy: 10.682186329053648, Test Loss Force: 12.31147052477703, time: 10.27781867980957

Epoch 16, Batch 100/140, Loss: 1.1076059341430664, Variance: 0.17064407467842102

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.112622418946457, Training Loss Force: 3.5750627054048745, time: 2.2859206199645996
Validation Loss Energy: 3.7915454201369063, Validation Loss Force: 3.6746158395432844, time: 0.13716602325439453
Test Loss Energy: 11.735638502494476, Test Loss Force: 12.750046053892557, time: 10.499890089035034

Epoch 17, Batch 100/140, Loss: 1.1922028064727783, Variance: 0.17013917863368988

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.113679606722246, Training Loss Force: 3.562946510556885, time: 2.235027551651001
Validation Loss Energy: 3.74633950124589, Validation Loss Force: 3.670257610573089, time: 0.13382458686828613
Test Loss Energy: 10.913650654431262, Test Loss Force: 12.498687310537166, time: 10.253177881240845

Epoch 18, Batch 100/140, Loss: 1.0089563131332397, Variance: 0.1673107147216797

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.105268319497171, Training Loss Force: 3.573461954410233, time: 2.2336156368255615
Validation Loss Energy: 3.5585722594813096, Validation Loss Force: 3.722016905097906, time: 0.13686251640319824
Test Loss Energy: 11.642949371786903, Test Loss Force: 13.207223531335112, time: 10.548545598983765

Epoch 19, Batch 100/140, Loss: 0.8641128540039062, Variance: 0.16650602221488953

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.064391854890887, Training Loss Force: 3.555516904795462, time: 2.2742202281951904
Validation Loss Energy: 3.7161323533554236, Validation Loss Force: 3.597547227661829, time: 0.13162016868591309
Test Loss Energy: 11.11093418870325, Test Loss Force: 12.439140945667258, time: 10.27653455734253

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.039 MB of 0.056 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–ƒâ–†â–‚â–ˆâ–â–„â–„â–‡â–ƒâ–ˆâ–„â–†â–‚â–„â–â–ˆâ–ƒâ–‡â–„
wandb:   test_error_force â–„â–â–‡â–ƒâ–‡â–„â–„â–…â–†â–…â–ˆâ–„â–…â–‚â–ƒâ–‚â–…â–„â–ˆâ–ƒ
wandb:          test_loss â–„â–â–†â–‚â–ˆâ–â–„â–ƒâ–‡â–‚â–ˆâ–ƒâ–†â–â–ƒâ–â–‡â–‚â–‡â–‚
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:  train_error_force â–ˆâ–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–â–â–‚â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–
wandb: valid_error_energy â–â–ˆâ–‚â–…â–…â–ƒâ–â–†â–…â–„â–„â–…â–‚â–‡â–‚â–…â–†â–†â–…â–†
wandb:  valid_error_force â–ƒâ–â–‚â–‚â–‚â–ˆâ–ƒâ–„â–ƒâ–‚â–…â–‚â–‚â–…â–ƒâ–„â–ƒâ–ƒâ–„â–
wandb:         valid_loss â–â–ˆâ–â–„â–…â–…â–â–‡â–…â–ƒâ–…â–„â–â–ˆâ–‚â–…â–‡â–†â–…â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 4468
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.11093
wandb:   test_error_force 12.43914
wandb:          test_loss 9.08825
wandb: train_error_energy 4.06439
wandb:  train_error_force 3.55552
wandb:         train_loss 1.44338
wandb: valid_error_energy 3.71613
wandb:  valid_error_force 3.59755
wandb:         valid_loss 1.31938
wandb: 
wandb: ğŸš€ View run al_71_40 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/81hnwapw
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_162825-81hnwapw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.294978618621826, Uncertainty Bias: -0.31818485260009766
0.0001950264 0.03972435
2.3710585 4.492821
(48745, 22, 3)
Found uncertainty sample 0 after 288 steps.
Found uncertainty sample 1 after 2244 steps.
Found uncertainty sample 2 after 951 steps.
Found uncertainty sample 3 after 1096 steps.
Found uncertainty sample 4 after 57 steps.
Found uncertainty sample 5 after 632 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 31 steps.
Found uncertainty sample 8 after 2972 steps.
Found uncertainty sample 9 after 552 steps.
Found uncertainty sample 10 after 1815 steps.
Found uncertainty sample 11 after 3067 steps.
Found uncertainty sample 12 after 351 steps.
Found uncertainty sample 13 after 326 steps.
Found uncertainty sample 14 after 821 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 1927 steps.
Found uncertainty sample 17 after 227 steps.
Found uncertainty sample 18 after 1661 steps.
Found uncertainty sample 19 after 967 steps.
Found uncertainty sample 20 after 2726 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 1930 steps.
Found uncertainty sample 23 after 1932 steps.
Found uncertainty sample 24 after 279 steps.
Did not find any uncertainty samples for sample 25.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 340 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 0 steps.
Found uncertainty sample 30 after 1423 steps.
Found uncertainty sample 31 after 1061 steps.
Found uncertainty sample 32 after 440 steps.
Found uncertainty sample 33 after 318 steps.
Found uncertainty sample 34 after 1910 steps.
Found uncertainty sample 35 after 1566 steps.
Found uncertainty sample 36 after 539 steps.
Found uncertainty sample 37 after 393 steps.
Found uncertainty sample 38 after 488 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 20 steps.
Found uncertainty sample 41 after 1989 steps.
Found uncertainty sample 42 after 513 steps.
Found uncertainty sample 43 after 16 steps.
Found uncertainty sample 44 after 40 steps.
Found uncertainty sample 45 after 12 steps.
Found uncertainty sample 46 after 125 steps.
Found uncertainty sample 47 after 3337 steps.
Found uncertainty sample 48 after 234 steps.
Found uncertainty sample 49 after 1252 steps.
Found uncertainty sample 50 after 686 steps.
Found uncertainty sample 51 after 2911 steps.
Found uncertainty sample 52 after 117 steps.
Found uncertainty sample 53 after 531 steps.
Found uncertainty sample 54 after 1282 steps.
Found uncertainty sample 55 after 4 steps.
Found uncertainty sample 56 after 824 steps.
Found uncertainty sample 57 after 840 steps.
Found uncertainty sample 58 after 192 steps.
Found uncertainty sample 59 after 28 steps.
Found uncertainty sample 60 after 1056 steps.
Found uncertainty sample 61 after 1386 steps.
Found uncertainty sample 62 after 562 steps.
Found uncertainty sample 63 after 581 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 1320 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 3060 steps.
Found uncertainty sample 68 after 117 steps.
Found uncertainty sample 69 after 3795 steps.
Found uncertainty sample 70 after 554 steps.
Found uncertainty sample 71 after 244 steps.
Found uncertainty sample 72 after 382 steps.
Found uncertainty sample 73 after 150 steps.
Found uncertainty sample 74 after 138 steps.
Found uncertainty sample 75 after 1148 steps.
Found uncertainty sample 76 after 1409 steps.
Found uncertainty sample 77 after 169 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1669 steps.
Found uncertainty sample 80 after 1034 steps.
Found uncertainty sample 81 after 2323 steps.
Found uncertainty sample 82 after 786 steps.
Found uncertainty sample 83 after 13 steps.
Found uncertainty sample 84 after 2747 steps.
Found uncertainty sample 85 after 1830 steps.
Found uncertainty sample 86 after 1318 steps.
Found uncertainty sample 87 after 365 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 591 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 2640 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 5 steps.
Found uncertainty sample 96 after 139 steps.
Found uncertainty sample 97 after 2052 steps.
Found uncertainty sample 98 after 2034 steps.
Found uncertainty sample 99 after 605 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_164737-350wm5o7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_41
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/350wm5o7
Training model 41. Added 91 samples to the dataset.
Epoch 0, Batch 100/143, Loss: 0.7807192802429199, Variance: 0.11839732527732849

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.4884334576223397, Training Loss Force: 3.913004235971501, time: 2.391812801361084
Validation Loss Energy: 3.5258390785763907, Validation Loss Force: 3.680319138600982, time: 0.1375417709350586
Test Loss Energy: 11.163008905537042, Test Loss Force: 13.211777057078983, time: 10.338448762893677

Epoch 1, Batch 100/143, Loss: 2.723470687866211, Variance: 0.14482322335243225

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.005148264402316, Training Loss Force: 3.665840892283436, time: 2.484262704849243
Validation Loss Energy: 5.829394219898422, Validation Loss Force: 3.6495816670279644, time: 0.15697836875915527
Test Loss Energy: 11.539558346898733, Test Loss Force: 11.721046659941013, time: 11.93479061126709

Epoch 2, Batch 100/143, Loss: 1.0178580284118652, Variance: 0.15503300726413727

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.011893266689554, Training Loss Force: 3.527651596600524, time: 2.6260664463043213
Validation Loss Energy: 3.6593814103632947, Validation Loss Force: 3.6587848054983056, time: 0.15651631355285645
Test Loss Energy: 10.676052701081222, Test Loss Force: 11.897597556305383, time: 11.643154382705688

Epoch 3, Batch 100/143, Loss: 0.9385419487953186, Variance: 0.16191138327121735

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.039017875313325, Training Loss Force: 3.5418512002507043, time: 2.558898687362671
Validation Loss Energy: 1.920658473598013, Validation Loss Force: 3.6638059947389774, time: 0.15046191215515137
Test Loss Energy: 10.48919289942848, Test Loss Force: 12.14967657256008, time: 11.717931985855103

Epoch 4, Batch 100/143, Loss: 1.7324970960617065, Variance: 0.16550691425800323

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.050095279934326, Training Loss Force: 3.5234095858722756, time: 2.7027504444122314
Validation Loss Energy: 5.055651002156373, Validation Loss Force: 3.6505810061548885, time: 0.15711283683776855
Test Loss Energy: 12.050900185021172, Test Loss Force: 12.611851333204003, time: 12.621409893035889

Epoch 5, Batch 100/143, Loss: 1.9423058032989502, Variance: 0.16644293069839478

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.078403627323261, Training Loss Force: 3.540412877466781, time: 2.7318012714385986
Validation Loss Energy: 5.564411998400463, Validation Loss Force: 3.6609006819459604, time: 0.1519925594329834
Test Loss Energy: 12.350626649268953, Test Loss Force: 12.493337934954765, time: 11.838436126708984

Epoch 6, Batch 100/143, Loss: 1.2530901432037354, Variance: 0.16594082117080688

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.077357093237288, Training Loss Force: 3.5391990191960954, time: 2.6486494541168213
Validation Loss Energy: 3.442903208922786, Validation Loss Force: 3.6183898019511567, time: 0.15100932121276855
Test Loss Energy: 11.151557588617743, Test Loss Force: 12.27381598774068, time: 11.830720901489258

Epoch 7, Batch 100/143, Loss: 1.113200068473816, Variance: 0.16733315587043762

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.08075455337326, Training Loss Force: 3.553635977255947, time: 2.480591297149658
Validation Loss Energy: 2.566492146714644, Validation Loss Force: 3.5895081292633493, time: 0.1537790298461914
Test Loss Energy: 10.696862524741787, Test Loss Force: 12.04175874222555, time: 11.686593055725098

Epoch 8, Batch 100/143, Loss: 1.8188337087631226, Variance: 0.16401143372058868

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.129728717532561, Training Loss Force: 3.5570448394536047, time: 2.5265302658081055
Validation Loss Energy: 5.264813186138279, Validation Loss Force: 3.68070146152454, time: 0.14144325256347656
Test Loss Energy: 11.570477265846314, Test Loss Force: 12.422559131106972, time: 11.909179210662842

Epoch 9, Batch 100/143, Loss: 1.5634599924087524, Variance: 0.16198697686195374

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.084134697479601, Training Loss Force: 3.5421509129211226, time: 2.3790388107299805
Validation Loss Energy: 6.327451510297763, Validation Loss Force: 3.700327148931372, time: 0.15733075141906738
Test Loss Energy: 11.733367319175068, Test Loss Force: 12.065588285930872, time: 11.840519666671753

Epoch 10, Batch 100/143, Loss: 1.0779005289077759, Variance: 0.16562575101852417

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.117171526952688, Training Loss Force: 3.5605209072480353, time: 2.3394126892089844
Validation Loss Energy: 3.31505697655786, Validation Loss Force: 3.7529927811031634, time: 0.1530611515045166
Test Loss Energy: 10.641789874465326, Test Loss Force: 12.181938132830481, time: 11.99807095527649

Epoch 11, Batch 100/143, Loss: 1.0301001071929932, Variance: 0.16714514791965485

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.063528534240569, Training Loss Force: 3.683596631192966, time: 2.6097426414489746
Validation Loss Energy: 2.046119770963955, Validation Loss Force: 5.41575540454932, time: 0.15639066696166992
Test Loss Energy: 10.839032470257063, Test Loss Force: 13.666068775032908, time: 11.919636487960815

Epoch 12, Batch 100/143, Loss: 1.748116374015808, Variance: 0.1681913435459137

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.079506359675986, Training Loss Force: 3.717854824288369, time: 2.5952420234680176
Validation Loss Energy: 5.047585183460382, Validation Loss Force: 3.6271894639290765, time: 0.16130828857421875
Test Loss Energy: 11.978431264065433, Test Loss Force: 12.670734806197448, time: 12.000556707382202

Epoch 13, Batch 100/143, Loss: 2.090169668197632, Variance: 0.16758093237876892

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.062993173771555, Training Loss Force: 3.5380704463225543, time: 2.4817113876342773
Validation Loss Energy: 5.617267567353731, Validation Loss Force: 3.683002598701606, time: 0.15176725387573242
Test Loss Energy: 12.542709746803599, Test Loss Force: 12.860770560028891, time: 11.873326063156128

Epoch 14, Batch 100/143, Loss: 1.232011318206787, Variance: 0.1694762408733368

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.125425570888177, Training Loss Force: 3.5404479548643355, time: 2.5304603576660156
Validation Loss Energy: 3.35113167911267, Validation Loss Force: 3.610378017634577, time: 0.1494767665863037
Test Loss Energy: 11.322042469155173, Test Loss Force: 12.63628673284183, time: 11.895143270492554

Epoch 15, Batch 100/143, Loss: 1.0924568176269531, Variance: 0.16990214586257935

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.1246907502496, Training Loss Force: 3.5525745990792887, time: 2.4710001945495605
Validation Loss Energy: 2.6572463011970164, Validation Loss Force: 3.6358427580550736, time: 0.1639847755432129
Test Loss Energy: 10.664979296837423, Test Loss Force: 12.279884157979046, time: 11.888845920562744

Epoch 16, Batch 100/143, Loss: 1.3911776542663574, Variance: 0.16735096275806427

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.120221734555586, Training Loss Force: 3.5622222365500895, time: 2.472738742828369
Validation Loss Energy: 5.42632016484701, Validation Loss Force: 3.576828531640686, time: 0.15104126930236816
Test Loss Energy: 11.425503792208259, Test Loss Force: 12.04998596846157, time: 11.172321557998657

Epoch 17, Batch 100/143, Loss: 1.5890343189239502, Variance: 0.16656798124313354

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.052846961397499, Training Loss Force: 3.5790228867647866, time: 2.6189913749694824
Validation Loss Energy: 5.919188818807943, Validation Loss Force: 3.686913708026779, time: 0.1611630916595459
Test Loss Energy: 11.829337548530502, Test Loss Force: 12.155538222411732, time: 11.867582321166992

Epoch 18, Batch 100/143, Loss: 1.2963826656341553, Variance: 0.16532540321350098

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.101438433594168, Training Loss Force: 3.5558987442311194, time: 2.3350143432617188
Validation Loss Energy: 3.6311506738120833, Validation Loss Force: 3.6195244496957564, time: 0.12762141227722168
Test Loss Energy: 10.867821678901967, Test Loss Force: 12.386851780958832, time: 9.973525762557983

Epoch 19, Batch 100/143, Loss: 1.0898783206939697, Variance: 0.16682153940200806

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.095777419355377, Training Loss Force: 3.5888843987288137, time: 2.3326103687286377
Validation Loss Energy: 2.0300667357804856, Validation Loss Force: 3.6435337503554197, time: 0.14017343521118164
Test Loss Energy: 10.82233868226155, Test Loss Force: 12.60380267130172, time: 9.925986289978027

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–…â–‚â–â–†â–‡â–ƒâ–‚â–…â–…â–‚â–‚â–†â–ˆâ–„â–‚â–„â–†â–‚â–‚
wandb:   test_error_force â–†â–â–‚â–ƒâ–„â–„â–ƒâ–‚â–„â–‚â–ƒâ–ˆâ–„â–…â–„â–ƒâ–‚â–ƒâ–ƒâ–„
wandb:          test_loss â–ˆâ–„â–‚â–‚â–…â–…â–ƒâ–â–ƒâ–ƒâ–â–„â–„â–…â–ƒâ–â–â–‚â–â–‚
wandb: train_error_energy â–â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb:  train_error_force â–ˆâ–„â–â–â–â–â–â–‚â–‚â–â–‚â–„â–„â–â–â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–„â–â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–â–â–â–â–â–‚â–‚
wandb: valid_error_energy â–„â–‡â–„â–â–†â–‡â–ƒâ–‚â–†â–ˆâ–ƒâ–â–†â–‡â–ƒâ–‚â–‡â–‡â–„â–
wandb:  valid_error_force â–â–â–â–â–â–â–â–â–â–â–‚â–ˆâ–â–â–â–â–â–â–â–
wandb:         valid_loss â–„â–ˆâ–ƒâ–â–…â–‡â–ƒâ–â–†â–ˆâ–ƒâ–„â–…â–‡â–‚â–‚â–†â–‡â–ƒâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 4549
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.82234
wandb:   test_error_force 12.6038
wandb:          test_loss 9.37918
wandb: train_error_energy 4.09578
wandb:  train_error_force 3.58888
wandb:         train_loss 1.46954
wandb: valid_error_energy 2.03007
wandb:  valid_error_force 3.64353
wandb:         valid_loss 1.01937
wandb: 
wandb: ğŸš€ View run al_71_41 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/350wm5o7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_164737-350wm5o7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.6893908977508545, Uncertainty Bias: -0.36773252487182617
1.335144e-05 0.0059109926
2.1491385 4.612195
(48745, 22, 3)
Found uncertainty sample 0 after 10 steps.
Found uncertainty sample 1 after 12 steps.
Found uncertainty sample 2 after 3292 steps.
Found uncertainty sample 3 after 66 steps.
Found uncertainty sample 4 after 651 steps.
Found uncertainty sample 5 after 1365 steps.
Found uncertainty sample 6 after 85 steps.
Found uncertainty sample 7 after 3338 steps.
Found uncertainty sample 8 after 18 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 2372 steps.
Found uncertainty sample 11 after 841 steps.
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 161 steps.
Found uncertainty sample 15 after 3 steps.
Found uncertainty sample 16 after 1273 steps.
Found uncertainty sample 17 after 3635 steps.
Found uncertainty sample 18 after 757 steps.
Found uncertainty sample 19 after 97 steps.
Found uncertainty sample 20 after 11 steps.
Found uncertainty sample 21 after 34 steps.
Found uncertainty sample 22 after 527 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 18 steps.
Found uncertainty sample 25 after 368 steps.
Found uncertainty sample 26 after 2170 steps.
Found uncertainty sample 27 after 23 steps.
Found uncertainty sample 28 after 13 steps.
Found uncertainty sample 29 after 79 steps.
Found uncertainty sample 30 after 335 steps.
Found uncertainty sample 31 after 6 steps.
Found uncertainty sample 32 after 204 steps.
Found uncertainty sample 33 after 129 steps.
Found uncertainty sample 34 after 157 steps.
Found uncertainty sample 35 after 10 steps.
Found uncertainty sample 36 after 32 steps.
Found uncertainty sample 37 after 56 steps.
Found uncertainty sample 38 after 2824 steps.
Found uncertainty sample 39 after 159 steps.
Found uncertainty sample 40 after 15 steps.
Found uncertainty sample 41 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 14 steps.
Found uncertainty sample 44 after 51 steps.
Found uncertainty sample 45 after 20 steps.
Found uncertainty sample 46 after 188 steps.
Found uncertainty sample 47 after 1137 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 3 steps.
Did not find any uncertainty samples for sample 50.
Found uncertainty sample 51 after 633 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 186 steps.
Found uncertainty sample 54 after 305 steps.
Found uncertainty sample 55 after 722 steps.
Found uncertainty sample 56 after 666 steps.
Found uncertainty sample 57 after 172 steps.
Found uncertainty sample 58 after 1818 steps.
Found uncertainty sample 59 after 33 steps.
Found uncertainty sample 60 after 18 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 659 steps.
Found uncertainty sample 63 after 176 steps.
Did not find any uncertainty samples for sample 64.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 301 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1108 steps.
Found uncertainty sample 70 after 15 steps.
Found uncertainty sample 71 after 2336 steps.
Found uncertainty sample 72 after 11 steps.
Found uncertainty sample 73 after 17 steps.
Found uncertainty sample 74 after 386 steps.
Found uncertainty sample 75 after 45 steps.
Found uncertainty sample 76 after 80 steps.
Found uncertainty sample 77 after 14 steps.
Found uncertainty sample 78 after 15 steps.
Found uncertainty sample 79 after 98 steps.
Found uncertainty sample 80 after 601 steps.
Found uncertainty sample 81 after 83 steps.
Found uncertainty sample 82 after 9 steps.
Found uncertainty sample 83 after 2831 steps.
Found uncertainty sample 84 after 416 steps.
Found uncertainty sample 85 after 1157 steps.
Found uncertainty sample 86 after 3623 steps.
Found uncertainty sample 87 after 858 steps.
Found uncertainty sample 88 after 91 steps.
Found uncertainty sample 89 after 31 steps.
Found uncertainty sample 90 after 565 steps.
Found uncertainty sample 91 after 40 steps.
Found uncertainty sample 92 after 539 steps.
Found uncertainty sample 93 after 213 steps.
Found uncertainty sample 94 after 355 steps.
Found uncertainty sample 95 after 125 steps.
Found uncertainty sample 96 after 1064 steps.
Found uncertainty sample 97 after 1695 steps.
Found uncertainty sample 98 after 33 steps.
Found uncertainty sample 99 after 17 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_170126-a2wfjsoo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_42
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/a2wfjsoo
Training model 42. Added 97 samples to the dataset.
Epoch 0, Batch 100/145, Loss: 1.0133578777313232, Variance: 0.1369582712650299

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.0631425661008276, Training Loss Force: 3.6596248573558947, time: 2.432971239089966
Validation Loss Energy: 2.178663431631373, Validation Loss Force: 3.5839160905555674, time: 0.14297986030578613
Test Loss Energy: 10.925957221389734, Test Loss Force: 13.480679121351304, time: 10.533717155456543

Epoch 1, Batch 100/145, Loss: 0.7952073812484741, Variance: 0.1268150806427002

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6386518580784797, Training Loss Force: 3.5248284427014633, time: 2.3849549293518066
Validation Loss Energy: 2.3797269903297877, Validation Loss Force: 3.5908119313358053, time: 0.13883256912231445
Test Loss Energy: 11.021854848945527, Test Loss Force: 13.186763799530684, time: 10.548195123672485

Epoch 2, Batch 100/145, Loss: 0.8805283308029175, Variance: 0.12448234856128693

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.610269753089399, Training Loss Force: 3.51329874489207, time: 2.5363142490386963
Validation Loss Energy: 3.1739972601813853, Validation Loss Force: 3.6012384140938742, time: 0.14400887489318848
Test Loss Energy: 11.401789537493405, Test Loss Force: 13.03760080241458, time: 10.521935224533081

Epoch 3, Batch 100/145, Loss: 1.4454073905944824, Variance: 0.12899740040302277

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.664537536347503, Training Loss Force: 3.520757984680984, time: 2.4421932697296143
Validation Loss Energy: 1.9318173151770381, Validation Loss Force: 3.6784185832174394, time: 0.14799213409423828
Test Loss Energy: 10.744689586294948, Test Loss Force: 13.111029877390786, time: 10.431490898132324

Epoch 4, Batch 100/145, Loss: 0.8659572601318359, Variance: 0.1283286213874817

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.653727416917915, Training Loss Force: 3.5146728899393156, time: 2.3369743824005127
Validation Loss Energy: 2.850103685414457, Validation Loss Force: 3.610745644394504, time: 0.14053702354431152
Test Loss Energy: 10.609343151393308, Test Loss Force: 12.565564848357287, time: 10.52713131904602

Epoch 5, Batch 100/145, Loss: 0.84991455078125, Variance: 0.12554678320884705

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.685789915795885, Training Loss Force: 3.521541227240908, time: 2.369697332382202
Validation Loss Energy: 3.6598585161199693, Validation Loss Force: 3.63375144943652, time: 0.14040017127990723
Test Loss Energy: 10.999112359689331, Test Loss Force: 12.8000481569896, time: 10.470601797103882

Epoch 6, Batch 100/145, Loss: 1.1697537899017334, Variance: 0.12490835785865784

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.682206434429725, Training Loss Force: 3.5116067373355393, time: 2.345536231994629
Validation Loss Energy: 2.4261238180362743, Validation Loss Force: 3.625979702459251, time: 0.14075207710266113
Test Loss Energy: 10.834561516294698, Test Loss Force: 13.075029606564076, time: 10.669342994689941

Epoch 7, Batch 100/145, Loss: 0.6965506076812744, Variance: 0.12511837482452393

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.665248948144425, Training Loss Force: 3.5141340474983123, time: 2.4176738262176514
Validation Loss Energy: 2.2020364180950245, Validation Loss Force: 3.653047215392135, time: 0.14054083824157715
Test Loss Energy: 10.978428890859716, Test Loss Force: 13.166363747720588, time: 10.503711462020874

Epoch 8, Batch 100/145, Loss: 0.8455008268356323, Variance: 0.12405814230442047

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6258878568202566, Training Loss Force: 3.51473601894566, time: 2.3440096378326416
Validation Loss Energy: 3.359841976540024, Validation Loss Force: 3.6334161097027526, time: 0.1382308006286621
Test Loss Energy: 11.581865116229988, Test Loss Force: 13.274836614126846, time: 10.448235988616943

Epoch 9, Batch 100/145, Loss: 1.4024478197097778, Variance: 0.12621206045150757

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6584747760889957, Training Loss Force: 3.5086074484371617, time: 2.516193151473999
Validation Loss Energy: 1.8351048683533573, Validation Loss Force: 3.6384871768321125, time: 0.14500951766967773
Test Loss Energy: 11.095095231056375, Test Loss Force: 13.536086092446169, time: 10.44871997833252

Epoch 10, Batch 100/145, Loss: 0.8472195863723755, Variance: 0.1265389621257782

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.677420321613358, Training Loss Force: 3.5130126488238376, time: 2.3569395542144775
Validation Loss Energy: 2.602899739642616, Validation Loss Force: 3.5514797276221692, time: 0.14249134063720703
Test Loss Energy: 10.588420496263533, Test Loss Force: 12.47557230862371, time: 11.189483404159546

Epoch 11, Batch 100/145, Loss: 0.7405687570571899, Variance: 0.1216190904378891

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.614332183503255, Training Loss Force: 3.5077243379974643, time: 2.3567090034484863
Validation Loss Energy: 3.8103518335328217, Validation Loss Force: 3.618891846652175, time: 0.14161968231201172
Test Loss Energy: 11.05002396015904, Test Loss Force: 12.66397130200412, time: 10.590450763702393

Epoch 12, Batch 100/145, Loss: 1.2115100622177124, Variance: 0.12012794613838196

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.665965284768595, Training Loss Force: 3.5354694034521876, time: 2.3253769874572754
Validation Loss Energy: 2.567403234636732, Validation Loss Force: 3.709253583876374, time: 0.1426091194152832
Test Loss Energy: 10.601162189044128, Test Loss Force: 12.546854204844454, time: 10.457676887512207

Epoch 13, Batch 100/145, Loss: 0.9345507621765137, Variance: 0.12323632836341858

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.632571478547933, Training Loss Force: 3.5307886446226546, time: 2.432047128677368
Validation Loss Energy: 2.2222548507683157, Validation Loss Force: 3.630362053920256, time: 0.1425483226776123
Test Loss Energy: 10.844152046617637, Test Loss Force: 12.866262760964203, time: 10.530925989151001

Epoch 14, Batch 100/145, Loss: 0.9853971600532532, Variance: 0.1255967617034912

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.687728564751465, Training Loss Force: 3.5309360884528407, time: 2.306586265563965
Validation Loss Energy: 3.42480933679384, Validation Loss Force: 3.676861012541675, time: 0.14408206939697266
Test Loss Energy: 11.231267537659612, Test Loss Force: 12.85419452693514, time: 10.47572946548462

Epoch 15, Batch 100/145, Loss: 1.2557581663131714, Variance: 0.12330027669668198

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6927604198442263, Training Loss Force: 3.531588831536759, time: 2.359361410140991
Validation Loss Energy: 1.8590517845406367, Validation Loss Force: 3.6053572737321975, time: 0.14022612571716309
Test Loss Energy: 10.84692613111764, Test Loss Force: 13.075793695155427, time: 10.40985655784607

Epoch 16, Batch 100/145, Loss: 0.9608307480812073, Variance: 0.1257709264755249

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6523059178215775, Training Loss Force: 3.536127323209836, time: 2.560850143432617
Validation Loss Energy: 2.745021439399069, Validation Loss Force: 3.693405107414963, time: 0.13788723945617676
Test Loss Energy: 10.82375907825116, Test Loss Force: 12.897464887086034, time: 10.477196455001831

Epoch 17, Batch 100/145, Loss: 0.976304292678833, Variance: 0.12132291495800018

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6728591798811916, Training Loss Force: 3.52530980732548, time: 2.3786888122558594
Validation Loss Energy: 3.4970319258294205, Validation Loss Force: 3.5898079903798656, time: 0.1364285945892334
Test Loss Energy: 10.907764434030293, Test Loss Force: 12.683614479236873, time: 10.501044034957886

Epoch 18, Batch 100/145, Loss: 1.4613004922866821, Variance: 0.12206217646598816

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6979828781902526, Training Loss Force: 3.5180016186069176, time: 2.343773603439331
Validation Loss Energy: 2.366007898910885, Validation Loss Force: 3.6312844979687857, time: 0.1364121437072754
Test Loss Energy: 10.407146230058263, Test Loss Force: 12.437208965566171, time: 10.698010683059692

Epoch 19, Batch 100/145, Loss: 0.7911970615386963, Variance: 0.12306368350982666

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6490828717697554, Training Loss Force: 3.5104833039257652, time: 2.3339340686798096
Validation Loss Energy: 2.2601680582380026, Validation Loss Force: 3.6360261632082698, time: 0.14408588409423828
Test Loss Energy: 10.890831086676362, Test Loss Force: 12.788730161764171, time: 10.520490884780884

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–…â–‡â–ƒâ–‚â–…â–„â–„â–ˆâ–…â–‚â–…â–‚â–„â–†â–„â–ƒâ–„â–â–„
wandb:   test_error_force â–ˆâ–†â–…â–…â–‚â–ƒâ–…â–†â–†â–ˆâ–â–‚â–‚â–„â–„â–…â–„â–ƒâ–â–ƒ
wandb:          test_loss â–â–„â–†â–„â–â–‚â–„â–…â–ˆâ–†â–â–„â–ƒâ–…â–†â–…â–ƒâ–ƒâ–‚â–†
wandb: train_error_energy â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–â–‚â–â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–‚â–â–‚â–‚â–â–â–â–‚â–â–‚â–â–‚â–‚â–â–‚â–‚â–
wandb: valid_error_energy â–‚â–ƒâ–†â–â–…â–‡â–ƒâ–‚â–†â–â–„â–ˆâ–„â–‚â–‡â–â–„â–‡â–ƒâ–ƒ
wandb:  valid_error_force â–‚â–ƒâ–ƒâ–‡â–„â–…â–„â–†â–…â–…â–â–„â–ˆâ–„â–‡â–ƒâ–‡â–ƒâ–…â–…
wandb:         valid_loss â–‚â–‚â–…â–â–„â–‡â–‚â–‚â–†â–â–ƒâ–ˆâ–ƒâ–‚â–†â–â–„â–†â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 4636
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.89083
wandb:   test_error_force 12.78873
wandb:          test_loss 12.64765
wandb: train_error_energy 2.64908
wandb:  train_error_force 3.51048
wandb:         train_loss 1.01379
wandb: valid_error_energy 2.26017
wandb:  valid_error_force 3.63603
wandb:         valid_loss 0.93808
wandb: 
wandb: ğŸš€ View run al_71_42 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/a2wfjsoo
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_170126-a2wfjsoo/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.648125171661377, Uncertainty Bias: -0.193607896566391
9.536743e-05 0.001552105
2.2887254 4.7525525
(48745, 22, 3)
Found uncertainty sample 0 after 580 steps.
Found uncertainty sample 1 after 537 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 184 steps.
Found uncertainty sample 4 after 5 steps.
Found uncertainty sample 5 after 48 steps.
Found uncertainty sample 6 after 310 steps.
Found uncertainty sample 7 after 1649 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 226 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 21 steps.
Found uncertainty sample 12 after 1129 steps.
Found uncertainty sample 13 after 246 steps.
Found uncertainty sample 14 after 2172 steps.
Found uncertainty sample 15 after 649 steps.
Found uncertainty sample 16 after 75 steps.
Found uncertainty sample 17 after 15 steps.
Found uncertainty sample 18 after 181 steps.
Found uncertainty sample 19 after 103 steps.
Found uncertainty sample 20 after 857 steps.
Found uncertainty sample 21 after 668 steps.
Found uncertainty sample 22 after 85 steps.
Found uncertainty sample 23 after 652 steps.
Found uncertainty sample 24 after 178 steps.
Found uncertainty sample 25 after 236 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 2507 steps.
Found uncertainty sample 28 after 98 steps.
Found uncertainty sample 29 after 120 steps.
Found uncertainty sample 30 after 39 steps.
Found uncertainty sample 31 after 440 steps.
Found uncertainty sample 32 after 1231 steps.
Found uncertainty sample 33 after 223 steps.
Found uncertainty sample 34 after 207 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1064 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 42 steps.
Found uncertainty sample 39 after 274 steps.
Found uncertainty sample 40 after 346 steps.
Found uncertainty sample 41 after 356 steps.
Found uncertainty sample 42 after 891 steps.
Found uncertainty sample 43 after 3873 steps.
Found uncertainty sample 44 after 603 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 399 steps.
Found uncertainty sample 47 after 1201 steps.
Found uncertainty sample 48 after 37 steps.
Found uncertainty sample 49 after 1552 steps.
Found uncertainty sample 50 after 1240 steps.
Found uncertainty sample 51 after 886 steps.
Found uncertainty sample 52 after 93 steps.
Found uncertainty sample 53 after 279 steps.
Found uncertainty sample 54 after 819 steps.
Found uncertainty sample 55 after 2326 steps.
Found uncertainty sample 56 after 1673 steps.
Found uncertainty sample 57 after 194 steps.
Found uncertainty sample 58 after 2469 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 63 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 67 steps.
Found uncertainty sample 63 after 21 steps.
Found uncertainty sample 64 after 20 steps.
Found uncertainty sample 65 after 2687 steps.
Found uncertainty sample 66 after 930 steps.
Found uncertainty sample 67 after 5 steps.
Found uncertainty sample 68 after 49 steps.
Found uncertainty sample 69 after 19 steps.
Found uncertainty sample 70 after 188 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 2336 steps.
Found uncertainty sample 73 after 275 steps.
Found uncertainty sample 74 after 416 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 591 steps.
Found uncertainty sample 77 after 152 steps.
Found uncertainty sample 78 after 1235 steps.
Found uncertainty sample 79 after 191 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 406 steps.
Found uncertainty sample 82 after 464 steps.
Found uncertainty sample 83 after 575 steps.
Found uncertainty sample 84 after 209 steps.
Found uncertainty sample 85 after 664 steps.
Found uncertainty sample 86 after 163 steps.
Found uncertainty sample 87 after 112 steps.
Found uncertainty sample 88 after 391 steps.
Found uncertainty sample 89 after 103 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 100 steps.
Found uncertainty sample 92 after 255 steps.
Found uncertainty sample 93 after 286 steps.
Found uncertainty sample 94 after 18 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 1674 steps.
Found uncertainty sample 97 after 2385 steps.
Found uncertainty sample 98 after 24 steps.
Found uncertainty sample 99 after 462 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_171427-wqkx94lt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_43
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/wqkx94lt
Training model 43. Added 99 samples to the dataset.
Epoch 0, Batch 100/148, Loss: 2.073284149169922, Variance: 0.1526864469051361

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.4241786210940734, Training Loss Force: 3.648455998767729, time: 2.48405122756958
Validation Loss Energy: 5.525298530011177, Validation Loss Force: 3.662067394047573, time: 0.1372387409210205
Test Loss Energy: 11.262485919794903, Test Loss Force: 11.609564915495538, time: 11.286594867706299

Epoch 1, Batch 100/148, Loss: 1.771159052848816, Variance: 0.15660278499126434

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.149798604159608, Training Loss Force: 3.5528544663673367, time: 2.3917503356933594
Validation Loss Energy: 4.793653734782626, Validation Loss Force: 3.646443739200501, time: 0.14053654670715332
Test Loss Energy: 11.515522725813376, Test Loss Force: 11.788389629903243, time: 10.586156606674194

Epoch 2, Batch 100/148, Loss: 1.9183366298675537, Variance: 0.16350269317626953

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.111858755379776, Training Loss Force: 3.57895790865908, time: 2.532599687576294
Validation Loss Energy: 5.5068630091893285, Validation Loss Force: 3.5979804675967557, time: 0.14498281478881836
Test Loss Energy: 11.312117244955568, Test Loss Force: 11.686250691109734, time: 10.5999596118927

Epoch 3, Batch 100/148, Loss: 2.132941722869873, Variance: 0.16034448146820068

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.1431527741900895, Training Loss Force: 3.5652619206685885, time: 2.3746321201324463
Validation Loss Energy: 4.75701501128118, Validation Loss Force: 3.698091031144916, time: 0.1393885612487793
Test Loss Energy: 12.029098124330206, Test Loss Force: 12.12825867793362, time: 10.55875825881958

Epoch 4, Batch 100/148, Loss: 1.9788703918457031, Variance: 0.1674489676952362

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.098260173856718, Training Loss Force: 3.5852602112901564, time: 2.4214110374450684
Validation Loss Energy: 5.1045916021859385, Validation Loss Force: 3.622089598256844, time: 0.14041781425476074
Test Loss Energy: 11.15687420502213, Test Loss Force: 11.873368835604035, time: 10.654170751571655

Epoch 5, Batch 100/148, Loss: 1.8539299964904785, Variance: 0.1615055799484253

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.105821028077439, Training Loss Force: 3.5399917695108005, time: 2.3819494247436523
Validation Loss Energy: 5.044316251952065, Validation Loss Force: 3.6082741653208026, time: 0.14644122123718262
Test Loss Energy: 11.72251803900439, Test Loss Force: 11.956817812301722, time: 10.448949575424194

Epoch 6, Batch 100/148, Loss: 1.8539457321166992, Variance: 0.1664687991142273

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.096370737386767, Training Loss Force: 3.560681968394828, time: 2.418177843093872
Validation Loss Energy: 5.616593770518154, Validation Loss Force: 3.6353473192053847, time: 0.13619256019592285
Test Loss Energy: 11.414805097002198, Test Loss Force: 11.964510525333269, time: 10.642558813095093

Epoch 7, Batch 100/148, Loss: 1.6229441165924072, Variance: 0.16244861483573914

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.118292622376627, Training Loss Force: 3.5329027361650875, time: 2.4122676849365234
Validation Loss Energy: 4.960993309099789, Validation Loss Force: 3.565173880096728, time: 0.14466166496276855
Test Loss Energy: 12.004030064524258, Test Loss Force: 12.349090483976369, time: 10.59006142616272

Epoch 8, Batch 100/148, Loss: 2.243621349334717, Variance: 0.16744326055049896

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.149348206915028, Training Loss Force: 3.5817894342131122, time: 2.4121222496032715
Validation Loss Energy: 5.202495662944052, Validation Loss Force: 3.6313745305267893, time: 0.14328670501708984
Test Loss Energy: 11.421771922607704, Test Loss Force: 11.976435975978475, time: 10.679320096969604

Epoch 9, Batch 100/148, Loss: 1.7856088876724243, Variance: 0.1622626781463623

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.096543465454115, Training Loss Force: 3.582879715631893, time: 2.350205898284912
Validation Loss Energy: 4.864621508957853, Validation Loss Force: 3.6343496872085788, time: 0.13689613342285156
Test Loss Energy: 12.064098426494185, Test Loss Force: 12.546463762074815, time: 10.434484958648682

Epoch 10, Batch 100/148, Loss: 1.9324663877487183, Variance: 0.16974735260009766

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.119093140524057, Training Loss Force: 3.5428939488875475, time: 2.362941026687622
Validation Loss Energy: 5.206548538292133, Validation Loss Force: 3.697836817203316, time: 0.1378188133239746
Test Loss Energy: 11.43289773220987, Test Loss Force: 12.267909827375984, time: 11.886797904968262

Epoch 11, Batch 100/148, Loss: 1.892378568649292, Variance: 0.16379624605178833

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.097446740146564, Training Loss Force: 3.5409790679541095, time: 2.789224863052368
Validation Loss Energy: 4.732654264486194, Validation Loss Force: 3.8182206642577143, time: 0.2180342674255371
Test Loss Energy: 12.0818671725249, Test Loss Force: 12.796207027952622, time: 12.192092657089233

Epoch 12, Batch 100/148, Loss: 1.9313912391662598, Variance: 0.17136678099632263

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.101635184394497, Training Loss Force: 3.5511301285374426, time: 2.7252938747406006
Validation Loss Energy: 5.119102653533859, Validation Loss Force: 3.767387896049103, time: 0.15568232536315918
Test Loss Energy: 11.29295249628189, Test Loss Force: 12.384988093834538, time: 12.174275398254395

Epoch 13, Batch 100/148, Loss: 1.8962737321853638, Variance: 0.16423256695270538

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.13309092795595, Training Loss Force: 3.5784621824458314, time: 2.669725179672241
Validation Loss Energy: 4.979583807647203, Validation Loss Force: 3.639080961221253, time: 0.19034862518310547
Test Loss Energy: 12.14693498143408, Test Loss Force: 13.033581402827242, time: 12.460694789886475

Epoch 14, Batch 100/148, Loss: 2.006033182144165, Variance: 0.17089296877384186

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 3.852442235192747, Training Loss Force: 3.6731822278783453, time: 2.775022029876709
Validation Loss Energy: 2.2316590594774297, Validation Loss Force: 3.9449584496142123, time: 0.1696622371673584
Test Loss Energy: 12.016622515349956, Test Loss Force: 15.326243635204284, time: 13.042339563369751

Epoch 15, Batch 100/148, Loss: 0.9596394300460815, Variance: 0.12836188077926636

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.606678667748298, Training Loss Force: 3.5553083134549297, time: 2.8729944229125977
Validation Loss Energy: 3.348458981354098, Validation Loss Force: 3.6072790569076516, time: 0.1645059585571289
Test Loss Energy: 10.877782213065402, Test Loss Force: 12.84072751097911, time: 12.31579303741455

Epoch 16, Batch 100/148, Loss: 0.944372296333313, Variance: 0.12560370564460754

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.659938101109718, Training Loss Force: 3.5065784647569127, time: 2.8376355171203613
Validation Loss Energy: 1.8168398749857655, Validation Loss Force: 3.565322919978403, time: 0.15550637245178223
Test Loss Energy: 10.896242982159064, Test Loss Force: 13.252330116859502, time: 12.204590797424316

Epoch 17, Batch 100/148, Loss: 0.8075559139251709, Variance: 0.1281018704175949

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.65981221104586, Training Loss Force: 3.4955974470587297, time: 2.932670831680298
Validation Loss Energy: 2.2133753990131044, Validation Loss Force: 3.6176108310556523, time: 0.1619734764099121
Test Loss Energy: 11.211057829497657, Test Loss Force: 13.282580380180583, time: 12.287273168563843

Epoch 18, Batch 100/148, Loss: 0.9288461208343506, Variance: 0.12683524191379547

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6273664955068257, Training Loss Force: 3.5025001454468536, time: 2.6612253189086914
Validation Loss Energy: 3.7464735480717386, Validation Loss Force: 3.6167525899441983, time: 0.16622614860534668
Test Loss Energy: 11.184339159452959, Test Loss Force: 12.925569920881028, time: 12.260209560394287

Epoch 19, Batch 100/148, Loss: 1.2231624126434326, Variance: 0.1206648200750351

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.65522663035808, Training Loss Force: 3.4981229198897688, time: 2.9270098209381104
Validation Loss Energy: 1.7198928091053256, Validation Loss Force: 3.6686160936293093, time: 0.15596675872802734
Test Loss Energy: 10.674553841614136, Test Loss Force: 13.081845119741375, time: 12.29409408569336

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–…â–„â–‡â–ƒâ–†â–…â–‡â–…â–ˆâ–…â–ˆâ–„â–ˆâ–‡â–‚â–‚â–„â–ƒâ–
wandb:   test_error_force â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–„â–ˆâ–ƒâ–„â–„â–ƒâ–„
wandb:          test_loss â–‚â–‚â–â–ƒâ–â–‚â–â–ƒâ–â–ƒâ–â–ƒâ–â–ƒâ–‡â–…â–‡â–ˆâ–‡â–†
wandb: train_error_energy â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–â–â–â–â–
wandb:  train_error_force â–‡â–ƒâ–„â–„â–…â–ƒâ–„â–‚â–„â–„â–ƒâ–ƒâ–ƒâ–„â–ˆâ–ƒâ–â–â–â–
wandb:         train_loss â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‡â–ˆâ–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–†â–‡â–‡â–‚â–„â–â–‚â–…â–
wandb:  valid_error_force â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–ƒâ–†â–…â–‚â–ˆâ–‚â–â–‚â–‚â–ƒ
wandb:         valid_loss â–ˆâ–†â–‡â–†â–‡â–‡â–ˆâ–†â–‡â–†â–‡â–†â–‡â–†â–ƒâ–„â–â–‚â–…â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 4725
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.67455
wandb:   test_error_force 13.08185
wandb:          test_loss 12.29734
wandb: train_error_energy 2.65523
wandb:  train_error_force 3.49812
wandb:         train_loss 1.01136
wandb: valid_error_energy 1.71989
wandb:  valid_error_force 3.66862
wandb:         valid_loss 0.78589
wandb: 
wandb: ğŸš€ View run al_71_43 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/wqkx94lt
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_171427-wqkx94lt/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.5876801013946533, Uncertainty Bias: -0.1916598081588745
0.00013065338 0.0020961761
2.3348765 4.7535524
(48745, 22, 3)
Found uncertainty sample 0 after 35 steps.
Found uncertainty sample 1 after 1823 steps.
Found uncertainty sample 2 after 587 steps.
Found uncertainty sample 3 after 531 steps.
Found uncertainty sample 4 after 3285 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 21 steps.
Found uncertainty sample 7 after 26 steps.
Found uncertainty sample 8 after 572 steps.
Found uncertainty sample 9 after 52 steps.
Found uncertainty sample 10 after 419 steps.
Found uncertainty sample 11 after 1896 steps.
Found uncertainty sample 12 after 5 steps.
Found uncertainty sample 13 after 128 steps.
Found uncertainty sample 14 after 210 steps.
Found uncertainty sample 15 after 1698 steps.
Found uncertainty sample 16 after 26 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 990 steps.
Found uncertainty sample 19 after 268 steps.
Found uncertainty sample 20 after 2374 steps.
Found uncertainty sample 21 after 176 steps.
Found uncertainty sample 22 after 17 steps.
Found uncertainty sample 23 after 13 steps.
Found uncertainty sample 24 after 917 steps.
Found uncertainty sample 25 after 639 steps.
Found uncertainty sample 26 after 494 steps.
Found uncertainty sample 27 after 1786 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 13 steps.
Found uncertainty sample 30 after 124 steps.
Found uncertainty sample 31 after 10 steps.
Found uncertainty sample 32 after 384 steps.
Found uncertainty sample 33 after 646 steps.
Found uncertainty sample 34 after 23 steps.
Found uncertainty sample 35 after 2 steps.
Found uncertainty sample 36 after 459 steps.
Found uncertainty sample 37 after 785 steps.
Found uncertainty sample 38 after 13 steps.
Found uncertainty sample 39 after 5 steps.
Found uncertainty sample 40 after 109 steps.
Found uncertainty sample 41 after 43 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 349 steps.
Found uncertainty sample 44 after 293 steps.
Found uncertainty sample 45 after 443 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1589 steps.
Found uncertainty sample 48 after 19 steps.
Found uncertainty sample 49 after 244 steps.
Found uncertainty sample 50 after 12 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1185 steps.
Found uncertainty sample 53 after 399 steps.
Found uncertainty sample 54 after 14 steps.
Found uncertainty sample 55 after 13 steps.
Found uncertainty sample 56 after 6 steps.
Found uncertainty sample 57 after 341 steps.
Found uncertainty sample 58 after 489 steps.
Found uncertainty sample 59 after 51 steps.
Found uncertainty sample 60 after 542 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 665 steps.
Found uncertainty sample 63 after 2611 steps.
Found uncertainty sample 64 after 603 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 127 steps.
Found uncertainty sample 67 after 155 steps.
Found uncertainty sample 68 after 20 steps.
Found uncertainty sample 69 after 268 steps.
Found uncertainty sample 70 after 24 steps.
Found uncertainty sample 71 after 225 steps.
Found uncertainty sample 72 after 11 steps.
Found uncertainty sample 73 after 1085 steps.
Found uncertainty sample 74 after 1730 steps.
Found uncertainty sample 75 after 45 steps.
Found uncertainty sample 76 after 1156 steps.
Found uncertainty sample 77 after 37 steps.
Found uncertainty sample 78 after 163 steps.
Found uncertainty sample 79 after 162 steps.
Found uncertainty sample 80 after 1447 steps.
Found uncertainty sample 81 after 467 steps.
Found uncertainty sample 82 after 2975 steps.
Found uncertainty sample 83 after 788 steps.
Found uncertainty sample 84 after 403 steps.
Found uncertainty sample 85 after 1784 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 72 steps.
Found uncertainty sample 88 after 901 steps.
Found uncertainty sample 89 after 1112 steps.
Found uncertainty sample 90 after 545 steps.
Found uncertainty sample 91 after 126 steps.
Found uncertainty sample 92 after 345 steps.
Found uncertainty sample 93 after 112 steps.
Found uncertainty sample 94 after 442 steps.
Found uncertainty sample 95 after 576 steps.
Found uncertainty sample 96 after 240 steps.
Found uncertainty sample 97 after 7 steps.
Found uncertainty sample 98 after 135 steps.
Found uncertainty sample 99 after 1276 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_172753-hn3wwazr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_44
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hn3wwazr
Training model 44. Added 98 samples to the dataset.
Epoch 0, Batch 100/151, Loss: 2.266803026199341, Variance: 0.15008877217769623

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.3150590622776175, Training Loss Force: 3.6761737936228776, time: 2.4239747524261475
Validation Loss Energy: 5.569328978480841, Validation Loss Force: 3.5924792036774638, time: 0.1386275291442871
Test Loss Energy: 12.139108079348224, Test Loss Force: 11.922088583592252, time: 9.923747539520264

Epoch 1, Batch 100/151, Loss: 1.2315394878387451, Variance: 0.1600283831357956

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.11323111505504, Training Loss Force: 3.5385788632268094, time: 2.479613780975342
Validation Loss Energy: 3.211274301901156, Validation Loss Force: 3.6056258127221863, time: 0.14490461349487305
Test Loss Energy: 11.08213184840777, Test Loss Force: 11.891208140940446, time: 10.04986023902893

Epoch 2, Batch 100/151, Loss: 1.1016919612884521, Variance: 0.1633162796497345

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.1292894763566785, Training Loss Force: 3.5475741098195095, time: 2.6003975868225098
Validation Loss Energy: 2.3942684731276507, Validation Loss Force: 3.6849593409329713, time: 0.13090896606445312
Test Loss Energy: 10.5814638411587, Test Loss Force: 11.912802972434006, time: 10.132654905319214

Epoch 3, Batch 100/151, Loss: 1.6029386520385742, Variance: 0.1601313203573227

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.126689534635568, Training Loss Force: 3.5475751804009663, time: 2.4968230724334717
Validation Loss Energy: 5.348024645971493, Validation Loss Force: 3.6111008016822588, time: 0.14247345924377441
Test Loss Energy: 11.408217223972077, Test Loss Force: 11.78721036011673, time: 10.028093338012695

Epoch 4, Batch 100/151, Loss: 2.188511610031128, Variance: 0.16109882295131683

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.115219936881264, Training Loss Force: 3.5508912283073264, time: 2.5060973167419434
Validation Loss Energy: 5.776348343048434, Validation Loss Force: 3.6321828195265073, time: 0.13429713249206543
Test Loss Energy: 11.41808338838669, Test Loss Force: 11.773172672284861, time: 10.248511552810669

Epoch 5, Batch 100/151, Loss: 1.2198903560638428, Variance: 0.16058005392551422

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.1048349988771555, Training Loss Force: 3.5622519379000117, time: 2.436713457107544
Validation Loss Energy: 3.703723150298965, Validation Loss Force: 3.600616647615395, time: 0.13523411750793457
Test Loss Energy: 10.675514872321505, Test Loss Force: 11.965210280028547, time: 10.836666822433472

Epoch 6, Batch 100/151, Loss: 1.0974452495574951, Variance: 0.16371971368789673

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.114027464712109, Training Loss Force: 3.554716555941547, time: 2.8243541717529297
Validation Loss Energy: 1.7638205699871918, Validation Loss Force: 3.6555404418190895, time: 0.16414403915405273
Test Loss Energy: 10.613983753597115, Test Loss Force: 12.11281577758312, time: 12.209899187088013

Epoch 7, Batch 100/151, Loss: 1.7349385023117065, Variance: 0.16987115144729614

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.079508838049681, Training Loss Force: 3.562516796213257, time: 2.710723876953125
Validation Loss Energy: 5.56507050528802, Validation Loss Force: 3.646625585669676, time: 0.16174817085266113
Test Loss Energy: 12.502423703469061, Test Loss Force: 12.51058041703167, time: 11.48507809638977

Epoch 8, Batch 100/151, Loss: 1.9252150058746338, Variance: 0.16693523526191711

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.105619485872341, Training Loss Force: 3.5443447038049967, time: 2.4812989234924316
Validation Loss Energy: 5.887242921779975, Validation Loss Force: 3.6777148744932595, time: 0.1403815746307373
Test Loss Energy: 12.27055916119553, Test Loss Force: 12.412251988518857, time: 10.878840923309326

Epoch 9, Batch 100/151, Loss: 1.1663825511932373, Variance: 0.16512930393218994

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.080462541061416, Training Loss Force: 3.572787413518437, time: 2.4053075313568115
Validation Loss Energy: 3.4782891958092406, Validation Loss Force: 3.615114635706753, time: 0.14016938209533691
Test Loss Energy: 11.317351755901267, Test Loss Force: 12.299699389678986, time: 10.513138771057129

Epoch 10, Batch 100/151, Loss: 0.8956009149551392, Variance: 0.16339771449565887

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.088299427365876, Training Loss Force: 3.5530127181802498, time: 2.4368810653686523
Validation Loss Energy: 2.2816095383068267, Validation Loss Force: 3.6228196561632924, time: 0.1441664695739746
Test Loss Energy: 10.491339098906021, Test Loss Force: 12.201050112413991, time: 10.57565712928772

Epoch 11, Batch 100/151, Loss: 1.801835060119629, Variance: 0.1614423394203186

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.091537005780435, Training Loss Force: 3.5549232091298997, time: 2.442359209060669
Validation Loss Energy: 5.667288854971534, Validation Loss Force: 3.6148506243227314, time: 0.16165399551391602
Test Loss Energy: 11.646729967458786, Test Loss Force: 12.165076552558343, time: 10.77677845954895

Epoch 12, Batch 100/151, Loss: 1.9359195232391357, Variance: 0.16370190680027008

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.054765832969088, Training Loss Force: 3.588897417103684, time: 2.5309231281280518
Validation Loss Energy: 5.459452443084483, Validation Loss Force: 3.639226215727447, time: 0.1490190029144287
Test Loss Energy: 11.546390554432937, Test Loss Force: 12.236395162374606, time: 10.607484817504883

Epoch 13, Batch 100/151, Loss: 1.3351361751556396, Variance: 0.164729505777359

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.118337258473404, Training Loss Force: 3.5274992903891045, time: 2.43916916847229
Validation Loss Energy: 3.979196205938166, Validation Loss Force: 3.5628077977521, time: 0.1485888957977295
Test Loss Energy: 10.91918679230255, Test Loss Force: 12.264409501044597, time: 10.766761064529419

Epoch 14, Batch 100/151, Loss: 0.9801614880561829, Variance: 0.16481873393058777

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.120228408174589, Training Loss Force: 3.541601739921899, time: 2.469348669052124
Validation Loss Energy: 2.019184375057654, Validation Loss Force: 3.606522649818935, time: 0.14147472381591797
Test Loss Energy: 11.017909557380538, Test Loss Force: 12.679704082608408, time: 10.703279495239258

Epoch 15, Batch 100/151, Loss: 1.9079360961914062, Variance: 0.16961580514907837

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.074741569053447, Training Loss Force: 3.5526734603542915, time: 2.517232656478882
Validation Loss Energy: 5.18332384947967, Validation Loss Force: 3.6319867964550423, time: 0.14078140258789062
Test Loss Energy: 12.310247565205625, Test Loss Force: 12.853649680017, time: 10.760148048400879

Epoch 16, Batch 100/151, Loss: 2.081517457962036, Variance: 0.17212112247943878

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.097022886358467, Training Loss Force: 3.5294002168878897, time: 2.4648780822753906
Validation Loss Energy: 5.376576471431132, Validation Loss Force: 3.6746646040862436, time: 0.13927745819091797
Test Loss Energy: 12.130159213887046, Test Loss Force: 12.764182620814902, time: 10.482984781265259

Epoch 17, Batch 100/151, Loss: 1.255428433418274, Variance: 0.17030754685401917

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.0946876164098915, Training Loss Force: 3.5588544048832818, time: 2.515829086303711
Validation Loss Energy: 3.5388017629065764, Validation Loss Force: 3.620157572300677, time: 0.14380621910095215
Test Loss Energy: 11.799447967256771, Test Loss Force: 13.11689033345602, time: 10.54942512512207

Epoch 18, Batch 100/151, Loss: 0.8155905604362488, Variance: 0.16393600404262543

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.08735651495994, Training Loss Force: 3.5231491330525615, time: 2.6841397285461426
Validation Loss Energy: 2.3488898372024427, Validation Loss Force: 3.5734370578790604, time: 0.13806557655334473
Test Loss Energy: 10.554994942942153, Test Loss Force: 12.511314110701697, time: 10.683937311172485

Epoch 19, Batch 100/151, Loss: 0.8746165633201599, Variance: 0.11928622424602509

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 3.0455510153270673, Training Loss Force: 5.030462282507477, time: 2.513671398162842
Validation Loss Energy: 1.8339844343225729, Validation Loss Force: 3.9233311892546356, time: 0.14084386825561523
Test Loss Energy: 11.208998300779196, Test Loss Force: 13.43954974718884, time: 10.57541799545288

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–ƒâ–â–„â–„â–‚â–â–ˆâ–‡â–„â–â–…â–…â–‚â–ƒâ–‡â–‡â–†â–â–ƒ
wandb:   test_error_force â–‚â–â–‚â–â–â–‚â–‚â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–†â–…â–‡â–„â–ˆ
wandb:          test_loss â–…â–ƒâ–â–‚â–‚â–â–‚â–…â–ƒâ–ƒâ–â–‚â–‚â–â–‚â–„â–„â–„â–â–ˆ
wandb: train_error_energy â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–
wandb:  train_error_force â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–…
wandb: valid_error_energy â–‡â–ƒâ–‚â–‡â–ˆâ–„â–â–‡â–ˆâ–„â–‚â–ˆâ–‡â–…â–â–‡â–‡â–„â–‚â–
wandb:  valid_error_force â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–â–ˆ
wandb:         valid_loss â–ˆâ–ƒâ–‚â–†â–‡â–ƒâ–â–‡â–ˆâ–ƒâ–‚â–‡â–‡â–„â–â–†â–‡â–ƒâ–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 4813
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.209
wandb:   test_error_force 13.43955
wandb:          test_loss 12.54447
wandb: train_error_energy 3.04555
wandb:  train_error_force 5.03046
wandb:         train_loss 1.69913
wandb: valid_error_energy 1.83398
wandb:  valid_error_force 3.92333
wandb:         valid_loss 0.92493
wandb: 
wandb: ğŸš€ View run al_71_44 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hn3wwazr
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_172753-hn3wwazr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.4552698135375977, Uncertainty Bias: -0.17690423130989075
0.00018692017 0.005915642
2.550329 4.9044113
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 29 steps.
Found uncertainty sample 3 after 84 steps.
Found uncertainty sample 4 after 35 steps.
Found uncertainty sample 5 after 20 steps.
Found uncertainty sample 6 after 225 steps.
Found uncertainty sample 7 after 84 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 11 steps.
Found uncertainty sample 11 after 373 steps.
Found uncertainty sample 12 after 57 steps.
Found uncertainty sample 13 after 786 steps.
Found uncertainty sample 14 after 199 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 26 steps.
Found uncertainty sample 17 after 57 steps.
Found uncertainty sample 18 after 49 steps.
Found uncertainty sample 19 after 4 steps.
Found uncertainty sample 20 after 399 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 75 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 429 steps.
Found uncertainty sample 25 after 117 steps.
Found uncertainty sample 26 after 610 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 61 steps.
Found uncertainty sample 29 after 12 steps.
Found uncertainty sample 30 after 146 steps.
Found uncertainty sample 31 after 647 steps.
Found uncertainty sample 32 after 86 steps.
Found uncertainty sample 33 after 161 steps.
Found uncertainty sample 34 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 45 steps.
Found uncertainty sample 37 after 296 steps.
Found uncertainty sample 38 after 1824 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 179 steps.
Found uncertainty sample 41 after 123 steps.
Found uncertainty sample 42 after 16 steps.
Found uncertainty sample 43 after 148 steps.
Found uncertainty sample 44 after 356 steps.
Found uncertainty sample 45 after 233 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 147 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 19 steps.
Found uncertainty sample 50 after 49 steps.
Found uncertainty sample 51 after 284 steps.
Found uncertainty sample 52 after 8 steps.
Found uncertainty sample 53 after 130 steps.
Found uncertainty sample 54 after 692 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 588 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1657 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 16 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 33 steps.
Found uncertainty sample 63 after 8 steps.
Found uncertainty sample 64 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 202 steps.
Found uncertainty sample 67 after 42 steps.
Found uncertainty sample 68 after 154 steps.
Found uncertainty sample 69 after 72 steps.
Found uncertainty sample 70 after 2 steps.
Found uncertainty sample 71 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 25 steps.
Found uncertainty sample 74 after 32 steps.
Found uncertainty sample 75 after 18 steps.
Found uncertainty sample 76 after 10 steps.
Found uncertainty sample 77 after 37 steps.
Found uncertainty sample 78 after 135 steps.
Found uncertainty sample 79 after 4 steps.
Found uncertainty sample 80 after 603 steps.
Found uncertainty sample 81 after 53 steps.
Found uncertainty sample 82 after 163 steps.
Found uncertainty sample 83 after 142 steps.
Found uncertainty sample 84 after 5 steps.
Found uncertainty sample 85 after 209 steps.
Found uncertainty sample 86 after 882 steps.
Found uncertainty sample 87 after 12 steps.
Found uncertainty sample 88 after 9 steps.
Found uncertainty sample 89 after 1058 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 48 steps.
Found uncertainty sample 93 after 109 steps.
Found uncertainty sample 94 after 24 steps.
Found uncertainty sample 95 after 146 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 391 steps.
Found uncertainty sample 99 after 1187 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_173700-6p8d9562
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_45
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/6p8d9562
Training model 45. Added 100 samples to the dataset.
Epoch 0, Batch 100/154, Loss: 0.7106841206550598, Variance: 0.10644174367189407

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.5737204621707743, Training Loss Force: 3.806578411336656, time: 2.611820936203003
Validation Loss Energy: 1.695960106631861, Validation Loss Force: 3.626100166467198, time: 0.14960622787475586
Test Loss Energy: 11.300571077974949, Test Loss Force: 14.35350815047538, time: 10.415058135986328

Epoch 1, Batch 100/154, Loss: 0.5961732268333435, Variance: 0.09582918137311935

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6939097105770649, Training Loss Force: 3.51155978580605, time: 2.4543352127075195
Validation Loss Energy: 1.4680665116446938, Validation Loss Force: 3.5957830200186436, time: 0.1465740203857422
Test Loss Energy: 10.92658481262882, Test Loss Force: 13.836254733317094, time: 10.428993940353394

Epoch 2, Batch 100/154, Loss: 0.5395762324333191, Variance: 0.08847934007644653

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6859355648166485, Training Loss Force: 3.5010255326272564, time: 2.754519462585449
Validation Loss Energy: 2.1191006226644684, Validation Loss Force: 3.6065099576776167, time: 0.14651083946228027
Test Loss Energy: 10.524714625115411, Test Loss Force: 13.173067819070674, time: 10.481590747833252

Epoch 3, Batch 100/154, Loss: 0.6802003979682922, Variance: 0.09074687212705612

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6853675325721038, Training Loss Force: 3.5002018722269512, time: 2.459726333618164
Validation Loss Energy: 1.6502744470659432, Validation Loss Force: 3.5622650788204253, time: 0.14466524124145508
Test Loss Energy: 10.675903538401206, Test Loss Force: 13.12307953778198, time: 10.427562475204468

Epoch 4, Batch 100/154, Loss: 0.43427008390426636, Variance: 0.08452204614877701

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.687129143011047, Training Loss Force: 3.507192791286597, time: 2.447077512741089
Validation Loss Energy: 1.9213650027980418, Validation Loss Force: 3.6084986158198986, time: 0.14317059516906738
Test Loss Energy: 10.281830921268352, Test Loss Force: 13.093870405438835, time: 10.659261703491211

Epoch 5, Batch 100/154, Loss: 0.7027895450592041, Variance: 0.08884494006633759

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6962300213990729, Training Loss Force: 3.5113766505548494, time: 2.4568586349487305
Validation Loss Energy: 1.501694117397281, Validation Loss Force: 3.6086933770995366, time: 0.14075016975402832
Test Loss Energy: 10.727977770259265, Test Loss Force: 13.532744087374958, time: 10.488742351531982

Epoch 6, Batch 100/154, Loss: 0.5792155265808105, Variance: 0.08585880696773529

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6793844592514133, Training Loss Force: 3.509384410909487, time: 2.4924747943878174
Validation Loss Energy: 1.930538971610267, Validation Loss Force: 3.6064824951496193, time: 0.1511702537536621
Test Loss Energy: 10.557512974245638, Test Loss Force: 13.287637609180301, time: 10.706640243530273

Epoch 7, Batch 100/154, Loss: 0.83400559425354, Variance: 0.0867987796664238

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7154091887288851, Training Loss Force: 3.504077519803748, time: 2.4993391036987305
Validation Loss Energy: 1.6317612518236906, Validation Loss Force: 3.579418099584778, time: 0.14171648025512695
Test Loss Energy: 10.6807899813318, Test Loss Force: 13.454106298431384, time: 10.49949336051941

Epoch 8, Batch 100/154, Loss: 0.28263038396835327, Variance: 0.08311503380537033

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.7070500003003433, Training Loss Force: 3.5037033503302806, time: 2.438760757446289
Validation Loss Energy: 1.8576853382151268, Validation Loss Force: 3.6127188641426677, time: 0.14221525192260742
Test Loss Energy: 10.980052514996943, Test Loss Force: 13.81492370064207, time: 10.662381172180176

Epoch 9, Batch 100/154, Loss: 0.5479082465171814, Variance: 0.08422994613647461

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.684053354816301, Training Loss Force: 3.525470450769243, time: 2.474443197250366
Validation Loss Energy: 1.6110729130294426, Validation Loss Force: 3.7192192899484695, time: 0.14337658882141113
Test Loss Energy: 10.75334477097137, Test Loss Force: 13.61238309006319, time: 10.5068998336792

Epoch 10, Batch 100/154, Loss: 0.5059029459953308, Variance: 0.08273296058177948

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6812786138609463, Training Loss Force: 3.5297135111122415, time: 2.443490505218506
Validation Loss Energy: 1.7926581912956294, Validation Loss Force: 3.644810980576461, time: 0.14425921440124512
Test Loss Energy: 10.60505837681631, Test Loss Force: 13.392520661906257, time: 10.416468620300293

Epoch 11, Batch 100/154, Loss: 0.9006003141403198, Variance: 0.08703058958053589

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.679256272311157, Training Loss Force: 3.5124826308518298, time: 2.5527141094207764
Validation Loss Energy: 1.5256530948299651, Validation Loss Force: 3.6103767558427506, time: 0.14325261116027832
Test Loss Energy: 10.916032356258702, Test Loss Force: 13.916333026090221, time: 11.514525413513184

Epoch 12, Batch 100/154, Loss: 0.6697786450386047, Variance: 0.08094607293605804

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.699759306396427, Training Loss Force: 3.5133820869618666, time: 2.488567590713501
Validation Loss Energy: 1.8369741387218967, Validation Loss Force: 3.5883531444034222, time: 0.14770150184631348
Test Loss Energy: 10.20042658826475, Test Loss Force: 12.970795311469946, time: 10.431579351425171

Epoch 13, Batch 100/154, Loss: 0.6133884787559509, Variance: 0.08625517785549164

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.698413202728875, Training Loss Force: 3.4992559563442076, time: 2.484092950820923
Validation Loss Energy: 1.452990511562031, Validation Loss Force: 3.5767183104134523, time: 0.14809012413024902
Test Loss Energy: 10.60674829806867, Test Loss Force: 13.332311190651392, time: 10.654762744903564

Epoch 14, Batch 100/154, Loss: 1.0405973196029663, Variance: 0.08273836970329285

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6949077534514598, Training Loss Force: 3.5193577879818574, time: 2.4402682781219482
Validation Loss Energy: 2.017051353085763, Validation Loss Force: 3.6166918012673595, time: 0.14422821998596191
Test Loss Energy: 10.504999769402197, Test Loss Force: 13.25907804587069, time: 10.524925947189331

Epoch 15, Batch 100/154, Loss: 0.5203433036804199, Variance: 0.08032233268022537

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6774091843923449, Training Loss Force: 3.5141236041358765, time: 2.4936399459838867
Validation Loss Energy: 1.4173145352245298, Validation Loss Force: 3.6340312703299027, time: 0.15327143669128418
Test Loss Energy: 10.45008119218504, Test Loss Force: 13.170646353896485, time: 10.638412714004517

Epoch 16, Batch 100/154, Loss: 0.45675283670425415, Variance: 0.08198373764753342

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6839722114323084, Training Loss Force: 3.5154546595622844, time: 2.5364859104156494
Validation Loss Energy: 1.9032191003119976, Validation Loss Force: 3.54629383743656, time: 0.1546342372894287
Test Loss Energy: 10.582395516844631, Test Loss Force: 13.17339957511358, time: 10.530989646911621

Epoch 17, Batch 100/154, Loss: 0.49671852588653564, Variance: 0.08243134617805481

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6805410429191856, Training Loss Force: 3.5046673139810722, time: 2.607358455657959
Validation Loss Energy: 1.6366125816175439, Validation Loss Force: 3.596455406616297, time: 0.14209246635437012
Test Loss Energy: 10.66586625682827, Test Loss Force: 13.364017756819116, time: 10.450200319290161

Epoch 18, Batch 100/154, Loss: 0.374578595161438, Variance: 0.07992379367351532

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6767668916261573, Training Loss Force: 3.5189059330027246, time: 2.6089184284210205
Validation Loss Energy: 4.489837123558277, Validation Loss Force: 4.252775237822105, time: 0.1608879566192627
Test Loss Energy: 11.22122070579535, Test Loss Force: 13.654708311788378, time: 10.530202150344849

Epoch 19, Batch 100/154, Loss: 0.9955388903617859, Variance: 0.10610446333885193

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.7880360044724632, Training Loss Force: 4.024822171376229, time: 2.532719135284424
Validation Loss Energy: 2.666726811200135, Validation Loss Force: 3.5814024527902375, time: 0.14702725410461426
Test Loss Energy: 10.60508077430129, Test Loss Force: 12.480433812609956, time: 10.518812894821167

wandb: - 0.039 MB of 0.049 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–†â–ƒâ–„â–‚â–„â–ƒâ–„â–†â–…â–„â–†â–â–„â–ƒâ–ƒâ–ƒâ–„â–‡â–„
wandb:   test_error_force â–ˆâ–†â–„â–ƒâ–ƒâ–…â–„â–…â–†â–…â–„â–†â–ƒâ–„â–„â–„â–„â–„â–…â–
wandb:          test_loss â–„â–†â–…â–†â–…â–‡â–†â–‡â–†â–ˆâ–†â–‡â–…â–ˆâ–†â–‡â–‡â–ˆâ–ˆâ–
wandb: train_error_energy â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:  train_error_force â–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb:         train_loss â–‡â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆ
wandb: valid_error_energy â–‚â–â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–â–ˆâ–„
wandb:  valid_error_force â–‚â–â–‚â–â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–â–â–‚â–‚â–â–â–ˆâ–
wandb:         valid_loss â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–ˆâ–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 4903
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.60508
wandb:   test_error_force 12.48043
wandb:          test_loss 12.71787
wandb: train_error_energy 2.78804
wandb:  train_error_force 4.02482
wandb:         train_loss 1.36462
wandb: valid_error_energy 2.66673
wandb:  valid_error_force 3.5814
wandb:         valid_loss 1.03827
wandb: 
wandb: ğŸš€ View run al_71_45 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/6p8d9562
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_173700-6p8d9562/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.8696658611297607, Uncertainty Bias: -0.0829741358757019
3.528595e-05 0.058135986
2.2714097 4.581925
(48745, 22, 3)
Found uncertainty sample 0 after 1583 steps.
Found uncertainty sample 1 after 1178 steps.
Found uncertainty sample 2 after 354 steps.
Found uncertainty sample 3 after 243 steps.
Found uncertainty sample 4 after 203 steps.
Found uncertainty sample 5 after 97 steps.
Found uncertainty sample 6 after 56 steps.
Found uncertainty sample 7 after 1451 steps.
Found uncertainty sample 8 after 104 steps.
Found uncertainty sample 9 after 13 steps.
Found uncertainty sample 10 after 1457 steps.
Found uncertainty sample 11 after 62 steps.
Found uncertainty sample 12 after 28 steps.
Found uncertainty sample 13 after 1858 steps.
Found uncertainty sample 14 after 4 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 2436 steps.
Found uncertainty sample 17 after 227 steps.
Found uncertainty sample 18 after 3600 steps.
Found uncertainty sample 19 after 494 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 175 steps.
Found uncertainty sample 22 after 215 steps.
Found uncertainty sample 23 after 149 steps.
Found uncertainty sample 24 after 18 steps.
Found uncertainty sample 25 after 11 steps.
Found uncertainty sample 26 after 934 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 1261 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 89 steps.
Found uncertainty sample 31 after 40 steps.
Found uncertainty sample 32 after 1287 steps.
Found uncertainty sample 33 after 797 steps.
Found uncertainty sample 34 after 3771 steps.
Found uncertainty sample 35 after 287 steps.
Found uncertainty sample 36 after 1450 steps.
Found uncertainty sample 37 after 618 steps.
Found uncertainty sample 38 after 612 steps.
Found uncertainty sample 39 after 48 steps.
Found uncertainty sample 40 after 452 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 610 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 210 steps.
Found uncertainty sample 45 after 207 steps.
Found uncertainty sample 46 after 2681 steps.
Found uncertainty sample 47 after 220 steps.
Found uncertainty sample 48 after 1324 steps.
Found uncertainty sample 49 after 986 steps.
Found uncertainty sample 50 after 525 steps.
Found uncertainty sample 51 after 7 steps.
Found uncertainty sample 52 after 1681 steps.
Found uncertainty sample 53 after 330 steps.
Found uncertainty sample 54 after 2344 steps.
Found uncertainty sample 55 after 1235 steps.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 28 steps.
Found uncertainty sample 58 after 2807 steps.
Found uncertainty sample 59 after 219 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 1071 steps.
Found uncertainty sample 62 after 1041 steps.
Found uncertainty sample 63 after 95 steps.
Found uncertainty sample 64 after 1450 steps.
Found uncertainty sample 65 after 747 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 1111 steps.
Found uncertainty sample 68 after 3513 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 281 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 68 steps.
Found uncertainty sample 74 after 31 steps.
Found uncertainty sample 75 after 98 steps.
Found uncertainty sample 76 after 655 steps.
Found uncertainty sample 77 after 2350 steps.
Found uncertainty sample 78 after 687 steps.
Found uncertainty sample 79 after 2768 steps.
Found uncertainty sample 80 after 270 steps.
Found uncertainty sample 81 after 1224 steps.
Found uncertainty sample 82 after 1074 steps.
Found uncertainty sample 83 after 1487 steps.
Found uncertainty sample 84 after 80 steps.
Found uncertainty sample 85 after 223 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 994 steps.
Found uncertainty sample 88 after 1673 steps.
Found uncertainty sample 89 after 2685 steps.
Found uncertainty sample 90 after 657 steps.
Found uncertainty sample 91 after 1129 steps.
Found uncertainty sample 92 after 2789 steps.
Found uncertainty sample 93 after 757 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 9 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 3107 steps.
Found uncertainty sample 99 after 1002 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_175552-27qpcosf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_46
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/27qpcosf
Training model 46. Added 90 samples to the dataset.
Epoch 0, Batch 100/156, Loss: 0.8798627257347107, Variance: 0.11368229985237122

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1218665863505852, Training Loss Force: 3.6972416339951812, time: 2.5027222633361816
Validation Loss Energy: 1.9191478861120388, Validation Loss Force: 3.570151723076993, time: 0.15265917778015137
Test Loss Energy: 10.705288292581098, Test Loss Force: 12.788600482051518, time: 10.619023084640503

Epoch 1, Batch 100/156, Loss: 0.9155161380767822, Variance: 0.1160697415471077

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6373282541973633, Training Loss Force: 3.4884994425613867, time: 2.480900764465332
Validation Loss Energy: 1.7841736432707214, Validation Loss Force: 3.570644044558639, time: 0.14490675926208496
Test Loss Energy: 10.470449058394184, Test Loss Force: 12.539487637992304, time: 10.547563076019287

Epoch 2, Batch 100/156, Loss: 0.7433218955993652, Variance: 0.1134708970785141

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6315401760034036, Training Loss Force: 3.4825891711051837, time: 2.7613377571105957
Validation Loss Energy: 1.8962381256949665, Validation Loss Force: 3.592960465924863, time: 0.14203476905822754
Test Loss Energy: 10.434243069256013, Test Loss Force: 12.667702257611955, time: 11.40868854522705

Epoch 3, Batch 100/156, Loss: 0.9406754970550537, Variance: 0.11739291250705719

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6562763808036323, Training Loss Force: 3.4846103946100553, time: 2.5346648693084717
Validation Loss Energy: 1.9216411125389925, Validation Loss Force: 3.594000405059394, time: 0.14273357391357422
Test Loss Energy: 10.646819291972138, Test Loss Force: 12.714289301505003, time: 10.581214904785156

Epoch 4, Batch 100/156, Loss: 0.854110836982727, Variance: 0.11964909732341766

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6586196060176945, Training Loss Force: 3.4895496801439165, time: 2.5429978370666504
Validation Loss Energy: 1.7171572815360783, Validation Loss Force: 3.5687929959485416, time: 0.15150237083435059
Test Loss Energy: 10.785156138529372, Test Loss Force: 13.025950165132885, time: 10.76171350479126

Epoch 5, Batch 100/156, Loss: 0.7848200798034668, Variance: 0.11616308987140656

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6425376759441255, Training Loss Force: 3.495039128324811, time: 2.5446860790252686
Validation Loss Energy: 1.602666299205609, Validation Loss Force: 3.5885235605231056, time: 0.14572429656982422
Test Loss Energy: 10.705932568850637, Test Loss Force: 12.913513293626929, time: 10.684122562408447

Epoch 6, Batch 100/156, Loss: 0.7725206613540649, Variance: 0.11953447014093399

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6994631615719684, Training Loss Force: 3.4935346000088665, time: 2.549987316131592
Validation Loss Energy: 1.8619183529022625, Validation Loss Force: 3.6029532634097126, time: 0.14741826057434082
Test Loss Energy: 10.572596143847338, Test Loss Force: 12.92635744685725, time: 10.784977912902832

Epoch 7, Batch 100/156, Loss: 0.8390933275222778, Variance: 0.11816905438899994

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6885128789091146, Training Loss Force: 3.50031605736168, time: 2.549931764602661
Validation Loss Energy: 1.7100468248106904, Validation Loss Force: 3.569702673632705, time: 0.14838075637817383
Test Loss Energy: 10.49714132057766, Test Loss Force: 12.96813270610298, time: 10.546608448028564

Epoch 8, Batch 100/156, Loss: 0.7132898569107056, Variance: 0.12135680019855499

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6788789070434618, Training Loss Force: 3.488785837904039, time: 2.5482735633850098
Validation Loss Energy: 1.6526207118565843, Validation Loss Force: 3.6310744467411276, time: 0.14928960800170898
Test Loss Energy: 10.711607605636704, Test Loss Force: 13.347461678615199, time: 11.124138116836548

Epoch 9, Batch 100/156, Loss: 0.8687459230422974, Variance: 0.12031285464763641

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7120922190079715, Training Loss Force: 3.5043392092335117, time: 2.7905306816101074
Validation Loss Energy: 1.7679740546777074, Validation Loss Force: 3.6608484289197154, time: 0.16531944274902344
Test Loss Energy: 10.85492898146622, Test Loss Force: 13.348316103262983, time: 12.411933183670044

Epoch 10, Batch 100/156, Loss: 0.7217690944671631, Variance: 0.12291989475488663

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6866603037307377, Training Loss Force: 3.4896577374838103, time: 2.855675220489502
Validation Loss Energy: 1.5334798781978025, Validation Loss Force: 3.6034855168915283, time: 0.1858508586883545
Test Loss Energy: 10.555674636429774, Test Loss Force: 13.045006409813107, time: 12.612745761871338

Epoch 11, Batch 100/156, Loss: 0.9395360946655273, Variance: 0.12045536935329437

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6710941874473932, Training Loss Force: 3.489672637760825, time: 2.92854380607605
Validation Loss Energy: 1.9121438281268803, Validation Loss Force: 3.571757167619132, time: 0.16546916961669922
Test Loss Energy: 10.849676866834116, Test Loss Force: 13.09619299152255, time: 12.47182035446167

Epoch 12, Batch 100/156, Loss: 0.8230066895484924, Variance: 0.11992350220680237

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.630221433687676, Training Loss Force: 3.4816389662090943, time: 2.8051211833953857
Validation Loss Energy: 1.9140675039900747, Validation Loss Force: 3.5551675740170254, time: 0.16040372848510742
Test Loss Energy: 10.86733527155947, Test Loss Force: 13.288361463438852, time: 12.56771993637085

Epoch 13, Batch 100/156, Loss: 0.8078239560127258, Variance: 0.11994785070419312

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.681438911376332, Training Loss Force: 3.4882434001635296, time: 2.853893518447876
Validation Loss Energy: 1.9332968774286796, Validation Loss Force: 3.552738847419343, time: 0.1692943572998047
Test Loss Energy: 10.934857897050836, Test Loss Force: 13.301323778208202, time: 12.447632551193237

Epoch 14, Batch 100/156, Loss: 1.0067414045333862, Variance: 0.11946417391300201

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.654066565810039, Training Loss Force: 3.4945687611645337, time: 2.883798599243164
Validation Loss Energy: 1.5902785466102085, Validation Loss Force: 3.59462454667543, time: 0.1692523956298828
Test Loss Energy: 10.632980033826083, Test Loss Force: 13.120840749908783, time: 12.595130681991577

Epoch 15, Batch 100/156, Loss: 0.8297590017318726, Variance: 0.11996139585971832

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6842720815071184, Training Loss Force: 3.491363033436868, time: 2.9558260440826416
Validation Loss Energy: 1.6387888756267934, Validation Loss Force: 3.6193212269885726, time: 0.17455625534057617
Test Loss Energy: 10.707464616512727, Test Loss Force: 13.258184776532103, time: 12.432071447372437

Epoch 16, Batch 100/156, Loss: 0.6780587434768677, Variance: 0.11981961131095886

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.647032346860855, Training Loss Force: 3.4953099653745756, time: 2.9399282932281494
Validation Loss Energy: 1.7875798503075029, Validation Loss Force: 3.56007104550255, time: 0.1715400218963623
Test Loss Energy: 10.74133972834747, Test Loss Force: 13.157611372042293, time: 12.73867654800415

Epoch 17, Batch 100/156, Loss: 0.6285169124603271, Variance: 0.12342753261327744

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6949739189293545, Training Loss Force: 3.492655296679992, time: 3.0455894470214844
Validation Loss Energy: 1.8144687825853536, Validation Loss Force: 3.5369935409955455, time: 0.17622733116149902
Test Loss Energy: 10.790261673909878, Test Loss Force: 13.157733338775522, time: 12.56593132019043

Epoch 18, Batch 100/156, Loss: 0.7644847631454468, Variance: 0.11870227009057999

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.652129160110212, Training Loss Force: 3.4974991727669225, time: 2.822915554046631
Validation Loss Energy: 1.7562978042265944, Validation Loss Force: 3.564173427624075, time: 0.16546845436096191
Test Loss Energy: 10.776796947598891, Test Loss Force: 13.201830223479963, time: 12.600749254226685

Epoch 19, Batch 100/156, Loss: 0.7994617223739624, Variance: 0.1217394769191742

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6948986881600896, Training Loss Force: 3.4867635138997333, time: 2.9290411472320557
Validation Loss Energy: 1.8064005145287574, Validation Loss Force: 3.5983207060517337, time: 0.15715742111206055
Test Loss Energy: 10.768982748146614, Test Loss Force: 12.883814508334467, time: 12.578927993774414

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–‚â–â–„â–†â–…â–ƒâ–‚â–…â–‡â–ƒâ–‡â–‡â–ˆâ–„â–…â–…â–†â–†â–†
wandb:   test_error_force â–ƒâ–â–‚â–ƒâ–…â–„â–„â–…â–ˆâ–ˆâ–…â–†â–‡â–ˆâ–†â–‡â–†â–†â–‡â–„
wandb:          test_loss â–ˆâ–ƒâ–â–‚â–†â–‡â–ƒâ–ƒâ–…â–…â–„â–…â–†â–‡â–…â–…â–‡â–„â–†â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–…â–‡â–ˆâ–„â–‚â–‡â–„â–ƒâ–…â–â–ˆâ–ˆâ–ˆâ–‚â–ƒâ–…â–†â–…â–†
wandb:  valid_error_force â–ƒâ–ƒâ–„â–„â–ƒâ–„â–…â–ƒâ–†â–ˆâ–…â–ƒâ–‚â–‚â–„â–†â–‚â–â–ƒâ–„
wandb:         valid_loss â–†â–„â–ˆâ–ˆâ–ƒâ–‚â–‡â–ƒâ–ƒâ–‡â–â–ˆâ–‡â–‡â–â–„â–…â–…â–„â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 4984
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.76898
wandb:   test_error_force 12.88381
wandb:          test_loss 12.65282
wandb: train_error_energy 2.6949
wandb:  train_error_force 3.48676
wandb:         train_loss 1.01324
wandb: valid_error_energy 1.8064
wandb:  valid_error_force 3.59832
wandb:         valid_loss 0.78879
wandb: 
wandb: ğŸš€ View run al_71_46 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/27qpcosf
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_175552-27qpcosf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.3281989097595215, Uncertainty Bias: -0.15094764530658722
2.7403235e-05 0.0068178177
2.1722717 4.7008004
(48745, 22, 3)
Found uncertainty sample 0 after 1472 steps.
Found uncertainty sample 1 after 271 steps.
Found uncertainty sample 2 after 42 steps.
Found uncertainty sample 3 after 754 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 785 steps.
Found uncertainty sample 6 after 732 steps.
Found uncertainty sample 7 after 137 steps.
Found uncertainty sample 8 after 299 steps.
Found uncertainty sample 9 after 1691 steps.
Found uncertainty sample 10 after 3089 steps.
Found uncertainty sample 11 after 161 steps.
Found uncertainty sample 12 after 2757 steps.
Found uncertainty sample 13 after 105 steps.
Found uncertainty sample 14 after 3691 steps.
Found uncertainty sample 15 after 926 steps.
Found uncertainty sample 16 after 162 steps.
Found uncertainty sample 17 after 292 steps.
Found uncertainty sample 18 after 133 steps.
Found uncertainty sample 19 after 1209 steps.
Found uncertainty sample 20 after 352 steps.
Found uncertainty sample 21 after 1393 steps.
Found uncertainty sample 22 after 867 steps.
Found uncertainty sample 23 after 1232 steps.
Found uncertainty sample 24 after 489 steps.
Found uncertainty sample 25 after 50 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 1614 steps.
Found uncertainty sample 28 after 272 steps.
Found uncertainty sample 29 after 444 steps.
Found uncertainty sample 30 after 234 steps.
Found uncertainty sample 31 after 135 steps.
Found uncertainty sample 32 after 1995 steps.
Found uncertainty sample 33 after 1746 steps.
Found uncertainty sample 34 after 207 steps.
Found uncertainty sample 35 after 436 steps.
Found uncertainty sample 36 after 847 steps.
Found uncertainty sample 37 after 753 steps.
Found uncertainty sample 38 after 911 steps.
Found uncertainty sample 39 after 19 steps.
Found uncertainty sample 40 after 1739 steps.
Found uncertainty sample 41 after 11 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 175 steps.
Found uncertainty sample 44 after 1290 steps.
Found uncertainty sample 45 after 1033 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1208 steps.
Found uncertainty sample 49 after 21 steps.
Found uncertainty sample 50 after 782 steps.
Found uncertainty sample 51 after 388 steps.
Found uncertainty sample 52 after 615 steps.
Found uncertainty sample 53 after 96 steps.
Found uncertainty sample 54 after 178 steps.
Found uncertainty sample 55 after 66 steps.
Found uncertainty sample 56 after 19 steps.
Found uncertainty sample 57 after 144 steps.
Found uncertainty sample 58 after 1372 steps.
Found uncertainty sample 59 after 85 steps.
Found uncertainty sample 60 after 1931 steps.
Found uncertainty sample 61 after 248 steps.
Found uncertainty sample 62 after 92 steps.
Found uncertainty sample 63 after 31 steps.
Found uncertainty sample 64 after 1336 steps.
Found uncertainty sample 65 after 49 steps.
Found uncertainty sample 66 after 987 steps.
Found uncertainty sample 67 after 155 steps.
Found uncertainty sample 68 after 300 steps.
Found uncertainty sample 69 after 394 steps.
Found uncertainty sample 70 after 178 steps.
Found uncertainty sample 71 after 12 steps.
Found uncertainty sample 72 after 55 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 84 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 221 steps.
Found uncertainty sample 77 after 27 steps.
Did not find any uncertainty samples for sample 78.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 44 steps.
Found uncertainty sample 81 after 62 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 161 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 577 steps.
Found uncertainty sample 86 after 3263 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 2114 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 1815 steps.
Found uncertainty sample 91 after 1912 steps.
Found uncertainty sample 92 after 5 steps.
Found uncertainty sample 93 after 36 steps.
Found uncertainty sample 94 after 765 steps.
Found uncertainty sample 95 after 1581 steps.
Found uncertainty sample 96 after 40 steps.
Found uncertainty sample 97 after 305 steps.
Found uncertainty sample 98 after 129 steps.
Found uncertainty sample 99 after 204 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_181247-958tw0gp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_47
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/958tw0gp
Training model 47. Added 92 samples to the dataset.
Epoch 0, Batch 100/159, Loss: 0.8194431066513062, Variance: 0.10658363997936249

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.0102844239329762, Training Loss Force: 3.7344707316113825, time: 2.513338088989258
Validation Loss Energy: 1.7830846427010372, Validation Loss Force: 3.5854271976028826, time: 0.14738988876342773
Test Loss Energy: 10.766404640474397, Test Loss Force: 13.157192334357413, time: 10.52008318901062

Epoch 1, Batch 100/159, Loss: 0.799213171005249, Variance: 0.11948517709970474

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.603198665968849, Training Loss Force: 3.4713595709379566, time: 2.6449389457702637
Validation Loss Energy: 1.997756774698926, Validation Loss Force: 3.582039286547591, time: 0.1484220027923584
Test Loss Energy: 10.43079363273383, Test Loss Force: 12.886019124071508, time: 10.45020318031311

Epoch 2, Batch 100/159, Loss: 0.780706524848938, Variance: 0.11514169722795486

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.647709051874515, Training Loss Force: 3.471582515969556, time: 2.8605258464813232
Validation Loss Energy: 1.8104524620887297, Validation Loss Force: 3.599147996151904, time: 0.14915013313293457
Test Loss Energy: 10.696941902211986, Test Loss Force: 12.993791326452985, time: 10.447959184646606

Epoch 3, Batch 100/159, Loss: 0.7332016229629517, Variance: 0.11682228744029999

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.62540190348785, Training Loss Force: 3.4778997614773615, time: 2.633103370666504
Validation Loss Energy: 2.364349904450307, Validation Loss Force: 3.550965658787271, time: 0.14833498001098633
Test Loss Energy: 10.569045491957358, Test Loss Force: 12.871543144085578, time: 10.501595258712769

Epoch 4, Batch 100/159, Loss: 0.7536276578903198, Variance: 0.11702357232570648

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6452063192955957, Training Loss Force: 3.4714434555868374, time: 2.6055541038513184
Validation Loss Energy: 1.7424910182757205, Validation Loss Force: 3.5386054429429166, time: 0.1605381965637207
Test Loss Energy: 10.732251325109246, Test Loss Force: 13.24718961193083, time: 10.706743955612183

Epoch 5, Batch 100/159, Loss: 0.7374492883682251, Variance: 0.1172083169221878

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6408687516414027, Training Loss Force: 3.485234376711172, time: 2.5625650882720947
Validation Loss Energy: 2.365895599438289, Validation Loss Force: 3.5927507858124472, time: 0.1530919075012207
Test Loss Energy: 10.690540665563592, Test Loss Force: 13.072619463707925, time: 10.462833404541016

Epoch 6, Batch 100/159, Loss: 0.6066111326217651, Variance: 0.11669418215751648

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.671864316360639, Training Loss Force: 3.4902414173582503, time: 2.746122360229492
Validation Loss Energy: 1.7673848721175824, Validation Loss Force: 3.550631358176185, time: 0.14699292182922363
Test Loss Energy: 10.758267437858523, Test Loss Force: 13.078297736774351, time: 10.58714246749878

Epoch 7, Batch 100/159, Loss: 0.7788894176483154, Variance: 0.12062223255634308

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.643219969069997, Training Loss Force: 3.4858639310080193, time: 2.663156747817993
Validation Loss Energy: 2.3580757559254995, Validation Loss Force: 3.5915966174060547, time: 0.15186190605163574
Test Loss Energy: 10.741640595166864, Test Loss Force: 12.920391583182077, time: 10.592121601104736

Epoch 8, Batch 100/159, Loss: 0.6418198943138123, Variance: 0.11681413650512695

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.673096686451168, Training Loss Force: 3.4805080350301463, time: 2.605894088745117
Validation Loss Energy: 1.646912773858347, Validation Loss Force: 3.6502241417370147, time: 0.15012073516845703
Test Loss Energy: 10.563972650469674, Test Loss Force: 12.876872803539204, time: 10.598178148269653

Epoch 9, Batch 100/159, Loss: 0.7499637603759766, Variance: 0.11896337568759918

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6524625759965628, Training Loss Force: 3.4830788063372777, time: 2.6255745887756348
Validation Loss Energy: 2.329129212293179, Validation Loss Force: 3.56852349205892, time: 0.15426349639892578
Test Loss Energy: 10.424863153634137, Test Loss Force: 12.648703137477018, time: 10.532181739807129

Epoch 10, Batch 100/159, Loss: 0.9295142889022827, Variance: 0.11924831569194794

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6653284041497924, Training Loss Force: 3.500387604475512, time: 2.6054282188415527
Validation Loss Energy: 1.88907675195821, Validation Loss Force: 3.5604693263815665, time: 0.15378689765930176
Test Loss Energy: 10.745749412154685, Test Loss Force: 13.041454037191924, time: 10.488847970962524

Epoch 11, Batch 100/159, Loss: 0.9297354817390442, Variance: 0.12048312276601791

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6797590726362497, Training Loss Force: 3.5002775302939413, time: 2.797994375228882
Validation Loss Energy: 2.3790703390184467, Validation Loss Force: 3.5483455109471387, time: 0.14885330200195312
Test Loss Energy: 10.506183212777945, Test Loss Force: 12.609761320962887, time: 10.619389533996582

Epoch 12, Batch 100/159, Loss: 0.8340650796890259, Variance: 0.11915033310651779

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6632840072988504, Training Loss Force: 3.4736269520326744, time: 2.5368916988372803
Validation Loss Energy: 1.8073674181834247, Validation Loss Force: 3.576619222643514, time: 0.1538705825805664
Test Loss Energy: 10.529169276432032, Test Loss Force: 12.783797627486171, time: 10.501153469085693

Epoch 13, Batch 100/159, Loss: 0.9505992531776428, Variance: 0.11925137042999268

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6876853148892677, Training Loss Force: 3.469802966562259, time: 2.645752429962158
Validation Loss Energy: 2.28402186161076, Validation Loss Force: 3.5475300965347514, time: 0.14830875396728516
Test Loss Energy: 10.499190361695927, Test Loss Force: 12.583623516610842, time: 10.571971654891968

Epoch 14, Batch 100/159, Loss: 0.6351624131202698, Variance: 0.11492134630680084

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6461323820678557, Training Loss Force: 3.4888120466927104, time: 2.5635032653808594
Validation Loss Energy: 1.7450510083349282, Validation Loss Force: 3.616198737555227, time: 0.1519618034362793
Test Loss Energy: 10.59062424464013, Test Loss Force: 12.902230659660836, time: 10.496313095092773

Epoch 15, Batch 100/159, Loss: 0.7658012509346008, Variance: 0.12164254486560822

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.649040924963924, Training Loss Force: 3.500076234585152, time: 2.5469484329223633
Validation Loss Energy: 2.3924736113653755, Validation Loss Force: 3.556745556031167, time: 0.15801310539245605
Test Loss Energy: 10.799352534669353, Test Loss Force: 12.959324213136677, time: 10.672025680541992

Epoch 16, Batch 100/159, Loss: 0.8378937244415283, Variance: 0.11946403235197067

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.659470867496364, Training Loss Force: 3.4839436855100643, time: 2.656714916229248
Validation Loss Energy: 1.7912224577542115, Validation Loss Force: 3.5701805958429147, time: 0.1503298282623291
Test Loss Energy: 10.50086995856108, Test Loss Force: 12.798149407052572, time: 11.320543766021729

Epoch 17, Batch 100/159, Loss: 0.6947978734970093, Variance: 0.12038683891296387

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.630984247685175, Training Loss Force: 3.478430747565611, time: 2.663407325744629
Validation Loss Energy: 2.2911188468528194, Validation Loss Force: 3.529831610585924, time: 0.15068864822387695
Test Loss Energy: 10.520450909676903, Test Loss Force: 12.941377826091411, time: 10.750234127044678

Epoch 18, Batch 100/159, Loss: 0.7520344257354736, Variance: 0.1215701624751091

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.651938141142682, Training Loss Force: 3.488333527897242, time: 2.5488264560699463
Validation Loss Energy: 1.80720772729427, Validation Loss Force: 3.68680155700028, time: 0.15900397300720215
Test Loss Energy: 10.812629332884407, Test Loss Force: 13.405849678950547, time: 10.445387125015259

Epoch 19, Batch 100/159, Loss: 0.6089150905609131, Variance: 0.1207583099603653

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.682582462089436, Training Loss Force: 3.487360770424776, time: 2.587975025177002
Validation Loss Energy: 2.447727030128091, Validation Loss Force: 3.596896217302185, time: 0.16005253791809082
Test Loss Energy: 10.93133500069502, Test Loss Force: 13.193861316769357, time: 10.419355869293213

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–â–…â–ƒâ–…â–…â–†â–…â–ƒâ–â–…â–‚â–‚â–‚â–ƒâ–†â–‚â–‚â–†â–ˆ
wandb:   test_error_force â–†â–„â–„â–ƒâ–‡â–…â–…â–„â–ƒâ–‚â–…â–â–ƒâ–â–„â–„â–ƒâ–„â–ˆâ–†
wandb:          test_loss â–ˆâ–ƒâ–‡â–‚â–†â–ƒâ–„â–ƒâ–„â–‚â–…â–â–ƒâ–â–„â–ƒâ–ƒâ–‚â–†â–„
wandb: train_error_energy â–ˆâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–â–‚â–â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–„â–‚â–‡â–‚â–‡â–‚â–‡â–â–‡â–ƒâ–‡â–‚â–‡â–‚â–ˆâ–‚â–‡â–‚â–ˆ
wandb:  valid_error_force â–ƒâ–ƒâ–„â–‚â–â–„â–‚â–„â–†â–ƒâ–‚â–‚â–ƒâ–‚â–…â–‚â–ƒâ–â–ˆâ–„
wandb:         valid_loss â–‚â–ƒâ–‚â–‡â–â–‡â–‚â–‡â–‚â–‡â–ƒâ–‡â–‚â–†â–‚â–‡â–‚â–…â–ƒâ–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5066
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.93134
wandb:   test_error_force 13.19386
wandb:          test_loss 12.59435
wandb: train_error_energy 2.68258
wandb:  train_error_force 3.48736
wandb:         train_loss 1.006
wandb: valid_error_energy 2.44773
wandb:  valid_error_force 3.5969
wandb:         valid_loss 0.9394
wandb: 
wandb: ğŸš€ View run al_71_47 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/958tw0gp
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_181247-958tw0gp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.5429928302764893, Uncertainty Bias: -0.1837637573480606
2.2888184e-05 0.0022621155
2.1560233 4.694809
(48745, 22, 3)
Found uncertainty sample 0 after 1677 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 2080 steps.
Found uncertainty sample 3 after 120 steps.
Found uncertainty sample 4 after 395 steps.
Found uncertainty sample 5 after 184 steps.
Found uncertainty sample 6 after 519 steps.
Found uncertainty sample 7 after 1588 steps.
Found uncertainty sample 8 after 32 steps.
Found uncertainty sample 9 after 1628 steps.
Found uncertainty sample 10 after 169 steps.
Found uncertainty sample 11 after 1096 steps.
Found uncertainty sample 12 after 308 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 818 steps.
Found uncertainty sample 16 after 199 steps.
Found uncertainty sample 17 after 206 steps.
Found uncertainty sample 18 after 99 steps.
Found uncertainty sample 19 after 408 steps.
Found uncertainty sample 20 after 17 steps.
Found uncertainty sample 21 after 167 steps.
Found uncertainty sample 22 after 1206 steps.
Found uncertainty sample 23 after 717 steps.
Found uncertainty sample 24 after 2563 steps.
Found uncertainty sample 25 after 2962 steps.
Found uncertainty sample 26 after 197 steps.
Found uncertainty sample 27 after 2731 steps.
Found uncertainty sample 28 after 63 steps.
Found uncertainty sample 29 after 91 steps.
Found uncertainty sample 30 after 223 steps.
Found uncertainty sample 31 after 377 steps.
Found uncertainty sample 32 after 667 steps.
Found uncertainty sample 33 after 217 steps.
Found uncertainty sample 34 after 316 steps.
Found uncertainty sample 35 after 2351 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 660 steps.
Found uncertainty sample 40 after 124 steps.
Found uncertainty sample 41 after 712 steps.
Found uncertainty sample 42 after 106 steps.
Found uncertainty sample 43 after 1159 steps.
Found uncertainty sample 44 after 620 steps.
Found uncertainty sample 45 after 80 steps.
Found uncertainty sample 46 after 1426 steps.
Found uncertainty sample 47 after 834 steps.
Found uncertainty sample 48 after 564 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1294 steps.
Found uncertainty sample 52 after 852 steps.
Found uncertainty sample 53 after 1719 steps.
Found uncertainty sample 54 after 174 steps.
Found uncertainty sample 55 after 494 steps.
Found uncertainty sample 56 after 733 steps.
Found uncertainty sample 57 after 24 steps.
Found uncertainty sample 58 after 968 steps.
Found uncertainty sample 59 after 1201 steps.
Found uncertainty sample 60 after 11 steps.
Found uncertainty sample 61 after 2934 steps.
Found uncertainty sample 62 after 2817 steps.
Found uncertainty sample 63 after 88 steps.
Found uncertainty sample 64 after 11 steps.
Found uncertainty sample 65 after 1222 steps.
Found uncertainty sample 66 after 1325 steps.
Found uncertainty sample 67 after 247 steps.
Found uncertainty sample 68 after 79 steps.
Found uncertainty sample 69 after 109 steps.
Found uncertainty sample 70 after 449 steps.
Found uncertainty sample 71 after 1575 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 440 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1869 steps.
Found uncertainty sample 76 after 458 steps.
Found uncertainty sample 77 after 1514 steps.
Found uncertainty sample 78 after 13 steps.
Found uncertainty sample 79 after 1696 steps.
Found uncertainty sample 80 after 214 steps.
Found uncertainty sample 81 after 10 steps.
Found uncertainty sample 82 after 129 steps.
Found uncertainty sample 83 after 185 steps.
Found uncertainty sample 84 after 1761 steps.
Found uncertainty sample 85 after 2914 steps.
Found uncertainty sample 86 after 198 steps.
Found uncertainty sample 87 after 10 steps.
Found uncertainty sample 88 after 2723 steps.
Found uncertainty sample 89 after 134 steps.
Found uncertainty sample 90 after 69 steps.
Found uncertainty sample 91 after 1221 steps.
Found uncertainty sample 92 after 1634 steps.
Found uncertainty sample 93 after 1795 steps.
Found uncertainty sample 94 after 251 steps.
Found uncertainty sample 95 after 3699 steps.
Found uncertainty sample 96 after 49 steps.
Found uncertainty sample 97 after 72 steps.
Found uncertainty sample 98 after 9 steps.
Found uncertainty sample 99 after 663 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_182753-9ti1m5eh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_48
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/9ti1m5eh
Training model 48. Added 99 samples to the dataset.
Epoch 0, Batch 100/162, Loss: 0.4990895390510559, Variance: 0.09740888327360153

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.125773770932263, Training Loss Force: 3.6730308571960566, time: 2.674727201461792
Validation Loss Energy: 1.6766876467135188, Validation Loss Force: 3.6420671029142415, time: 0.16359853744506836
Test Loss Energy: 10.769909933143802, Test Loss Force: 13.848047067450478, time: 10.517870903015137

Epoch 1, Batch 100/162, Loss: 0.6452000141143799, Variance: 0.08913230895996094

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6916731936409022, Training Loss Force: 3.4804877855976026, time: 2.630674362182617
Validation Loss Energy: 1.7486459645844077, Validation Loss Force: 3.537289022952192, time: 0.1489882469177246
Test Loss Energy: 10.840007560597417, Test Loss Force: 13.58081271813789, time: 10.499418258666992

Epoch 2, Batch 100/162, Loss: 0.6663641333580017, Variance: 0.08560246974229813

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6312706872978207, Training Loss Force: 3.4729036593677916, time: 2.767890214920044
Validation Loss Energy: 2.031752118546013, Validation Loss Force: 3.5655510074068815, time: 0.1536390781402588
Test Loss Energy: 10.490140709883937, Test Loss Force: 13.251643106766858, time: 10.754323482513428

Epoch 3, Batch 100/162, Loss: 0.4583595395088196, Variance: 0.08387546241283417

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6644322186943385, Training Loss Force: 3.4975479986866325, time: 2.639622926712036
Validation Loss Energy: 1.4954706632146968, Validation Loss Force: 3.6435278867345766, time: 0.15056538581848145
Test Loss Energy: 10.5755890969623, Test Loss Force: 13.37445863146518, time: 10.492685556411743

Epoch 4, Batch 100/162, Loss: 0.5975358486175537, Variance: 0.0827307403087616

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.670809794288427, Training Loss Force: 3.5159834837714565, time: 2.6152870655059814
Validation Loss Energy: 2.091678546228218, Validation Loss Force: 3.6049455584813774, time: 0.14850974082946777
Test Loss Energy: 10.515984824124766, Test Loss Force: 12.990614707432654, time: 10.70647144317627

Epoch 5, Batch 100/162, Loss: 0.690590500831604, Variance: 0.08148880302906036

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6802176179349104, Training Loss Force: 3.5050489731099495, time: 2.644000291824341
Validation Loss Energy: 1.573067743202488, Validation Loss Force: 3.5928673159988165, time: 0.17584538459777832
Test Loss Energy: 9.945395347530077, Test Loss Force: 12.726756586622942, time: 10.574763774871826

Epoch 6, Batch 100/162, Loss: 0.6266729235649109, Variance: 0.0814957395195961

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6558996840939817, Training Loss Force: 3.5094779380200016, time: 2.691439151763916
Validation Loss Energy: 1.8914705194999824, Validation Loss Force: 3.655208887460803, time: 0.1516585350036621
Test Loss Energy: 10.147286636226523, Test Loss Force: 12.699970071271352, time: 10.67823839187622

Epoch 7, Batch 100/162, Loss: 0.771994948387146, Variance: 0.08288270980119705

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.7136442022435072, Training Loss Force: 3.5170883350855293, time: 2.6544313430786133
Validation Loss Energy: 1.4142032905830535, Validation Loss Force: 3.555679861498757, time: 0.15331411361694336
Test Loss Energy: 10.226920103662897, Test Loss Force: 13.052768768157629, time: 10.43123722076416

Epoch 8, Batch 100/162, Loss: 0.6292254328727722, Variance: 0.08509954810142517

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6628871113054913, Training Loss Force: 3.4874359398821233, time: 2.7115864753723145
Validation Loss Energy: 1.9554825920062437, Validation Loss Force: 3.6095588178467093, time: 0.15295696258544922
Test Loss Energy: 10.28578403306701, Test Loss Force: 12.998380987552077, time: 10.597877740859985

Epoch 9, Batch 100/162, Loss: 0.2919920086860657, Variance: 0.08213721215724945

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6815405901714293, Training Loss Force: 3.5134610148378393, time: 2.6311705112457275
Validation Loss Energy: 1.63202129227461, Validation Loss Force: 3.5830117569309063, time: 0.14989113807678223
Test Loss Energy: 10.449618432072679, Test Loss Force: 13.18362566146935, time: 10.466089725494385

Epoch 10, Batch 100/162, Loss: 0.6417890191078186, Variance: 0.08240518718957901

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6535366349292158, Training Loss Force: 3.5092481489594074, time: 2.658799409866333
Validation Loss Energy: 2.0919746219692534, Validation Loss Force: 3.56027063128035, time: 0.15641546249389648
Test Loss Energy: 10.446384399386906, Test Loss Force: 12.89809099075533, time: 11.355845928192139

Epoch 11, Batch 100/162, Loss: 0.5514034628868103, Variance: 0.08153609931468964

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6687452705689272, Training Loss Force: 3.5068594725689723, time: 2.859161138534546
Validation Loss Energy: 1.4668274691821785, Validation Loss Force: 3.6083880977004696, time: 0.1595005989074707
Test Loss Energy: 10.226063911651627, Test Loss Force: 12.948995717626694, time: 10.511390686035156

Epoch 12, Batch 100/162, Loss: 0.6607254147529602, Variance: 0.08052065968513489

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6674684128376294, Training Loss Force: 3.501530593054653, time: 2.643266439437866
Validation Loss Energy: 2.086356234079204, Validation Loss Force: 3.743624422126544, time: 0.1500248908996582
Test Loss Energy: 10.680170690638885, Test Loss Force: 13.689909230692185, time: 10.537482023239136

Epoch 13, Batch 100/162, Loss: 0.2577003240585327, Variance: 0.08037307858467102

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6582006803847058, Training Loss Force: 3.499515053798256, time: 2.6663107872009277
Validation Loss Energy: 1.621218906886691, Validation Loss Force: 3.5525901786529115, time: 0.15298748016357422
Test Loss Energy: 10.417395585922685, Test Loss Force: 13.10501652806298, time: 10.854727029800415

Epoch 14, Batch 100/162, Loss: 0.8798909783363342, Variance: 0.08048957586288452

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6982051036941357, Training Loss Force: 3.5282325121144433, time: 2.624335289001465
Validation Loss Energy: 2.0195726358251034, Validation Loss Force: 3.5733552640794564, time: 0.15064144134521484
Test Loss Energy: 10.329335450643962, Test Loss Force: 12.870418042357679, time: 10.519079208374023

Epoch 15, Batch 100/162, Loss: 0.4504919648170471, Variance: 0.0789373442530632

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.661978183411569, Training Loss Force: 3.510378994834516, time: 2.610790967941284
Validation Loss Energy: 1.4192472662996585, Validation Loss Force: 3.5624702313268735, time: 0.15650248527526855
Test Loss Energy: 10.236522086874261, Test Loss Force: 13.049646135330015, time: 10.819137573242188

Epoch 16, Batch 100/162, Loss: 0.6458350419998169, Variance: 0.08460552990436554

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6786905089481086, Training Loss Force: 3.505046116897076, time: 2.6762890815734863
Validation Loss Energy: 1.9117921593224116, Validation Loss Force: 3.639380673891672, time: 0.1787881851196289
Test Loss Energy: 10.42830932123336, Test Loss Force: 13.072658126532826, time: 10.52759838104248

Epoch 17, Batch 100/162, Loss: 0.7107188105583191, Variance: 0.08332426846027374

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.688441109065483, Training Loss Force: 3.529878092082886, time: 2.7024683952331543
Validation Loss Energy: 1.5731944048349964, Validation Loss Force: 3.8667754528804363, time: 0.15503191947937012
Test Loss Energy: 10.621957794521956, Test Loss Force: 13.665035002629052, time: 10.698946475982666

Epoch 18, Batch 100/162, Loss: 1.6267966032028198, Variance: 0.1775570809841156

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.686383970927257, Training Loss Force: 4.63820401522906, time: 2.6173906326293945
Validation Loss Energy: 2.89295035048132, Validation Loss Force: 4.21606214282592, time: 0.16221237182617188
Test Loss Energy: 10.510406651914602, Test Loss Force: 13.108865579924649, time: 10.544176578521729

Epoch 19, Batch 100/162, Loss: 0.5451777577400208, Variance: 0.11166974902153015

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6575039314606066, Training Loss Force: 3.6449115101978435, time: 2.660489082336426
Validation Loss Energy: 2.600220139140521, Validation Loss Force: 3.5940749250002315, time: 0.15515875816345215
Test Loss Energy: 10.165216865186125, Test Loss Force: 12.183743496959835, time: 10.590704917907715

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–ˆâ–…â–†â–…â–â–ƒâ–ƒâ–„â–…â–…â–ƒâ–‡â–…â–„â–ƒâ–…â–†â–…â–ƒ
wandb:   test_error_force â–ˆâ–‡â–…â–†â–„â–ƒâ–ƒâ–…â–„â–…â–„â–„â–‡â–…â–„â–…â–…â–‡â–…â–
wandb:          test_loss â–…â–‡â–†â–ˆâ–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–…â–
wandb: train_error_energy â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ƒ
wandb:  train_error_force â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–‚
wandb:         train_loss â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ƒ
wandb: valid_error_energy â–‚â–ƒâ–„â–â–„â–‚â–ƒâ–â–„â–‚â–„â–â–„â–‚â–„â–â–ƒâ–‚â–ˆâ–‡
wandb:  valid_error_force â–‚â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–ƒâ–â–â–â–‚â–„â–ˆâ–‚
wandb:         valid_loss â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–â–ƒâ–‚â–ƒâ–â–„â–‚â–ƒâ–â–ƒâ–‚â–ˆâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 5155
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.16522
wandb:   test_error_force 12.18374
wandb:          test_loss 12.19466
wandb: train_error_energy 2.6575
wandb:  train_error_force 3.64491
wandb:         train_loss 1.08484
wandb: valid_error_energy 2.60022
wandb:  valid_error_force 3.59407
wandb:         valid_loss 1.0001
wandb: 
wandb: ğŸš€ View run al_71_48 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/9ti1m5eh
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_182753-9ti1m5eh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.7690794467926025, Uncertainty Bias: -0.0812125951051712
1.9073486e-05 0.01935339
2.3016956 4.5482817
(48745, 22, 3)
Found uncertainty sample 0 after 5 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 389 steps.
Found uncertainty sample 3 after 17 steps.
Found uncertainty sample 4 after 20 steps.
Found uncertainty sample 5 after 290 steps.
Found uncertainty sample 6 after 53 steps.
Found uncertainty sample 7 after 399 steps.
Found uncertainty sample 8 after 1273 steps.
Found uncertainty sample 9 after 3109 steps.
Found uncertainty sample 10 after 687 steps.
Found uncertainty sample 11 after 431 steps.
Found uncertainty sample 12 after 1033 steps.
Found uncertainty sample 13 after 459 steps.
Found uncertainty sample 14 after 12 steps.
Found uncertainty sample 15 after 1525 steps.
Found uncertainty sample 16 after 1387 steps.
Found uncertainty sample 17 after 900 steps.
Found uncertainty sample 18 after 919 steps.
Found uncertainty sample 19 after 99 steps.
Found uncertainty sample 20 after 880 steps.
Found uncertainty sample 21 after 548 steps.
Found uncertainty sample 22 after 3548 steps.
Found uncertainty sample 23 after 507 steps.
Found uncertainty sample 24 after 596 steps.
Found uncertainty sample 25 after 1109 steps.
Found uncertainty sample 26 after 104 steps.
Found uncertainty sample 27 after 683 steps.
Found uncertainty sample 28 after 830 steps.
Found uncertainty sample 29 after 825 steps.
Found uncertainty sample 30 after 410 steps.
Found uncertainty sample 31 after 225 steps.
Found uncertainty sample 32 after 382 steps.
Found uncertainty sample 33 after 198 steps.
Found uncertainty sample 34 after 245 steps.
Found uncertainty sample 35 after 263 steps.
Found uncertainty sample 36 after 180 steps.
Found uncertainty sample 37 after 265 steps.
Found uncertainty sample 38 after 47 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 1554 steps.
Found uncertainty sample 41 after 2784 steps.
Found uncertainty sample 42 after 3817 steps.
Found uncertainty sample 43 after 2730 steps.
Found uncertainty sample 44 after 1826 steps.
Found uncertainty sample 45 after 3137 steps.
Found uncertainty sample 46 after 3147 steps.
Found uncertainty sample 47 after 2367 steps.
Found uncertainty sample 48 after 1168 steps.
Found uncertainty sample 49 after 206 steps.
Found uncertainty sample 50 after 303 steps.
Found uncertainty sample 51 after 1949 steps.
Found uncertainty sample 52 after 218 steps.
Found uncertainty sample 53 after 670 steps.
Found uncertainty sample 54 after 366 steps.
Found uncertainty sample 55 after 403 steps.
Found uncertainty sample 56 after 129 steps.
Found uncertainty sample 57 after 172 steps.
Found uncertainty sample 58 after 686 steps.
Found uncertainty sample 59 after 3 steps.
Found uncertainty sample 60 after 2383 steps.
Found uncertainty sample 61 after 2544 steps.
Found uncertainty sample 62 after 3005 steps.
Found uncertainty sample 63 after 13 steps.
Found uncertainty sample 64 after 2417 steps.
Found uncertainty sample 65 after 2055 steps.
Found uncertainty sample 66 after 1956 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 376 steps.
Found uncertainty sample 69 after 101 steps.
Found uncertainty sample 70 after 161 steps.
Found uncertainty sample 71 after 417 steps.
Found uncertainty sample 72 after 1419 steps.
Found uncertainty sample 73 after 387 steps.
Found uncertainty sample 74 after 771 steps.
Found uncertainty sample 75 after 3767 steps.
Found uncertainty sample 76 after 282 steps.
Found uncertainty sample 77 after 758 steps.
Found uncertainty sample 78 after 490 steps.
Found uncertainty sample 79 after 22 steps.
Found uncertainty sample 80 after 2145 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 817 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2256 steps.
Found uncertainty sample 86 after 549 steps.
Found uncertainty sample 87 after 34 steps.
Found uncertainty sample 88 after 36 steps.
Found uncertainty sample 89 after 72 steps.
Found uncertainty sample 90 after 1735 steps.
Found uncertainty sample 91 after 315 steps.
Found uncertainty sample 92 after 286 steps.
Found uncertainty sample 93 after 1486 steps.
Found uncertainty sample 94 after 2321 steps.
Found uncertainty sample 95 after 190 steps.
Found uncertainty sample 96 after 2796 steps.
Found uncertainty sample 97 after 609 steps.
Found uncertainty sample 98 after 1045 steps.
Found uncertainty sample 99 after 117 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_184539-gjqzn6v1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_49
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/gjqzn6v1
Training model 49. Added 97 samples to the dataset.
Epoch 0, Batch 100/164, Loss: 0.6457189321517944, Variance: 0.07953906059265137

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.8106448296856286, Training Loss Force: 3.8947670984362084, time: 2.6679635047912598
Validation Loss Energy: 2.508894021845309, Validation Loss Force: 3.695102143199128, time: 0.1576523780822754
Test Loss Energy: 9.895678960424053, Test Loss Force: 12.204681452141294, time: 10.446927547454834

Epoch 1, Batch 100/164, Loss: 1.6523171663284302, Variance: 0.13384252786636353

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.1308340330306255, Training Loss Force: 3.5433495376295046, time: 2.73195219039917
Validation Loss Energy: 1.8264022530115371, Validation Loss Force: 3.5735607134780984, time: 0.15480756759643555
Test Loss Energy: 9.87173041579863, Test Loss Force: 11.765564428968693, time: 10.314717292785645

Epoch 2, Batch 100/164, Loss: 1.8730144500732422, Variance: 0.14801979064941406

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.104242903311141, Training Loss Force: 3.492964987023941, time: 2.8346076011657715
Validation Loss Energy: 2.337564587548361, Validation Loss Force: 3.5286942908115737, time: 0.15024566650390625
Test Loss Energy: 10.088738006333042, Test Loss Force: 11.741209306804103, time: 10.392801523208618

Epoch 3, Batch 100/164, Loss: 1.6256966590881348, Variance: 0.14926186203956604

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.181994261110201, Training Loss Force: 3.500819414404181, time: 2.6527137756347656
Validation Loss Energy: 1.9622779347786192, Validation Loss Force: 3.6289475110592253, time: 0.15108203887939453
Test Loss Energy: 10.106221805805392, Test Loss Force: 11.804176275456317, time: 11.344494819641113

Epoch 4, Batch 100/164, Loss: 1.712650179862976, Variance: 0.15674711763858795

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.198287994399145, Training Loss Force: 3.5010940279170906, time: 2.6764845848083496
Validation Loss Energy: 2.7112434948614563, Validation Loss Force: 3.5860134112410615, time: 0.15586352348327637
Test Loss Energy: 10.24089780839059, Test Loss Force: 11.62816696175035, time: 10.745275735855103

Epoch 5, Batch 100/164, Loss: 1.6554994583129883, Variance: 0.1522149294614792

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.161826414612616, Training Loss Force: 3.5076905986497238, time: 2.7030465602874756
Validation Loss Energy: 1.905851086728388, Validation Loss Force: 3.5449459468732107, time: 0.16670989990234375
Test Loss Energy: 10.005246962653878, Test Loss Force: 11.581321966726907, time: 10.46463131904602

Epoch 6, Batch 100/164, Loss: 1.7446179389953613, Variance: 0.15687841176986694

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.159103385480633, Training Loss Force: 3.494486728333291, time: 2.6492230892181396
Validation Loss Energy: 2.5025268718405527, Validation Loss Force: 3.5298638233890345, time: 0.15102362632751465
Test Loss Energy: 10.307362519945773, Test Loss Force: 11.718176718122397, time: 10.539286851882935

Epoch 7, Batch 100/164, Loss: 2.081939935684204, Variance: 0.15475863218307495

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.131482551487187, Training Loss Force: 3.4889069287346834, time: 2.694049596786499
Validation Loss Energy: 2.1151710318001835, Validation Loss Force: 3.753398113171801, time: 0.15145254135131836
Test Loss Energy: 10.184824573905285, Test Loss Force: 11.747046614604622, time: 10.40564513206482

Epoch 8, Batch 100/164, Loss: 1.654907464981079, Variance: 0.16362372040748596

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.199526615283795, Training Loss Force: 3.5117046010338155, time: 2.640455484390259
Validation Loss Energy: 2.49112673113039, Validation Loss Force: 3.539130496789015, time: 0.15574121475219727
Test Loss Energy: 10.245072628710172, Test Loss Force: 11.49187221742784, time: 10.609503030776978

Epoch 9, Batch 100/164, Loss: 1.6678895950317383, Variance: 0.1552274078130722

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.127132379790984, Training Loss Force: 3.5165458410188495, time: 2.631373167037964
Validation Loss Energy: 2.21079147202071, Validation Loss Force: 3.573215947417154, time: 0.15073919296264648
Test Loss Energy: 10.32405539394856, Test Loss Force: 11.841469715418008, time: 10.330167770385742

Epoch 10, Batch 100/164, Loss: 1.7071807384490967, Variance: 0.1626567244529724

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.141095542390859, Training Loss Force: 3.5200474789826846, time: 2.6250181198120117
Validation Loss Energy: 2.2653351699923054, Validation Loss Force: 3.643388644295323, time: 0.15767526626586914
Test Loss Energy: 9.939773494861486, Test Loss Force: 11.645850230808403, time: 10.438524723052979

Epoch 11, Batch 100/164, Loss: 1.7627229690551758, Variance: 0.15570884943008423

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.112653167868034, Training Loss Force: 3.5000523735060085, time: 2.847238302230835
Validation Loss Energy: 1.7503794809969797, Validation Loss Force: 3.5876798671291854, time: 0.15682625770568848
Test Loss Energy: 10.32174915714287, Test Loss Force: 11.892674245892744, time: 10.44137167930603

Epoch 12, Batch 100/164, Loss: 1.4533509016036987, Variance: 0.1610962152481079

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.153245553583082, Training Loss Force: 3.5101519915352415, time: 2.608081102371216
Validation Loss Energy: 2.220119602104966, Validation Loss Force: 3.574350892053948, time: 0.1540212631225586
Test Loss Energy: 10.098533271026302, Test Loss Force: 11.784169496995373, time: 10.395858764648438

Epoch 13, Batch 100/164, Loss: 1.5329992771148682, Variance: 0.15435928106307983

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.1232122955867485, Training Loss Force: 3.526013796917418, time: 2.7354469299316406
Validation Loss Energy: 1.9451392019511304, Validation Loss Force: 3.6235736881144525, time: 0.15148568153381348
Test Loss Energy: 10.383944207643609, Test Loss Force: 11.84067756351943, time: 10.642247676849365

Epoch 14, Batch 100/164, Loss: 1.6554491519927979, Variance: 0.16095644235610962

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.150713992082954, Training Loss Force: 3.5223926467204603, time: 2.6774163246154785
Validation Loss Energy: 2.3984412186992885, Validation Loss Force: 3.730946828526194, time: 0.1580657958984375
Test Loss Energy: 10.395639271500238, Test Loss Force: 11.848122188148201, time: 10.45855164527893

Epoch 15, Batch 100/164, Loss: 1.544555425643921, Variance: 0.15503308176994324

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.126680959508645, Training Loss Force: 3.529795471202508, time: 2.658447504043579
Validation Loss Energy: 1.8879330861132007, Validation Loss Force: 3.6198326980167685, time: 0.16115331649780273
Test Loss Energy: 10.286676150588232, Test Loss Force: 11.921947777417149, time: 10.600262641906738

Epoch 16, Batch 100/164, Loss: 1.6866410970687866, Variance: 0.16053825616836548

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.11844203207085, Training Loss Force: 3.517849185985945, time: 2.660191774368286
Validation Loss Energy: 2.2301816496941993, Validation Loss Force: 3.6469736175447607, time: 0.1542985439300537
Test Loss Energy: 10.24799284479187, Test Loss Force: 12.01338909821031, time: 10.546674251556396

Epoch 17, Batch 100/164, Loss: 1.7721396684646606, Variance: 0.16190779209136963

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.127552476362767, Training Loss Force: 3.5246967482936773, time: 2.7126076221466064
Validation Loss Energy: 2.008190976654053, Validation Loss Force: 3.5919725571474292, time: 0.16221308708190918
Test Loss Energy: 10.296215147576474, Test Loss Force: 12.082282176517399, time: 10.624687433242798

Epoch 18, Batch 100/164, Loss: 1.69853937625885, Variance: 0.16195634007453918

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.094544542550433, Training Loss Force: 3.505460064789939, time: 2.701429605484009
Validation Loss Energy: 1.804586688777554, Validation Loss Force: 4.2232957727005305, time: 0.15058565139770508
Test Loss Energy: 10.052671503676397, Test Loss Force: 12.354558081719768, time: 10.532843828201294

Epoch 19, Batch 100/164, Loss: 1.5695254802703857, Variance: 0.15640968084335327

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.159969922149222, Training Loss Force: 3.5180552852888423, time: 2.618319272994995
Validation Loss Energy: 2.247132709543195, Validation Loss Force: 3.553435727623048, time: 0.15371155738830566
Test Loss Energy: 10.422500179331097, Test Loss Force: 12.073683183740629, time: 10.582224607467651

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–„â–„â–†â–ƒâ–‡â–…â–†â–‡â–‚â–‡â–„â–ˆâ–ˆâ–†â–†â–†â–ƒâ–ˆ
wandb:   test_error_force â–‡â–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–ƒâ–â–„â–‚â–„â–ƒâ–„â–„â–„â–…â–†â–ˆâ–†
wandb:          test_loss â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb: train_error_energy â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:  train_error_force â–ˆâ–‚â–â–â–â–â–â–â–â–â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–‚â–…â–ƒâ–ˆâ–‚â–†â–„â–†â–„â–…â–â–„â–‚â–†â–‚â–„â–ƒâ–â–…
wandb:  valid_error_force â–ƒâ–â–â–‚â–‚â–â–â–ƒâ–â–â–‚â–‚â–â–‚â–ƒâ–‚â–‚â–‚â–ˆâ–
wandb:         valid_loss â–„â–â–ƒâ–ƒâ–†â–‚â–…â–†â–…â–„â–…â–‚â–„â–ƒâ–†â–ƒâ–…â–ƒâ–ˆâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 5242
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.4225
wandb:   test_error_force 12.07368
wandb:          test_loss 9.47762
wandb: train_error_energy 4.15997
wandb:  train_error_force 3.51806
wandb:         train_loss 1.45478
wandb: valid_error_energy 2.24713
wandb:  valid_error_force 3.55344
wandb:         valid_loss 1.01417
wandb: 
wandb: ğŸš€ View run al_71_49 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/gjqzn6v1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_184539-gjqzn6v1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1255123615264893, Uncertainty Bias: -0.25408345460891724
7.247925e-05 0.0018377304
2.1535094 4.373509
(48745, 22, 3)
Found uncertainty sample 0 after 247 steps.
Found uncertainty sample 1 after 1023 steps.
Found uncertainty sample 2 after 3232 steps.
Found uncertainty sample 3 after 38 steps.
Found uncertainty sample 4 after 3514 steps.
Found uncertainty sample 5 after 1484 steps.
Found uncertainty sample 6 after 2044 steps.
Found uncertainty sample 7 after 264 steps.
Found uncertainty sample 8 after 504 steps.
Found uncertainty sample 9 after 602 steps.
Found uncertainty sample 10 after 2243 steps.
Found uncertainty sample 11 after 55 steps.
Found uncertainty sample 12 after 2131 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2719 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 2062 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 90 steps.
Found uncertainty sample 19 after 3788 steps.
Found uncertainty sample 20 after 409 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 127 steps.
Found uncertainty sample 23 after 1255 steps.
Found uncertainty sample 24 after 965 steps.
Found uncertainty sample 25 after 178 steps.
Found uncertainty sample 26 after 3531 steps.
Found uncertainty sample 27 after 182 steps.
Found uncertainty sample 28 after 405 steps.
Found uncertainty sample 29 after 2588 steps.
Found uncertainty sample 30 after 192 steps.
Found uncertainty sample 31 after 210 steps.
Found uncertainty sample 32 after 37 steps.
Found uncertainty sample 33 after 907 steps.
Found uncertainty sample 34 after 277 steps.
Found uncertainty sample 35 after 689 steps.
Found uncertainty sample 36 after 643 steps.
Found uncertainty sample 37 after 18 steps.
Found uncertainty sample 38 after 2293 steps.
Found uncertainty sample 39 after 2436 steps.
Found uncertainty sample 40 after 168 steps.
Found uncertainty sample 41 after 433 steps.
Found uncertainty sample 42 after 141 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 2610 steps.
Found uncertainty sample 45 after 872 steps.
Found uncertainty sample 46 after 1651 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 3218 steps.
Found uncertainty sample 49 after 498 steps.
Found uncertainty sample 50 after 234 steps.
Found uncertainty sample 51 after 1218 steps.
Found uncertainty sample 52 after 914 steps.
Found uncertainty sample 53 after 23 steps.
Found uncertainty sample 54 after 900 steps.
Found uncertainty sample 55 after 3279 steps.
Found uncertainty sample 56 after 2066 steps.
Found uncertainty sample 57 after 357 steps.
Found uncertainty sample 58 after 1055 steps.
Found uncertainty sample 59 after 90 steps.
Found uncertainty sample 60 after 632 steps.
Found uncertainty sample 61 after 466 steps.
Found uncertainty sample 62 after 2474 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 38 steps.
Found uncertainty sample 65 after 1104 steps.
Found uncertainty sample 66 after 2758 steps.
Found uncertainty sample 67 after 456 steps.
Found uncertainty sample 68 after 492 steps.
Found uncertainty sample 69 after 1383 steps.
Found uncertainty sample 70 after 1694 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 2580 steps.
Found uncertainty sample 73 after 2584 steps.
Found uncertainty sample 74 after 7 steps.
Found uncertainty sample 75 after 837 steps.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 2408 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 962 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2785 steps.
Found uncertainty sample 86 after 3067 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1802 steps.
Found uncertainty sample 89 after 1648 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 2756 steps.
Found uncertainty sample 92 after 3629 steps.
Found uncertainty sample 93 after 381 steps.
Found uncertainty sample 94 after 35 steps.
Found uncertainty sample 95 after 1574 steps.
Found uncertainty sample 96 after 1142 steps.
Found uncertainty sample 97 after 262 steps.
Found uncertainty sample 98 after 180 steps.
Found uncertainty sample 99 after 116 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_190723-v1po9y4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_50
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/v1po9y4s
Training model 50. Added 89 samples to the dataset.
Epoch 0, Batch 100/167, Loss: 0.8070664405822754, Variance: 0.1297762542963028

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.996393015352259, Training Loss Force: 3.565237957699076, time: 2.6818997859954834
Validation Loss Energy: 3.581129534640379, Validation Loss Force: 3.534654030502331, time: 0.15936636924743652
Test Loss Energy: 11.34525654601469, Test Loss Force: 13.09927666774189, time: 10.808588027954102

Epoch 1, Batch 100/167, Loss: 1.1712359189987183, Variance: 0.12635275721549988

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6871978500193485, Training Loss Force: 3.4720399853398782, time: 2.688002347946167
Validation Loss Energy: 2.2826056858426593, Validation Loss Force: 3.523565181258176, time: 0.18132972717285156
Test Loss Energy: 10.796504029710714, Test Loss Force: 13.158560744679193, time: 10.930609464645386

Epoch 2, Batch 100/167, Loss: 0.7755405902862549, Variance: 0.11857359856367111

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6518789587688443, Training Loss Force: 3.4757068061672967, time: 2.71075177192688
Validation Loss Energy: 2.2879095906756106, Validation Loss Force: 3.547056850376606, time: 0.1680004596710205
Test Loss Energy: 10.289464854308603, Test Loss Force: 12.260337882628022, time: 10.76695990562439

Epoch 3, Batch 100/167, Loss: 0.8159102201461792, Variance: 0.12311145663261414

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6372958850634522, Training Loss Force: 3.481464455607924, time: 2.7431323528289795
Validation Loss Energy: 4.162124995607832, Validation Loss Force: 3.5416312197438806, time: 0.16106367111206055
Test Loss Energy: 10.806829909149686, Test Loss Force: 12.467163500945313, time: 10.86123275756836

Epoch 4, Batch 100/167, Loss: 1.305155873298645, Variance: 0.11506757140159607

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.664709176957448, Training Loss Force: 3.4936663354670983, time: 2.920825719833374
Validation Loss Energy: 2.842967510402714, Validation Loss Force: 3.534384452389661, time: 0.18251609802246094
Test Loss Energy: 10.442319994984338, Test Loss Force: 12.27567975413985, time: 10.80005168914795

Epoch 5, Batch 100/167, Loss: 0.6733822822570801, Variance: 0.11704662442207336

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6647042342357476, Training Loss Force: 3.4723282685497057, time: 2.792863368988037
Validation Loss Energy: 1.7110773376180444, Validation Loss Force: 3.553815452739904, time: 0.15968537330627441
Test Loss Energy: 10.280434638831697, Test Loss Force: 12.646442376472605, time: 11.867645025253296

Epoch 6, Batch 100/167, Loss: 0.8676855564117432, Variance: 0.11736129224300385

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6563733554773923, Training Loss Force: 3.4829667310013486, time: 2.9471328258514404
Validation Loss Energy: 3.212497720411932, Validation Loss Force: 3.5754849460141576, time: 0.17721796035766602
Test Loss Energy: 10.558759618353003, Test Loss Force: 12.365674667456574, time: 12.318719625473022

Epoch 7, Batch 100/167, Loss: 1.2716103792190552, Variance: 0.12185284495353699

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.667429970584866, Training Loss Force: 3.483359890045559, time: 3.075666666030884
Validation Loss Energy: 2.0735964013409576, Validation Loss Force: 3.594613635887279, time: 0.17598676681518555
Test Loss Energy: 10.576517135330246, Test Loss Force: 12.846813984388112, time: 12.018073558807373

Epoch 8, Batch 100/167, Loss: 0.953725278377533, Variance: 0.12575674057006836

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.7017738529793465, Training Loss Force: 3.4770461544133116, time: 2.890876531600952
Validation Loss Energy: 2.2416963907914655, Validation Loss Force: 3.5290421742405615, time: 0.1712663173675537
Test Loss Energy: 10.276165043062445, Test Loss Force: 12.397687341912183, time: 12.391444683074951

Epoch 9, Batch 100/167, Loss: 0.6260653138160706, Variance: 0.11456877738237381

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6348637239830843, Training Loss Force: 3.4707564430485562, time: 2.94736909866333
Validation Loss Energy: 3.874098744197807, Validation Loss Force: 3.5146306560008718, time: 0.18307852745056152
Test Loss Energy: 10.92771979107402, Test Loss Force: 12.490589423598141, time: 12.1263587474823

Epoch 10, Batch 100/167, Loss: 1.52737557888031, Variance: 0.12267576158046722

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6815755374189325, Training Loss Force: 3.4783411354986633, time: 3.0892560482025146
Validation Loss Energy: 2.7285595836636767, Validation Loss Force: 3.539402153147046, time: 0.17221546173095703
Test Loss Energy: 10.721150827509685, Test Loss Force: 12.851659624072068, time: 12.281476020812988

Epoch 11, Batch 100/167, Loss: 0.967357873916626, Variance: 0.1184285432100296

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.686831216603815, Training Loss Force: 3.4685828161801147, time: 2.9554941654205322
Validation Loss Energy: 1.7203854645906906, Validation Loss Force: 3.569518377802926, time: 0.1778116226196289
Test Loss Energy: 10.491102164945072, Test Loss Force: 12.934624124003733, time: 12.02413272857666

Epoch 12, Batch 100/167, Loss: 0.9353675246238708, Variance: 0.11749053001403809

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6807781898743737, Training Loss Force: 3.4861923909167567, time: 2.9584085941314697
Validation Loss Energy: 3.487483427513745, Validation Loss Force: 3.585064395341117, time: 0.1827707290649414
Test Loss Energy: 10.992573170761478, Test Loss Force: 12.598367408382568, time: 12.167747735977173

Epoch 13, Batch 100/167, Loss: 1.5008755922317505, Variance: 0.12212590128183365

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.663707689139933, Training Loss Force: 3.4819198793454076, time: 3.0041449069976807
Validation Loss Energy: 2.179856567138303, Validation Loss Force: 3.5158751538574626, time: 0.1908576488494873
Test Loss Energy: 10.36214640914321, Test Loss Force: 12.51338586174123, time: 12.034093856811523

Epoch 14, Batch 100/167, Loss: 0.9858819842338562, Variance: 0.11713950335979462

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6687827683139638, Training Loss Force: 3.4653754578018146, time: 3.0012893676757812
Validation Loss Energy: 2.3006386950717186, Validation Loss Force: 3.5412405838388747, time: 0.16118454933166504
Test Loss Energy: 10.46202412527272, Test Loss Force: 12.49676344445312, time: 12.259609937667847

Epoch 15, Batch 100/167, Loss: 0.9218844175338745, Variance: 0.11801886558532715

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6577244591876292, Training Loss Force: 3.4809773288507837, time: 3.0273725986480713
Validation Loss Energy: 3.852786931794961, Validation Loss Force: 3.559696590222148, time: 0.18100738525390625
Test Loss Energy: 10.997772437028539, Test Loss Force: 12.664172692839832, time: 12.781957149505615

Epoch 16, Batch 100/167, Loss: 1.4992172718048096, Variance: 0.11642982065677643

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.651869880001092, Training Loss Force: 3.4721138301620877, time: 2.9143481254577637
Validation Loss Energy: 2.582056348899753, Validation Loss Force: 3.6579260448255106, time: 0.24538111686706543
Test Loss Energy: 10.452918440110835, Test Loss Force: 12.39660620509963, time: 12.14344310760498

Epoch 17, Batch 100/167, Loss: 0.8368578553199768, Variance: 0.11416119337081909

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6821880549055424, Training Loss Force: 3.4784240844222403, time: 2.8761229515075684
Validation Loss Energy: 1.6934549674056003, Validation Loss Force: 3.5820228090307924, time: 0.17043828964233398
Test Loss Energy: 10.349777867882134, Test Loss Force: 12.651255891177586, time: 12.11569857597351

Epoch 18, Batch 100/167, Loss: 0.742423951625824, Variance: 0.11805637925863266

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.639056313997898, Training Loss Force: 3.469673760326995, time: 3.059755325317383
Validation Loss Energy: 3.540173182176346, Validation Loss Force: 3.5316327813073336, time: 0.24382638931274414
Test Loss Energy: 10.80345555992777, Test Loss Force: 12.652218404791517, time: 12.23506760597229

Epoch 19, Batch 100/167, Loss: 1.3467768430709839, Variance: 0.12361697852611542

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.654699920178224, Training Loss Force: 3.473370884158482, time: 2.907254934310913
Validation Loss Energy: 2.0039099396544455, Validation Loss Force: 3.556680536640556, time: 0.19474482536315918
Test Loss Energy: 10.326364046306452, Test Loss Force: 12.547702795793775, time: 12.001160383224487

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.039 MB of 0.056 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–„â–â–„â–‚â–â–ƒâ–ƒâ–â–…â–„â–‚â–†â–‚â–‚â–†â–‚â–â–„â–
wandb:   test_error_force â–ˆâ–ˆâ–â–ƒâ–â–„â–‚â–†â–‚â–ƒâ–†â–†â–„â–ƒâ–ƒâ–„â–‚â–„â–„â–ƒ
wandb:          test_loss â–ˆâ–†â–â–„â–‚â–ƒâ–…â–‡â–â–…â–„â–…â–‡â–„â–‚â–‚â–ƒâ–†â–†â–ƒ
wandb: train_error_energy â–ˆâ–‚â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–
wandb:  train_error_force â–ˆâ–â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–
wandb: valid_error_energy â–†â–ƒâ–ƒâ–ˆâ–„â–â–…â–‚â–ƒâ–‡â–„â–â–†â–‚â–ƒâ–‡â–„â–â–†â–‚
wandb:  valid_error_force â–‚â–â–ƒâ–‚â–‚â–ƒâ–„â–…â–‚â–â–‚â–„â–„â–â–‚â–ƒâ–ˆâ–„â–‚â–ƒ
wandb:         valid_loss â–†â–‚â–‚â–ˆâ–„â–â–…â–‚â–‚â–‡â–ƒâ–â–†â–‚â–‚â–‡â–ƒâ–â–†â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 5322
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.32636
wandb:   test_error_force 12.5477
wandb:          test_loss 12.49588
wandb: train_error_energy 2.6547
wandb:  train_error_force 3.47337
wandb:         train_loss 1.00032
wandb: valid_error_energy 2.00391
wandb:  valid_error_force 3.55668
wandb:         valid_loss 0.81133
wandb: 
wandb: ğŸš€ View run al_71_50 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/v1po9y4s
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_190723-v1po9y4s/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.202648639678955, Uncertainty Bias: -0.13534648716449738
6.1035156e-05 0.013442993
2.1798506 4.6167135
(48745, 22, 3)
Found uncertainty sample 0 after 414 steps.
Found uncertainty sample 1 after 1420 steps.
Found uncertainty sample 2 after 552 steps.
Found uncertainty sample 3 after 1080 steps.
Found uncertainty sample 4 after 5 steps.
Found uncertainty sample 5 after 1221 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 994 steps.
Found uncertainty sample 8 after 2116 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 16 steps.
Found uncertainty sample 11 after 276 steps.
Found uncertainty sample 12 after 2569 steps.
Found uncertainty sample 13 after 175 steps.
Found uncertainty sample 14 after 1319 steps.
Found uncertainty sample 15 after 410 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 790 steps.
Found uncertainty sample 18 after 87 steps.
Found uncertainty sample 19 after 22 steps.
Found uncertainty sample 20 after 1294 steps.
Found uncertainty sample 21 after 16 steps.
Found uncertainty sample 22 after 1572 steps.
Found uncertainty sample 23 after 369 steps.
Found uncertainty sample 24 after 2461 steps.
Found uncertainty sample 25 after 6 steps.
Found uncertainty sample 26 after 1277 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 104 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1329 steps.
Found uncertainty sample 32 after 568 steps.
Found uncertainty sample 33 after 16 steps.
Found uncertainty sample 34 after 1435 steps.
Found uncertainty sample 35 after 216 steps.
Found uncertainty sample 36 after 592 steps.
Found uncertainty sample 37 after 3347 steps.
Found uncertainty sample 38 after 808 steps.
Found uncertainty sample 39 after 1950 steps.
Found uncertainty sample 40 after 141 steps.
Found uncertainty sample 41 after 2760 steps.
Found uncertainty sample 42 after 1642 steps.
Found uncertainty sample 43 after 920 steps.
Found uncertainty sample 44 after 157 steps.
Found uncertainty sample 45 after 287 steps.
Found uncertainty sample 46 after 1271 steps.
Found uncertainty sample 47 after 2059 steps.
Found uncertainty sample 48 after 66 steps.
Found uncertainty sample 49 after 15 steps.
Found uncertainty sample 50 after 518 steps.
Found uncertainty sample 51 after 18 steps.
Found uncertainty sample 52 after 144 steps.
Found uncertainty sample 53 after 1683 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 16 steps.
Found uncertainty sample 56 after 115 steps.
Found uncertainty sample 57 after 218 steps.
Found uncertainty sample 58 after 14 steps.
Found uncertainty sample 59 after 796 steps.
Found uncertainty sample 60 after 1871 steps.
Found uncertainty sample 61 after 248 steps.
Found uncertainty sample 62 after 14 steps.
Found uncertainty sample 63 after 1197 steps.
Found uncertainty sample 64 after 428 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 458 steps.
Found uncertainty sample 67 after 615 steps.
Found uncertainty sample 68 after 171 steps.
Found uncertainty sample 69 after 126 steps.
Found uncertainty sample 70 after 946 steps.
Found uncertainty sample 71 after 1762 steps.
Found uncertainty sample 72 after 549 steps.
Found uncertainty sample 73 after 307 steps.
Found uncertainty sample 74 after 143 steps.
Found uncertainty sample 75 after 2273 steps.
Found uncertainty sample 76 after 3141 steps.
Found uncertainty sample 77 after 25 steps.
Found uncertainty sample 78 after 1199 steps.
Did not find any uncertainty samples for sample 79.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 13 steps.
Found uncertainty sample 82 after 2967 steps.
Found uncertainty sample 83 after 661 steps.
Found uncertainty sample 84 after 1254 steps.
Found uncertainty sample 85 after 317 steps.
Found uncertainty sample 86 after 720 steps.
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 188 steps.
Found uncertainty sample 91 after 139 steps.
Found uncertainty sample 92 after 2098 steps.
Found uncertainty sample 93 after 47 steps.
Found uncertainty sample 94 after 286 steps.
Found uncertainty sample 95 after 3669 steps.
Found uncertainty sample 96 after 616 steps.
Found uncertainty sample 97 after 1646 steps.
Found uncertainty sample 98 after 2542 steps.
Found uncertainty sample 99 after 3055 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_192534-h7y0zlr4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_51
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/h7y0zlr4
Training model 51. Added 94 samples to the dataset.
Epoch 0, Batch 100/169, Loss: 1.417961597442627, Variance: 0.11312955617904663

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.2488705372064763, Training Loss Force: 3.599985856229067, time: 2.863311767578125
Validation Loss Energy: 2.186586742073971, Validation Loss Force: 3.491408564995112, time: 0.1587071418762207
Test Loss Energy: 10.304520328998098, Test Loss Force: 12.345453441120998, time: 10.59706974029541

Epoch 1, Batch 100/169, Loss: 0.5507329702377319, Variance: 0.11622445285320282

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6373507642583185, Training Loss Force: 3.4507597590270507, time: 2.7776522636413574
Validation Loss Energy: 2.38976395052858, Validation Loss Force: 3.519936490633088, time: 0.15670990943908691
Test Loss Energy: 10.592937019291737, Test Loss Force: 12.571498726950324, time: 10.748798131942749

Epoch 2, Batch 100/169, Loss: 0.9579455256462097, Variance: 0.12107095122337341

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.626371368977102, Training Loss Force: 3.456781743308293, time: 2.758293867111206
Validation Loss Energy: 3.3416018107822034, Validation Loss Force: 3.521517664238321, time: 0.163405179977417
Test Loss Energy: 10.940662543601126, Test Loss Force: 12.811139565957427, time: 10.624891757965088

Epoch 3, Batch 100/169, Loss: 1.5353590250015259, Variance: 0.11863715946674347

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.664375499900971, Training Loss Force: 3.458820362346103, time: 2.701125383377075
Validation Loss Energy: 1.7468898472535237, Validation Loss Force: 3.5525562587196817, time: 0.16232752799987793
Test Loss Energy: 10.264413176423268, Test Loss Force: 12.66165560350844, time: 10.57950210571289

Epoch 4, Batch 100/169, Loss: 0.7824203968048096, Variance: 0.12233990430831909

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6574469550840134, Training Loss Force: 3.4785839500286584, time: 2.8078246116638184
Validation Loss Energy: 2.527821047695268, Validation Loss Force: 3.542939326753158, time: 0.15958857536315918
Test Loss Energy: 10.33977674498056, Test Loss Force: 12.399592918815275, time: 10.777709245681763

Epoch 5, Batch 100/169, Loss: 0.9807586073875427, Variance: 0.11749939620494843

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.644147011322157, Training Loss Force: 3.46685588442337, time: 2.8134689331054688
Validation Loss Energy: 3.921960883330596, Validation Loss Force: 3.5549238255252837, time: 0.16153407096862793
Test Loss Energy: 10.982307600852652, Test Loss Force: 12.527431267215748, time: 10.626484870910645

Epoch 6, Batch 100/169, Loss: 0.801814079284668, Variance: 0.11573521792888641

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6593008845585344, Training Loss Force: 3.464492688872225, time: 2.690112352371216
Validation Loss Energy: 2.284151009715398, Validation Loss Force: 3.5505109683912863, time: 0.16495585441589355
Test Loss Energy: 10.40305408877444, Test Loss Force: 12.873847688139437, time: 10.711703538894653

Epoch 7, Batch 100/169, Loss: 0.6362037062644958, Variance: 0.1186302974820137

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6817966351785283, Training Loss Force: 3.462882169054641, time: 2.7253267765045166
Validation Loss Energy: 2.2932658969809037, Validation Loss Force: 3.54472016035924, time: 0.156036376953125
Test Loss Energy: 10.408110012070262, Test Loss Force: 12.450626910371986, time: 10.5866379737854

Epoch 8, Batch 100/169, Loss: 1.1745787858963013, Variance: 0.11958388984203339

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6350419434585546, Training Loss Force: 3.4574535137228866, time: 2.8221523761749268
Validation Loss Energy: 3.196277534922919, Validation Loss Force: 3.5203329901589364, time: 0.15625357627868652
Test Loss Energy: 10.740286612873094, Test Loss Force: 12.397342365617364, time: 10.748849153518677

Epoch 9, Batch 100/169, Loss: 1.3654344081878662, Variance: 0.12395550310611725

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6762859644759973, Training Loss Force: 3.46451992433918, time: 2.7164978981018066
Validation Loss Energy: 1.8866213421745401, Validation Loss Force: 3.590258078946834, time: 0.15883851051330566
Test Loss Energy: 10.41878179112323, Test Loss Force: 12.780536305589052, time: 10.567015647888184

Epoch 10, Batch 100/169, Loss: 0.6712349653244019, Variance: 0.11717046797275543

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.662206017942585, Training Loss Force: 3.4698667968510715, time: 2.7719762325286865
Validation Loss Energy: 2.765673854237775, Validation Loss Force: 3.5348288257818843, time: 0.16124987602233887
Test Loss Energy: 10.433110734861193, Test Loss Force: 12.429598353330082, time: 11.626605749130249

Epoch 11, Batch 100/169, Loss: 1.0496220588684082, Variance: 0.12009382992982864

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6749716861935373, Training Loss Force: 3.4777660783502524, time: 2.820023536682129
Validation Loss Energy: 3.9032183759795616, Validation Loss Force: 3.570743577666869, time: 0.15782570838928223
Test Loss Energy: 10.962573896371108, Test Loss Force: 12.372203867749647, time: 10.532432556152344

Epoch 12, Batch 100/169, Loss: 1.2575345039367676, Variance: 0.11769786477088928

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6786928097675893, Training Loss Force: 3.4713462396431316, time: 2.7469489574432373
Validation Loss Energy: 2.412100041002694, Validation Loss Force: 3.5421798022963986, time: 0.15904617309570312
Test Loss Energy: 10.611255396712712, Test Loss Force: 12.740926901327672, time: 10.534836769104004

Epoch 13, Batch 100/169, Loss: 0.8465606570243835, Variance: 0.12191279232501984

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6851869606788488, Training Loss Force: 3.471053354614809, time: 2.907676935195923
Validation Loss Energy: 2.463870914323099, Validation Loss Force: 3.580888829293631, time: 0.15819764137268066
Test Loss Energy: 10.507304252834091, Test Loss Force: 12.662798332238884, time: 10.524686336517334

Epoch 14, Batch 100/169, Loss: 1.0885114669799805, Variance: 0.12290074676275253

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6653872765490205, Training Loss Force: 3.4748188716969577, time: 2.7622435092926025
Validation Loss Energy: 3.530977169118674, Validation Loss Force: 3.561891959554899, time: 0.1593647003173828
Test Loss Energy: 10.987643816805107, Test Loss Force: 12.574380480725939, time: 10.83861756324768

Epoch 15, Batch 100/169, Loss: 1.4308216571807861, Variance: 0.1167573407292366

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6468208606359562, Training Loss Force: 3.480766899517666, time: 2.9697768688201904
Validation Loss Energy: 1.8511430218155738, Validation Loss Force: 3.5316757564524983, time: 0.16585135459899902
Test Loss Energy: 10.31411301373246, Test Loss Force: 12.411563402255085, time: 12.240386962890625

Epoch 16, Batch 100/169, Loss: 0.702380359172821, Variance: 0.12255655229091644

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6555437531437187, Training Loss Force: 3.463689855925366, time: 2.982665538787842
Validation Loss Energy: 2.9089322799841892, Validation Loss Force: 3.527083427986883, time: 0.19028806686401367
Test Loss Energy: 10.6030913199128, Test Loss Force: 12.186336862714683, time: 11.803815603256226

Epoch 17, Batch 100/169, Loss: 0.5974459648132324, Variance: 0.11494499444961548

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6587635239600256, Training Loss Force: 3.4537718309153953, time: 2.8553733825683594
Validation Loss Energy: 3.721398478656616, Validation Loss Force: 3.512664710111475, time: 0.17480230331420898
Test Loss Energy: 10.599760744413874, Test Loss Force: 12.34508949015069, time: 12.005491971969604

Epoch 18, Batch 100/169, Loss: 1.1382403373718262, Variance: 0.11650320887565613

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6658709504965126, Training Loss Force: 3.460305171596338, time: 2.881594657897949
Validation Loss Energy: 2.24573813268225, Validation Loss Force: 3.5165528102915693, time: 0.19522643089294434
Test Loss Energy: 10.35779036644983, Test Loss Force: 12.460150999019717, time: 11.952679634094238

Epoch 19, Batch 100/169, Loss: 0.7434008121490479, Variance: 0.12035956233739853

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.689256581888596, Training Loss Force: 3.4506926526072856, time: 3.056823253631592
Validation Loss Energy: 2.1582133488232684, Validation Loss Force: 3.5360592212592326, time: 0.20180249214172363
Test Loss Energy: 10.681817108635492, Test Loss Force: 13.163967030298389, time: 12.19336223602295

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–„â–ˆâ–â–‚â–ˆâ–‚â–‚â–†â–‚â–ƒâ–ˆâ–„â–ƒâ–ˆâ–â–„â–„â–‚â–…
wandb:   test_error_force â–‚â–„â–…â–„â–ƒâ–ƒâ–†â–ƒâ–ƒâ–…â–ƒâ–‚â–…â–„â–„â–ƒâ–â–‚â–ƒâ–ˆ
wandb:          test_loss â–‚â–…â–ˆâ–„â–‚â–„â–„â–„â–ˆâ–…â–â–ƒâ–ƒâ–…â–ˆâ–‚â–ƒâ–‚â–‚â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–‚â–â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–ƒâ–†â–â–„â–ˆâ–ƒâ–ƒâ–†â–â–„â–ˆâ–ƒâ–ƒâ–‡â–â–…â–‡â–ƒâ–‚
wandb:  valid_error_force â–â–ƒâ–ƒâ–…â–…â–…â–…â–…â–ƒâ–ˆâ–„â–‡â–…â–‡â–†â–„â–„â–ƒâ–ƒâ–„
wandb:         valid_loss â–‚â–ƒâ–†â–â–ƒâ–ˆâ–‚â–‚â–…â–â–„â–ˆâ–‚â–ƒâ–‡â–â–„â–‡â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 5406
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.68182
wandb:   test_error_force 13.16397
wandb:          test_loss 12.98718
wandb: train_error_energy 2.68926
wandb:  train_error_force 3.45069
wandb:         train_loss 1.00067
wandb: valid_error_energy 2.15821
wandb:  valid_error_force 3.53606
wandb:         valid_loss 0.84905
wandb: 
wandb: ğŸš€ View run al_71_51 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/h7y0zlr4
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_192534-h7y0zlr4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.369320869445801, Uncertainty Bias: -0.1534310132265091
7.6293945e-06 0.008621216
2.1647344 4.632229
(48745, 22, 3)
Found uncertainty sample 0 after 2139 steps.
Found uncertainty sample 1 after 1543 steps.
Found uncertainty sample 2 after 2228 steps.
Found uncertainty sample 3 after 99 steps.
Found uncertainty sample 4 after 555 steps.
Found uncertainty sample 5 after 9 steps.
Found uncertainty sample 6 after 719 steps.
Found uncertainty sample 7 after 34 steps.
Found uncertainty sample 8 after 11 steps.
Found uncertainty sample 9 after 20 steps.
Found uncertainty sample 10 after 16 steps.
Found uncertainty sample 11 after 2854 steps.
Found uncertainty sample 12 after 693 steps.
Found uncertainty sample 13 after 1285 steps.
Found uncertainty sample 14 after 47 steps.
Found uncertainty sample 15 after 1909 steps.
Found uncertainty sample 16 after 1304 steps.
Found uncertainty sample 17 after 173 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 243 steps.
Found uncertainty sample 20 after 16 steps.
Found uncertainty sample 21 after 24 steps.
Found uncertainty sample 22 after 26 steps.
Found uncertainty sample 23 after 47 steps.
Found uncertainty sample 24 after 952 steps.
Found uncertainty sample 25 after 253 steps.
Found uncertainty sample 26 after 867 steps.
Found uncertainty sample 27 after 215 steps.
Found uncertainty sample 28 after 654 steps.
Found uncertainty sample 29 after 28 steps.
Found uncertainty sample 30 after 1684 steps.
Found uncertainty sample 31 after 207 steps.
Found uncertainty sample 32 after 2646 steps.
Found uncertainty sample 33 after 496 steps.
Found uncertainty sample 34 after 417 steps.
Found uncertainty sample 35 after 584 steps.
Found uncertainty sample 36 after 366 steps.
Found uncertainty sample 37 after 2311 steps.
Found uncertainty sample 38 after 2473 steps.
Found uncertainty sample 39 after 1737 steps.
Found uncertainty sample 40 after 1483 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 1330 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 11 steps.
Found uncertainty sample 45 after 1152 steps.
Found uncertainty sample 46 after 115 steps.
Found uncertainty sample 47 after 3487 steps.
Found uncertainty sample 48 after 2554 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 2509 steps.
Found uncertainty sample 51 after 1641 steps.
Found uncertainty sample 52 after 234 steps.
Found uncertainty sample 53 after 146 steps.
Found uncertainty sample 54 after 28 steps.
Found uncertainty sample 55 after 2740 steps.
Found uncertainty sample 56 after 2135 steps.
Found uncertainty sample 57 after 3198 steps.
Found uncertainty sample 58 after 263 steps.
Found uncertainty sample 59 after 45 steps.
Found uncertainty sample 60 after 39 steps.
Found uncertainty sample 61 after 606 steps.
Found uncertainty sample 62 after 1207 steps.
Found uncertainty sample 63 after 130 steps.
Found uncertainty sample 64 after 715 steps.
Found uncertainty sample 65 after 21 steps.
Found uncertainty sample 66 after 2208 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 1002 steps.
Found uncertainty sample 69 after 25 steps.
Found uncertainty sample 70 after 43 steps.
Found uncertainty sample 71 after 60 steps.
Found uncertainty sample 72 after 22 steps.
Found uncertainty sample 73 after 3184 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 184 steps.
Found uncertainty sample 76 after 585 steps.
Found uncertainty sample 77 after 587 steps.
Found uncertainty sample 78 after 1270 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1145 steps.
Found uncertainty sample 81 after 1571 steps.
Found uncertainty sample 82 after 813 steps.
Found uncertainty sample 83 after 497 steps.
Found uncertainty sample 84 after 32 steps.
Found uncertainty sample 85 after 394 steps.
Found uncertainty sample 86 after 778 steps.
Found uncertainty sample 87 after 20 steps.
Found uncertainty sample 88 after 137 steps.
Found uncertainty sample 89 after 222 steps.
Found uncertainty sample 90 after 3460 steps.
Found uncertainty sample 91 after 192 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 165 steps.
Found uncertainty sample 94 after 515 steps.
Found uncertainty sample 95 after 16 steps.
Found uncertainty sample 96 after 38 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 201 steps.
Found uncertainty sample 99 after 2219 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_194319-duhbsila
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_52
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/duhbsila
Training model 52. Added 94 samples to the dataset.
Epoch 0, Batch 100/172, Loss: 0.9694738388061523, Variance: 0.11828157305717468

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.083569413091901, Training Loss Force: 3.70376894505692, time: 2.8816452026367188
Validation Loss Energy: 2.0224163063762233, Validation Loss Force: 3.510652152147531, time: 0.16644954681396484
Test Loss Energy: 10.465527506018573, Test Loss Force: 12.677326932091677, time: 10.695611238479614

Epoch 1, Batch 100/172, Loss: 0.9147129058837891, Variance: 0.1230110377073288

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.629055813172755, Training Loss Force: 3.4412738002611105, time: 2.8209266662597656
Validation Loss Energy: 3.663752702590052, Validation Loss Force: 3.5276457418033993, time: 0.17619657516479492
Test Loss Energy: 10.809345842000491, Test Loss Force: 12.607299323009292, time: 10.735436916351318

Epoch 2, Batch 100/172, Loss: 1.2119548320770264, Variance: 0.11589713394641876

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6513390130360057, Training Loss Force: 3.4481620625228575, time: 2.7623980045318604
Validation Loss Energy: 1.7226064408752233, Validation Loss Force: 3.5565319420971737, time: 0.16541218757629395
Test Loss Energy: 10.30721489613106, Test Loss Force: 12.756543917292214, time: 10.682698488235474

Epoch 3, Batch 100/172, Loss: 0.7942630052566528, Variance: 0.11930382251739502

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.656746690537311, Training Loss Force: 3.456174851871299, time: 2.786583662033081
Validation Loss Energy: 2.015502763398884, Validation Loss Force: 3.51180105071747, time: 0.16099214553833008
Test Loss Energy: 10.225418194444577, Test Loss Force: 12.559962520625112, time: 10.692083358764648

Epoch 4, Batch 100/172, Loss: 1.0340567827224731, Variance: 0.12321345508098602

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6566778203945223, Training Loss Force: 3.456398749581294, time: 2.97847056388855
Validation Loss Energy: 3.839066207468988, Validation Loss Force: 3.503908691286157, time: 0.18141508102416992
Test Loss Energy: 10.899563405810227, Test Loss Force: 12.627700064977159, time: 10.645789384841919

Epoch 5, Batch 100/172, Loss: 1.5449590682983398, Variance: 0.11485078930854797

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6941660547370896, Training Loss Force: 3.460274523556961, time: 2.8845021724700928
Validation Loss Energy: 1.8705189857752476, Validation Loss Force: 3.5263223448461023, time: 0.1602916717529297
Test Loss Energy: 10.247919921229482, Test Loss Force: 12.558182757618503, time: 11.519865274429321

Epoch 6, Batch 100/172, Loss: 0.7565528154373169, Variance: 0.12105321884155273

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.629679312673114, Training Loss Force: 3.4623711594053717, time: 2.792908191680908
Validation Loss Energy: 2.2988908907222827, Validation Loss Force: 3.514188731969254, time: 0.17846059799194336
Test Loss Energy: 10.715934484501002, Test Loss Force: 13.009988573911148, time: 10.952857971191406

Epoch 7, Batch 100/172, Loss: 0.9114446640014648, Variance: 0.12066248059272766

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6704601828628647, Training Loss Force: 3.4654471562978006, time: 2.8324239253997803
Validation Loss Energy: 3.6670552571394044, Validation Loss Force: 3.580067922375644, time: 0.17129087448120117
Test Loss Energy: 10.769447737282052, Test Loss Force: 12.555264411491304, time: 10.62259554862976

Epoch 8, Batch 100/172, Loss: 1.2154338359832764, Variance: 0.1170104444026947

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.642184614511449, Training Loss Force: 3.4811259161486077, time: 2.7859058380126953
Validation Loss Energy: 1.9922635268688873, Validation Loss Force: 3.522424595669685, time: 0.166182279586792
Test Loss Energy: 10.350251787099547, Test Loss Force: 12.605135720364158, time: 10.956105470657349

Epoch 9, Batch 100/172, Loss: 0.8595490455627441, Variance: 0.11849465221166611

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.657655628580164, Training Loss Force: 3.472068011039522, time: 2.8113644123077393
Validation Loss Energy: 2.2979617665993746, Validation Loss Force: 3.513390685750249, time: 0.16350865364074707
Test Loss Energy: 10.40615553189317, Test Loss Force: 12.542021827381458, time: 10.727407455444336

Epoch 10, Batch 100/172, Loss: 0.9060996174812317, Variance: 0.1210952028632164

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.662491981716307, Training Loss Force: 3.458248800063999, time: 2.7643043994903564
Validation Loss Energy: 3.873105342914774, Validation Loss Force: 3.511786354553839, time: 0.1666414737701416
Test Loss Energy: 10.95523515500155, Test Loss Force: 12.617676922312086, time: 10.751535654067993

Epoch 11, Batch 100/172, Loss: 1.0727134943008423, Variance: 0.11686164140701294

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6725849850636476, Training Loss Force: 3.4543420517744634, time: 2.7754011154174805
Validation Loss Energy: 1.7860747182156043, Validation Loss Force: 3.56695745995561, time: 0.16692805290222168
Test Loss Energy: 10.21289499255745, Test Loss Force: 12.59207233478708, time: 10.78636121749878

Epoch 12, Batch 100/172, Loss: 0.8841277360916138, Variance: 0.12354724854230881

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.663362302844692, Training Loss Force: 3.4583755790724915, time: 2.7509806156158447
Validation Loss Energy: 2.2417316467395705, Validation Loss Force: 3.502600213161637, time: 0.1599428653717041
Test Loss Energy: 10.543529379860745, Test Loss Force: 12.779739331912529, time: 10.691674947738647

Epoch 13, Batch 100/172, Loss: 0.9174647927284241, Variance: 0.12147454917430878

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6300244679735343, Training Loss Force: 3.4590609119822515, time: 2.991502285003662
Validation Loss Energy: 3.738743217555612, Validation Loss Force: 3.543478037640206, time: 0.1602306365966797
Test Loss Energy: 10.80501152528723, Test Loss Force: 12.316861342468327, time: 10.703735589981079

Epoch 14, Batch 100/172, Loss: 1.086828351020813, Variance: 0.11924208700656891

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6615204594029795, Training Loss Force: 3.470387269372181, time: 2.7655534744262695
Validation Loss Energy: 1.6932481077690333, Validation Loss Force: 3.5504498828222535, time: 0.17332792282104492
Test Loss Energy: 10.635553907287111, Test Loss Force: 13.102099095928507, time: 10.66041374206543

Epoch 15, Batch 100/172, Loss: 0.8381475210189819, Variance: 0.1200418546795845

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.655131629795079, Training Loss Force: 3.470634408821576, time: 2.7589056491851807
Validation Loss Energy: 2.422620564363393, Validation Loss Force: 3.5679187226585802, time: 0.16270089149475098
Test Loss Energy: 10.357554514357814, Test Loss Force: 12.617016809719622, time: 10.80124807357788

Epoch 16, Batch 100/172, Loss: 1.0501242876052856, Variance: 0.12270274758338928

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.691964328761211, Training Loss Force: 3.462751097697727, time: 2.7975871562957764
Validation Loss Energy: 3.9083989963287173, Validation Loss Force: 3.561206509700684, time: 0.16163086891174316
Test Loss Energy: 11.110923929601238, Test Loss Force: 12.588789431469909, time: 10.728801250457764

Epoch 17, Batch 100/172, Loss: 1.417833685874939, Variance: 0.11687739193439484

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6594804320065677, Training Loss Force: 3.461639216147182, time: 2.7812349796295166
Validation Loss Energy: 1.7686612487735645, Validation Loss Force: 3.5422260432557025, time: 0.17050814628601074
Test Loss Energy: 10.22373124366857, Test Loss Force: 12.691366499204502, time: 10.830459594726562

Epoch 18, Batch 100/172, Loss: 0.8828269839286804, Variance: 0.12260323017835617

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.690614613907523, Training Loss Force: 3.4688728175408547, time: 2.7357470989227295
Validation Loss Energy: 2.6692271630397584, Validation Loss Force: 3.5194469154042416, time: 0.16806864738464355
Test Loss Energy: 10.3590781728724, Test Loss Force: 12.635887579161935, time: 10.803516387939453

Epoch 19, Batch 100/172, Loss: 0.8546350598335266, Variance: 0.12024447321891785

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.694169462092858, Training Loss Force: 3.4601174524825056, time: 2.8129942417144775
Validation Loss Energy: 3.8478997893801377, Validation Loss Force: 3.510202434695806, time: 0.16109228134155273
Test Loss Energy: 10.87719614787332, Test Loss Force: 12.610005501580439, time: 10.78611946105957

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–†â–‚â–â–†â–â–…â–…â–‚â–ƒâ–‡â–â–„â–†â–„â–‚â–ˆâ–â–‚â–†
wandb:   test_error_force â–„â–„â–…â–ƒâ–„â–ƒâ–‡â–ƒâ–„â–ƒâ–„â–ƒâ–…â–â–ˆâ–„â–ƒâ–„â–„â–„
wandb:          test_loss â–…â–ƒâ–â–‚â–‚â–â–†â–‚â–„â–…â–„â–ƒâ–„â–„â–ˆâ–ƒâ–„â–‚â–â–ƒ
wandb: train_error_energy â–ˆâ–â–â–â–â–‚â–â–‚â–â–â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–‡â–â–‚â–ˆâ–‚â–ƒâ–‡â–‚â–ƒâ–ˆâ–â–ƒâ–‡â–â–ƒâ–ˆâ–â–„â–ˆ
wandb:  valid_error_force â–‚â–ƒâ–†â–‚â–â–ƒâ–‚â–ˆâ–ƒâ–‚â–‚â–‡â–â–…â–…â–‡â–†â–…â–ƒâ–‚
wandb:         valid_loss â–‚â–‡â–â–‚â–ˆâ–â–‚â–‡â–‚â–‚â–ˆâ–â–‚â–ˆâ–â–ƒâ–ˆâ–â–„â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5490
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.8772
wandb:   test_error_force 12.61001
wandb:          test_loss 12.40804
wandb: train_error_energy 2.69417
wandb:  train_error_force 3.46012
wandb:         train_loss 1.00788
wandb: valid_error_energy 3.8479
wandb:  valid_error_force 3.5102
wandb:         valid_loss 1.48872
wandb: 
wandb: ğŸš€ View run al_71_52 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/duhbsila
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_194319-duhbsila/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.177316188812256, Uncertainty Bias: -0.15429554879665375
1.5258789e-05 0.044412613
2.1729953 4.5042844
(48745, 22, 3)
Found uncertainty sample 0 after 2618 steps.
Found uncertainty sample 1 after 333 steps.
Found uncertainty sample 2 after 314 steps.
Found uncertainty sample 3 after 2291 steps.
Found uncertainty sample 4 after 291 steps.
Found uncertainty sample 5 after 3041 steps.
Found uncertainty sample 6 after 32 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 2478 steps.
Found uncertainty sample 9 after 34 steps.
Found uncertainty sample 10 after 952 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 4 steps.
Found uncertainty sample 13 after 2778 steps.
Found uncertainty sample 14 after 3097 steps.
Found uncertainty sample 15 after 1367 steps.
Found uncertainty sample 16 after 24 steps.
Found uncertainty sample 17 after 2909 steps.
Found uncertainty sample 18 after 415 steps.
Found uncertainty sample 19 after 2669 steps.
Found uncertainty sample 20 after 549 steps.
Found uncertainty sample 21 after 962 steps.
Found uncertainty sample 22 after 1011 steps.
Found uncertainty sample 23 after 976 steps.
Found uncertainty sample 24 after 350 steps.
Found uncertainty sample 25 after 1404 steps.
Found uncertainty sample 26 after 636 steps.
Found uncertainty sample 27 after 2639 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1036 steps.
Found uncertainty sample 30 after 448 steps.
Found uncertainty sample 31 after 1862 steps.
Found uncertainty sample 32 after 1220 steps.
Found uncertainty sample 33 after 752 steps.
Found uncertainty sample 34 after 2837 steps.
Found uncertainty sample 35 after 1451 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 1701 steps.
Found uncertainty sample 38 after 2086 steps.
Found uncertainty sample 39 after 1749 steps.
Found uncertainty sample 40 after 2540 steps.
Found uncertainty sample 41 after 190 steps.
Found uncertainty sample 42 after 581 steps.
Found uncertainty sample 43 after 832 steps.
Found uncertainty sample 44 after 330 steps.
Found uncertainty sample 45 after 641 steps.
Found uncertainty sample 46 after 448 steps.
Found uncertainty sample 47 after 391 steps.
Found uncertainty sample 48 after 1462 steps.
Found uncertainty sample 49 after 43 steps.
Found uncertainty sample 50 after 447 steps.
Found uncertainty sample 51 after 2769 steps.
Found uncertainty sample 52 after 171 steps.
Found uncertainty sample 53 after 2087 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 2320 steps.
Found uncertainty sample 57 after 2861 steps.
Found uncertainty sample 58 after 46 steps.
Found uncertainty sample 59 after 2023 steps.
Found uncertainty sample 60 after 685 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 2970 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 1044 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 2844 steps.
Found uncertainty sample 67 after 666 steps.
Found uncertainty sample 68 after 431 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 50 steps.
Found uncertainty sample 71 after 2200 steps.
Found uncertainty sample 72 after 78 steps.
Found uncertainty sample 73 after 557 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 3824 steps.
Found uncertainty sample 76 after 1818 steps.
Found uncertainty sample 77 after 235 steps.
Found uncertainty sample 78 after 16 steps.
Found uncertainty sample 79 after 1387 steps.
Found uncertainty sample 80 after 775 steps.
Found uncertainty sample 81 after 24 steps.
Found uncertainty sample 82 after 1254 steps.
Found uncertainty sample 83 after 65 steps.
Found uncertainty sample 84 after 455 steps.
Found uncertainty sample 85 after 1514 steps.
Found uncertainty sample 86 after 994 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 2134 steps.
Found uncertainty sample 90 after 1268 steps.
Found uncertainty sample 91 after 453 steps.
Found uncertainty sample 92 after 1198 steps.
Found uncertainty sample 93 after 28 steps.
Found uncertainty sample 94 after 1527 steps.
Found uncertainty sample 95 after 1803 steps.
Found uncertainty sample 96 after 1823 steps.
Found uncertainty sample 97 after 1014 steps.
Found uncertainty sample 98 after 3030 steps.
Found uncertainty sample 99 after 492 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_200410-i7o3yege
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_53
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/i7o3yege
Training model 53. Added 93 samples to the dataset.
Epoch 0, Batch 100/175, Loss: 2.2567124366760254, Variance: 0.14509962499141693

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.410785410720224, Training Loss Force: 3.635416263510808, time: 3.2114500999450684
Validation Loss Energy: 6.152467407308703, Validation Loss Force: 3.5753188348312954, time: 0.18382620811462402
Test Loss Energy: 11.460006803055148, Test Loss Force: 12.005765414945612, time: 11.907498836517334

Epoch 1, Batch 100/175, Loss: 1.0663442611694336, Variance: 0.1535845696926117

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.1624628311446275, Training Loss Force: 3.4926434320930424, time: 3.0722508430480957
Validation Loss Energy: 3.738428066491076, Validation Loss Force: 3.557946781850399, time: 0.1823873519897461
Test Loss Energy: 10.521495123048672, Test Loss Force: 11.977375625784608, time: 12.120998859405518

Epoch 2, Batch 100/175, Loss: 0.976582407951355, Variance: 0.15917527675628662

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.187693343557798, Training Loss Force: 3.526064127949455, time: 3.0330519676208496
Validation Loss Energy: 1.9146602863680589, Validation Loss Force: 3.6566458456825237, time: 0.1951606273651123
Test Loss Energy: 10.241937251279923, Test Loss Force: 12.136258730345283, time: 12.079909801483154

Epoch 3, Batch 100/175, Loss: 1.625898838043213, Variance: 0.16634821891784668

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.158534820807146, Training Loss Force: 3.535514709098703, time: 3.1111860275268555
Validation Loss Energy: 4.814728627214204, Validation Loss Force: 3.6315834684087185, time: 0.1801447868347168
Test Loss Energy: 11.37551313267319, Test Loss Force: 12.042919918686703, time: 12.0256826877594

Epoch 4, Batch 100/175, Loss: 1.7783814668655396, Variance: 0.16583168506622314

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.134805668470542, Training Loss Force: 3.5344049095342625, time: 3.1121902465820312
Validation Loss Energy: 5.451822313684024, Validation Loss Force: 3.5715957324883347, time: 0.17178797721862793
Test Loss Energy: 11.672457570943362, Test Loss Force: 12.145614457802553, time: 11.90255331993103

Epoch 5, Batch 100/175, Loss: 1.511495590209961, Variance: 0.16451433300971985

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.136082277126569, Training Loss Force: 3.4970179366933447, time: 3.0604398250579834
Validation Loss Energy: 3.4483742640612802, Validation Loss Force: 3.4915052776274926, time: 0.19065380096435547
Test Loss Energy: 10.76731262229791, Test Loss Force: 12.057937775114414, time: 12.123970746994019

Epoch 6, Batch 100/175, Loss: 1.0223057270050049, Variance: 0.16369760036468506

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.158649197040643, Training Loss Force: 3.521498980388386, time: 3.0761234760284424
Validation Loss Energy: 2.6140022459277215, Validation Loss Force: 3.5152812210222244, time: 0.18589997291564941
Test Loss Energy: 10.33166947562642, Test Loss Force: 12.03974121520337, time: 11.921956777572632

Epoch 7, Batch 100/175, Loss: 1.6070802211761475, Variance: 0.16240932047367096

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.131808026446331, Training Loss Force: 3.523436788532107, time: 3.1057631969451904
Validation Loss Energy: 5.123923292097097, Validation Loss Force: 3.7963796415180475, time: 0.17519736289978027
Test Loss Energy: 11.506128988383157, Test Loss Force: 12.539248377080487, time: 12.125528335571289

Epoch 8, Batch 100/175, Loss: 1.645179271697998, Variance: 0.16255931556224823

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.127151155102078, Training Loss Force: 3.530444481750123, time: 3.1208102703094482
Validation Loss Energy: 5.925379763295222, Validation Loss Force: 3.5056118133603102, time: 0.18595314025878906
Test Loss Energy: 11.735251207403863, Test Loss Force: 12.01269088238033, time: 11.954431056976318

Epoch 9, Batch 100/175, Loss: 1.2340689897537231, Variance: 0.16080114245414734

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.1251030325281794, Training Loss Force: 3.4873563033656096, time: 3.1099934577941895
Validation Loss Energy: 2.8191868909559186, Validation Loss Force: 3.72727419785631, time: 0.17910027503967285
Test Loss Energy: 10.544264057471917, Test Loss Force: 12.041677942470752, time: 12.088996887207031

Epoch 10, Batch 100/175, Loss: 1.7163901329040527, Variance: 0.16278567910194397

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.992988689892057, Training Loss Force: 3.9901142166274584, time: 3.1726558208465576
Validation Loss Energy: 7.352730850491777, Validation Loss Force: 3.9404679551713353, time: 0.1999950408935547
Test Loss Energy: 12.627492952360463, Test Loss Force: 11.923019387254513, time: 11.857046365737915

Epoch 11, Batch 100/175, Loss: 0.713782787322998, Variance: 0.1094624251127243

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.8854291932402196, Training Loss Force: 4.596876372221896, time: 3.052924871444702
Validation Loss Energy: 3.2298722227534338, Validation Loss Force: 4.639104127761455, time: 0.19045686721801758
Test Loss Energy: 11.170575468919674, Test Loss Force: 13.788596676479884, time: 12.36359977722168

Epoch 12, Batch 100/175, Loss: 1.0083874464035034, Variance: 0.1147802323102951

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.7738840134361316, Training Loss Force: 3.6130071218639563, time: 3.1935226917266846
Validation Loss Energy: 3.840412613571365, Validation Loss Force: 3.494978099850431, time: 0.19614458084106445
Test Loss Energy: 10.752268423294167, Test Loss Force: 12.200864222148777, time: 11.248427867889404

Epoch 13, Batch 100/175, Loss: 0.9352614879608154, Variance: 0.11432827264070511

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5993671988271347, Training Loss Force: 3.4316288300497173, time: 2.9614744186401367
Validation Loss Energy: 2.169147739903198, Validation Loss Force: 3.492498307795282, time: 0.1825733184814453
Test Loss Energy: 10.297144720146493, Test Loss Force: 12.671480778147293, time: 12.3951735496521

Epoch 14, Batch 100/175, Loss: 1.027388334274292, Variance: 0.11688582599163055

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.645103992347789, Training Loss Force: 3.435138080520797, time: 2.8874948024749756
Validation Loss Energy: 2.077964873091866, Validation Loss Force: 3.507679593272444, time: 0.15694260597229004
Test Loss Energy: 10.200060772003166, Test Loss Force: 12.250683176958775, time: 10.731284856796265

Epoch 15, Batch 100/175, Loss: 0.9260162711143494, Variance: 0.12316971272230148

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6225349647111984, Training Loss Force: 3.435180252784641, time: 2.8386354446411133
Validation Loss Energy: 3.290861895607444, Validation Loss Force: 3.523017340936145, time: 0.16514039039611816
Test Loss Energy: 10.934485137242481, Test Loss Force: 12.892331353683494, time: 9.91437029838562

Epoch 16, Batch 100/175, Loss: 1.461958646774292, Variance: 0.122627392411232

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6174893206725254, Training Loss Force: 3.431260653615099, time: 3.047795534133911
Validation Loss Energy: 1.7360181375381725, Validation Loss Force: 3.516412454506586, time: 0.148637056350708
Test Loss Energy: 10.223148372787797, Test Loss Force: 12.593331658211884, time: 10.032490253448486

Epoch 17, Batch 100/175, Loss: 0.8500262498855591, Variance: 0.12114507704973221

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6023160712188074, Training Loss Force: 3.43084103042861, time: 2.926138162612915
Validation Loss Energy: 2.5991018946896713, Validation Loss Force: 3.48702606508801, time: 0.1567211151123047
Test Loss Energy: 10.520975303676213, Test Loss Force: 12.544482664191232, time: 10.119697093963623

Epoch 18, Batch 100/175, Loss: 0.5819963812828064, Variance: 0.11819436401128769

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6343743468199348, Training Loss Force: 3.445561486627467, time: 2.9570939540863037
Validation Loss Energy: 4.048469913251178, Validation Loss Force: 3.4874935960714093, time: 0.16198205947875977
Test Loss Energy: 10.936167653288479, Test Loss Force: 12.41508902193616, time: 10.238379955291748

Epoch 19, Batch 100/175, Loss: 1.5462868213653564, Variance: 0.11482759565114975

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6339344613773643, Training Loss Force: 3.437238354435364, time: 2.9200737476348877
Validation Loss Energy: 2.573416043807269, Validation Loss Force: 3.5103698076704952, time: 0.15489840507507324
Test Loss Energy: 10.307059862446755, Test Loss Force: 12.471717996640653, time: 9.9440758228302

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–‚â–â–„â–…â–ƒâ–â–…â–…â–‚â–ˆâ–„â–ƒâ–â–â–ƒâ–â–‚â–ƒâ–
wandb:   test_error_force â–â–â–‚â–â–‚â–‚â–â–ƒâ–â–â–â–ˆâ–‚â–„â–‚â–…â–„â–ƒâ–ƒâ–ƒ
wandb:          test_loss â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–â–â–ˆâ–„â–„â–„â–…â–„â–„â–„â–„
wandb: train_error_energy â–†â–†â–†â–†â–…â–…â–†â–…â–…â–…â–ˆâ–‚â–‚â–â–â–â–â–â–â–
wandb:  train_error_force â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–„â–ˆâ–‚â–â–â–â–â–â–â–
wandb:         train_loss â–‡â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ˆâ–…â–‚â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–ƒâ–â–…â–†â–ƒâ–‚â–…â–†â–‚â–ˆâ–ƒâ–„â–‚â–â–ƒâ–â–‚â–„â–‚
wandb:  valid_error_force â–‚â–â–‚â–‚â–‚â–â–â–ƒâ–â–‚â–„â–ˆâ–â–â–â–â–â–â–â–
wandb:         valid_loss â–‡â–ƒâ–‚â–…â–†â–ƒâ–‚â–…â–†â–ƒâ–ˆâ–†â–„â–â–â–ƒâ–â–‚â–…â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 5573
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.30706
wandb:   test_error_force 12.47172
wandb:          test_loss 12.23936
wandb: train_error_energy 2.63393
wandb:  train_error_force 3.43724
wandb:         train_loss 0.97878
wandb: valid_error_energy 2.57342
wandb:  valid_error_force 3.51037
wandb:         valid_loss 0.95241
wandb: 
wandb: ğŸš€ View run al_71_53 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/i7o3yege
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_200410-i7o3yege/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.327636480331421, Uncertainty Bias: -0.1656063050031662
0.0001373291 0.012746811
2.14493 4.580076
(48745, 22, 3)
Found uncertainty sample 0 after 2083 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 2152 steps.
Found uncertainty sample 3 after 1235 steps.
Found uncertainty sample 4 after 272 steps.
Found uncertainty sample 5 after 19 steps.
Found uncertainty sample 6 after 930 steps.
Found uncertainty sample 7 after 502 steps.
Found uncertainty sample 8 after 1217 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 156 steps.
Found uncertainty sample 11 after 260 steps.
Found uncertainty sample 12 after 1723 steps.
Found uncertainty sample 13 after 3367 steps.
Found uncertainty sample 14 after 3289 steps.
Found uncertainty sample 15 after 1 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 1154 steps.
Found uncertainty sample 18 after 737 steps.
Found uncertainty sample 19 after 10 steps.
Found uncertainty sample 20 after 730 steps.
Found uncertainty sample 21 after 36 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 893 steps.
Found uncertainty sample 24 after 83 steps.
Found uncertainty sample 25 after 183 steps.
Found uncertainty sample 26 after 1416 steps.
Found uncertainty sample 27 after 495 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 244 steps.
Found uncertainty sample 30 after 242 steps.
Found uncertainty sample 31 after 246 steps.
Found uncertainty sample 32 after 1005 steps.
Found uncertainty sample 33 after 1391 steps.
Found uncertainty sample 34 after 13 steps.
Found uncertainty sample 35 after 1444 steps.
Found uncertainty sample 36 after 3092 steps.
Found uncertainty sample 37 after 100 steps.
Found uncertainty sample 38 after 588 steps.
Found uncertainty sample 39 after 389 steps.
Found uncertainty sample 40 after 2054 steps.
Found uncertainty sample 41 after 2870 steps.
Found uncertainty sample 42 after 1119 steps.
Found uncertainty sample 43 after 272 steps.
Found uncertainty sample 44 after 18 steps.
Found uncertainty sample 45 after 960 steps.
Found uncertainty sample 46 after 1565 steps.
Found uncertainty sample 47 after 1526 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 1637 steps.
Found uncertainty sample 50 after 293 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1214 steps.
Found uncertainty sample 53 after 13 steps.
Found uncertainty sample 54 after 17 steps.
Found uncertainty sample 55 after 92 steps.
Found uncertainty sample 56 after 196 steps.
Found uncertainty sample 57 after 489 steps.
Found uncertainty sample 58 after 3430 steps.
Found uncertainty sample 59 after 1055 steps.
Found uncertainty sample 60 after 1022 steps.
Found uncertainty sample 61 after 681 steps.
Found uncertainty sample 62 after 222 steps.
Found uncertainty sample 63 after 1129 steps.
Found uncertainty sample 64 after 441 steps.
Found uncertainty sample 65 after 2502 steps.
Found uncertainty sample 66 after 984 steps.
Found uncertainty sample 67 after 1160 steps.
Found uncertainty sample 68 after 88 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 346 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 3340 steps.
Found uncertainty sample 73 after 1095 steps.
Found uncertainty sample 74 after 514 steps.
Found uncertainty sample 75 after 404 steps.
Found uncertainty sample 76 after 493 steps.
Found uncertainty sample 77 after 150 steps.
Found uncertainty sample 78 after 650 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 1040 steps.
Found uncertainty sample 81 after 1005 steps.
Found uncertainty sample 82 after 1264 steps.
Found uncertainty sample 83 after 144 steps.
Found uncertainty sample 84 after 1346 steps.
Found uncertainty sample 85 after 6 steps.
Found uncertainty sample 86 after 2748 steps.
Found uncertainty sample 87 after 563 steps.
Found uncertainty sample 88 after 2411 steps.
Found uncertainty sample 89 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2370 steps.
Found uncertainty sample 92 after 940 steps.
Found uncertainty sample 93 after 85 steps.
Found uncertainty sample 94 after 640 steps.
Found uncertainty sample 95 after 4 steps.
Found uncertainty sample 96 after 191 steps.
Found uncertainty sample 97 after 44 steps.
Found uncertainty sample 98 after 27 steps.
Found uncertainty sample 99 after 1240 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_202359-hb7bm1v1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_54
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hb7bm1v1
Training model 54. Added 90 samples to the dataset.
Epoch 0, Batch 100/177, Loss: 0.7900828123092651, Variance: 0.10107126832008362

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.6573692681742247, Training Loss Force: 3.9909683613742497, time: 2.9017415046691895
Validation Loss Energy: 2.555533484697808, Validation Loss Force: 3.545814059650134, time: 0.1719813346862793
Test Loss Energy: 10.085137005568324, Test Loss Force: 12.138326750802271, time: 10.689682006835938

Epoch 1, Batch 100/177, Loss: 1.8435816764831543, Variance: 0.14197193086147308

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.049170063043076, Training Loss Force: 3.498511324538752, time: 2.859964609146118
Validation Loss Energy: 3.2321460458485998, Validation Loss Force: 3.542872748678526, time: 0.17466187477111816
Test Loss Energy: 10.161713764233966, Test Loss Force: 11.864278958785704, time: 10.886333703994751

Epoch 2, Batch 100/177, Loss: 0.7395168542861938, Variance: 0.15351176261901855

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.088149024172008, Training Loss Force: 3.4795466323786326, time: 2.8319599628448486
Validation Loss Energy: 5.61288232119383, Validation Loss Force: 3.563077004749527, time: 0.1678452491760254
Test Loss Energy: 11.471604638604083, Test Loss Force: 11.96828064739464, time: 10.704527616500854

Epoch 3, Batch 100/177, Loss: 1.2168279886245728, Variance: 0.1611609160900116

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.108836231379269, Training Loss Force: 3.4925983680129598, time: 2.831221580505371
Validation Loss Energy: 4.856131728208016, Validation Loss Force: 3.660886431120621, time: 0.16719889640808105
Test Loss Energy: 11.261016322084691, Test Loss Force: 12.292533355628557, time: 10.757915258407593

Epoch 4, Batch 100/177, Loss: 1.8517096042633057, Variance: 0.1629045605659485

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.118516081217712, Training Loss Force: 3.5027448952697435, time: 3.091099500656128
Validation Loss Energy: 1.9742967245992176, Validation Loss Force: 3.5236497213105795, time: 0.17419791221618652
Test Loss Energy: 10.111884722861399, Test Loss Force: 12.026803181863187, time: 10.700848579406738

Epoch 5, Batch 100/177, Loss: 1.7505488395690918, Variance: 0.16513791680335999

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.103244810987222, Training Loss Force: 3.4901937538419423, time: 2.841470718383789
Validation Loss Energy: 3.6811368539278777, Validation Loss Force: 3.5067692475808427, time: 0.16947269439697266
Test Loss Energy: 10.430239823091116, Test Loss Force: 11.900330319940426, time: 11.626648426055908

Epoch 6, Batch 100/177, Loss: 0.914262056350708, Variance: 0.1651359498500824

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.1233387093360605, Training Loss Force: 3.50395689349517, time: 2.8790197372436523
Validation Loss Energy: 6.105859192359683, Validation Loss Force: 3.5404558235455768, time: 0.1686100959777832
Test Loss Energy: 11.707337739022785, Test Loss Force: 11.886992148979202, time: 10.991865634918213

Epoch 7, Batch 100/177, Loss: 1.138938069343567, Variance: 0.1622658371925354

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.151699815527168, Training Loss Force: 3.496565439000773, time: 2.953059196472168
Validation Loss Energy: 5.242260661254323, Validation Loss Force: 3.499039510849105, time: 0.17228293418884277
Test Loss Energy: 11.284403777768203, Test Loss Force: 11.883947821079303, time: 10.852871417999268

Epoch 8, Batch 100/177, Loss: 1.5932402610778809, Variance: 0.15887627005577087

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.133812982302322, Training Loss Force: 3.5043615022035874, time: 3.0395419597625732
Validation Loss Energy: 2.5505782797473797, Validation Loss Force: 3.5996544200127922, time: 0.18363404273986816
Test Loss Energy: 10.146709981428922, Test Loss Force: 11.6890477653247, time: 10.938678741455078

Epoch 9, Batch 100/177, Loss: 1.4099231958389282, Variance: 0.16349416971206665

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.126240599691725, Training Loss Force: 3.5091717419919104, time: 2.9143214225769043
Validation Loss Energy: 3.461967992751026, Validation Loss Force: 3.4778070428852583, time: 0.17663812637329102
Test Loss Energy: 10.814615947366937, Test Loss Force: 12.04879909294988, time: 10.697420597076416

Epoch 10, Batch 100/177, Loss: 0.8235379457473755, Variance: 0.1649230420589447

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.1372534476791145, Training Loss Force: 3.5153899225639877, time: 2.8603756427764893
Validation Loss Energy: 5.560442846651251, Validation Loss Force: 3.618499206724856, time: 0.16887545585632324
Test Loss Energy: 11.909159895361357, Test Loss Force: 12.098962096072285, time: 10.842109441757202

Epoch 11, Batch 100/177, Loss: 1.1999644041061401, Variance: 0.16749754548072815

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.135925321833006, Training Loss Force: 3.4998160776726444, time: 2.950336217880249
Validation Loss Energy: 4.8896435831162455, Validation Loss Force: 3.6005625390129343, time: 0.1718430519104004
Test Loss Energy: 11.322259099420597, Test Loss Force: 12.123369581626182, time: 10.748564004898071

Epoch 12, Batch 100/177, Loss: 2.050931215286255, Variance: 0.1669909805059433

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.140535581454957, Training Loss Force: 3.5111016808965902, time: 2.9124534130096436
Validation Loss Energy: 2.1058213770318215, Validation Loss Force: 3.551282145866683, time: 0.16902518272399902
Test Loss Energy: 10.555459069380376, Test Loss Force: 12.23477516156568, time: 10.907420873641968

Epoch 13, Batch 100/177, Loss: 1.6579933166503906, Variance: 0.16814792156219482

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.10613609876909, Training Loss Force: 3.5116951721675904, time: 2.8549997806549072
Validation Loss Energy: 4.0068514169702025, Validation Loss Force: 3.505520741679625, time: 0.1683485507965088
Test Loss Energy: 10.864682034617932, Test Loss Force: 11.955407376967518, time: 10.806715726852417

Epoch 14, Batch 100/177, Loss: 0.9532163143157959, Variance: 0.16641762852668762

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.115220173405267, Training Loss Force: 3.500223235125071, time: 2.8961617946624756
Validation Loss Energy: 6.01537135017584, Validation Loss Force: 3.5196008540071255, time: 0.17248773574829102
Test Loss Energy: 11.325227505302381, Test Loss Force: 11.974189493778772, time: 10.724961042404175

Epoch 15, Batch 100/177, Loss: 1.2232149839401245, Variance: 0.16052237153053284

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.067787768740878, Training Loss Force: 3.5150367038816226, time: 3.115217924118042
Validation Loss Energy: 5.262778880821082, Validation Loss Force: 3.569235810656743, time: 0.16717004776000977
Test Loss Energy: 11.264936907866748, Test Loss Force: 11.769664469761064, time: 10.674650192260742

Epoch 16, Batch 100/177, Loss: 1.9625279903411865, Variance: 0.15771879255771637

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.108035119370949, Training Loss Force: 3.4918050582308715, time: 2.9485020637512207
Validation Loss Energy: 2.2286318724385414, Validation Loss Force: 3.5243359133952805, time: 0.18240857124328613
Test Loss Energy: 10.319307248376308, Test Loss Force: 11.911808660267068, time: 10.751659393310547

Epoch 17, Batch 100/177, Loss: 1.747016191482544, Variance: 0.16514340043067932

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.1332646110967906, Training Loss Force: 3.5188351463930085, time: 2.9344215393066406
Validation Loss Energy: 3.096626412045856, Validation Loss Force: 3.580599953188314, time: 0.16987204551696777
Test Loss Energy: 10.695816809810543, Test Loss Force: 12.059291081240099, time: 11.005967617034912

Epoch 18, Batch 100/177, Loss: 0.9057556390762329, Variance: 0.16415321826934814

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.098934098606354, Training Loss Force: 3.515650699584313, time: 2.882230043411255
Validation Loss Energy: 5.632787903717094, Validation Loss Force: 3.5245475233069605, time: 0.17232537269592285
Test Loss Energy: 11.925791438587027, Test Loss Force: 12.240885151634433, time: 10.784537553787231

Epoch 19, Batch 100/177, Loss: 1.1733635663986206, Variance: 0.16531449556350708

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.093530619150585, Training Loss Force: 3.5215788793831844, time: 2.91379976272583
Validation Loss Energy: 5.1688001802134815, Validation Loss Force: 3.563649932461999, time: 0.1713860034942627
Test Loss Energy: 11.380581054455138, Test Loss Force: 11.975810138113186, time: 10.875839710235596

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–†â–…â–â–‚â–‡â–†â–â–„â–ˆâ–†â–ƒâ–„â–†â–…â–‚â–ƒâ–ˆâ–†
wandb:   test_error_force â–†â–ƒâ–„â–ˆâ–…â–ƒâ–ƒâ–ƒâ–â–…â–†â–†â–‡â–„â–„â–‚â–„â–…â–‡â–„
wandb:          test_loss â–ˆâ–ƒâ–†â–†â–ƒâ–‚â–…â–„â–â–„â–‡â–†â–„â–„â–„â–„â–‚â–„â–ˆâ–†
wandb: train_error_energy â–â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–‡
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–ƒâ–‡â–†â–â–„â–ˆâ–‡â–‚â–„â–‡â–†â–â–„â–ˆâ–‡â–â–ƒâ–‡â–†
wandb:  valid_error_force â–„â–ƒâ–„â–ˆâ–ƒâ–‚â–ƒâ–‚â–†â–â–†â–†â–„â–‚â–ƒâ–„â–ƒâ–…â–ƒâ–„
wandb:         valid_loss â–â–‚â–‡â–†â–â–ƒâ–ˆâ–†â–‚â–ƒâ–‡â–†â–â–„â–ˆâ–†â–â–‚â–‡â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 5654
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.38058
wandb:   test_error_force 11.97581
wandb:          test_loss 10.2339
wandb: train_error_energy 4.09353
wandb:  train_error_force 3.52158
wandb:         train_loss 1.44286
wandb: valid_error_energy 5.1688
wandb:  valid_error_force 3.56365
wandb:         valid_loss 1.74681
wandb: 
wandb: ğŸš€ View run al_71_54 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hb7bm1v1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_202359-hb7bm1v1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.3183341026306152, Uncertainty Bias: -0.2872398793697357
0.000102996826 1.0088139
2.0354974 4.443959
(48745, 22, 3)
Found uncertainty sample 0 after 3200 steps.
Found uncertainty sample 1 after 1331 steps.
Found uncertainty sample 2 after 308 steps.
Found uncertainty sample 3 after 1207 steps.
Found uncertainty sample 4 after 3077 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 80 steps.
Found uncertainty sample 7 after 166 steps.
Found uncertainty sample 8 after 931 steps.
Found uncertainty sample 9 after 2964 steps.
Found uncertainty sample 10 after 12 steps.
Found uncertainty sample 11 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 177 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 17 steps.
Found uncertainty sample 16 after 737 steps.
Found uncertainty sample 17 after 43 steps.
Found uncertainty sample 18 after 3013 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 569 steps.
Found uncertainty sample 21 after 15 steps.
Found uncertainty sample 22 after 1453 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 2723 steps.
Found uncertainty sample 25 after 1088 steps.
Found uncertainty sample 26 after 1442 steps.
Found uncertainty sample 27 after 205 steps.
Found uncertainty sample 28 after 12 steps.
Found uncertainty sample 29 after 1033 steps.
Found uncertainty sample 30 after 566 steps.
Found uncertainty sample 31 after 1085 steps.
Found uncertainty sample 32 after 959 steps.
Found uncertainty sample 33 after 718 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 3119 steps.
Found uncertainty sample 36 after 43 steps.
Found uncertainty sample 37 after 773 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 101 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1093 steps.
Found uncertainty sample 42 after 851 steps.
Found uncertainty sample 43 after 368 steps.
Found uncertainty sample 44 after 170 steps.
Found uncertainty sample 45 after 183 steps.
Found uncertainty sample 46 after 3639 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 807 steps.
Found uncertainty sample 49 after 21 steps.
Found uncertainty sample 50 after 277 steps.
Found uncertainty sample 51 after 1002 steps.
Found uncertainty sample 52 after 1046 steps.
Found uncertainty sample 53 after 177 steps.
Found uncertainty sample 54 after 588 steps.
Found uncertainty sample 55 after 757 steps.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 1019 steps.
Found uncertainty sample 58 after 49 steps.
Found uncertainty sample 59 after 32 steps.
Found uncertainty sample 60 after 2417 steps.
Found uncertainty sample 61 after 905 steps.
Found uncertainty sample 62 after 949 steps.
Found uncertainty sample 63 after 562 steps.
Found uncertainty sample 64 after 2142 steps.
Found uncertainty sample 65 after 2352 steps.
Found uncertainty sample 66 after 248 steps.
Found uncertainty sample 67 after 449 steps.
Found uncertainty sample 68 after 1616 steps.
Found uncertainty sample 69 after 836 steps.
Found uncertainty sample 70 after 77 steps.
Found uncertainty sample 71 after 31 steps.
Found uncertainty sample 72 after 310 steps.
Found uncertainty sample 73 after 49 steps.
Found uncertainty sample 74 after 218 steps.
Found uncertainty sample 75 after 1036 steps.
Found uncertainty sample 76 after 1776 steps.
Found uncertainty sample 77 after 2754 steps.
Found uncertainty sample 78 after 79 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 846 steps.
Found uncertainty sample 82 after 331 steps.
Found uncertainty sample 83 after 1132 steps.
Found uncertainty sample 84 after 1040 steps.
Found uncertainty sample 85 after 1775 steps.
Found uncertainty sample 86 after 3 steps.
Found uncertainty sample 87 after 958 steps.
Found uncertainty sample 88 after 1643 steps.
Found uncertainty sample 89 after 9 steps.
Found uncertainty sample 90 after 2220 steps.
Found uncertainty sample 91 after 129 steps.
Found uncertainty sample 92 after 13 steps.
Found uncertainty sample 93 after 1073 steps.
Found uncertainty sample 94 after 1743 steps.
Found uncertainty sample 95 after 704 steps.
Found uncertainty sample 96 after 166 steps.
Found uncertainty sample 97 after 953 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 14 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_204246-cfxsw43g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_55
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/cfxsw43g
Training model 55. Added 91 samples to the dataset.
Epoch 0, Batch 100/180, Loss: 0.9994499087333679, Variance: 0.16855771839618683

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.496578551945168, Training Loss Force: 3.5773944885718403, time: 2.9114575386047363
Validation Loss Energy: 2.8672437913683457, Validation Loss Force: 3.631434354303995, time: 0.16530632972717285
Test Loss Energy: 10.738450555375813, Test Loss Force: 12.151359876691645, time: 10.004881381988525

Epoch 1, Batch 100/180, Loss: 0.8522697687149048, Variance: 0.16110675036907196

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.054593126113942, Training Loss Force: 3.5239965396388193, time: 2.9614129066467285
Validation Loss Energy: 3.8663188326851867, Validation Loss Force: 3.661093810318528, time: 0.1591341495513916
Test Loss Energy: 10.596860962598086, Test Loss Force: 12.161790203115542, time: 11.406420707702637

Epoch 2, Batch 100/180, Loss: 1.0139117240905762, Variance: 0.16266030073165894

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.083823465902427, Training Loss Force: 3.51099112633658, time: 3.2971746921539307
Validation Loss Energy: 3.2965353569995863, Validation Loss Force: 3.5284957441564546, time: 0.19889140129089355
Test Loss Energy: 10.84785287777138, Test Loss Force: 12.31815861058089, time: 12.898147106170654

Epoch 3, Batch 100/180, Loss: 0.8788545727729797, Variance: 0.16291402280330658

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.1282076725553765, Training Loss Force: 3.4971898823733385, time: 3.1018078327178955
Validation Loss Energy: 3.6641207004652223, Validation Loss Force: 3.5246973054191213, time: 0.17177057266235352
Test Loss Energy: 10.668205369392679, Test Loss Force: 11.636885645604998, time: 10.815613985061646

Epoch 4, Batch 100/180, Loss: 0.9966107606887817, Variance: 0.16889265179634094

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.124076890839865, Training Loss Force: 3.490614231165589, time: 2.9115707874298096
Validation Loss Energy: 3.4338108854640677, Validation Loss Force: 3.650248783789164, time: 0.18005704879760742
Test Loss Energy: 10.800393371863915, Test Loss Force: 12.32398688570841, time: 10.831826448440552

Epoch 5, Batch 100/180, Loss: 0.9745373725891113, Variance: 0.16320639848709106

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.109184907632025, Training Loss Force: 3.5313357579853784, time: 2.951605796813965
Validation Loss Energy: 3.5858986624078604, Validation Loss Force: 3.5241185140858065, time: 0.17244744300842285
Test Loss Energy: 10.842296417551655, Test Loss Force: 11.971664085248294, time: 10.693326950073242

Epoch 6, Batch 100/180, Loss: 1.0760623216629028, Variance: 0.16711750626564026

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.11078348839774, Training Loss Force: 3.5234976667360796, time: 3.099372625350952
Validation Loss Energy: 3.19870255036817, Validation Loss Force: 3.5460005821713736, time: 0.16870832443237305
Test Loss Energy: 10.659093808563279, Test Loss Force: 12.19807901118632, time: 10.863691568374634

Epoch 7, Batch 100/180, Loss: 0.967454731464386, Variance: 0.16810797154903412

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.151274049171814, Training Loss Force: 3.517965487679977, time: 2.884122610092163
Validation Loss Energy: 3.9723075647936468, Validation Loss Force: 3.5212164671069295, time: 0.1708667278289795
Test Loss Energy: 10.92339103292876, Test Loss Force: 12.070191894745468, time: 10.696960687637329

Epoch 8, Batch 100/180, Loss: 1.0564520359039307, Variance: 0.16685271263122559

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.132725041308591, Training Loss Force: 3.523053189431588, time: 2.9997944831848145
Validation Loss Energy: 3.243293160185939, Validation Loss Force: 3.5707279908266543, time: 0.1817331314086914
Test Loss Energy: 10.81152729081989, Test Loss Force: 12.280053428499006, time: 10.874063730239868

Epoch 9, Batch 100/180, Loss: 0.9010735750198364, Variance: 0.16365501284599304

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.078357365770953, Training Loss Force: 3.5532122680016514, time: 2.9084250926971436
Validation Loss Energy: 3.5451089406880296, Validation Loss Force: 3.566023882449784, time: 0.17283225059509277
Test Loss Energy: 10.882985312506035, Test Loss Force: 12.38592239958117, time: 10.729333877563477

Epoch 10, Batch 100/180, Loss: 0.96177077293396, Variance: 0.16740305721759796

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.114815160694925, Training Loss Force: 3.482927373089053, time: 2.8907430171966553
Validation Loss Energy: 3.2331749257488833, Validation Loss Force: 3.5593721612765314, time: 0.1685185432434082
Test Loss Energy: 10.758897938951998, Test Loss Force: 12.525473877184266, time: 10.827245473861694

Epoch 11, Batch 100/180, Loss: 0.9408811330795288, Variance: 0.1655934453010559

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.11929877095004, Training Loss Force: 3.5121464662300905, time: 2.9394094944000244
Validation Loss Energy: 3.8508663361882847, Validation Loss Force: 3.523579283637418, time: 0.16755104064941406
Test Loss Energy: 10.687207618553728, Test Loss Force: 12.069488241591422, time: 10.750530004501343

Epoch 12, Batch 100/180, Loss: 0.9330765008926392, Variance: 0.16858503222465515

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.12958223331568, Training Loss Force: 3.485855675234336, time: 2.970839500427246
Validation Loss Energy: 3.3305596478356443, Validation Loss Force: 3.4778412597106825, time: 0.1725919246673584
Test Loss Energy: 10.786923198902404, Test Loss Force: 12.210076163628129, time: 10.823712348937988

Epoch 13, Batch 100/180, Loss: 0.9485375881195068, Variance: 0.16591592133045197

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.094659675352136, Training Loss Force: 3.485737340190835, time: 2.910907506942749
Validation Loss Energy: 3.204695764653865, Validation Loss Force: 3.5254668499874473, time: 0.17914438247680664
Test Loss Energy: 10.525136735867667, Test Loss Force: 12.124224300057344, time: 10.761979341506958

Epoch 14, Batch 100/180, Loss: 0.9436404705047607, Variance: 0.1672564446926117

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.108243083945493, Training Loss Force: 3.512934408218074, time: 2.878279685974121
Validation Loss Energy: 3.0772712676513265, Validation Loss Force: 3.507982942872246, time: 0.17417168617248535
Test Loss Energy: 10.796590266789575, Test Loss Force: 12.545040069182877, time: 10.768641948699951

Epoch 15, Batch 100/180, Loss: 0.9746520519256592, Variance: 0.16711662709712982

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.103528353013312, Training Loss Force: 3.5283337746197074, time: 2.9302918910980225
Validation Loss Energy: 3.842544151082283, Validation Loss Force: 3.498574651895457, time: 0.17011332511901855
Test Loss Energy: 10.862495087852492, Test Loss Force: 12.17638806123337, time: 10.789590120315552

Epoch 16, Batch 100/180, Loss: 0.8867852687835693, Variance: 0.16585740447044373

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.1128897838868665, Training Loss Force: 3.4877245472725145, time: 2.938035249710083
Validation Loss Energy: 3.3088338192674702, Validation Loss Force: 3.5948998399628453, time: 0.17734575271606445
Test Loss Energy: 11.07919387941636, Test Loss Force: 12.612690247954841, time: 10.705039024353027

Epoch 17, Batch 100/180, Loss: 0.9093074798583984, Variance: 0.1625504195690155

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.095946493335487, Training Loss Force: 3.497799716259697, time: 3.0593032836914062
Validation Loss Energy: 3.4965559334184757, Validation Loss Force: 3.509069642527422, time: 0.17627811431884766
Test Loss Energy: 10.648373046961439, Test Loss Force: 12.143820173503366, time: 10.782645225524902

Epoch 18, Batch 100/180, Loss: 0.9923349022865295, Variance: 0.164603590965271

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.07050916534243, Training Loss Force: 3.503341572764292, time: 3.005882740020752
Validation Loss Energy: 2.930667757548687, Validation Loss Force: 3.6456183046765473, time: 0.1726512908935547
Test Loss Energy: 10.53645207771626, Test Loss Force: 12.40689216264209, time: 10.661864280700684

Epoch 19, Batch 100/180, Loss: 0.9411090612411499, Variance: 0.16366010904312134

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.097337997362218, Training Loss Force: 3.512553874069988, time: 2.9887189865112305
Validation Loss Energy: 3.406347392566675, Validation Loss Force: 3.5136959651493154, time: 0.17264103889465332
Test Loss Energy: 10.542936949362188, Test Loss Force: 12.040104466689451, time: 10.816051721572876

wandb: - 0.039 MB of 0.056 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–‚â–…â–ƒâ–„â–…â–ƒâ–†â–…â–†â–„â–ƒâ–„â–â–„â–…â–ˆâ–ƒâ–â–
wandb:   test_error_force â–…â–…â–†â–â–†â–ƒâ–…â–„â–†â–†â–‡â–„â–…â–„â–ˆâ–…â–ˆâ–…â–‡â–„
wandb:          test_loss â–†â–…â–†â–â–…â–ƒâ–…â–„â–‡â–†â–…â–‚â–…â–â–…â–…â–ˆâ–‚â–…â–ƒ
wandb: train_error_energy â–ˆâ–â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:  train_error_force â–ˆâ–„â–ƒâ–‚â–‚â–…â–„â–„â–„â–†â–â–ƒâ–â–â–ƒâ–„â–â–‚â–ƒâ–ƒ
wandb:         train_loss â–ˆâ–â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–â–â–‚
wandb: valid_error_energy â–â–‡â–„â–†â–…â–†â–ƒâ–ˆâ–ƒâ–…â–ƒâ–‡â–„â–ƒâ–‚â–‡â–„â–…â–â–„
wandb:  valid_error_force â–‡â–ˆâ–ƒâ–ƒâ–ˆâ–ƒâ–„â–ƒâ–…â–„â–„â–ƒâ–â–ƒâ–‚â–‚â–…â–‚â–‡â–‚
wandb:         valid_loss â–â–ˆâ–ƒâ–…â–…â–…â–‚â–‡â–ƒâ–…â–ƒâ–†â–‚â–‚â–â–†â–ƒâ–„â–â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5735
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.54294
wandb:   test_error_force 12.0401
wandb:          test_loss 9.42622
wandb: train_error_energy 4.09734
wandb:  train_error_force 3.51255
wandb:         train_loss 1.44071
wandb: valid_error_energy 3.40635
wandb:  valid_error_force 3.5137
wandb:         valid_loss 1.2167
wandb: 
wandb: ğŸš€ View run al_71_55 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/cfxsw43g
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_204246-cfxsw43g/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.155592441558838, Uncertainty Bias: -0.2849922478199005
6.67572e-05 0.02510643
2.166451 4.3051634
(48745, 22, 3)
Found uncertainty sample 0 after 3149 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 3130 steps.
Found uncertainty sample 4 after 2273 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 136 steps.
Found uncertainty sample 8 after 365 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 3158 steps.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 1102 steps.
Found uncertainty sample 15 after 3636 steps.
Found uncertainty sample 16 after 1050 steps.
Found uncertainty sample 17 after 232 steps.
Found uncertainty sample 18 after 250 steps.
Found uncertainty sample 19 after 1338 steps.
Found uncertainty sample 20 after 2475 steps.
Found uncertainty sample 21 after 1068 steps.
Found uncertainty sample 22 after 2436 steps.
Found uncertainty sample 23 after 345 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 1963 steps.
Found uncertainty sample 26 after 141 steps.
Found uncertainty sample 27 after 1 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1184 steps.
Found uncertainty sample 30 after 149 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 1620 steps.
Found uncertainty sample 33 after 987 steps.
Found uncertainty sample 34 after 2542 steps.
Found uncertainty sample 35 after 3568 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 4 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 1646 steps.
Found uncertainty sample 40 after 2260 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 19 steps.
Found uncertainty sample 43 after 2528 steps.
Found uncertainty sample 44 after 31 steps.
Found uncertainty sample 45 after 461 steps.
Found uncertainty sample 46 after 490 steps.
Found uncertainty sample 47 after 23 steps.
Found uncertainty sample 48 after 234 steps.
Found uncertainty sample 49 after 2 steps.
Found uncertainty sample 50 after 470 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 543 steps.
Found uncertainty sample 53 after 158 steps.
Found uncertainty sample 54 after 319 steps.
Found uncertainty sample 55 after 339 steps.
Found uncertainty sample 56 after 2168 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 1244 steps.
Found uncertainty sample 59 after 1841 steps.
Found uncertainty sample 60 after 401 steps.
Found uncertainty sample 61 after 617 steps.
Found uncertainty sample 62 after 2033 steps.
Found uncertainty sample 63 after 581 steps.
Found uncertainty sample 64 after 199 steps.
Found uncertainty sample 65 after 146 steps.
Found uncertainty sample 66 after 3840 steps.
Found uncertainty sample 67 after 1786 steps.
Found uncertainty sample 68 after 703 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 2857 steps.
Found uncertainty sample 71 after 2911 steps.
Found uncertainty sample 72 after 1746 steps.
Found uncertainty sample 73 after 1004 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 512 steps.
Found uncertainty sample 76 after 862 steps.
Found uncertainty sample 77 after 37 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 502 steps.
Found uncertainty sample 80 after 1128 steps.
Found uncertainty sample 81 after 565 steps.
Found uncertainty sample 82 after 313 steps.
Found uncertainty sample 83 after 414 steps.
Found uncertainty sample 84 after 471 steps.
Found uncertainty sample 85 after 2651 steps.
Found uncertainty sample 86 after 535 steps.
Found uncertainty sample 87 after 1375 steps.
Found uncertainty sample 88 after 1109 steps.
Found uncertainty sample 89 after 1231 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 310 steps.
Found uncertainty sample 92 after 1005 steps.
Found uncertainty sample 93 after 590 steps.
Found uncertainty sample 94 after 534 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 307 steps.
Found uncertainty sample 97 after 1807 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 588 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_210635-j4hf2gy7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_56
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j4hf2gy7
Training model 56. Added 81 samples to the dataset.
Epoch 0, Batch 100/182, Loss: 0.9291831851005554, Variance: 0.16687056422233582

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.513799523815396, Training Loss Force: 3.5771413944331023, time: 3.0575079917907715
Validation Loss Energy: 5.475727034068149, Validation Loss Force: 3.516805985110489, time: 0.17612814903259277
Test Loss Energy: 11.418260805566291, Test Loss Force: 12.158577262210864, time: 11.000723838806152

Epoch 1, Batch 100/182, Loss: 1.926945686340332, Variance: 0.1622985303401947

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.095262948077326, Training Loss Force: 3.513745753725653, time: 2.934964656829834
Validation Loss Energy: 3.7366876844465047, Validation Loss Force: 3.5075714502905715, time: 0.17679476737976074
Test Loss Energy: 10.88760747630559, Test Loss Force: 12.098365338097967, time: 11.054273843765259

Epoch 2, Batch 100/182, Loss: 0.9552364945411682, Variance: 0.16436301171779633

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.10125747003448, Training Loss Force: 3.503233516430352, time: 2.9770376682281494
Validation Loss Energy: 4.781758600561066, Validation Loss Force: 3.5729782109923516, time: 0.17335104942321777
Test Loss Energy: 11.38269052634748, Test Loss Force: 12.120218782900324, time: 10.941839694976807

Epoch 3, Batch 100/182, Loss: 1.775127649307251, Variance: 0.1692502349615097

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.04133126909961, Training Loss Force: 3.508579364626528, time: 2.98360538482666
Validation Loss Energy: 3.4000785881013704, Validation Loss Force: 3.520205994522709, time: 0.1731271743774414
Test Loss Energy: 11.056791512034437, Test Loss Force: 12.513011824208355, time: 11.031836032867432

Epoch 4, Batch 100/182, Loss: 0.9431345462799072, Variance: 0.16246990859508514

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.105258038728124, Training Loss Force: 3.501770649878647, time: 3.239187240600586
Validation Loss Energy: 5.57059106719996, Validation Loss Force: 3.5615264110403335, time: 0.17758727073669434
Test Loss Energy: 11.401921255036774, Test Loss Force: 11.958103688147823, time: 11.017090797424316

Epoch 5, Batch 100/182, Loss: 2.1068549156188965, Variance: 0.1601610779762268

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.109814894285953, Training Loss Force: 3.5262153323383556, time: 2.8701252937316895
Validation Loss Energy: 3.3411613712115584, Validation Loss Force: 3.662562090666776, time: 0.17629456520080566
Test Loss Energy: 10.815367132805891, Test Loss Force: 12.326258941051485, time: 10.936718940734863

Epoch 6, Batch 100/182, Loss: 0.9700631499290466, Variance: 0.16606152057647705

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.109911470910715, Training Loss Force: 3.4986489067539774, time: 3.163468360900879
Validation Loss Energy: 4.939052679028843, Validation Loss Force: 3.5583287336587137, time: 0.18129825592041016
Test Loss Energy: 11.533012825307006, Test Loss Force: 12.280308455785324, time: 10.939011573791504

Epoch 7, Batch 100/182, Loss: 1.9158167839050293, Variance: 0.16842439770698547

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.1057680342673635, Training Loss Force: 3.497862228895983, time: 2.945061445236206
Validation Loss Energy: 3.399387200403264, Validation Loss Force: 3.6472892139475888, time: 0.17658090591430664
Test Loss Energy: 10.626027958011335, Test Loss Force: 12.037097262749151, time: 10.790708541870117

Epoch 8, Batch 100/182, Loss: 0.8680763840675354, Variance: 0.16552285850048065

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.1039714894362875, Training Loss Force: 3.52289343167372, time: 3.0170094966888428
Validation Loss Energy: 5.324164459416552, Validation Loss Force: 3.5271381303737335, time: 0.2023789882659912
Test Loss Energy: 11.575391231149668, Test Loss Force: 11.868538216178065, time: 12.591847658157349

Epoch 9, Batch 100/182, Loss: 1.798840045928955, Variance: 0.16003404557704926

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.097172405740697, Training Loss Force: 3.495025583026253, time: 3.3413658142089844
Validation Loss Energy: 3.5494583371154373, Validation Loss Force: 3.549149495010036, time: 0.2020282745361328
Test Loss Energy: 10.466719068074589, Test Loss Force: 11.925861993229498, time: 12.414983987808228

Epoch 10, Batch 100/182, Loss: 0.9746059775352478, Variance: 0.1667734980583191

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.078745344715117, Training Loss Force: 3.4903610547017996, time: 3.20577335357666
Validation Loss Energy: 4.75165640406138, Validation Loss Force: 3.6396705148966206, time: 0.1928420066833496
Test Loss Energy: 11.515793715329778, Test Loss Force: 12.292648103684554, time: 12.441271543502808

Epoch 11, Batch 100/182, Loss: 1.8514931201934814, Variance: 0.1686294972896576

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.0865442410154325, Training Loss Force: 3.512994064286534, time: 3.3731281757354736
Validation Loss Energy: 3.5227706235224567, Validation Loss Force: 3.544133798274963, time: 0.1941838264465332
Test Loss Energy: 10.874851532699651, Test Loss Force: 12.287940388300683, time: 12.412663459777832

Epoch 12, Batch 100/182, Loss: 0.9419135451316833, Variance: 0.16711923480033875

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.138409245066455, Training Loss Force: 3.487463919902158, time: 3.4250078201293945
Validation Loss Energy: 5.443809341798666, Validation Loss Force: 3.4984346672257165, time: 0.20313286781311035
Test Loss Energy: 11.342293955178452, Test Loss Force: 12.045244579906962, time: 12.36110520362854

Epoch 13, Batch 100/182, Loss: 1.7520217895507812, Variance: 0.16161088645458221

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.067159705721006, Training Loss Force: 3.5351311431919274, time: 3.4246487617492676
Validation Loss Energy: 3.6449551217369254, Validation Loss Force: 3.5654750030050715, time: 0.20128798484802246
Test Loss Energy: 10.957429082619583, Test Loss Force: 12.124119265505662, time: 12.556822776794434

Epoch 14, Batch 100/182, Loss: 1.055765151977539, Variance: 0.1641099900007248

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.066450696573951, Training Loss Force: 3.5474514515052498, time: 3.439032554626465
Validation Loss Energy: 4.9253747071788885, Validation Loss Force: 3.514758764982829, time: 0.21696996688842773
Test Loss Energy: 11.36077693522797, Test Loss Force: 12.379800471648263, time: 12.45091199874878

Epoch 15, Batch 100/182, Loss: 1.7888649702072144, Variance: 0.16757754981517792

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.036072296738866, Training Loss Force: 3.4794727524647056, time: 3.236829996109009
Validation Loss Energy: 3.1423027566191695, Validation Loss Force: 3.4813131170339884, time: 0.19019103050231934
Test Loss Energy: 10.718115420365308, Test Loss Force: 12.400994279507426, time: 12.44576096534729

Epoch 16, Batch 100/182, Loss: 1.114059567451477, Variance: 0.16533413529396057

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.1006392689131586, Training Loss Force: 3.5020369707413743, time: 3.3473424911499023
Validation Loss Energy: 5.249274294012757, Validation Loss Force: 3.5903250281367094, time: 0.19516777992248535
Test Loss Energy: 11.551944440550708, Test Loss Force: 12.504213772209969, time: 13.254883289337158

Epoch 17, Batch 100/182, Loss: 2.207509994506836, Variance: 0.16081391274929047

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.091694936221869, Training Loss Force: 3.5347668846086973, time: 3.344698429107666
Validation Loss Energy: 3.2277849964025553, Validation Loss Force: 3.501494809298203, time: 0.19397449493408203
Test Loss Energy: 10.805065287644995, Test Loss Force: 12.361983069494643, time: 12.610189199447632

Epoch 18, Batch 100/182, Loss: 0.9250074625015259, Variance: 0.16825398802757263

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.101865573354724, Training Loss Force: 3.4790261185320506, time: 3.3441195487976074
Validation Loss Energy: 4.8796341596685, Validation Loss Force: 3.5383916063397773, time: 0.19646024703979492
Test Loss Energy: 11.255427237920568, Test Loss Force: 12.308510633949915, time: 12.4406099319458

Epoch 19, Batch 100/182, Loss: 1.7365868091583252, Variance: 0.16950231790542603

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.081783129948933, Training Loss Force: 3.5050059848541606, time: 3.362642288208008
Validation Loss Energy: 3.0417518277234126, Validation Loss Force: 3.5803305235217415, time: 0.2005321979522705
Test Loss Energy: 10.763819883038783, Test Loss Force: 12.522802094584227, time: 12.594054937362671

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–„â–‡â–…â–‡â–ƒâ–ˆâ–‚â–ˆâ–â–ˆâ–„â–‡â–„â–‡â–ƒâ–ˆâ–ƒâ–†â–ƒ
wandb:   test_error_force â–„â–ƒâ–„â–ˆâ–‚â–†â–…â–ƒâ–â–‚â–†â–…â–ƒâ–„â–†â–‡â–ˆâ–†â–†â–ˆ
wandb:          test_loss â–„â–„â–†â–†â–…â–…â–‡â–ƒâ–„â–â–ˆâ–…â–ƒâ–ƒâ–†â–„â–…â–ƒâ–…â–…
wandb: train_error_energy â–ˆâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–„â–‚â–‚â–ƒâ–‚â–…â–†â–â–ƒâ–…â–â–ƒ
wandb:         train_loss â–ˆâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚
wandb: valid_error_energy â–ˆâ–ƒâ–†â–‚â–ˆâ–‚â–†â–‚â–‡â–‚â–†â–‚â–ˆâ–ƒâ–†â–â–‡â–‚â–†â–
wandb:  valid_error_force â–‚â–‚â–…â–ƒâ–„â–ˆâ–„â–‡â–ƒâ–„â–‡â–ƒâ–‚â–„â–‚â–â–…â–‚â–ƒâ–…
wandb:         valid_loss â–‡â–‚â–†â–‚â–ˆâ–‚â–†â–‚â–‡â–‚â–†â–‚â–‡â–‚â–†â–â–‡â–â–†â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 5807
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.76382
wandb:   test_error_force 12.5228
wandb:          test_loss 9.87733
wandb: train_error_energy 4.08178
wandb:  train_error_force 3.50501
wandb:         train_loss 1.4305
wandb: valid_error_energy 3.04175
wandb:  valid_error_force 3.58033
wandb:         valid_loss 1.15786
wandb: 
wandb: ğŸš€ View run al_71_56 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j4hf2gy7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_210635-j4hf2gy7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.079878807067871, Uncertainty Bias: -0.2552924156188965
4.7683716e-05 0.0083293915
2.2150471 4.386441
(48745, 22, 3)
Found uncertainty sample 0 after 3029 steps.
Found uncertainty sample 1 after 415 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 196 steps.
Found uncertainty sample 5 after 900 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 2861 steps.
Found uncertainty sample 8 after 49 steps.
Found uncertainty sample 9 after 390 steps.
Found uncertainty sample 10 after 1771 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 927 steps.
Found uncertainty sample 13 after 417 steps.
Found uncertainty sample 14 after 928 steps.
Found uncertainty sample 15 after 1947 steps.
Found uncertainty sample 16 after 1622 steps.
Found uncertainty sample 17 after 1551 steps.
Found uncertainty sample 18 after 66 steps.
Found uncertainty sample 19 after 132 steps.
Found uncertainty sample 20 after 351 steps.
Found uncertainty sample 21 after 477 steps.
Found uncertainty sample 22 after 3014 steps.
Found uncertainty sample 23 after 1773 steps.
Found uncertainty sample 24 after 623 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1159 steps.
Found uncertainty sample 27 after 1796 steps.
Found uncertainty sample 28 after 1050 steps.
Found uncertainty sample 29 after 7 steps.
Found uncertainty sample 30 after 104 steps.
Found uncertainty sample 31 after 2 steps.
Found uncertainty sample 32 after 1788 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 214 steps.
Found uncertainty sample 35 after 2657 steps.
Found uncertainty sample 36 after 2900 steps.
Found uncertainty sample 37 after 2589 steps.
Found uncertainty sample 38 after 3365 steps.
Found uncertainty sample 39 after 3877 steps.
Found uncertainty sample 40 after 1289 steps.
Found uncertainty sample 41 after 1244 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 188 steps.
Found uncertainty sample 44 after 666 steps.
Found uncertainty sample 45 after 1375 steps.
Found uncertainty sample 46 after 1208 steps.
Found uncertainty sample 47 after 3153 steps.
Found uncertainty sample 48 after 1107 steps.
Found uncertainty sample 49 after 1886 steps.
Found uncertainty sample 50 after 1969 steps.
Found uncertainty sample 51 after 576 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 244 steps.
Found uncertainty sample 54 after 1186 steps.
Found uncertainty sample 55 after 1248 steps.
Found uncertainty sample 56 after 1620 steps.
Found uncertainty sample 57 after 3089 steps.
Found uncertainty sample 58 after 3884 steps.
Found uncertainty sample 59 after 1436 steps.
Found uncertainty sample 60 after 350 steps.
Found uncertainty sample 61 after 1807 steps.
Found uncertainty sample 62 after 348 steps.
Found uncertainty sample 63 after 73 steps.
Found uncertainty sample 64 after 2599 steps.
Found uncertainty sample 65 after 538 steps.
Found uncertainty sample 66 after 635 steps.
Found uncertainty sample 67 after 926 steps.
Found uncertainty sample 68 after 38 steps.
Found uncertainty sample 69 after 1082 steps.
Found uncertainty sample 70 after 22 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 3241 steps.
Found uncertainty sample 73 after 828 steps.
Found uncertainty sample 74 after 3 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1811 steps.
Found uncertainty sample 77 after 528 steps.
Found uncertainty sample 78 after 590 steps.
Found uncertainty sample 79 after 1228 steps.
Found uncertainty sample 80 after 615 steps.
Found uncertainty sample 81 after 749 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 2010 steps.
Found uncertainty sample 84 after 741 steps.
Found uncertainty sample 85 after 570 steps.
Found uncertainty sample 86 after 689 steps.
Found uncertainty sample 87 after 525 steps.
Found uncertainty sample 88 after 51 steps.
Found uncertainty sample 89 after 2675 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 189 steps.
Found uncertainty sample 92 after 1399 steps.
Found uncertainty sample 93 after 10 steps.
Found uncertainty sample 94 after 3032 steps.
Found uncertainty sample 95 after 2368 steps.
Found uncertainty sample 96 after 479 steps.
Found uncertainty sample 97 after 20 steps.
Found uncertainty sample 98 after 3089 steps.
Found uncertainty sample 99 after 253 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_212652-w3kw1758
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_57
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/w3kw1758
Training model 57. Added 96 samples to the dataset.
Epoch 0, Batch 100/185, Loss: 1.0218666791915894, Variance: 0.16741788387298584

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.457788070735496, Training Loss Force: 3.578306781949792, time: 3.0806663036346436
Validation Loss Energy: 5.288995649170426, Validation Loss Force: 3.4825191673513824, time: 0.17557787895202637
Test Loss Energy: 11.611513823114946, Test Loss Force: 12.275098422124378, time: 10.620632410049438

Epoch 1, Batch 100/185, Loss: 1.187341570854187, Variance: 0.16176825761795044

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.098189879163226, Training Loss Force: 3.504331034073926, time: 3.03147554397583
Validation Loss Energy: 5.130054969480035, Validation Loss Force: 3.622029069360676, time: 0.18073534965515137
Test Loss Energy: 11.378462685123411, Test Loss Force: 12.13622576598523, time: 10.780749559402466

Epoch 2, Batch 100/185, Loss: 2.007059097290039, Variance: 0.15680667757987976

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.070391062580294, Training Loss Force: 3.5266615092735063, time: 3.048757553100586
Validation Loss Energy: 2.5463011209020636, Validation Loss Force: 3.6776348991583467, time: 0.17893218994140625
Test Loss Energy: 10.317749240522899, Test Loss Force: 12.032006034152726, time: 10.678555011749268

Epoch 3, Batch 100/185, Loss: 1.5601800680160522, Variance: 0.1577189862728119

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.092252323939298, Training Loss Force: 3.4784166086993635, time: 2.970912218093872
Validation Loss Energy: 3.4715484812006663, Validation Loss Force: 3.408477167280188, time: 0.18676018714904785
Test Loss Energy: 10.786871180591977, Test Loss Force: 12.44720440865581, time: 10.53160548210144

Epoch 4, Batch 100/185, Loss: 0.9094282388687134, Variance: 0.16394011676311493

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.0570462003623975, Training Loss Force: 3.5303423795042006, time: 3.2498137950897217
Validation Loss Energy: 5.899513030843277, Validation Loss Force: 3.5896796401730002, time: 0.1822643280029297
Test Loss Energy: 12.241413094350388, Test Loss Force: 12.558241047970837, time: 10.759432077407837

Epoch 5, Batch 100/185, Loss: 1.281627893447876, Variance: 0.1687798798084259

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.072275875656424, Training Loss Force: 3.500950679326192, time: 3.004791259765625
Validation Loss Energy: 5.151497228890871, Validation Loss Force: 3.5532559779549975, time: 0.1859753131866455
Test Loss Energy: 11.72309268861387, Test Loss Force: 12.48918033593688, time: 10.745962381362915

Epoch 6, Batch 100/185, Loss: 2.074883222579956, Variance: 0.169386625289917

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.106307636741423, Training Loss Force: 3.4902491845258816, time: 3.0218732357025146
Validation Loss Energy: 1.7113705093823017, Validation Loss Force: 3.5352634759547352, time: 0.17704391479492188
Test Loss Energy: 10.20963171210449, Test Loss Force: 12.059140734100122, time: 11.036978483200073

Epoch 7, Batch 100/185, Loss: 1.563138723373413, Variance: 0.16931873559951782

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.077284485895776, Training Loss Force: 3.512454836216722, time: 3.090996742248535
Validation Loss Energy: 4.012302394774039, Validation Loss Force: 3.7329632832001916, time: 0.1784677505493164
Test Loss Energy: 10.848502041104059, Test Loss Force: 12.293856486004644, time: 10.569037675857544

Epoch 8, Batch 100/185, Loss: 0.9485877156257629, Variance: 0.16518177092075348

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.121039722322172, Training Loss Force: 3.496831909156733, time: 3.06522274017334
Validation Loss Energy: 5.633959811432111, Validation Loss Force: 3.677275121485814, time: 0.17778396606445312
Test Loss Energy: 11.668899751671658, Test Loss Force: 12.18724738599756, time: 10.820796012878418

Epoch 9, Batch 100/185, Loss: 1.1136568784713745, Variance: 0.16303451359272003

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.068182765397895, Training Loss Force: 3.503784818233646, time: 2.9591376781463623
Validation Loss Energy: 5.441941630114147, Validation Loss Force: 3.615997741539503, time: 0.17813491821289062
Test Loss Energy: 11.403615545733949, Test Loss Force: 11.892178936447635, time: 10.638217687606812

Epoch 10, Batch 100/185, Loss: 1.9418020248413086, Variance: 0.16031727194786072

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.081095286582163, Training Loss Force: 3.5040802821360373, time: 3.0402565002441406
Validation Loss Energy: 2.260195873325481, Validation Loss Force: 3.8061436093439323, time: 0.18543362617492676
Test Loss Energy: 10.352454607493733, Test Loss Force: 12.415477133091706, time: 10.737643241882324

Epoch 11, Batch 100/185, Loss: 1.516528606414795, Variance: 0.16278696060180664

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.084438027094873, Training Loss Force: 3.528455851875646, time: 2.982511043548584
Validation Loss Energy: 2.97200645149204, Validation Loss Force: 3.43501331740784, time: 0.17808914184570312
Test Loss Energy: 10.704711507213107, Test Loss Force: 12.4696087718994, time: 11.599031686782837

Epoch 12, Batch 100/185, Loss: 0.872355580329895, Variance: 0.1625049114227295

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.061908107743094, Training Loss Force: 3.499231229384955, time: 3.025038242340088
Validation Loss Energy: 5.440864468120722, Validation Loss Force: 3.5760635332340893, time: 0.1787407398223877
Test Loss Energy: 11.630421296826324, Test Loss Force: 12.299079110381694, time: 10.840736389160156

Epoch 13, Batch 100/185, Loss: 1.1589304208755493, Variance: 0.16793300211429596

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.107451070488954, Training Loss Force: 3.4871143676471705, time: 2.94970703125
Validation Loss Energy: 5.018681429415604, Validation Loss Force: 3.5661538098964325, time: 0.18010330200195312
Test Loss Energy: 11.489678979967573, Test Loss Force: 12.370477284197282, time: 10.755714178085327

Epoch 14, Batch 100/185, Loss: 1.6856778860092163, Variance: 0.1684781312942505

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.12480502207311, Training Loss Force: 3.5198892221860665, time: 2.9983270168304443
Validation Loss Energy: 1.8289029666885226, Validation Loss Force: 3.4254117681681473, time: 0.18028569221496582
Test Loss Energy: 10.111541644500456, Test Loss Force: 11.996819081617335, time: 10.680780410766602

Epoch 15, Batch 100/185, Loss: 1.7622668743133545, Variance: 0.16649916768074036

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.065908973396168, Training Loss Force: 3.4937447181992938, time: 3.1488332748413086
Validation Loss Energy: 3.5603810009536816, Validation Loss Force: 3.549128304329648, time: 0.18414592742919922
Test Loss Energy: 10.48073235789658, Test Loss Force: 12.006615456158906, time: 10.619128942489624

Epoch 16, Batch 100/185, Loss: 1.0174739360809326, Variance: 0.16691848635673523

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.093369443610055, Training Loss Force: 3.490846495111642, time: 3.0926167964935303
Validation Loss Energy: 5.893238214311031, Validation Loss Force: 3.53488031802188, time: 0.18083953857421875
Test Loss Energy: 11.685721368078232, Test Loss Force: 12.04025619414497, time: 10.675045013427734

Epoch 17, Batch 100/185, Loss: 1.1331899166107178, Variance: 0.1634080708026886

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.114001421433222, Training Loss Force: 3.5218322909270485, time: 3.19402813911438
Validation Loss Energy: 5.1526570011374355, Validation Loss Force: 3.505599158330517, time: 0.1806657314300537
Test Loss Energy: 11.04912505297758, Test Loss Force: 11.740260400490024, time: 10.610138177871704

Epoch 18, Batch 100/185, Loss: 2.1044418811798096, Variance: 0.16036272048950195

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.056406528476146, Training Loss Force: 3.497573073639698, time: 2.952449321746826
Validation Loss Energy: 2.386505391691311, Validation Loss Force: 3.519210199546358, time: 0.1790633201599121
Test Loss Energy: 10.178387066668051, Test Loss Force: 12.211153546541992, time: 10.630863904953003

Epoch 19, Batch 100/185, Loss: 1.8216116428375244, Variance: 0.16104301810264587

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.0419863969525665, Training Loss Force: 3.5181975745940277, time: 2.942934274673462
Validation Loss Energy: 3.158979673638004, Validation Loss Force: 3.557459581759109, time: 0.1800236701965332
Test Loss Energy: 10.604698720679162, Test Loss Force: 12.270609794527186, time: 10.842456579208374

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–…â–‚â–ƒâ–ˆâ–†â–â–ƒâ–†â–…â–‚â–ƒâ–†â–†â–â–‚â–†â–„â–â–ƒ
wandb:   test_error_force â–†â–„â–ƒâ–‡â–ˆâ–‡â–„â–†â–…â–‚â–‡â–‡â–†â–†â–ƒâ–ƒâ–„â–â–…â–†
wandb:          test_loss â–„â–ƒâ–‚â–„â–ˆâ–‡â–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–…â–…â–â–â–„â–‚â–â–ƒ
wandb: train_error_energy â–ˆâ–‚â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–
wandb:  train_error_force â–ˆâ–ƒâ–„â–â–…â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–…â–‚â–‚â–„â–‚â–‚â–„â–‚â–„
wandb:         train_loss â–ˆâ–â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–‚â–â–â–‚â–â–
wandb: valid_error_energy â–‡â–‡â–‚â–„â–ˆâ–‡â–â–…â–ˆâ–‡â–‚â–ƒâ–‡â–‡â–â–„â–ˆâ–‡â–‚â–ƒ
wandb:  valid_error_force â–‚â–…â–†â–â–„â–„â–ƒâ–‡â–†â–…â–ˆâ–â–„â–„â–â–ƒâ–ƒâ–ƒâ–ƒâ–„
wandb:         valid_loss â–†â–†â–‚â–ƒâ–ˆâ–†â–â–„â–‡â–‡â–‚â–‚â–‡â–†â–â–ƒâ–‡â–†â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5893
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.6047
wandb:   test_error_force 12.27061
wandb:          test_loss 9.59528
wandb: train_error_energy 4.04199
wandb:  train_error_force 3.5182
wandb:         train_loss 1.42964
wandb: valid_error_energy 3.15898
wandb:  valid_error_force 3.55746
wandb:         valid_loss 1.18485
wandb: 
wandb: ğŸš€ View run al_71_57 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/w3kw1758
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_212652-w3kw1758/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.319932699203491, Uncertainty Bias: -0.30400243401527405
9.536743e-05 0.002571106
2.1213078 4.3891253
(48745, 22, 3)
Found uncertainty sample 0 after 116 steps.
Found uncertainty sample 1 after 1643 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 855 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 78 steps.
Found uncertainty sample 6 after 36 steps.
Found uncertainty sample 7 after 993 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 2604 steps.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 1887 steps.
Found uncertainty sample 14 after 1282 steps.
Found uncertainty sample 15 after 954 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 33 steps.
Found uncertainty sample 18 after 1045 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 796 steps.
Found uncertainty sample 21 after 2562 steps.
Found uncertainty sample 22 after 1160 steps.
Found uncertainty sample 23 after 146 steps.
Found uncertainty sample 24 after 628 steps.
Found uncertainty sample 25 after 113 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 3824 steps.
Found uncertainty sample 28 after 125 steps.
Found uncertainty sample 29 after 246 steps.
Found uncertainty sample 30 after 12 steps.
Found uncertainty sample 31 after 501 steps.
Found uncertainty sample 32 after 2150 steps.
Found uncertainty sample 33 after 21 steps.
Found uncertainty sample 34 after 1166 steps.
Found uncertainty sample 35 after 2043 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 3658 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 176 steps.
Found uncertainty sample 40 after 279 steps.
Found uncertainty sample 41 after 3217 steps.
Found uncertainty sample 42 after 3263 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 2497 steps.
Found uncertainty sample 45 after 3593 steps.
Found uncertainty sample 46 after 77 steps.
Found uncertainty sample 47 after 2853 steps.
Found uncertainty sample 48 after 2060 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1795 steps.
Found uncertainty sample 51 after 1421 steps.
Found uncertainty sample 52 after 354 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 1962 steps.
Found uncertainty sample 55 after 93 steps.
Found uncertainty sample 56 after 5 steps.
Found uncertainty sample 57 after 299 steps.
Found uncertainty sample 58 after 1920 steps.
Found uncertainty sample 59 after 982 steps.
Found uncertainty sample 60 after 2973 steps.
Found uncertainty sample 61 after 513 steps.
Found uncertainty sample 62 after 86 steps.
Found uncertainty sample 63 after 3837 steps.
Found uncertainty sample 64 after 725 steps.
Found uncertainty sample 65 after 1481 steps.
Found uncertainty sample 66 after 1037 steps.
Found uncertainty sample 67 after 906 steps.
Found uncertainty sample 68 after 761 steps.
Found uncertainty sample 69 after 2880 steps.
Found uncertainty sample 70 after 1191 steps.
Found uncertainty sample 71 after 1270 steps.
Found uncertainty sample 72 after 63 steps.
Found uncertainty sample 73 after 755 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 2210 steps.
Found uncertainty sample 76 after 3232 steps.
Found uncertainty sample 77 after 2053 steps.
Found uncertainty sample 78 after 501 steps.
Found uncertainty sample 79 after 1574 steps.
Found uncertainty sample 80 after 2572 steps.
Found uncertainty sample 81 after 349 steps.
Found uncertainty sample 82 after 460 steps.
Found uncertainty sample 83 after 1237 steps.
Found uncertainty sample 84 after 238 steps.
Found uncertainty sample 85 after 679 steps.
Found uncertainty sample 86 after 114 steps.
Found uncertainty sample 87 after 3317 steps.
Found uncertainty sample 88 after 3567 steps.
Found uncertainty sample 89 after 1426 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 1018 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1642 steps.
Found uncertainty sample 94 after 20 steps.
Found uncertainty sample 95 after 1231 steps.
Found uncertainty sample 96 after 132 steps.
Found uncertainty sample 97 after 338 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 423 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_214951-a1yhkr2e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_58
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/a1yhkr2e
Training model 58. Added 87 samples to the dataset.
Epoch 0, Batch 100/187, Loss: 1.2505123615264893, Variance: 0.14086395502090454

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.972706678976628, Training Loss Force: 3.5556245986683526, time: 3.5705795288085938
Validation Loss Energy: 1.7962929589328251, Validation Loss Force: 3.4583169238767724, time: 0.2071080207824707
Test Loss Energy: 10.441667422831856, Test Loss Force: 13.147524879422303, time: 12.589339971542358

Epoch 1, Batch 100/187, Loss: 0.6552683115005493, Variance: 0.12943747639656067

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6213802942077122, Training Loss Force: 3.443463459973614, time: 3.414154529571533
Validation Loss Energy: 2.7800471661631754, Validation Loss Force: 3.4702193134013366, time: 0.19713854789733887
Test Loss Energy: 10.728950664219143, Test Loss Force: 12.46320332901918, time: 12.91406774520874

Epoch 2, Batch 100/187, Loss: 0.8240913152694702, Variance: 0.12370399385690689

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6553813941117586, Training Loss Force: 3.4561007460924884, time: 3.730573892593384
Validation Loss Energy: 3.6790777395566723, Validation Loss Force: 3.604699397740554, time: 0.21108388900756836
Test Loss Energy: 10.95729166661137, Test Loss Force: 12.375375753593373, time: 12.693103313446045

Epoch 3, Batch 100/187, Loss: 1.0916556119918823, Variance: 0.12125580757856369

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.646842828973228, Training Loss Force: 3.4500696022412782, time: 3.5896401405334473
Validation Loss Energy: 2.0701186432852845, Validation Loss Force: 3.4234123662395755, time: 0.21509075164794922
Test Loss Energy: 10.28353913170166, Test Loss Force: 12.535947907382736, time: 12.969345808029175

Epoch 4, Batch 100/187, Loss: 0.8217838406562805, Variance: 0.12433107942342758

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.639341665096646, Training Loss Force: 3.4572452762192545, time: 3.453860282897949
Validation Loss Energy: 2.237424000777025, Validation Loss Force: 3.6049423297787615, time: 0.21527838706970215
Test Loss Energy: 10.474146583319143, Test Loss Force: 12.685282334093449, time: 12.746233224868774

Epoch 5, Batch 100/187, Loss: 0.9315131902694702, Variance: 0.12554194033145905

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.636235094125747, Training Loss Force: 3.4405991607733197, time: 3.484354019165039
Validation Loss Energy: 3.2356608597613077, Validation Loss Force: 3.4178880003963936, time: 0.2176656723022461
Test Loss Energy: 10.589159566635656, Test Loss Force: 12.470481984693553, time: 12.852192163467407

Epoch 6, Batch 100/187, Loss: 1.427999496459961, Variance: 0.12170381098985672

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6393629157111533, Training Loss Force: 3.440233282025726, time: 3.3910980224609375
Validation Loss Energy: 1.8161671822180367, Validation Loss Force: 3.501595233083016, time: 0.2082216739654541
Test Loss Energy: 10.031618413391001, Test Loss Force: 12.353572191342932, time: 12.666687250137329

Epoch 7, Batch 100/187, Loss: 0.8260137438774109, Variance: 0.12744882702827454

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.652345046678541, Training Loss Force: 3.454052449532166, time: 3.586728572845459
Validation Loss Energy: 2.752995648297273, Validation Loss Force: 3.5698068085699504, time: 0.27126312255859375
Test Loss Energy: 10.404794713306877, Test Loss Force: 12.288114900289123, time: 12.320987224578857

Epoch 8, Batch 100/187, Loss: 0.8630164265632629, Variance: 0.11666083335876465

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6585756598130876, Training Loss Force: 3.4482735481472484, time: 3.3450286388397217
Validation Loss Energy: 3.623978315064128, Validation Loss Force: 3.513224073561808, time: 0.19787931442260742
Test Loss Energy: 10.586977199160295, Test Loss Force: 12.278812907210254, time: 12.9940927028656

Epoch 9, Batch 100/187, Loss: 1.5014368295669556, Variance: 0.11818677186965942

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6371869618578, Training Loss Force: 3.451346470476115, time: 3.635474681854248
Validation Loss Energy: 2.205771247679671, Validation Loss Force: 3.4111328865163824, time: 0.20443010330200195
Test Loss Energy: 10.204287111667755, Test Loss Force: 12.194556282706722, time: 12.085449934005737

Epoch 10, Batch 100/187, Loss: 0.7001309394836426, Variance: 0.11794169247150421

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.637332661305371, Training Loss Force: 3.4586681523043823, time: 3.4195008277893066
Validation Loss Energy: 2.042025562655662, Validation Loss Force: 3.355587901062836, time: 0.19998764991760254
Test Loss Energy: 10.186895289708163, Test Loss Force: 12.357436599215019, time: 12.039137125015259

Epoch 11, Batch 100/187, Loss: 0.8921905159950256, Variance: 0.12656140327453613

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6746831659821364, Training Loss Force: 3.4397179053568947, time: 3.564523935317993
Validation Loss Energy: 3.3147270551066437, Validation Loss Force: 3.4967260712017265, time: 0.20093870162963867
Test Loss Energy: 10.607324766337145, Test Loss Force: 12.426193478616455, time: 10.452999353408813

Epoch 12, Batch 100/187, Loss: 1.2742362022399902, Variance: 0.12133153527975082

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6316927906057153, Training Loss Force: 3.4436030044649524, time: 3.434939384460449
Validation Loss Energy: 1.6551704034677925, Validation Loss Force: 3.5500271083371855, time: 0.20105767250061035
Test Loss Energy: 9.722472573284428, Test Loss Force: 11.963293758528899, time: 10.430771350860596

Epoch 13, Batch 100/187, Loss: 0.6853653192520142, Variance: 0.12004827708005905

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6280527400834957, Training Loss Force: 3.4417744025718062, time: 3.269266128540039
Validation Loss Energy: 2.943018213062145, Validation Loss Force: 3.479369289613088, time: 0.1560382843017578
Test Loss Energy: 10.190815544401016, Test Loss Force: 12.052957923060008, time: 8.989980459213257

Epoch 14, Batch 100/187, Loss: 0.9159287214279175, Variance: 0.11994500458240509

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6559000965288493, Training Loss Force: 3.448481055548604, time: 3.0758049488067627
Validation Loss Energy: 3.8402618665595716, Validation Loss Force: 3.4938367720314543, time: 0.1591031551361084
Test Loss Energy: 10.95460229431461, Test Loss Force: 12.249041175039265, time: 9.017510652542114

Epoch 15, Batch 100/187, Loss: 1.3229378461837769, Variance: 0.11726037412881851

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6143244650507533, Training Loss Force: 3.452444704472433, time: 3.1215860843658447
Validation Loss Energy: 2.246144392538995, Validation Loss Force: 3.4745362001334383, time: 0.15236520767211914
Test Loss Energy: 10.168191412539429, Test Loss Force: 12.325066530287936, time: 10.580772876739502

Epoch 16, Batch 100/187, Loss: 0.6575452089309692, Variance: 0.12057068198919296

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.648465719702488, Training Loss Force: 3.4408306164199307, time: 3.4024696350097656
Validation Loss Energy: 2.5720144528682263, Validation Loss Force: 3.5788923220639557, time: 0.19706249237060547
Test Loss Energy: 10.31705023194413, Test Loss Force: 12.404026100629324, time: 12.062987327575684

Epoch 17, Batch 100/187, Loss: 1.2286676168441772, Variance: 0.12336653470993042

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.635238897967864, Training Loss Force: 3.437503535717521, time: 3.089606523513794
Validation Loss Energy: 3.477153620276533, Validation Loss Force: 3.48340388753544, time: 0.17179441452026367
Test Loss Energy: 10.605629598953074, Test Loss Force: 12.417344387585558, time: 10.001761674880981

Epoch 18, Batch 100/187, Loss: 1.3713656663894653, Variance: 0.12541913986206055

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.660281999674702, Training Loss Force: 3.4480342133469555, time: 3.1095073223114014
Validation Loss Energy: 2.009858443831215, Validation Loss Force: 3.528842310126329, time: 0.16465091705322266
Test Loss Energy: 10.04151902973546, Test Loss Force: 12.266629356211352, time: 9.871999502182007

Epoch 19, Batch 100/187, Loss: 0.8781614303588867, Variance: 0.12383750081062317

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6145550020382946, Training Loss Force: 3.4379696298697127, time: 3.0083889961242676
Validation Loss Energy: 2.7131154929270336, Validation Loss Force: 3.4087302864318216, time: 0.16584277153015137
Test Loss Energy: 10.515548621096928, Test Loss Force: 12.082803310920209, time: 9.883689880371094

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–‡â–ˆâ–„â–…â–†â–ƒâ–…â–†â–„â–„â–†â–â–„â–ˆâ–„â–„â–†â–ƒâ–…
wandb:   test_error_force â–ˆâ–„â–ƒâ–„â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–â–‚â–ƒâ–ƒâ–„â–„â–ƒâ–‚
wandb:          test_loss â–„â–†â–†â–…â–ˆâ–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–â–ƒâ–‡â–ƒâ–†â–ˆâ–ƒâ–„
wandb: train_error_energy â–ˆâ–â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–â–â–‚â–â–‚â–â–‚â–
wandb:  train_error_force â–ˆâ–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–â–â–‚â–
wandb:         train_loss â–ˆâ–â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–‚â–
wandb: valid_error_energy â–â–…â–‡â–‚â–ƒâ–†â–‚â–…â–‡â–ƒâ–‚â–†â–â–…â–ˆâ–ƒâ–„â–‡â–‚â–„
wandb:  valid_error_force â–„â–„â–ˆâ–ƒâ–ˆâ–ƒâ–…â–‡â–…â–ƒâ–â–…â–†â–„â–…â–„â–‡â–…â–†â–‚
wandb:         valid_loss â–â–„â–‡â–â–ƒâ–…â–â–„â–‡â–‚â–â–†â–â–„â–ˆâ–‚â–„â–‡â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5971
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.51555
wandb:   test_error_force 12.0828
wandb:          test_loss 11.99616
wandb: train_error_energy 2.61456
wandb:  train_error_force 3.43797
wandb:         train_loss 0.97417
wandb: valid_error_energy 2.71312
wandb:  valid_error_force 3.40873
wandb:         valid_loss 0.97129
wandb: 
wandb: ğŸš€ View run al_71_58 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/a1yhkr2e
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_214951-a1yhkr2e/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.205796003341675, Uncertainty Bias: -0.1577501744031906
0.00010061264 0.0070991516
2.0970924 4.5581837
(48745, 22, 3)
Found uncertainty sample 0 after 116 steps.
Found uncertainty sample 1 after 1185 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 603 steps.
Found uncertainty sample 4 after 463 steps.
Found uncertainty sample 5 after 1618 steps.
Found uncertainty sample 6 after 387 steps.
Found uncertainty sample 7 after 2706 steps.
Found uncertainty sample 8 after 800 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1170 steps.
Found uncertainty sample 11 after 3184 steps.
Found uncertainty sample 12 after 2395 steps.
Found uncertainty sample 13 after 457 steps.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 3713 steps.
Found uncertainty sample 17 after 993 steps.
Found uncertainty sample 18 after 13 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 160 steps.
Found uncertainty sample 21 after 3522 steps.
Found uncertainty sample 22 after 181 steps.
Found uncertainty sample 23 after 585 steps.
Found uncertainty sample 24 after 768 steps.
Found uncertainty sample 25 after 470 steps.
Found uncertainty sample 26 after 2905 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 182 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 2431 steps.
Found uncertainty sample 31 after 766 steps.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 1045 steps.
Found uncertainty sample 34 after 1331 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 144 steps.
Found uncertainty sample 37 after 1520 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 1759 steps.
Found uncertainty sample 40 after 16 steps.
Found uncertainty sample 41 after 883 steps.
Found uncertainty sample 42 after 656 steps.
Found uncertainty sample 43 after 889 steps.
Did not find any uncertainty samples for sample 44.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 521 steps.
Found uncertainty sample 47 after 557 steps.
Found uncertainty sample 48 after 1167 steps.
Found uncertainty sample 49 after 75 steps.
Found uncertainty sample 50 after 222 steps.
Found uncertainty sample 51 after 436 steps.
Found uncertainty sample 52 after 2534 steps.
Found uncertainty sample 53 after 3626 steps.
Found uncertainty sample 54 after 492 steps.
Found uncertainty sample 55 after 1785 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 3266 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 492 steps.
Found uncertainty sample 62 after 2567 steps.
Found uncertainty sample 63 after 2997 steps.
Found uncertainty sample 64 after 2010 steps.
Found uncertainty sample 65 after 2915 steps.
Found uncertainty sample 66 after 7 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 823 steps.
Found uncertainty sample 69 after 2242 steps.
Found uncertainty sample 70 after 286 steps.
Found uncertainty sample 71 after 601 steps.
Found uncertainty sample 72 after 105 steps.
Found uncertainty sample 73 after 2180 steps.
Found uncertainty sample 74 after 1490 steps.
Found uncertainty sample 75 after 1134 steps.
Found uncertainty sample 76 after 444 steps.
Found uncertainty sample 77 after 3770 steps.
Found uncertainty sample 78 after 147 steps.
Found uncertainty sample 79 after 3528 steps.
Found uncertainty sample 80 after 85 steps.
Found uncertainty sample 81 after 827 steps.
Found uncertainty sample 82 after 1055 steps.
Found uncertainty sample 83 after 1242 steps.
Found uncertainty sample 84 after 1 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 2956 steps.
Found uncertainty sample 87 after 259 steps.
Found uncertainty sample 88 after 3643 steps.
Found uncertainty sample 89 after 1113 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 2515 steps.
Found uncertainty sample 92 after 499 steps.
Found uncertainty sample 93 after 220 steps.
Found uncertainty sample 94 after 905 steps.
Found uncertainty sample 95 after 1090 steps.
Found uncertainty sample 96 after 1077 steps.
Found uncertainty sample 97 after 111 steps.
Found uncertainty sample 98 after 375 steps.
Found uncertainty sample 99 after 2359 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_221407-6df3b4g7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_59
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/6df3b4g7
Training model 59. Added 84 samples to the dataset.
Epoch 0, Batch 100/189, Loss: 1.3129709959030151, Variance: 0.11200650036334991

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.6429709631067517, Training Loss Force: 3.925486787992894, time: 3.0573928356170654
Validation Loss Energy: 1.6916159150957395, Validation Loss Force: 3.4457407375166302, time: 0.18685317039489746
Test Loss Energy: 10.28077584257491, Test Loss Force: 12.718330576295767, time: 10.619564294815063

Epoch 1, Batch 100/189, Loss: 0.5873010158538818, Variance: 0.09053155779838562

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6315291233625637, Training Loss Force: 3.41942225384252, time: 3.143134832382202
Validation Loss Energy: 1.6830476341940814, Validation Loss Force: 3.4296126442752213, time: 0.1943645477294922
Test Loss Energy: 9.945173377868379, Test Loss Force: 12.504783935793933, time: 10.918099403381348

Epoch 2, Batch 100/189, Loss: 0.41295844316482544, Variance: 0.0834302306175232

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6439720537257683, Training Loss Force: 3.4188349174334616, time: 3.11057186126709
Validation Loss Energy: 1.6150052315907653, Validation Loss Force: 3.436559921628168, time: 0.1816561222076416
Test Loss Energy: 10.201220899237976, Test Loss Force: 12.873460416364965, time: 10.71698808670044

Epoch 3, Batch 100/189, Loss: 0.5016366243362427, Variance: 0.08391613513231277

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6627366059113207, Training Loss Force: 3.425156532186948, time: 3.159468650817871
Validation Loss Energy: 1.831711297790607, Validation Loss Force: 3.5734776764424296, time: 0.18211770057678223
Test Loss Energy: 9.993353674293475, Test Loss Force: 12.337768231223603, time: 10.599249362945557

Epoch 4, Batch 100/189, Loss: 0.24008935689926147, Variance: 0.08384309709072113

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6570121721350586, Training Loss Force: 3.432127506720854, time: 3.281522274017334
Validation Loss Energy: 1.7349011827127936, Validation Loss Force: 3.4754432985666623, time: 0.18381929397583008
Test Loss Energy: 10.072165485586531, Test Loss Force: 12.521383246786376, time: 10.767518281936646

Epoch 5, Batch 100/189, Loss: 0.5687951445579529, Variance: 0.08386276662349701

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6501167655007483, Training Loss Force: 3.4400337293682526, time: 3.1172971725463867
Validation Loss Energy: 1.7414456237471136, Validation Loss Force: 3.51041588162794, time: 0.18106555938720703
Test Loss Energy: 10.293001143455767, Test Loss Force: 13.138255934641593, time: 10.900766611099243

Epoch 6, Batch 100/189, Loss: 0.6429137587547302, Variance: 0.08457839488983154

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.656095342529994, Training Loss Force: 3.445781857969186, time: 3.1326165199279785
Validation Loss Energy: 1.498480157334365, Validation Loss Force: 3.62547548673406, time: 0.22986626625061035
Test Loss Energy: 10.097896232086907, Test Loss Force: 12.995875402404621, time: 11.703436851501465

Epoch 7, Batch 100/189, Loss: 0.6484990119934082, Variance: 0.08331382274627686

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6610346278146315, Training Loss Force: 3.465453967769441, time: 3.0620200634002686
Validation Loss Energy: 1.807241545170808, Validation Loss Force: 3.4790171940260897, time: 0.18318748474121094
Test Loss Energy: 10.263907090257822, Test Loss Force: 12.874833116856678, time: 10.819844484329224

Epoch 8, Batch 100/189, Loss: 0.30372536182403564, Variance: 0.08476832509040833

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6654341725146904, Training Loss Force: 3.435454752804388, time: 3.0467214584350586
Validation Loss Energy: 1.7685724594516263, Validation Loss Force: 3.4546168306788902, time: 0.1816847324371338
Test Loss Energy: 9.827204649334607, Test Loss Force: 12.578469721481829, time: 10.97075605392456

Epoch 9, Batch 100/189, Loss: 0.6383955478668213, Variance: 0.08280736953020096

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6476207021039362, Training Loss Force: 3.429589850141354, time: 3.024247169494629
Validation Loss Energy: 1.6302446801044197, Validation Loss Force: 3.542538633625061, time: 0.18897056579589844
Test Loss Energy: 9.811652867269173, Test Loss Force: 12.774661011131725, time: 10.72640585899353

Epoch 10, Batch 100/189, Loss: 0.636058509349823, Variance: 0.08288247883319855

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.673546042783397, Training Loss Force: 3.4456902302047085, time: 3.089939832687378
Validation Loss Energy: 1.5382927047851724, Validation Loss Force: 3.482394249960517, time: 0.1853315830230713
Test Loss Energy: 9.828961972887184, Test Loss Force: 12.241678697838234, time: 10.935856342315674

Epoch 11, Batch 100/189, Loss: 0.30515170097351074, Variance: 0.08227435499429703

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6417562094348848, Training Loss Force: 3.451379720240927, time: 3.0362162590026855
Validation Loss Energy: 1.8849703339584152, Validation Loss Force: 3.474642580317906, time: 0.1839127540588379
Test Loss Energy: 10.091107672368532, Test Loss Force: 12.538108283073969, time: 10.63389253616333

Epoch 12, Batch 100/189, Loss: 0.5349349975585938, Variance: 0.08215761184692383

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6760211075372335, Training Loss Force: 3.443703049454539, time: 3.082566022872925
Validation Loss Energy: 1.7953271871796497, Validation Loss Force: 3.526378810030357, time: 0.1811375617980957
Test Loss Energy: 10.012659751423817, Test Loss Force: 12.37703475297962, time: 10.768391609191895

Epoch 13, Batch 100/189, Loss: 1.0364168882369995, Variance: 0.0805121511220932

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6388472653086663, Training Loss Force: 3.4706035393213917, time: 3.0734570026397705
Validation Loss Energy: 1.517792252360682, Validation Loss Force: 3.6338944401397297, time: 0.18116974830627441
Test Loss Energy: 9.820384883293206, Test Loss Force: 12.422787666593958, time: 10.71807074546814

Epoch 14, Batch 100/189, Loss: 0.45635396242141724, Variance: 0.08332861959934235

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6503129063865762, Training Loss Force: 3.4668407865545885, time: 3.119830846786499
Validation Loss Energy: 1.5421138464975095, Validation Loss Force: 3.6273488579260365, time: 0.18117594718933105
Test Loss Energy: 9.821341152844594, Test Loss Force: 12.665987196896436, time: 10.881526708602905

Epoch 15, Batch 100/189, Loss: 0.6849561333656311, Variance: 0.0757363885641098

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6478510250599507, Training Loss Force: 3.4510285757103727, time: 3.123945474624634
Validation Loss Energy: 1.9783188344936353, Validation Loss Force: 3.5403401654800293, time: 0.1975722312927246
Test Loss Energy: 9.940901385199838, Test Loss Force: 12.353869332243006, time: 10.7160005569458

Epoch 16, Batch 100/189, Loss: 0.7130520939826965, Variance: 0.07967770099639893

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6543607168951946, Training Loss Force: 3.462694279419717, time: 3.2083208560943604
Validation Loss Energy: 1.9437172460631165, Validation Loss Force: 3.49140605996722, time: 0.18749284744262695
Test Loss Energy: 9.990738419345517, Test Loss Force: 12.39113045603336, time: 10.789735078811646

Epoch 17, Batch 100/189, Loss: 0.580807089805603, Variance: 0.08160745352506638

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6759585990652326, Training Loss Force: 3.4622215104097425, time: 3.2956631183624268
Validation Loss Energy: 1.616086318127888, Validation Loss Force: 3.431174000326467, time: 0.1801316738128662
Test Loss Energy: 9.821465670363642, Test Loss Force: 12.622276231630293, time: 10.687940835952759

Epoch 18, Batch 100/189, Loss: 0.5776790380477905, Variance: 0.07973647117614746

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.642821056292954, Training Loss Force: 3.444745050340421, time: 3.0738677978515625
Validation Loss Energy: 1.3065092097089819, Validation Loss Force: 3.4417737753473534, time: 0.18791818618774414
Test Loss Energy: 9.711493338270639, Test Loss Force: 12.339058262006017, time: 10.70597791671753

Epoch 19, Batch 100/189, Loss: 0.36401939392089844, Variance: 0.0779002457857132

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6389251476714215, Training Loss Force: 3.4501454050797236, time: 3.234830856323242
Validation Loss Energy: 1.9169661923760004, Validation Loss Force: 3.540908250138578, time: 0.18961429595947266
Test Loss Energy: 10.08802302619773, Test Loss Force: 12.617312742843595, time: 10.74847149848938

wandb: - 0.039 MB of 0.059 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–„â–‡â–„â–…â–ˆâ–†â–ˆâ–‚â–‚â–‚â–†â–…â–‚â–‚â–„â–„â–‚â–â–†
wandb:   test_error_force â–…â–ƒâ–†â–‚â–ƒâ–ˆâ–‡â–†â–„â–…â–â–ƒâ–‚â–‚â–„â–‚â–‚â–„â–‚â–„
wandb:          test_loss â–â–ƒâ–†â–‚â–„â–‡â–†â–†â–ƒâ–„â–†â–†â–ˆâ–†â–†â–†â–†â–†â–†â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–‚â–â–â–â–â–â–‚â–‚â–â–‚â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–…â–„â–†â–…â–†â–ƒâ–†â–†â–„â–ƒâ–‡â–†â–ƒâ–ƒâ–ˆâ–ˆâ–„â–â–‡
wandb:  valid_error_force â–‚â–â–â–†â–ƒâ–„â–ˆâ–ƒâ–‚â–…â–ƒâ–ƒâ–„â–ˆâ–ˆâ–…â–ƒâ–â–â–…
wandb:         valid_loss â–„â–…â–…â–‡â–…â–‡â–…â–†â–†â–†â–„â–‡â–†â–…â–…â–ˆâ–ˆâ–„â–â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6046
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.08802
wandb:   test_error_force 12.61731
wandb:          test_loss 16.11737
wandb: train_error_energy 1.63893
wandb:  train_error_force 3.45015
wandb:         train_loss 0.55461
wandb: valid_error_energy 1.91697
wandb:  valid_error_force 3.54091
wandb:         valid_loss 0.72261
wandb: 
wandb: ğŸš€ View run al_71_59 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/6df3b4g7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_221407-6df3b4g7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.57494854927063, Uncertainty Bias: -0.04980652034282684
0.00011444092 0.008377075
2.3014674 4.8986
(48745, 22, 3)
Found uncertainty sample 0 after 3762 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 18 steps.
Found uncertainty sample 3 after 758 steps.
Found uncertainty sample 4 after 355 steps.
Found uncertainty sample 5 after 584 steps.
Found uncertainty sample 6 after 38 steps.
Found uncertainty sample 7 after 3446 steps.
Found uncertainty sample 8 after 4 steps.
Found uncertainty sample 9 after 917 steps.
Did not find any uncertainty samples for sample 10.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 3817 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2425 steps.
Found uncertainty sample 15 after 1214 steps.
Found uncertainty sample 16 after 1080 steps.
Found uncertainty sample 17 after 131 steps.
Found uncertainty sample 18 after 59 steps.
Found uncertainty sample 19 after 130 steps.
Found uncertainty sample 20 after 14 steps.
Found uncertainty sample 21 after 159 steps.
Found uncertainty sample 22 after 66 steps.
Found uncertainty sample 23 after 487 steps.
Found uncertainty sample 24 after 19 steps.
Found uncertainty sample 25 after 329 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 450 steps.
Found uncertainty sample 28 after 3083 steps.
Found uncertainty sample 29 after 1782 steps.
Found uncertainty sample 30 after 294 steps.
Found uncertainty sample 31 after 1565 steps.
Found uncertainty sample 32 after 17 steps.
Found uncertainty sample 33 after 1627 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 3712 steps.
Found uncertainty sample 36 after 2067 steps.
Found uncertainty sample 37 after 118 steps.
Found uncertainty sample 38 after 295 steps.
Found uncertainty sample 39 after 134 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 4 steps.
Found uncertainty sample 42 after 482 steps.
Found uncertainty sample 43 after 56 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 116 steps.
Found uncertainty sample 46 after 1031 steps.
Found uncertainty sample 47 after 357 steps.
Found uncertainty sample 48 after 1960 steps.
Found uncertainty sample 49 after 566 steps.
Found uncertainty sample 50 after 45 steps.
Found uncertainty sample 51 after 2853 steps.
Found uncertainty sample 52 after 3788 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 1147 steps.
Found uncertainty sample 55 after 352 steps.
Found uncertainty sample 56 after 2476 steps.
Found uncertainty sample 57 after 3492 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 2999 steps.
Found uncertainty sample 60 after 408 steps.
Found uncertainty sample 61 after 2711 steps.
Found uncertainty sample 62 after 2769 steps.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 874 steps.
Found uncertainty sample 67 after 1636 steps.
Found uncertainty sample 68 after 399 steps.
Found uncertainty sample 69 after 612 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 814 steps.
Found uncertainty sample 72 after 677 steps.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 28 steps.
Found uncertainty sample 76 after 230 steps.
Found uncertainty sample 77 after 809 steps.
Found uncertainty sample 78 after 1328 steps.
Found uncertainty sample 79 after 908 steps.
Found uncertainty sample 80 after 367 steps.
Found uncertainty sample 81 after 1526 steps.
Found uncertainty sample 82 after 1809 steps.
Found uncertainty sample 83 after 1416 steps.
Found uncertainty sample 84 after 2235 steps.
Found uncertainty sample 85 after 1840 steps.
Found uncertainty sample 86 after 3400 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 2908 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 165 steps.
Found uncertainty sample 91 after 1854 steps.
Found uncertainty sample 92 after 483 steps.
Found uncertainty sample 93 after 2902 steps.
Found uncertainty sample 94 after 476 steps.
Found uncertainty sample 95 after 2458 steps.
Found uncertainty sample 96 after 625 steps.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 2757 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_223706-hry9zmu3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_60
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hry9zmu3
Training model 60. Added 85 samples to the dataset.
Epoch 0, Batch 100/192, Loss: 0.5123714804649353, Variance: 0.08067896962165833

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.0420945668961967, Training Loss Force: 3.5325800668824563, time: 3.238075017929077
Validation Loss Energy: 1.6121813414821091, Validation Loss Force: 3.4924602870386146, time: 0.18509292602539062
Test Loss Energy: 10.014862487879899, Test Loss Force: 12.609464753639877, time: 10.86523175239563

Epoch 1, Batch 100/192, Loss: 0.5886942744255066, Variance: 0.0792057067155838

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6523371392200399, Training Loss Force: 3.4527654596624253, time: 3.0876595973968506
Validation Loss Energy: 1.4893747427854425, Validation Loss Force: 3.5261950523176804, time: 0.18207168579101562
Test Loss Energy: 9.663101410560733, Test Loss Force: 12.41706896800238, time: 11.074091911315918

Epoch 2, Batch 100/192, Loss: 0.7520202994346619, Variance: 0.07752060890197754

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7011994582877257, Training Loss Force: 3.448608861083072, time: 3.0811266899108887
Validation Loss Energy: 1.648504412913439, Validation Loss Force: 3.4846337448956852, time: 0.18822455406188965
Test Loss Energy: 9.949543275012703, Test Loss Force: 12.585268862064963, time: 10.90971064567566

Epoch 3, Batch 100/192, Loss: 0.5110453963279724, Variance: 0.0793883353471756

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6916890537920306, Training Loss Force: 3.4604463297945114, time: 3.121976852416992
Validation Loss Energy: 1.7468878246754485, Validation Loss Force: 3.5329606873148363, time: 0.18230104446411133
Test Loss Energy: 9.93085968754981, Test Loss Force: 12.677837205836036, time: 11.908558130264282

Epoch 4, Batch 100/192, Loss: 0.6505815386772156, Variance: 0.08101523667573929

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.655769106293641, Training Loss Force: 3.455559878581829, time: 3.0989439487457275
Validation Loss Energy: 1.4712135132398534, Validation Loss Force: 3.569878953373231, time: 0.18147754669189453
Test Loss Energy: 9.823438169254448, Test Loss Force: 12.590433070182517, time: 10.846548557281494

Epoch 5, Batch 100/192, Loss: 0.5004948377609253, Variance: 0.07629610598087311

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6352659554554254, Training Loss Force: 3.448584337323906, time: 3.1023623943328857
Validation Loss Energy: 1.411823115137064, Validation Loss Force: 3.4884595904937306, time: 0.18996882438659668
Test Loss Energy: 9.980419959102594, Test Loss Force: 12.757277578319643, time: 10.84654426574707

Epoch 6, Batch 100/192, Loss: 0.49219799041748047, Variance: 0.08237773925065994

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.664058187919949, Training Loss Force: 3.452772124660246, time: 3.291301727294922
Validation Loss Energy: 1.7386450860651368, Validation Loss Force: 3.5152249785071152, time: 0.18675899505615234
Test Loss Energy: 9.973173097318003, Test Loss Force: 12.70586488133864, time: 10.907510995864868

Epoch 7, Batch 100/192, Loss: 0.5275471210479736, Variance: 0.08149020373821259

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6750711000269316, Training Loss Force: 3.456989101370199, time: 3.2082700729370117
Validation Loss Energy: 1.457602138020293, Validation Loss Force: 3.564759567039206, time: 0.18335866928100586
Test Loss Energy: 9.998954010827212, Test Loss Force: 12.771247807892557, time: 10.930996179580688

Epoch 8, Batch 100/192, Loss: 0.6251906752586365, Variance: 0.07905154675245285

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6733998190607793, Training Loss Force: 3.4469369938708088, time: 3.136631965637207
Validation Loss Energy: 1.2901956330690147, Validation Loss Force: 3.457644574390643, time: 0.20389413833618164
Test Loss Energy: 9.75383126729264, Test Loss Force: 12.429449218629433, time: 11.085348844528198

Epoch 9, Batch 100/192, Loss: 0.7733179330825806, Variance: 0.07905834913253784

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6801027071683219, Training Loss Force: 3.4693460119694186, time: 3.0738813877105713
Validation Loss Energy: 1.5579265845609984, Validation Loss Force: 3.4378222886837193, time: 0.184920072555542
Test Loss Energy: 9.669329492220088, Test Loss Force: 12.133793407509993, time: 10.960750102996826

Epoch 10, Batch 100/192, Loss: 0.7901677489280701, Variance: 0.07736727595329285

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6535162535291867, Training Loss Force: 3.4559044811201205, time: 3.225632429122925
Validation Loss Energy: 1.2321211817850155, Validation Loss Force: 3.487247323996013, time: 0.19115471839904785
Test Loss Energy: 9.6599145837339, Test Loss Force: 12.456670202287976, time: 11.161414623260498

Epoch 11, Batch 100/192, Loss: 0.5790233612060547, Variance: 0.08114954829216003

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6579112420440907, Training Loss Force: 3.461630916893762, time: 3.1210248470306396
Validation Loss Energy: 1.531134838087464, Validation Loss Force: 3.4773159460839147, time: 0.1905839443206787
Test Loss Energy: 9.713657449207647, Test Loss Force: 12.404523098163036, time: 10.868212938308716

Epoch 12, Batch 100/192, Loss: 0.1156930923461914, Variance: 0.07879287004470825

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6355640360885753, Training Loss Force: 3.453781596393045, time: 3.1477885246276855
Validation Loss Energy: 1.4782896046937195, Validation Loss Force: 3.4719706062340587, time: 0.18516230583190918
Test Loss Energy: 9.779006194298951, Test Loss Force: 12.29932099202775, time: 11.094080209732056

Epoch 13, Batch 100/192, Loss: 0.5497996807098389, Variance: 0.07978588342666626

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6564118025121022, Training Loss Force: 3.458211607169424, time: 3.053316593170166
Validation Loss Energy: 1.3383407661305158, Validation Loss Force: 3.471896735744897, time: 0.18296313285827637
Test Loss Energy: 9.650884158411724, Test Loss Force: 12.407154749021299, time: 10.950118780136108

Epoch 14, Batch 100/192, Loss: 0.2311762571334839, Variance: 0.07678339630365372

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6612306637812473, Training Loss Force: 3.4465946400589442, time: 3.148550510406494
Validation Loss Energy: 1.4080254093523359, Validation Loss Force: 3.5337515524538854, time: 0.18749356269836426
Test Loss Energy: 9.929967658629925, Test Loss Force: 12.689979846334051, time: 11.15123176574707

Epoch 15, Batch 100/192, Loss: 0.45878952741622925, Variance: 0.07783635705709457

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6678527340063016, Training Loss Force: 3.4441121145080813, time: 3.1670169830322266
Validation Loss Energy: 1.4287218106252426, Validation Loss Force: 3.4755145526933586, time: 0.18822526931762695
Test Loss Energy: 9.840399957637095, Test Loss Force: 12.584977135172814, time: 10.965896368026733

Epoch 16, Batch 100/192, Loss: 0.5489594340324402, Variance: 0.07620793581008911

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6535967397274263, Training Loss Force: 3.4473822915719343, time: 3.096437454223633
Validation Loss Energy: 1.4211470237854282, Validation Loss Force: 3.4946403849791077, time: 0.1854541301727295
Test Loss Energy: 10.101160976793516, Test Loss Force: 12.697317836923872, time: 11.110949754714966

Epoch 17, Batch 100/192, Loss: 0.6147032380104065, Variance: 0.07869893312454224

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6748240486095252, Training Loss Force: 3.4474220032224285, time: 3.1277682781219482
Validation Loss Energy: 1.474085277214884, Validation Loss Force: 3.4568284363035313, time: 0.18781471252441406
Test Loss Energy: 9.692909044925532, Test Loss Force: 12.353303262973391, time: 10.996835708618164

Epoch 18, Batch 100/192, Loss: 0.4646739959716797, Variance: 0.07936049997806549

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6572129834951774, Training Loss Force: 3.439522118269824, time: 3.0972938537597656
Validation Loss Energy: 1.4129438367821618, Validation Loss Force: 3.5101759556232026, time: 0.1844921112060547
Test Loss Energy: 9.749900767487459, Test Loss Force: 12.373872575976986, time: 11.0918128490448

Epoch 19, Batch 100/192, Loss: 0.9002379179000854, Variance: 0.08001118898391724

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6594451059940927, Training Loss Force: 3.460489810584195, time: 3.061497926712036
Validation Loss Energy: 1.6379198072281183, Validation Loss Force: 3.4535954473065194, time: 0.19675421714782715
Test Loss Energy: 9.672734432218911, Test Loss Force: 12.397162108531798, time: 11.002901554107666

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–â–†â–…â–„â–†â–†â–†â–ƒâ–â–â–‚â–ƒâ–â–…â–„â–ˆâ–‚â–ƒâ–
wandb:   test_error_force â–†â–„â–†â–‡â–†â–ˆâ–‡â–ˆâ–„â–â–…â–„â–ƒâ–„â–‡â–†â–‡â–ƒâ–„â–„
wandb:          test_loss â–„â–â–…â–‚â–…â–‡â–…â–‡â–†â–…â–ƒâ–ƒâ–„â–ƒâ–‡â–ƒâ–ˆâ–‚â–ƒâ–…
wandb: train_error_energy â–ˆâ–â–‚â–‚â–â–â–â–‚â–‚â–‚â–â–â–â–â–â–‚â–â–‚â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–ƒ
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–„â–‡â–ˆâ–„â–ƒâ–ˆâ–„â–‚â–…â–â–…â–„â–‚â–ƒâ–„â–„â–„â–ƒâ–‡
wandb:  valid_error_force â–„â–†â–ƒâ–†â–ˆâ–„â–…â–ˆâ–‚â–â–„â–ƒâ–ƒâ–ƒâ–†â–ƒâ–„â–‚â–…â–‚
wandb:         valid_loss â–†â–…â–†â–ˆâ–…â–ƒâ–ˆâ–„â–‚â–…â–â–…â–ƒâ–‚â–ƒâ–„â–„â–„â–ƒâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 6122
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.67273
wandb:   test_error_force 12.39716
wandb:          test_loss 16.19509
wandb: train_error_energy 1.65945
wandb:  train_error_force 3.46049
wandb:         train_loss 0.56371
wandb: valid_error_energy 1.63792
wandb:  valid_error_force 3.4536
wandb:         valid_loss 0.59838
wandb: 
wandb: ğŸš€ View run al_71_60 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hry9zmu3
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_223706-hry9zmu3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.5864334106445312, Uncertainty Bias: -0.04474321007728577
4.196167e-05 0.0003938675
2.2576115 5.0734563
(48745, 22, 3)
Found uncertainty sample 0 after 361 steps.
Found uncertainty sample 1 after 2013 steps.
Found uncertainty sample 2 after 106 steps.
Found uncertainty sample 3 after 368 steps.
Found uncertainty sample 4 after 36 steps.
Found uncertainty sample 5 after 89 steps.
Found uncertainty sample 6 after 142 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 2192 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 21 steps.
Found uncertainty sample 12 after 1129 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 1153 steps.
Found uncertainty sample 15 after 2325 steps.
Found uncertainty sample 16 after 63 steps.
Found uncertainty sample 17 after 337 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 2106 steps.
Found uncertainty sample 20 after 849 steps.
Found uncertainty sample 21 after 701 steps.
Found uncertainty sample 22 after 19 steps.
Found uncertainty sample 23 after 1416 steps.
Found uncertainty sample 24 after 939 steps.
Found uncertainty sample 25 after 38 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 1067 steps.
Found uncertainty sample 28 after 2922 steps.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 194 steps.
Found uncertainty sample 32 after 865 steps.
Found uncertainty sample 33 after 359 steps.
Did not find any uncertainty samples for sample 34.
Did not find any uncertainty samples for sample 35.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1111 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 75 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 3052 steps.
Found uncertainty sample 42 after 1518 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 328 steps.
Found uncertainty sample 45 after 334 steps.
Found uncertainty sample 46 after 212 steps.
Found uncertainty sample 47 after 779 steps.
Found uncertainty sample 48 after 377 steps.
Found uncertainty sample 49 after 9 steps.
Found uncertainty sample 50 after 3887 steps.
Found uncertainty sample 51 after 1271 steps.
Found uncertainty sample 52 after 3270 steps.
Found uncertainty sample 53 after 144 steps.
Found uncertainty sample 54 after 63 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 2351 steps.
Found uncertainty sample 57 after 1778 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 985 steps.
Found uncertainty sample 61 after 1819 steps.
Found uncertainty sample 62 after 570 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1077 steps.
Found uncertainty sample 65 after 8 steps.
Found uncertainty sample 66 after 284 steps.
Found uncertainty sample 67 after 2606 steps.
Found uncertainty sample 68 after 16 steps.
Did not find any uncertainty samples for sample 69.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 554 steps.
Found uncertainty sample 72 after 83 steps.
Found uncertainty sample 73 after 396 steps.
Found uncertainty sample 74 after 47 steps.
Found uncertainty sample 75 after 440 steps.
Found uncertainty sample 76 after 8 steps.
Found uncertainty sample 77 after 2825 steps.
Found uncertainty sample 78 after 116 steps.
Found uncertainty sample 79 after 44 steps.
Found uncertainty sample 80 after 1154 steps.
Found uncertainty sample 81 after 1438 steps.
Found uncertainty sample 82 after 2251 steps.
Found uncertainty sample 83 after 477 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 2767 steps.
Found uncertainty sample 86 after 2791 steps.
Found uncertainty sample 87 after 829 steps.
Found uncertainty sample 88 after 15 steps.
Found uncertainty sample 89 after 1641 steps.
Found uncertainty sample 90 after 1024 steps.
Found uncertainty sample 91 after 1109 steps.
Found uncertainty sample 92 after 2571 steps.
Found uncertainty sample 93 after 3734 steps.
Found uncertainty sample 94 after 1342 steps.
Found uncertainty sample 95 after 1316 steps.
Found uncertainty sample 96 after 1787 steps.
Found uncertainty sample 97 after 57 steps.
Found uncertainty sample 98 after 3987 steps.
Found uncertainty sample 99 after 3618 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_225938-sueuic4x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_61
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sueuic4x
Training model 61. Added 84 samples to the dataset.
Epoch 0, Batch 100/194, Loss: 1.1036720275878906, Variance: 0.11162112653255463

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.928430389214988, Training Loss Force: 3.844046986085604, time: 3.504953145980835
Validation Loss Energy: 1.9402184179395099, Validation Loss Force: 3.495038907083032, time: 0.21259403228759766
Test Loss Energy: 9.735316362782953, Test Loss Force: 11.948672910696116, time: 13.5024094581604

Epoch 1, Batch 100/194, Loss: 1.0126721858978271, Variance: 0.11168915778398514

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6520614834196823, Training Loss Force: 3.432138884272506, time: 3.5866055488586426
Validation Loss Energy: 1.8871635693785174, Validation Loss Force: 3.5018887666846354, time: 0.2058577537536621
Test Loss Energy: 9.662948960304844, Test Loss Force: 11.922206185543901, time: 12.590801477432251

Epoch 2, Batch 100/194, Loss: 0.7927231788635254, Variance: 0.112271249294281

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6505778779248543, Training Loss Force: 3.431315941576984, time: 3.544313669204712
Validation Loss Energy: 3.7516117685043575, Validation Loss Force: 3.4903201582098164, time: 0.19523358345031738
Test Loss Energy: 10.87135514679608, Test Loss Force: 12.218459603345185, time: 12.455040693283081

Epoch 3, Batch 100/194, Loss: 1.081632375717163, Variance: 0.10690728574991226

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.673251157970056, Training Loss Force: 3.4274203563467687, time: 3.479215383529663
Validation Loss Energy: 2.5895921779169115, Validation Loss Force: 3.528728748861906, time: 0.2111973762512207
Test Loss Energy: 10.041341990277342, Test Loss Force: 12.226276223012889, time: 12.605252027511597

Epoch 4, Batch 100/194, Loss: 0.9634640216827393, Variance: 0.11201941967010498

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6923585236378207, Training Loss Force: 3.429539746528849, time: 3.562899351119995
Validation Loss Energy: 1.8517114404778408, Validation Loss Force: 3.4452736889830464, time: 0.21709609031677246
Test Loss Energy: 10.068671431297055, Test Loss Force: 12.513030295255204, time: 12.455987691879272

Epoch 5, Batch 100/194, Loss: 0.8191821575164795, Variance: 0.11459885537624359

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.656485987642123, Training Loss Force: 3.433057830655517, time: 3.377349615097046
Validation Loss Energy: 3.970527658975673, Validation Loss Force: 3.549300650248596, time: 0.21423912048339844
Test Loss Energy: 10.925330208390942, Test Loss Force: 12.156318100693717, time: 12.675649881362915

Epoch 6, Batch 100/194, Loss: 1.1880954504013062, Variance: 0.10909241437911987

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.681336058302333, Training Loss Force: 3.437873606461291, time: 3.4421546459198
Validation Loss Energy: 2.333110777847276, Validation Loss Force: 3.4456571252523345, time: 0.20917582511901855
Test Loss Energy: 9.962110982749646, Test Loss Force: 12.016445345321452, time: 12.417579889297485

Epoch 7, Batch 100/194, Loss: 0.8473110198974609, Variance: 0.11591492593288422

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6823562451149896, Training Loss Force: 3.4131770202660348, time: 3.606476306915283
Validation Loss Energy: 1.8163580653433833, Validation Loss Force: 3.4451109178373733, time: 0.27948451042175293
Test Loss Energy: 9.901072813876901, Test Loss Force: 12.369022949232926, time: 11.24216914176941

Epoch 8, Batch 100/194, Loss: 0.762673556804657, Variance: 0.1126726046204567

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6679740053811343, Training Loss Force: 3.430681732790438, time: 3.6346395015716553
Validation Loss Energy: 3.794216781880602, Validation Loss Force: 3.485067782728385, time: 0.20397138595581055
Test Loss Energy: 10.98281123416007, Test Loss Force: 12.203622419030351, time: 11.320046663284302

Epoch 9, Batch 100/194, Loss: 1.3580082654953003, Variance: 0.10965664684772491

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.658420618100371, Training Loss Force: 3.4230173184629957, time: 3.255930185317993
Validation Loss Energy: 2.0988468598200223, Validation Loss Force: 3.5344043627224595, time: 0.17606687545776367
Test Loss Energy: 9.822412632111572, Test Loss Force: 12.170666751234004, time: 10.366990327835083

Epoch 10, Batch 100/194, Loss: 0.9565895199775696, Variance: 0.11000455915927887

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.648632949592497, Training Loss Force: 3.4358376093176974, time: 3.19083833694458
Validation Loss Energy: 2.0350446065943935, Validation Loss Force: 3.470736462492754, time: 0.17474913597106934
Test Loss Energy: 10.133171119116216, Test Loss Force: 12.433088537140161, time: 10.230802059173584

Epoch 11, Batch 100/194, Loss: 0.9160594940185547, Variance: 0.11619777232408524

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.64636789476449, Training Loss Force: 3.4196514814954075, time: 3.1912190914154053
Validation Loss Energy: 3.7773726960004623, Validation Loss Force: 3.484966078595983, time: 0.18220233917236328
Test Loss Energy: 10.952508999160848, Test Loss Force: 12.243307269660567, time: 10.403945684432983

Epoch 12, Batch 100/194, Loss: 1.285742163658142, Variance: 0.11049363017082214

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.635821703811014, Training Loss Force: 3.4131865181381102, time: 3.2406952381134033
Validation Loss Energy: 1.943967053011204, Validation Loss Force: 3.4520765653948597, time: 0.1790142059326172
Test Loss Energy: 10.21542400110009, Test Loss Force: 12.63953125779102, time: 10.21003270149231

Epoch 13, Batch 100/194, Loss: 1.0095144510269165, Variance: 0.1135403960943222

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.650834071976704, Training Loss Force: 3.4205060879730365, time: 3.122825860977173
Validation Loss Energy: 1.7701308929703314, Validation Loss Force: 3.529413734822066, time: 0.17632770538330078
Test Loss Energy: 10.15051149167922, Test Loss Force: 12.479798014089264, time: 10.482075214385986

Epoch 14, Batch 100/194, Loss: 0.8131922483444214, Variance: 0.11238329112529755

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.663127303241327, Training Loss Force: 3.4299764030195714, time: 3.29789662361145
Validation Loss Energy: 3.6555086557985077, Validation Loss Force: 3.4605766817042203, time: 0.1825265884399414
Test Loss Energy: 10.683951482166352, Test Loss Force: 12.421440501939797, time: 10.210140705108643

Epoch 15, Batch 100/194, Loss: 1.312918782234192, Variance: 0.11113139986991882

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6511240049952782, Training Loss Force: 3.409979266365808, time: 3.1974356174468994
Validation Loss Energy: 2.4010854333629434, Validation Loss Force: 3.4796047065676197, time: 0.1742112636566162
Test Loss Energy: 10.441659681873064, Test Loss Force: 12.64518661109091, time: 10.471996307373047

Epoch 16, Batch 100/194, Loss: 1.0394734144210815, Variance: 0.11390283703804016

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.648065985740243, Training Loss Force: 3.4253699993388196, time: 3.1165738105773926
Validation Loss Energy: 1.9114526259863962, Validation Loss Force: 3.496692281158832, time: 0.1768021583557129
Test Loss Energy: 10.018872957385256, Test Loss Force: 12.41846892039962, time: 10.181790113449097

Epoch 17, Batch 100/194, Loss: 0.7513951659202576, Variance: 0.11372852325439453

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6421251746485583, Training Loss Force: 3.4075252561074567, time: 3.1264636516571045
Validation Loss Energy: 3.872922308876822, Validation Loss Force: 3.5075768500173607, time: 0.1772768497467041
Test Loss Energy: 10.717826738561167, Test Loss Force: 12.235505530354008, time: 11.635491609573364

Epoch 18, Batch 100/194, Loss: 1.079329252243042, Variance: 0.10968920588493347

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.668299386409517, Training Loss Force: 3.4361947040565775, time: 3.7462856769561768
Validation Loss Energy: 2.2421448650071265, Validation Loss Force: 3.458168125220006, time: 0.20824050903320312
Test Loss Energy: 10.173790429379592, Test Loss Force: 12.416553863153618, time: 12.628642320632935

Epoch 19, Batch 100/194, Loss: 0.9306342005729675, Variance: 0.11252574622631073

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.661547427774259, Training Loss Force: 3.4033689987652225, time: 3.283463716506958
Validation Loss Energy: 1.8144537709737656, Validation Loss Force: 3.4723734147190424, time: 0.18886756896972656
Test Loss Energy: 10.016130778692505, Test Loss Force: 12.280516967370476, time: 11.789090394973755

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‡â–ƒâ–ƒâ–ˆâ–ƒâ–‚â–ˆâ–‚â–ƒâ–ˆâ–„â–„â–†â–…â–ƒâ–‡â–„â–ƒ
wandb:   test_error_force â–â–â–„â–„â–‡â–ƒâ–‚â–…â–„â–ƒâ–†â–„â–ˆâ–†â–†â–ˆâ–†â–„â–†â–„
wandb:          test_loss â–…â–‚â–†â–‚â–„â–„â–ƒâ–ƒâ–†â–â–…â–†â–ˆâ–†â–„â–ˆâ–„â–ƒâ–…â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–â–‡â–„â–â–ˆâ–ƒâ–â–‡â–‚â–‚â–‡â–‚â–â–‡â–ƒâ–â–ˆâ–ƒâ–
wandb:  valid_error_force â–„â–…â–„â–‡â–â–ˆâ–â–â–„â–‡â–ƒâ–„â–â–‡â–‚â–ƒâ–„â–…â–‚â–ƒ
wandb:         valid_loss â–â–â–‡â–ƒâ–â–ˆâ–‚â–â–‡â–‚â–‚â–‡â–â–â–‡â–ƒâ–â–ˆâ–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 6197
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.01613
wandb:   test_error_force 12.28052
wandb:          test_loss 12.62231
wandb: train_error_energy 2.66155
wandb:  train_error_force 3.40337
wandb:         train_loss 0.97232
wandb: valid_error_energy 1.81445
wandb:  valid_error_force 3.47237
wandb:         valid_loss 0.7237
wandb: 
wandb: ğŸš€ View run al_71_61 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sueuic4x
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_225938-sueuic4x/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.0820140838623047, Uncertainty Bias: -0.10742031037807465
3.8146973e-06 0.0002644062
2.0162213 4.6792235
(48745, 22, 3)
Found uncertainty sample 0 after 1637 steps.
Found uncertainty sample 1 after 147 steps.
Found uncertainty sample 2 after 3560 steps.
Found uncertainty sample 3 after 1826 steps.
Found uncertainty sample 4 after 3963 steps.
Found uncertainty sample 5 after 402 steps.
Found uncertainty sample 6 after 592 steps.
Found uncertainty sample 7 after 309 steps.
Found uncertainty sample 8 after 274 steps.
Found uncertainty sample 9 after 237 steps.
Found uncertainty sample 10 after 885 steps.
Found uncertainty sample 11 after 39 steps.
Found uncertainty sample 12 after 40 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 8 steps.
Found uncertainty sample 15 after 1461 steps.
Found uncertainty sample 16 after 4 steps.
Found uncertainty sample 17 after 33 steps.
Found uncertainty sample 18 after 282 steps.
Found uncertainty sample 19 after 2004 steps.
Found uncertainty sample 20 after 4 steps.
Found uncertainty sample 21 after 2812 steps.
Found uncertainty sample 22 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 401 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 3572 steps.
Found uncertainty sample 27 after 86 steps.
Found uncertainty sample 28 after 84 steps.
Found uncertainty sample 29 after 2359 steps.
Found uncertainty sample 30 after 3302 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 32 steps.
Found uncertainty sample 33 after 433 steps.
Found uncertainty sample 34 after 370 steps.
Found uncertainty sample 35 after 622 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 656 steps.
Found uncertainty sample 38 after 418 steps.
Found uncertainty sample 39 after 2840 steps.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 46 steps.
Found uncertainty sample 43 after 332 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 4 steps.
Found uncertainty sample 46 after 1469 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 110 steps.
Found uncertainty sample 49 after 14 steps.
Found uncertainty sample 50 after 483 steps.
Found uncertainty sample 51 after 3902 steps.
Found uncertainty sample 52 after 1431 steps.
Found uncertainty sample 53 after 765 steps.
Found uncertainty sample 54 after 1148 steps.
Found uncertainty sample 55 after 3442 steps.
Found uncertainty sample 56 after 371 steps.
Found uncertainty sample 57 after 1307 steps.
Found uncertainty sample 58 after 590 steps.
Found uncertainty sample 59 after 38 steps.
Found uncertainty sample 60 after 1746 steps.
Found uncertainty sample 61 after 16 steps.
Found uncertainty sample 62 after 914 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 347 steps.
Found uncertainty sample 65 after 244 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1298 steps.
Found uncertainty sample 68 after 1429 steps.
Found uncertainty sample 69 after 691 steps.
Found uncertainty sample 70 after 64 steps.
Found uncertainty sample 71 after 35 steps.
Found uncertainty sample 72 after 96 steps.
Found uncertainty sample 73 after 411 steps.
Found uncertainty sample 74 after 2198 steps.
Found uncertainty sample 75 after 3997 steps.
Found uncertainty sample 76 after 2168 steps.
Found uncertainty sample 77 after 337 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 1971 steps.
Found uncertainty sample 80 after 316 steps.
Found uncertainty sample 81 after 5 steps.
Found uncertainty sample 82 after 104 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 411 steps.
Found uncertainty sample 85 after 1664 steps.
Found uncertainty sample 86 after 623 steps.
Found uncertainty sample 87 after 267 steps.
Found uncertainty sample 88 after 650 steps.
Found uncertainty sample 89 after 538 steps.
Found uncertainty sample 90 after 1082 steps.
Found uncertainty sample 91 after 2498 steps.
Found uncertainty sample 92 after 2897 steps.
Found uncertainty sample 93 after 164 steps.
Found uncertainty sample 94 after 1026 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 1601 steps.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 147 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_232009-2gjzovzt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_62
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/2gjzovzt
Training model 62. Added 89 samples to the dataset.
Epoch 0, Batch 100/197, Loss: 2.8493266105651855, Variance: 0.10122254490852356

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.881263263465105, Training Loss Force: 3.9062673853482845, time: 3.177517890930176
Validation Loss Energy: 1.9999359448513103, Validation Loss Force: 3.423376906178086, time: 0.18373799324035645
Test Loss Energy: 10.0105063099529, Test Loss Force: 12.495528183285764, time: 10.64225149154663

Epoch 1, Batch 100/197, Loss: 1.0144689083099365, Variance: 0.11379572004079819

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.578740998409845, Training Loss Force: 3.381657410412714, time: 3.33575701713562
Validation Loss Energy: 2.189975265469413, Validation Loss Force: 3.412332739209214, time: 0.20139551162719727
Test Loss Energy: 10.281307585821844, Test Loss Force: 12.286314507161014, time: 10.948461771011353

Epoch 2, Batch 100/197, Loss: 0.6671805381774902, Variance: 0.11055635660886765

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6318044913657332, Training Loss Force: 3.3970896142527636, time: 3.2881205081939697
Validation Loss Energy: 3.3185949851732692, Validation Loss Force: 3.494810003201018, time: 0.19746923446655273
Test Loss Energy: 10.611725958405831, Test Loss Force: 12.42774713567796, time: 10.785746574401855

Epoch 3, Batch 100/197, Loss: 1.4728058576583862, Variance: 0.11040462553501129

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6356588598075863, Training Loss Force: 3.394520959500353, time: 3.227529764175415
Validation Loss Energy: 2.8530479653080114, Validation Loss Force: 3.4878436319688073, time: 0.18433499336242676
Test Loss Energy: 10.352394806255464, Test Loss Force: 12.493851477622087, time: 10.70131540298462

Epoch 4, Batch 100/197, Loss: 0.7789269685745239, Variance: 0.1087362989783287

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.639062843167042, Training Loss Force: 3.4090703683402173, time: 3.4354114532470703
Validation Loss Energy: 2.041205945363145, Validation Loss Force: 3.5232013213179676, time: 0.21517562866210938
Test Loss Energy: 9.957440223449154, Test Loss Force: 12.52423287558301, time: 10.709946393966675

Epoch 5, Batch 100/197, Loss: 0.8159348368644714, Variance: 0.11439263820648193

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6346851688884025, Training Loss Force: 3.4222776609438563, time: 3.1918952465057373
Validation Loss Energy: 3.3164433888834552, Validation Loss Force: 3.485627405959388, time: 0.18643832206726074
Test Loss Energy: 10.276493520855936, Test Loss Force: 12.371808010972085, time: 10.861735343933105

Epoch 6, Batch 100/197, Loss: 1.4372608661651611, Variance: 0.11613388359546661

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.651758945622093, Training Loss Force: 3.420684090205666, time: 3.383981227874756
Validation Loss Energy: 2.0506706087453783, Validation Loss Force: 3.4329058652762066, time: 0.18671059608459473
Test Loss Energy: 10.072930893871456, Test Loss Force: 12.410931157114268, time: 10.626978158950806

Epoch 7, Batch 100/197, Loss: 0.989825963973999, Variance: 0.11466242372989655

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6528737765662385, Training Loss Force: 3.415137866334, time: 3.268369197845459
Validation Loss Energy: 2.0914651549633705, Validation Loss Force: 3.4882604686099845, time: 0.1855933666229248
Test Loss Energy: 10.214402432364396, Test Loss Force: 12.40196298122516, time: 10.727152585983276

Epoch 8, Batch 100/197, Loss: 0.5900611877441406, Variance: 0.11172851175069809

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.636087348148716, Training Loss Force: 3.4144943400689662, time: 3.172015428543091
Validation Loss Energy: 3.8405854048286043, Validation Loss Force: 3.5265523249623216, time: 0.1840205192565918
Test Loss Energy: 10.861560283140445, Test Loss Force: 12.411474521125841, time: 10.865302324295044

Epoch 9, Batch 100/197, Loss: 1.337607502937317, Variance: 0.10886135697364807

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.670823087816568, Training Loss Force: 3.434823791207923, time: 3.284824848175049
Validation Loss Energy: 2.7356588512644815, Validation Loss Force: 3.498739835290211, time: 0.18883895874023438
Test Loss Energy: 10.395110736554297, Test Loss Force: 12.338005915964292, time: 10.755507230758667

Epoch 10, Batch 100/197, Loss: 0.807517409324646, Variance: 0.11034408211708069

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6801449917361806, Training Loss Force: 3.4080381600825134, time: 3.22861909866333
Validation Loss Energy: 1.5049798140367106, Validation Loss Force: 3.506477588111202, time: 0.18525004386901855
Test Loss Energy: 9.827815075230738, Test Loss Force: 12.16454823678413, time: 10.768222093582153

Epoch 11, Batch 100/197, Loss: 0.9414430856704712, Variance: 0.11429505795240402

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6416804184438445, Training Loss Force: 3.4262311561510037, time: 3.265970468521118
Validation Loss Energy: 3.3456202280799516, Validation Loss Force: 3.5029781198138004, time: 0.18498992919921875
Test Loss Energy: 10.529177856771897, Test Loss Force: 12.522033067634228, time: 10.714836359024048

Epoch 12, Batch 100/197, Loss: 1.1937329769134521, Variance: 0.11730986088514328

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.667599107594989, Training Loss Force: 3.4115864483075837, time: 3.2432003021240234
Validation Loss Energy: 2.260790121608676, Validation Loss Force: 3.382861976740093, time: 0.2031548023223877
Test Loss Energy: 10.287463886409714, Test Loss Force: 12.591755375167208, time: 11.011282682418823

Epoch 13, Batch 100/197, Loss: 0.9071796536445618, Variance: 0.1170269101858139

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6566726110970373, Training Loss Force: 3.4072828934667316, time: 3.1122241020202637
Validation Loss Energy: 2.2410714883891814, Validation Loss Force: 3.4245980237743545, time: 0.1840980052947998
Test Loss Energy: 10.238686123199619, Test Loss Force: 12.322246951333737, time: 10.775202512741089

Epoch 14, Batch 100/197, Loss: 0.7569410800933838, Variance: 0.11215558648109436

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6194187885698046, Training Loss Force: 3.40673663518826, time: 3.238292694091797
Validation Loss Energy: 4.0379107939051835, Validation Loss Force: 3.426987319377446, time: 0.19083666801452637
Test Loss Energy: 10.918903309997837, Test Loss Force: 12.311378966954807, time: 10.795316219329834

Epoch 15, Batch 100/197, Loss: 1.4079304933547974, Variance: 0.1113143190741539

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6608183669829417, Training Loss Force: 3.4081911744765745, time: 3.267618417739868
Validation Loss Energy: 2.891275247608029, Validation Loss Force: 3.4480546596138253, time: 0.18946409225463867
Test Loss Energy: 10.444291388690063, Test Loss Force: 12.378900333374558, time: 10.625125885009766

Epoch 16, Batch 100/197, Loss: 0.8897011876106262, Variance: 0.11519388854503632

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6490202615010623, Training Loss Force: 3.40992406037, time: 3.2077014446258545
Validation Loss Energy: 1.7963173133706607, Validation Loss Force: 3.500000086988876, time: 0.18169689178466797
Test Loss Energy: 10.057382301454101, Test Loss Force: 12.768987639118123, time: 10.706876516342163

Epoch 17, Batch 100/197, Loss: 0.8337820768356323, Variance: 0.11865216493606567

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6697246117516458, Training Loss Force: 3.4296356272350117, time: 3.1607625484466553
Validation Loss Energy: 3.600133232899586, Validation Loss Force: 3.42683328799575, time: 0.18964409828186035
Test Loss Energy: 10.683409512460004, Test Loss Force: 12.37203360862531, time: 10.560068130493164

Epoch 18, Batch 100/197, Loss: 1.4202653169631958, Variance: 0.11827747523784637

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.682445349100277, Training Loss Force: 3.430154736832183, time: 3.260857105255127
Validation Loss Energy: 2.067333219271587, Validation Loss Force: 3.5195593447671727, time: 0.18337798118591309
Test Loss Energy: 10.072283371995098, Test Loss Force: 12.510885396818328, time: 11.557814836502075

Epoch 19, Batch 100/197, Loss: 0.8605083227157593, Variance: 0.12006218731403351

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6770565896146907, Training Loss Force: 3.42054385272544, time: 3.417569637298584
Validation Loss Energy: 2.1731851211898743, Validation Loss Force: 3.413246295047669, time: 0.19753670692443848
Test Loss Energy: 10.126593393270632, Test Loss Force: 12.280421581037912, time: 10.785319089889526

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–†â–„â–‚â–„â–ƒâ–ƒâ–ˆâ–…â–â–…â–„â–„â–ˆâ–…â–‚â–†â–ƒâ–ƒ
wandb:   test_error_force â–…â–‚â–„â–…â–…â–ƒâ–„â–„â–„â–ƒâ–â–…â–†â–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–…â–‚
wandb:          test_loss â–‡â–…â–„â–ƒâ–„â–†â–…â–„â–„â–ƒâ–…â–‡â–‡â–„â–†â–„â–„â–ˆâ–„â–
wandb: train_error_energy â–ˆâ–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–‚â–‚
wandb: valid_error_energy â–‚â–ƒâ–†â–…â–‚â–†â–ƒâ–ƒâ–‡â–„â–â–†â–ƒâ–ƒâ–ˆâ–…â–‚â–‡â–ƒâ–ƒ
wandb:  valid_error_force â–ƒâ–‚â–†â–†â–ˆâ–†â–ƒâ–†â–ˆâ–‡â–‡â–‡â–â–ƒâ–ƒâ–„â–‡â–ƒâ–ˆâ–‚
wandb:         valid_loss â–‚â–‚â–†â–„â–‚â–†â–‚â–‚â–‡â–„â–â–†â–‚â–‚â–ˆâ–„â–‚â–†â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 6277
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.12659
wandb:   test_error_force 12.28042
wandb:          test_loss 12.05984
wandb: train_error_energy 2.67706
wandb:  train_error_force 3.42054
wandb:         train_loss 0.98767
wandb: valid_error_energy 2.17319
wandb:  valid_error_force 3.41325
wandb:         valid_loss 0.79966
wandb: 
wandb: ğŸš€ View run al_71_62 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/2gjzovzt
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_232009-2gjzovzt/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.264744997024536, Uncertainty Bias: -0.1488911509513855
6.020069e-06 0.0036182404
1.989641 4.657894
(48745, 22, 3)
Found uncertainty sample 0 after 2353 steps.
Found uncertainty sample 1 after 39 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 131 steps.
Did not find any uncertainty samples for sample 5.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 3798 steps.
Found uncertainty sample 9 after 1945 steps.
Found uncertainty sample 10 after 3874 steps.
Found uncertainty sample 11 after 2841 steps.
Found uncertainty sample 12 after 3200 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 2620 steps.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 599 steps.
Found uncertainty sample 18 after 165 steps.
Found uncertainty sample 19 after 3556 steps.
Found uncertainty sample 20 after 29 steps.
Found uncertainty sample 21 after 3247 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 745 steps.
Found uncertainty sample 24 after 132 steps.
Found uncertainty sample 25 after 133 steps.
Found uncertainty sample 26 after 430 steps.
Found uncertainty sample 27 after 1373 steps.
Found uncertainty sample 28 after 2459 steps.
Found uncertainty sample 29 after 61 steps.
Found uncertainty sample 30 after 3790 steps.
Found uncertainty sample 31 after 1770 steps.
Found uncertainty sample 32 after 1812 steps.
Found uncertainty sample 33 after 107 steps.
Found uncertainty sample 34 after 150 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 64 steps.
Found uncertainty sample 37 after 329 steps.
Found uncertainty sample 38 after 3836 steps.
Found uncertainty sample 39 after 581 steps.
Found uncertainty sample 40 after 49 steps.
Found uncertainty sample 41 after 69 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 3847 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 1298 steps.
Found uncertainty sample 46 after 2483 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 1252 steps.
Found uncertainty sample 49 after 511 steps.
Found uncertainty sample 50 after 3287 steps.
Found uncertainty sample 51 after 414 steps.
Found uncertainty sample 52 after 992 steps.
Found uncertainty sample 53 after 3069 steps.
Found uncertainty sample 54 after 2422 steps.
Found uncertainty sample 55 after 208 steps.
Found uncertainty sample 56 after 1420 steps.
Found uncertainty sample 57 after 3586 steps.
Found uncertainty sample 58 after 868 steps.
Found uncertainty sample 59 after 1193 steps.
Found uncertainty sample 60 after 474 steps.
Found uncertainty sample 61 after 897 steps.
Found uncertainty sample 62 after 228 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 226 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 111 steps.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 1801 steps.
Found uncertainty sample 71 after 10 steps.
Found uncertainty sample 72 after 1927 steps.
Found uncertainty sample 73 after 3096 steps.
Found uncertainty sample 74 after 966 steps.
Found uncertainty sample 75 after 1995 steps.
Found uncertainty sample 76 after 2212 steps.
Found uncertainty sample 77 after 220 steps.
Found uncertainty sample 78 after 1782 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 592 steps.
Found uncertainty sample 81 after 2871 steps.
Found uncertainty sample 82 after 3256 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 341 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 213 steps.
Found uncertainty sample 87 after 1247 steps.
Found uncertainty sample 88 after 877 steps.
Found uncertainty sample 89 after 2337 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 175 steps.
Did not find any uncertainty samples for sample 92.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 2977 steps.
Found uncertainty sample 95 after 1732 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 287 steps.
Found uncertainty sample 98 after 495 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241203_234606-kljz694n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_63
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/kljz694n
Training model 63. Added 80 samples to the dataset.
Epoch 0, Batch 100/199, Loss: 0.5315770506858826, Variance: 0.08739154040813446

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.988476469993626, Training Loss Force: 3.5465189246127617, time: 3.2753992080688477
Validation Loss Energy: 1.532470630293885, Validation Loss Force: 3.484215961910639, time: 0.18280792236328125
Test Loss Energy: 10.266847922471337, Test Loss Force: 13.016642599496329, time: 10.865159749984741

Epoch 1, Batch 100/199, Loss: 0.552263081073761, Variance: 0.0834154263138771

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6444399468344058, Training Loss Force: 3.407824012377961, time: 3.2282004356384277
Validation Loss Energy: 1.8403020682760038, Validation Loss Force: 3.5490948482904723, time: 0.18793249130249023
Test Loss Energy: 9.956481454407458, Test Loss Force: 12.480287419887489, time: 10.713326692581177

Epoch 2, Batch 100/199, Loss: 0.5324867963790894, Variance: 0.0809706300497055

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6387425113975513, Training Loss Force: 3.4185023697461676, time: 3.2841920852661133
Validation Loss Energy: 1.8885944691438927, Validation Loss Force: 3.480688492968137, time: 0.1972804069519043
Test Loss Energy: 10.152399241060957, Test Loss Force: 12.7639216572391, time: 10.691654682159424

Epoch 3, Batch 100/199, Loss: 0.31712406873703003, Variance: 0.07724952697753906

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6377155064684512, Training Loss Force: 3.4139111825997936, time: 3.1764187812805176
Validation Loss Energy: 1.556920982864345, Validation Loss Force: 3.4679865542078323, time: 0.18127703666687012
Test Loss Energy: 10.068564606065749, Test Loss Force: 12.895381048959036, time: 10.632656812667847

Epoch 4, Batch 100/199, Loss: 0.5131967663764954, Variance: 0.08179101347923279

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6656725705174629, Training Loss Force: 3.420105170232067, time: 3.4317328929901123
Validation Loss Energy: 1.4908572182917432, Validation Loss Force: 3.5655929063178404, time: 0.19106125831604004
Test Loss Energy: 9.955627482092806, Test Loss Force: 13.004389417174144, time: 10.634707927703857

Epoch 5, Batch 100/199, Loss: 0.4509199857711792, Variance: 0.08291773498058319

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6313849404076026, Training Loss Force: 3.428665937308858, time: 3.2564377784729004
Validation Loss Energy: 1.8662232340562468, Validation Loss Force: 3.41503416128761, time: 0.1856703758239746
Test Loss Energy: 10.070388576475573, Test Loss Force: 12.745020091837837, time: 10.660331726074219

Epoch 6, Batch 100/199, Loss: 0.7973021268844604, Variance: 0.07861059159040451

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6677600738993428, Training Loss Force: 3.46844757724719, time: 3.4050168991088867
Validation Loss Energy: 1.7924023722806017, Validation Loss Force: 3.528610678547278, time: 0.18335795402526855
Test Loss Energy: 10.246828529978147, Test Loss Force: 12.759531470388938, time: 10.665491580963135

Epoch 7, Batch 100/199, Loss: 0.42854994535446167, Variance: 0.07954432815313339

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6529004255181752, Training Loss Force: 3.427445901727987, time: 3.24033260345459
Validation Loss Energy: 1.5245113495063394, Validation Loss Force: 3.44685992831135, time: 0.19022226333618164
Test Loss Energy: 10.143752841906437, Test Loss Force: 12.971411962775704, time: 10.648473262786865

Epoch 8, Batch 100/199, Loss: 0.541365385055542, Variance: 0.078565314412117

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6545153900055085, Training Loss Force: 3.4316671526213836, time: 3.2536542415618896
Validation Loss Energy: 1.5090332553355368, Validation Loss Force: 3.5166619181730736, time: 0.18598055839538574
Test Loss Energy: 10.051776821379672, Test Loss Force: 13.25017733756699, time: 10.779377460479736

Epoch 9, Batch 100/199, Loss: 0.6519798040390015, Variance: 0.07628258317708969

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6647491721992345, Training Loss Force: 3.426005721330803, time: 3.248103380203247
Validation Loss Energy: 1.7102816572587147, Validation Loss Force: 3.5346343799502025, time: 0.18326997756958008
Test Loss Energy: 9.877215817747242, Test Loss Force: 12.655321587294361, time: 10.664253950119019

Epoch 10, Batch 100/199, Loss: 0.6548573970794678, Variance: 0.08113972842693329

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6818059566268495, Training Loss Force: 3.439408844112694, time: 3.1587741374969482
Validation Loss Energy: 1.9323909294527102, Validation Loss Force: 3.5005794721065797, time: 0.18939495086669922
Test Loss Energy: 10.059083806165278, Test Loss Force: 12.621369937895388, time: 10.916865587234497

Epoch 11, Batch 100/199, Loss: 0.601566731929779, Variance: 0.0799180120229721

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6590828360332714, Training Loss Force: 3.42461392503683, time: 3.205504894256592
Validation Loss Energy: 1.3751094541901336, Validation Loss Force: 3.451037479825387, time: 0.18010592460632324
Test Loss Energy: 9.884065349174165, Test Loss Force: 12.830394944470921, time: 10.63307237625122

Epoch 12, Batch 100/199, Loss: 0.5316786766052246, Variance: 0.07727901637554169

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6509616665244506, Training Loss Force: 3.4235058875606, time: 3.2509307861328125
Validation Loss Energy: 1.4337003030571827, Validation Loss Force: 3.5526049490395915, time: 0.18395638465881348
Test Loss Energy: 9.867126289365704, Test Loss Force: 12.696000171851223, time: 11.584061622619629

Epoch 13, Batch 100/199, Loss: 0.3194848895072937, Variance: 0.07862939685583115

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.662507560788239, Training Loss Force: 3.4228476898735822, time: 3.506063222885132
Validation Loss Energy: 1.713499894424809, Validation Loss Force: 3.483690351391676, time: 0.21196436882019043
Test Loss Energy: 10.084281424439949, Test Loss Force: 12.465566098140181, time: 12.261249542236328

Epoch 14, Batch 100/199, Loss: 0.5765009522438049, Variance: 0.07978203892707825

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.65873924579223, Training Loss Force: 3.419606480860424, time: 3.618427276611328
Validation Loss Energy: 2.2323855546775437, Validation Loss Force: 3.415324633188473, time: 0.21222734451293945
Test Loss Energy: 10.150534462741268, Test Loss Force: 12.45219811516116, time: 12.674798965454102

Epoch 15, Batch 100/199, Loss: 0.6620743870735168, Variance: 0.07892972230911255

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.647867381663099, Training Loss Force: 3.438656051201572, time: 3.605191469192505
Validation Loss Energy: 1.3370572176724584, Validation Loss Force: 3.4669569877443487, time: 0.2149033546447754
Test Loss Energy: 10.008831544192146, Test Loss Force: 12.722100623745488, time: 12.312748670578003

Epoch 16, Batch 100/199, Loss: 0.5464964509010315, Variance: 0.07780639827251434

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.650457847370397, Training Loss Force: 3.4373255538212746, time: 3.6183056831359863
Validation Loss Energy: 1.5927080339941404, Validation Loss Force: 3.471148790083726, time: 0.21121668815612793
Test Loss Energy: 9.945848813828665, Test Loss Force: 12.685643986092265, time: 12.46677565574646

Epoch 17, Batch 100/199, Loss: 0.7652082443237305, Variance: 0.07728345692157745

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6483525455625383, Training Loss Force: 3.4257876353648844, time: 3.5734710693359375
Validation Loss Energy: 1.7672043802190458, Validation Loss Force: 3.5583926768151195, time: 0.2324504852294922
Test Loss Energy: 10.02466265828921, Test Loss Force: 12.662996202337316, time: 12.373655796051025

Epoch 18, Batch 100/199, Loss: 0.5052123069763184, Variance: 0.0770573765039444

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6373429821062546, Training Loss Force: 3.4167413622726177, time: 3.7181966304779053
Validation Loss Energy: 1.8773818529661257, Validation Loss Force: 3.5270299502803364, time: 0.21754693984985352
Test Loss Energy: 9.871991332742148, Test Loss Force: 12.55497074935946, time: 12.404980659484863

Epoch 19, Batch 100/199, Loss: 0.3909311294555664, Variance: 0.07762964069843292

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6743491714266938, Training Loss Force: 3.417545061378684, time: 3.714252471923828
Validation Loss Energy: 1.5002406631029999, Validation Loss Force: 3.463957244677696, time: 0.21619248390197754
Test Loss Energy: 9.923266105423064, Test Loss Force: 12.732296109997893, time: 13.412994623184204

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–ƒâ–†â–…â–ƒâ–…â–ˆâ–†â–„â–â–„â–â–â–…â–†â–ƒâ–‚â–„â–â–‚
wandb:   test_error_force â–†â–â–„â–…â–†â–„â–„â–†â–ˆâ–ƒâ–‚â–„â–ƒâ–â–â–ƒâ–ƒâ–ƒâ–‚â–ƒ
wandb:          test_loss â–„â–â–‚â–„â–„â–…â–„â–‡â–…â–ƒâ–„â–†â–ƒâ–…â–ƒâ–ˆâ–…â–„â–…â–„
wandb: train_error_energy â–ˆâ–â–â–â–‚â–â–‚â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚
wandb:  train_error_force â–ˆâ–â–‚â–â–‚â–‚â–„â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–…â–…â–ƒâ–‚â–…â–…â–‚â–‚â–„â–†â–â–‚â–„â–ˆâ–â–ƒâ–„â–…â–‚
wandb:  valid_error_force â–„â–‡â–„â–ƒâ–ˆâ–â–†â–‚â–†â–‡â–…â–ƒâ–‡â–„â–â–ƒâ–„â–ˆâ–†â–ƒ
wandb:         valid_loss â–ƒâ–…â–…â–ƒâ–ƒâ–…â–…â–ƒâ–ƒâ–„â–†â–â–‚â–„â–ˆâ–â–ƒâ–…â–†â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6349
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.92327
wandb:   test_error_force 12.7323
wandb:          test_loss 16.38104
wandb: train_error_energy 1.67435
wandb:  train_error_force 3.41755
wandb:         train_loss 0.55302
wandb: valid_error_energy 1.50024
wandb:  valid_error_force 3.46396
wandb:         valid_loss 0.51725
wandb: 
wandb: ğŸš€ View run al_71_63 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/kljz694n
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241203_234606-kljz694n/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.8784985542297363, Uncertainty Bias: -0.06949256360530853
3.0517578e-05 0.008243561
2.1369462 5.1635647
(48745, 22, 3)
Found uncertainty sample 0 after 1959 steps.
Found uncertainty sample 1 after 1554 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 852 steps.
Found uncertainty sample 4 after 1534 steps.
Found uncertainty sample 5 after 979 steps.
Found uncertainty sample 6 after 2219 steps.
Found uncertainty sample 7 after 2061 steps.
Found uncertainty sample 8 after 2202 steps.
Found uncertainty sample 9 after 1866 steps.
Found uncertainty sample 10 after 596 steps.
Found uncertainty sample 11 after 700 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 3139 steps.
Found uncertainty sample 14 after 705 steps.
Found uncertainty sample 15 after 996 steps.
Found uncertainty sample 16 after 426 steps.
Found uncertainty sample 17 after 1226 steps.
Found uncertainty sample 18 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 3345 steps.
Found uncertainty sample 21 after 185 steps.
Found uncertainty sample 22 after 2980 steps.
Found uncertainty sample 23 after 32 steps.
Found uncertainty sample 24 after 3796 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 1147 steps.
Found uncertainty sample 27 after 543 steps.
Found uncertainty sample 28 after 3929 steps.
Found uncertainty sample 29 after 752 steps.
Found uncertainty sample 30 after 149 steps.
Found uncertainty sample 31 after 183 steps.
Found uncertainty sample 32 after 520 steps.
Found uncertainty sample 33 after 177 steps.
Found uncertainty sample 34 after 359 steps.
Found uncertainty sample 35 after 1586 steps.
Found uncertainty sample 36 after 1633 steps.
Found uncertainty sample 37 after 3708 steps.
Found uncertainty sample 38 after 673 steps.
Found uncertainty sample 39 after 286 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 3438 steps.
Found uncertainty sample 42 after 2361 steps.
Found uncertainty sample 43 after 258 steps.
Found uncertainty sample 44 after 107 steps.
Found uncertainty sample 45 after 3390 steps.
Found uncertainty sample 46 after 969 steps.
Found uncertainty sample 47 after 2594 steps.
Found uncertainty sample 48 after 2608 steps.
Found uncertainty sample 49 after 1239 steps.
Found uncertainty sample 50 after 1681 steps.
Found uncertainty sample 51 after 3277 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 47 steps.
Found uncertainty sample 55 after 495 steps.
Found uncertainty sample 56 after 2070 steps.
Found uncertainty sample 57 after 2270 steps.
Found uncertainty sample 58 after 1383 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 380 steps.
Found uncertainty sample 61 after 4 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 728 steps.
Found uncertainty sample 64 after 508 steps.
Found uncertainty sample 65 after 19 steps.
Found uncertainty sample 66 after 3117 steps.
Found uncertainty sample 67 after 1591 steps.
Found uncertainty sample 68 after 1039 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 178 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 1621 steps.
Found uncertainty sample 73 after 2591 steps.
Found uncertainty sample 74 after 416 steps.
Found uncertainty sample 75 after 3893 steps.
Found uncertainty sample 76 after 3153 steps.
Found uncertainty sample 77 after 1083 steps.
Found uncertainty sample 78 after 221 steps.
Found uncertainty sample 79 after 737 steps.
Found uncertainty sample 80 after 619 steps.
Found uncertainty sample 81 after 5 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 37 steps.
Found uncertainty sample 84 after 3604 steps.
Found uncertainty sample 85 after 106 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 316 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 1739 steps.
Found uncertainty sample 90 after 133 steps.
Found uncertainty sample 91 after 3458 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 1692 steps.
Found uncertainty sample 94 after 3978 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1207 steps.
Found uncertainty sample 97 after 3 steps.
Found uncertainty sample 98 after 206 steps.
Found uncertainty sample 99 after 145 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_001004-b0cvlleb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_64
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/b0cvlleb
Training model 64. Added 88 samples to the dataset.
Epoch 0, Batch 100/201, Loss: 1.3651118278503418, Variance: 0.08043795824050903
Epoch 0, Batch 200/201, Loss: 0.8788443207740784, Variance: 0.07569722831249237

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.767700460298432, Training Loss Force: 4.09728407597909, time: 3.302269220352173
Validation Loss Energy: 1.3556756426419236, Validation Loss Force: 3.698325817203535, time: 0.1848921775817871
Test Loss Energy: 9.934307034945766, Test Loss Force: 12.061407818441955, time: 10.537112712860107

Epoch 1, Batch 100/201, Loss: 0.8618684411048889, Variance: 0.07723194360733032
Epoch 1, Batch 200/201, Loss: 0.6132152676582336, Variance: 0.07685510814189911

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6107596419128696, Training Loss Force: 3.3977164207456054, time: 3.6175894737243652
Validation Loss Energy: 1.503839636674987, Validation Loss Force: 3.410045760078676, time: 0.22267580032348633
Test Loss Energy: 9.818648199188585, Test Loss Force: 12.643952790861062, time: 12.813616037368774

Epoch 2, Batch 100/201, Loss: 0.5838221311569214, Variance: 0.07595798373222351
Epoch 2, Batch 200/201, Loss: 0.508095383644104, Variance: 0.07870946079492569

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6114353397447376, Training Loss Force: 3.3928351308735585, time: 3.7913272380828857
Validation Loss Energy: 1.2739501096867722, Validation Loss Force: 3.414750703464844, time: 0.21578073501586914
Test Loss Energy: 9.844033498408098, Test Loss Force: 12.690189612327588, time: 10.818222522735596

Epoch 3, Batch 100/201, Loss: 0.6274512410163879, Variance: 0.07357580959796906
Epoch 3, Batch 200/201, Loss: 0.7842594385147095, Variance: 0.07760444283485413

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6300828667961114, Training Loss Force: 3.39170340330741, time: 3.2772514820098877
Validation Loss Energy: 1.8409030523008627, Validation Loss Force: 3.4118212125137317, time: 0.19132232666015625
Test Loss Energy: 10.06611952164079, Test Loss Force: 12.869356253278875, time: 10.723430871963501

Epoch 4, Batch 100/201, Loss: 0.481701135635376, Variance: 0.07439669966697693
Epoch 4, Batch 200/201, Loss: 0.646468997001648, Variance: 0.07604820281267166

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.623071687552558, Training Loss Force: 3.3902109207062714, time: 3.3156630992889404
Validation Loss Energy: 1.8237743952522272, Validation Loss Force: 3.4054902276894263, time: 0.18968772888183594
Test Loss Energy: 10.206827643047244, Test Loss Force: 12.68045953956438, time: 10.597588062286377

Epoch 5, Batch 100/201, Loss: 0.8552231192588806, Variance: 0.07989800721406937
Epoch 5, Batch 200/201, Loss: 0.4386354684829712, Variance: 0.07929769903421402

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6442627833675303, Training Loss Force: 3.390777594849944, time: 3.2112503051757812
Validation Loss Energy: 1.8182173527118803, Validation Loss Force: 3.4458944348946066, time: 0.18866491317749023
Test Loss Energy: 10.152061907797139, Test Loss Force: 12.767782157325968, time: 10.690889358520508

Epoch 6, Batch 100/201, Loss: 0.6097334623336792, Variance: 0.07930926233530045
Epoch 6, Batch 200/201, Loss: 0.6036006808280945, Variance: 0.07793569564819336

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6414396338276507, Training Loss Force: 3.3978377067461216, time: 3.4580278396606445
Validation Loss Energy: 1.3708575033477128, Validation Loss Force: 3.42770025031542, time: 0.18924403190612793
Test Loss Energy: 10.055202750568565, Test Loss Force: 12.72751890318758, time: 10.616918563842773

Epoch 7, Batch 100/201, Loss: 0.6302881836891174, Variance: 0.0760386660695076
Epoch 7, Batch 200/201, Loss: 0.3882707953453064, Variance: 0.07811050117015839

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6460561030628844, Training Loss Force: 3.4121298466284258, time: 3.3382863998413086
Validation Loss Energy: 1.8338463330626924, Validation Loss Force: 3.4663680278533695, time: 0.18541598320007324
Test Loss Energy: 10.31305594384058, Test Loss Force: 12.817506485909593, time: 10.503493785858154

Epoch 8, Batch 100/201, Loss: 0.3529399037361145, Variance: 0.07777661085128784
Epoch 8, Batch 200/201, Loss: 0.6299407482147217, Variance: 0.0778447613120079

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6489042275508259, Training Loss Force: 3.409898699570616, time: 3.492990016937256
Validation Loss Energy: 1.698114486285999, Validation Loss Force: 3.483942524199439, time: 0.18863344192504883
Test Loss Energy: 10.071894076379596, Test Loss Force: 12.636239997968937, time: 10.607327938079834

Epoch 9, Batch 100/201, Loss: 0.7215059995651245, Variance: 0.07507084310054779
Epoch 9, Batch 200/201, Loss: 0.47942841053009033, Variance: 0.07990008592605591

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6352781298572254, Training Loss Force: 3.419700783701496, time: 3.280369997024536
Validation Loss Energy: 1.7024827817571668, Validation Loss Force: 3.4405000521653606, time: 0.1899871826171875
Test Loss Energy: 9.862129845545406, Test Loss Force: 12.640633155295463, time: 10.782889127731323

Epoch 10, Batch 100/201, Loss: 0.6705488562583923, Variance: 0.08125952631235123
Epoch 10, Batch 200/201, Loss: 0.7023211717605591, Variance: 0.0775301605463028

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6711656291435812, Training Loss Force: 3.410836120057872, time: 3.407041549682617
Validation Loss Energy: 1.2117127752187231, Validation Loss Force: 3.45582937551304, time: 0.19283080101013184
Test Loss Energy: 9.711001675469975, Test Loss Force: 12.607587216727872, time: 10.9161856174469

Epoch 11, Batch 100/201, Loss: 0.27730250358581543, Variance: 0.07761932164430618
Epoch 11, Batch 200/201, Loss: 0.3851768970489502, Variance: 0.07672706246376038

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6414809361847036, Training Loss Force: 3.4298539749788484, time: 3.2254257202148438
Validation Loss Energy: 2.1815824756883897, Validation Loss Force: 3.477067948322637, time: 0.18734216690063477
Test Loss Energy: 10.114969611520067, Test Loss Force: 12.789519996038333, time: 10.603193044662476

Epoch 12, Batch 100/201, Loss: 0.24316442012786865, Variance: 0.07676447927951813
Epoch 12, Batch 200/201, Loss: 0.7118543386459351, Variance: 0.07907426357269287

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6440860361325784, Training Loss Force: 3.4150574529233966, time: 3.3369767665863037
Validation Loss Energy: 1.8028726651931315, Validation Loss Force: 3.508548299833757, time: 0.1971592903137207
Test Loss Energy: 10.037139521677842, Test Loss Force: 12.549610340579168, time: 10.721232891082764

Epoch 13, Batch 100/201, Loss: 0.4535278081893921, Variance: 0.07898858189582825
Epoch 13, Batch 200/201, Loss: 0.7691918611526489, Variance: 0.07748980820178986

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6650040526118177, Training Loss Force: 3.4234735933309137, time: 3.2574596405029297
Validation Loss Energy: 1.3981867731395166, Validation Loss Force: 3.4347658763194278, time: 0.18474793434143066
Test Loss Energy: 10.029246420404645, Test Loss Force: 13.126297002018001, time: 10.612889051437378

Epoch 14, Batch 100/201, Loss: 0.5834556221961975, Variance: 0.0795944333076477
Epoch 14, Batch 200/201, Loss: 0.3580701947212219, Variance: 0.07634027302265167

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6446153578039733, Training Loss Force: 3.4226347964425905, time: 3.2888638973236084
Validation Loss Energy: 1.5299399119494859, Validation Loss Force: 3.422654289132264, time: 0.18697786331176758
Test Loss Energy: 9.926337680565256, Test Loss Force: 12.814191812018787, time: 10.726468801498413

Epoch 15, Batch 100/201, Loss: 0.4674975275993347, Variance: 0.07533515244722366
Epoch 15, Batch 200/201, Loss: 0.45672786235809326, Variance: 0.07719668745994568

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6380164070026924, Training Loss Force: 3.41640411492912, time: 3.2563979625701904
Validation Loss Energy: 2.0958340906083333, Validation Loss Force: 3.446846289826162, time: 0.21050500869750977
Test Loss Energy: 10.256598193548662, Test Loss Force: 12.762820967510764, time: 10.597063541412354

Epoch 16, Batch 100/201, Loss: 0.3091362714767456, Variance: 0.07293055951595306
Epoch 16, Batch 200/201, Loss: 0.39358335733413696, Variance: 0.07918492704629898

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6436915195807158, Training Loss Force: 3.4237834103062084, time: 3.3697614669799805
Validation Loss Energy: 1.8344070974730682, Validation Loss Force: 3.4500822291893574, time: 0.18884944915771484
Test Loss Energy: 10.186301842625863, Test Loss Force: 12.582949824697899, time: 10.779783248901367

Epoch 17, Batch 100/201, Loss: 0.721839964389801, Variance: 0.07876242697238922
Epoch 17, Batch 200/201, Loss: 0.6559633016586304, Variance: 0.0756310522556305

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6290859761989376, Training Loss Force: 3.4154490714529424, time: 3.3414976596832275
Validation Loss Energy: 1.5273559200119236, Validation Loss Force: 3.438007159742126, time: 0.18923449516296387
Test Loss Energy: 9.75998798290782, Test Loss Force: 12.53308632206903, time: 10.5207998752594

Epoch 18, Batch 100/201, Loss: 0.570724606513977, Variance: 0.08025427162647247
Epoch 18, Batch 200/201, Loss: 0.5666522979736328, Variance: 0.07633814215660095

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6433247059509042, Training Loss Force: 3.4120995614089664, time: 3.3381528854370117
Validation Loss Energy: 1.4489403917446317, Validation Loss Force: 3.4141875175275063, time: 0.18667912483215332
Test Loss Energy: 9.93787471228186, Test Loss Force: 13.061521666117482, time: 10.806241035461426

Epoch 19, Batch 100/201, Loss: 0.36491507291793823, Variance: 0.07501626014709473
Epoch 19, Batch 200/201, Loss: 0.5660862922668457, Variance: 0.07727298140525818

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6298013686833905, Training Loss Force: 3.4163114391443394, time: 3.2822203636169434
Validation Loss Energy: 1.8747863100666189, Validation Loss Force: 3.4218975480398286, time: 0.19198131561279297
Test Loss Energy: 10.052882677964842, Test Loss Force: 12.600031439392035, time: 10.716270685195923

wandb: - 0.039 MB of 0.051 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–‚â–ƒâ–…â–‡â–†â–…â–ˆâ–…â–ƒâ–â–†â–…â–…â–„â–‡â–‡â–‚â–„â–…
wandb:   test_error_force â–â–…â–…â–†â–…â–†â–…â–†â–…â–…â–…â–†â–„â–ˆâ–†â–†â–„â–„â–ˆâ–…
wandb:          test_loss â–â–ƒâ–…â–ƒâ–„â–…â–ˆâ–…â–‚â–‚â–‚â–‚â–‚â–‡â–…â–„â–ƒâ–…â–„â–‚
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–ƒâ–â–†â–…â–…â–‚â–…â–…â–…â–â–ˆâ–…â–‚â–ƒâ–‡â–…â–ƒâ–ƒâ–†
wandb:  valid_error_force â–ˆâ–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–â–
wandb:         valid_loss â–ƒâ–ƒâ–â–…â–…â–†â–‚â–…â–„â–„â–â–ˆâ–…â–‚â–ƒâ–‡â–…â–ƒâ–‚â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 6428
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.05288
wandb:   test_error_force 12.60003
wandb:          test_loss 16.28001
wandb: train_error_energy 1.6298
wandb:  train_error_force 3.41631
wandb:         train_loss 0.52412
wandb: valid_error_energy 1.87479
wandb:  valid_error_force 3.4219
wandb:         valid_loss 0.64717
wandb: 
wandb: ğŸš€ View run al_71_64 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/b0cvlleb
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_001004-b0cvlleb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.5592308044433594, Uncertainty Bias: -0.05044819414615631
7.6293945e-06 0.009098053
2.1454768 5.180383
(48745, 22, 3)
Found uncertainty sample 0 after 3272 steps.
Found uncertainty sample 1 after 626 steps.
Found uncertainty sample 2 after 1640 steps.
Found uncertainty sample 3 after 1115 steps.
Found uncertainty sample 4 after 2548 steps.
Found uncertainty sample 5 after 3340 steps.
Found uncertainty sample 6 after 237 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1793 steps.
Found uncertainty sample 9 after 409 steps.
Found uncertainty sample 10 after 419 steps.
Found uncertainty sample 11 after 202 steps.
Found uncertainty sample 12 after 2210 steps.
Found uncertainty sample 13 after 536 steps.
Found uncertainty sample 14 after 139 steps.
Found uncertainty sample 15 after 1439 steps.
Found uncertainty sample 16 after 128 steps.
Found uncertainty sample 17 after 208 steps.
Found uncertainty sample 18 after 1506 steps.
Found uncertainty sample 19 after 321 steps.
Found uncertainty sample 20 after 327 steps.
Found uncertainty sample 21 after 239 steps.
Found uncertainty sample 22 after 2913 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 293 steps.
Found uncertainty sample 26 after 2906 steps.
Found uncertainty sample 27 after 725 steps.
Found uncertainty sample 28 after 1689 steps.
Found uncertainty sample 29 after 400 steps.
Found uncertainty sample 30 after 7 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 1388 steps.
Found uncertainty sample 33 after 2479 steps.
Found uncertainty sample 34 after 558 steps.
Found uncertainty sample 35 after 1887 steps.
Found uncertainty sample 36 after 431 steps.
Found uncertainty sample 37 after 2463 steps.
Found uncertainty sample 38 after 1058 steps.
Found uncertainty sample 39 after 1097 steps.
Found uncertainty sample 40 after 3545 steps.
Found uncertainty sample 41 after 1202 steps.
Found uncertainty sample 42 after 2752 steps.
Found uncertainty sample 43 after 2033 steps.
Found uncertainty sample 44 after 53 steps.
Found uncertainty sample 45 after 3799 steps.
Found uncertainty sample 46 after 2191 steps.
Found uncertainty sample 47 after 474 steps.
Found uncertainty sample 48 after 2851 steps.
Found uncertainty sample 49 after 593 steps.
Found uncertainty sample 50 after 29 steps.
Found uncertainty sample 51 after 145 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 2660 steps.
Found uncertainty sample 56 after 2412 steps.
Found uncertainty sample 57 after 1310 steps.
Found uncertainty sample 58 after 4 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 898 steps.
Found uncertainty sample 61 after 430 steps.
Found uncertainty sample 62 after 3296 steps.
Found uncertainty sample 63 after 11 steps.
Found uncertainty sample 64 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 35 steps.
Found uncertainty sample 68 after 304 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 3727 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 3110 steps.
Found uncertainty sample 73 after 1132 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 2188 steps.
Found uncertainty sample 76 after 2238 steps.
Found uncertainty sample 77 after 578 steps.
Found uncertainty sample 78 after 3857 steps.
Found uncertainty sample 79 after 298 steps.
Found uncertainty sample 80 after 1586 steps.
Found uncertainty sample 81 after 1416 steps.
Found uncertainty sample 82 after 394 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 3595 steps.
Found uncertainty sample 85 after 869 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 225 steps.
Found uncertainty sample 88 after 389 steps.
Found uncertainty sample 89 after 17 steps.
Found uncertainty sample 90 after 2962 steps.
Found uncertainty sample 91 after 1408 steps.
Found uncertainty sample 92 after 2248 steps.
Found uncertainty sample 93 after 1100 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1827 steps.
Found uncertainty sample 96 after 1414 steps.
Found uncertainty sample 97 after 31 steps.
Found uncertainty sample 98 after 140 steps.
Found uncertainty sample 99 after 14 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_003355-y9gg2b7i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_65
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/y9gg2b7i
Training model 65. Added 86 samples to the dataset.
Epoch 0, Batch 100/204, Loss: 0.8583986759185791, Variance: 0.10786011815071106
Epoch 0, Batch 200/204, Loss: 1.131661295890808, Variance: 0.10422274470329285

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.0375812771892865, Training Loss Force: 3.5508251938507454, time: 3.7019686698913574
Validation Loss Energy: 2.0742636851040164, Validation Loss Force: 3.4396306404850856, time: 0.22684097290039062
Test Loss Energy: 9.747282849836207, Test Loss Force: 12.085600935396101, time: 13.341136693954468

Epoch 1, Batch 100/204, Loss: 0.9749550819396973, Variance: 0.11359871923923492
Epoch 1, Batch 200/204, Loss: 1.2655797004699707, Variance: 0.10781575739383698

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6776993742116177, Training Loss Force: 3.4171465208725773, time: 3.7610952854156494
Validation Loss Energy: 2.160314844555248, Validation Loss Force: 3.4446950275400665, time: 0.2111649513244629
Test Loss Energy: 9.853820462555186, Test Loss Force: 12.184366005280202, time: 12.489004611968994

Epoch 2, Batch 100/204, Loss: 0.9894585609436035, Variance: 0.10773048549890518
Epoch 2, Batch 200/204, Loss: 1.3840163946151733, Variance: 0.10354326665401459

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6801306484062946, Training Loss Force: 3.4047346326881827, time: 3.663149356842041
Validation Loss Energy: 2.4690052566014264, Validation Loss Force: 3.437884691711865, time: 0.2123105525970459
Test Loss Energy: 9.986787228796873, Test Loss Force: 12.001583114808026, time: 12.356449365615845

Epoch 3, Batch 100/204, Loss: 1.0827823877334595, Variance: 0.10878418385982513
Epoch 3, Batch 200/204, Loss: 1.3490771055221558, Variance: 0.11044048517942429

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6888676672226755, Training Loss Force: 3.411126554635424, time: 3.7004597187042236
Validation Loss Energy: 2.2416328929733518, Validation Loss Force: 3.454961800853219, time: 0.20820927619934082
Test Loss Energy: 10.117076308791637, Test Loss Force: 12.24338184866896, time: 12.599640369415283

Epoch 4, Batch 100/204, Loss: 1.0790987014770508, Variance: 0.11574351787567139
Epoch 4, Batch 200/204, Loss: 1.5416523218154907, Variance: 0.10820627212524414

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6847837377151045, Training Loss Force: 3.402514747919071, time: 3.6603705883026123
Validation Loss Energy: 2.1803796169620875, Validation Loss Force: 3.468729155809502, time: 0.21497130393981934
Test Loss Energy: 9.863507638282298, Test Loss Force: 12.24866358412651, time: 12.386439323425293

Epoch 5, Batch 100/204, Loss: 0.8784543871879578, Variance: 0.11451061069965363
Epoch 5, Batch 200/204, Loss: 1.168868064880371, Variance: 0.10703152418136597

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6699754787246577, Training Loss Force: 3.4087868716827727, time: 3.5389063358306885
Validation Loss Energy: 1.9117801723000563, Validation Loss Force: 3.4616858523873733, time: 0.17547082901000977
Test Loss Energy: 9.912830052554947, Test Loss Force: 12.44121355940401, time: 11.75759744644165

Epoch 6, Batch 100/204, Loss: 0.9458644390106201, Variance: 0.11211246252059937
Epoch 6, Batch 200/204, Loss: 1.2971376180648804, Variance: 0.10810060799121857

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.651104057944801, Training Loss Force: 3.4119442153865487, time: 3.734292507171631
Validation Loss Energy: 2.2946364997971376, Validation Loss Force: 3.4243647778439947, time: 0.2093977928161621
Test Loss Energy: 10.231172133051565, Test Loss Force: 12.489651382605732, time: 10.180679559707642

Epoch 7, Batch 100/204, Loss: 0.8833320736885071, Variance: 0.1122380718588829
Epoch 7, Batch 200/204, Loss: 1.4595038890838623, Variance: 0.11133909970521927

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.654403474933075, Training Loss Force: 3.405961790730791, time: 3.4223854541778564
Validation Loss Energy: 2.3687995762164302, Validation Loss Force: 3.3893699835354107, time: 0.1756749153137207
Test Loss Energy: 10.12221914067118, Test Loss Force: 12.526831576070903, time: 10.211525678634644

Epoch 8, Batch 100/204, Loss: 0.8699368238449097, Variance: 0.11622238904237747
Epoch 8, Batch 200/204, Loss: 0.9911848306655884, Variance: 0.10782191157341003

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.675465506038446, Training Loss Force: 3.4162641845582, time: 3.260329484939575
Validation Loss Energy: 2.321015000169473, Validation Loss Force: 3.4879804621589807, time: 0.1742866039276123
Test Loss Energy: 10.259775162311108, Test Loss Force: 12.686669975568277, time: 9.931455135345459

Epoch 9, Batch 100/204, Loss: 0.9563331604003906, Variance: 0.11424046754837036
Epoch 9, Batch 200/204, Loss: 0.9977995157241821, Variance: 0.10853689908981323

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.680574480952626, Training Loss Force: 3.407365992574722, time: 3.3806657791137695
Validation Loss Energy: 2.013314630813376, Validation Loss Force: 3.451986662727191, time: 0.17980599403381348
Test Loss Energy: 10.019627598276298, Test Loss Force: 12.365258101282633, time: 10.113025426864624

Epoch 10, Batch 100/204, Loss: 0.7966634035110474, Variance: 0.11548015475273132
Epoch 10, Batch 200/204, Loss: 1.1607967615127563, Variance: 0.10827289521694183

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.661603229626588, Training Loss Force: 3.399884956540193, time: 3.3472180366516113
Validation Loss Energy: 2.015574945313689, Validation Loss Force: 3.43202081108843, time: 0.17378830909729004
Test Loss Energy: 10.160746792573887, Test Loss Force: 12.52069379782539, time: 9.919172763824463

Epoch 11, Batch 100/204, Loss: 0.933090329170227, Variance: 0.11250022798776627
Epoch 11, Batch 200/204, Loss: 1.4421672821044922, Variance: 0.11203072965145111

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.674614574805767, Training Loss Force: 3.3924392648665176, time: 3.475248098373413
Validation Loss Energy: 2.05651161011407, Validation Loss Force: 3.4392867836564864, time: 0.19301795959472656
Test Loss Energy: 10.066459806728052, Test Loss Force: 12.612140103609484, time: 10.103190422058105

Epoch 12, Batch 100/204, Loss: 0.903235912322998, Variance: 0.11578713357448578
Epoch 12, Batch 200/204, Loss: 1.1109213829040527, Variance: 0.11385387182235718

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.652692732444009, Training Loss Force: 3.406863219832917, time: 3.5753228664398193
Validation Loss Energy: 2.320063367907064, Validation Loss Force: 3.438192364804251, time: 0.2166764736175537
Test Loss Energy: 10.309531475976868, Test Loss Force: 12.651675558398274, time: 11.475692749023438

Epoch 13, Batch 100/204, Loss: 0.9337815642356873, Variance: 0.1148289293050766
Epoch 13, Batch 200/204, Loss: 1.1803038120269775, Variance: 0.10739761590957642

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.664262671476126, Training Loss Force: 3.3879552693064587, time: 3.7222402095794678
Validation Loss Energy: 2.19908037384826, Validation Loss Force: 3.4619041794849554, time: 0.18157124519348145
Test Loss Energy: 10.257171696004008, Test Loss Force: 12.79787957867999, time: 11.617927074432373

Epoch 14, Batch 100/204, Loss: 0.9967416524887085, Variance: 0.11208988726139069
Epoch 14, Batch 200/204, Loss: 1.3179230690002441, Variance: 0.11089646816253662

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6719397993475917, Training Loss Force: 3.3897735549896733, time: 3.4424400329589844
Validation Loss Energy: 2.18604131400102, Validation Loss Force: 3.49037326480112, time: 0.20287680625915527
Test Loss Energy: 10.237250874828232, Test Loss Force: 12.623306268788376, time: 11.49839973449707

Epoch 15, Batch 100/204, Loss: 0.8775284290313721, Variance: 0.11584784090518951
Epoch 15, Batch 200/204, Loss: 1.160609245300293, Variance: 0.11203975975513458

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.67828650035669, Training Loss Force: 3.3849560567163017, time: 3.8580985069274902
Validation Loss Energy: 2.2157897448951136, Validation Loss Force: 3.4372701248691047, time: 0.22955703735351562
Test Loss Energy: 10.292183842767578, Test Loss Force: 12.85399197495561, time: 12.817081689834595

Epoch 16, Batch 100/204, Loss: 0.9165230989456177, Variance: 0.11617209762334824
Epoch 16, Batch 200/204, Loss: 1.2531129121780396, Variance: 0.11294682323932648

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6766252526343983, Training Loss Force: 3.4103250232076787, time: 4.004292726516724
Validation Loss Energy: 2.2896666357946023, Validation Loss Force: 3.4356025556352514, time: 0.23232650756835938
Test Loss Energy: 10.252675672526909, Test Loss Force: 12.676082181254957, time: 12.279495000839233

Epoch 17, Batch 100/204, Loss: 0.919195830821991, Variance: 0.11229190230369568
Epoch 17, Batch 200/204, Loss: 1.3451908826828003, Variance: 0.11231633275747299

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6778485974678836, Training Loss Force: 3.3917137454811916, time: 3.5456454753875732
Validation Loss Energy: 2.1941286573811176, Validation Loss Force: 3.4103256551951073, time: 0.20995187759399414
Test Loss Energy: 10.301552035033788, Test Loss Force: 12.64823049598413, time: 12.173487186431885

Epoch 18, Batch 100/204, Loss: 0.9773087501525879, Variance: 0.11558786034584045
Epoch 18, Batch 200/204, Loss: 1.1101503372192383, Variance: 0.1099332794547081

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6583251965169308, Training Loss Force: 3.3885667013169307, time: 3.7486610412597656
Validation Loss Energy: 2.1746850762241303, Validation Loss Force: 3.42003480929957, time: 0.22067856788635254
Test Loss Energy: 10.289141081712351, Test Loss Force: 12.727357336320583, time: 12.026651620864868

Epoch 19, Batch 100/204, Loss: 1.049299716949463, Variance: 0.11072325706481934
Epoch 19, Batch 200/204, Loss: 1.343354344367981, Variance: 0.11071884632110596

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.655532176647418, Training Loss Force: 3.391060134040298, time: 3.6403884887695312
Validation Loss Energy: 2.2650812343871367, Validation Loss Force: 3.4092037367033505, time: 0.22565031051635742
Test Loss Energy: 10.060100581102393, Test Loss Force: 12.588662796845268, time: 12.397475481033325

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–„â–†â–‚â–ƒâ–‡â–†â–‡â–„â–†â–…â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–…
wandb:   test_error_force â–‚â–ƒâ–â–ƒâ–ƒâ–…â–…â–…â–‡â–„â–…â–†â–†â–ˆâ–†â–ˆâ–‡â–†â–‡â–†
wandb:          test_loss â–„â–‚â–‚â–‚â–â–‚â–ˆâ–ƒâ–„â–â–‡â–„â–…â–ˆâ–‡â–†â–†â–†â–‡â–‚
wandb: train_error_energy â–ˆâ–â–‚â–‚â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–„â–ˆâ–…â–„â–â–†â–‡â–†â–‚â–‚â–ƒâ–†â–…â–„â–…â–†â–…â–„â–…
wandb:  valid_error_force â–„â–…â–„â–†â–†â–†â–ƒâ–â–ˆâ–…â–„â–„â–„â–†â–ˆâ–„â–„â–‚â–ƒâ–‚
wandb:         valid_loss â–‚â–„â–ˆâ–…â–…â–â–…â–†â–‡â–ƒâ–‚â–ƒâ–†â–…â–…â–…â–†â–„â–„â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 6505
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.0601
wandb:   test_error_force 12.58866
wandb:          test_loss 12.66366
wandb: train_error_energy 2.65553
wandb:  train_error_force 3.39106
wandb:         train_loss 0.96767
wandb: valid_error_energy 2.26508
wandb:  valid_error_force 3.4092
wandb:         valid_loss 0.8288
wandb: 
wandb: ğŸš€ View run al_71_65 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/y9gg2b7i
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_003355-y9gg2b7i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.20587420463562, Uncertainty Bias: -0.1274884045124054
0.00020217896 0.004011631
1.9666508 4.842402
(48745, 22, 3)
Found uncertainty sample 0 after 1865 steps.
Found uncertainty sample 1 after 1625 steps.
Found uncertainty sample 2 after 2712 steps.
Found uncertainty sample 3 after 1663 steps.
Found uncertainty sample 4 after 1163 steps.
Found uncertainty sample 5 after 427 steps.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1007 steps.
Found uncertainty sample 9 after 224 steps.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 3138 steps.
Found uncertainty sample 13 after 784 steps.
Found uncertainty sample 14 after 710 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 350 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 1567 steps.
Found uncertainty sample 19 after 554 steps.
Found uncertainty sample 20 after 1631 steps.
Found uncertainty sample 21 after 1534 steps.
Found uncertainty sample 22 after 463 steps.
Found uncertainty sample 23 after 311 steps.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 2729 steps.
Found uncertainty sample 27 after 491 steps.
Found uncertainty sample 28 after 1017 steps.
Found uncertainty sample 29 after 10 steps.
Found uncertainty sample 30 after 1065 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 15 steps.
Found uncertainty sample 33 after 1768 steps.
Found uncertainty sample 34 after 867 steps.
Found uncertainty sample 35 after 45 steps.
Found uncertainty sample 36 after 148 steps.
Found uncertainty sample 37 after 3632 steps.
Found uncertainty sample 38 after 1702 steps.
Found uncertainty sample 39 after 2554 steps.
Found uncertainty sample 40 after 1001 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 1276 steps.
Found uncertainty sample 43 after 321 steps.
Did not find any uncertainty samples for sample 44.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 28 steps.
Found uncertainty sample 47 after 353 steps.
Found uncertainty sample 48 after 1892 steps.
Found uncertainty sample 49 after 3843 steps.
Found uncertainty sample 50 after 62 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 405 steps.
Found uncertainty sample 53 after 1602 steps.
Found uncertainty sample 54 after 1202 steps.
Found uncertainty sample 55 after 159 steps.
Found uncertainty sample 56 after 466 steps.
Found uncertainty sample 57 after 692 steps.
Found uncertainty sample 58 after 3210 steps.
Found uncertainty sample 59 after 414 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 3640 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 1302 steps.
Found uncertainty sample 67 after 563 steps.
Found uncertainty sample 68 after 3034 steps.
Found uncertainty sample 69 after 636 steps.
Found uncertainty sample 70 after 1287 steps.
Found uncertainty sample 71 after 190 steps.
Found uncertainty sample 72 after 117 steps.
Found uncertainty sample 73 after 577 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 1152 steps.
Found uncertainty sample 76 after 2772 steps.
Found uncertainty sample 77 after 3984 steps.
Found uncertainty sample 78 after 3132 steps.
Found uncertainty sample 79 after 1269 steps.
Found uncertainty sample 80 after 3155 steps.
Found uncertainty sample 81 after 2741 steps.
Did not find any uncertainty samples for sample 82.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 1774 steps.
Found uncertainty sample 85 after 323 steps.
Found uncertainty sample 86 after 218 steps.
Found uncertainty sample 87 after 893 steps.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1246 steps.
Found uncertainty sample 91 after 3554 steps.
Found uncertainty sample 92 after 1780 steps.
Found uncertainty sample 93 after 1947 steps.
Found uncertainty sample 94 after 2043 steps.
Found uncertainty sample 95 after 3193 steps.
Found uncertainty sample 96 after 77 steps.
Found uncertainty sample 97 after 3617 steps.
Found uncertainty sample 98 after 2254 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_010140-nf4hn5em
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_66
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/nf4hn5em
Training model 66. Added 76 samples to the dataset.
Epoch 0, Batch 100/206, Loss: 0.9055362343788147, Variance: 0.10650309920310974
Epoch 0, Batch 200/206, Loss: 1.2372881174087524, Variance: 0.11185796558856964

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.087838847163794, Training Loss Force: 3.6466956860963635, time: 3.412594795227051
Validation Loss Energy: 2.12496756498866, Validation Loss Force: 3.417617123380372, time: 0.1885848045349121
Test Loss Energy: 10.180529563542413, Test Loss Force: 12.388010527878096, time: 11.691060781478882

Epoch 1, Batch 100/206, Loss: 0.46429872512817383, Variance: 0.10952617228031158
Epoch 1, Batch 200/206, Loss: 1.0707848072052002, Variance: 0.11010311543941498

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6058478508506737, Training Loss Force: 3.3676545858277134, time: 3.3742623329162598
Validation Loss Energy: 3.2581786668142607, Validation Loss Force: 3.429371660592041, time: 0.21211862564086914
Test Loss Energy: 10.401623746119107, Test Loss Force: 12.500515748319593, time: 10.893836975097656

Epoch 2, Batch 100/206, Loss: 1.3782485723495483, Variance: 0.11368410289287567
Epoch 2, Batch 200/206, Loss: 0.6082478165626526, Variance: 0.11078695952892303

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6208148866337964, Training Loss Force: 3.374809889127948, time: 3.338923215866089
Validation Loss Energy: 2.56011161800537, Validation Loss Force: 3.397794002664151, time: 0.18921709060668945
Test Loss Energy: 10.437644022194734, Test Loss Force: 12.459832318668647, time: 10.721204042434692

Epoch 3, Batch 100/206, Loss: 0.6631695032119751, Variance: 0.11393212527036667
Epoch 3, Batch 200/206, Loss: 1.280822992324829, Variance: 0.1145501434803009

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6537380817859963, Training Loss Force: 3.385400230435383, time: 3.3416056632995605
Validation Loss Energy: 2.1272026622081968, Validation Loss Force: 3.428612136135281, time: 0.18967938423156738
Test Loss Energy: 10.189140302990987, Test Loss Force: 12.542871775330184, time: 10.944652557373047

Epoch 4, Batch 100/206, Loss: 0.6283639669418335, Variance: 0.11201316118240356
Epoch 4, Batch 200/206, Loss: 0.779821515083313, Variance: 0.11172601580619812

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6480858854774425, Training Loss Force: 3.3961885505555847, time: 3.3325159549713135
Validation Loss Energy: 2.907980220939251, Validation Loss Force: 3.470100575079271, time: 0.1890404224395752
Test Loss Energy: 10.489167484834862, Test Loss Force: 12.644585231857462, time: 10.821419477462769

Epoch 5, Batch 100/206, Loss: 1.1936746835708618, Variance: 0.11789564788341522
Epoch 5, Batch 200/206, Loss: 0.5463005900382996, Variance: 0.11018458008766174

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.65229327762956, Training Loss Force: 3.406067663911644, time: 3.434971570968628
Validation Loss Energy: 2.8602273752455885, Validation Loss Force: 3.4396291374683505, time: 0.19655537605285645
Test Loss Energy: 10.555492406798681, Test Loss Force: 12.472575270520855, time: 10.668050050735474

Epoch 6, Batch 100/206, Loss: 0.9183050394058228, Variance: 0.11155524849891663
Epoch 6, Batch 200/206, Loss: 1.3876570463180542, Variance: 0.11512470245361328

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.632848597989468, Training Loss Force: 3.391609607099844, time: 3.5415854454040527
Validation Loss Energy: 2.1082416607577215, Validation Loss Force: 3.416452229743451, time: 0.203871488571167
Test Loss Energy: 10.159373770319428, Test Loss Force: 12.24595779726653, time: 10.686412572860718

Epoch 7, Batch 100/206, Loss: 0.4650576114654541, Variance: 0.10952825844287872
Epoch 7, Batch 200/206, Loss: 0.8100759983062744, Variance: 0.1146981343626976

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6412642183839177, Training Loss Force: 3.389188286593252, time: 3.375445604324341
Validation Loss Energy: 3.3347548644352596, Validation Loss Force: 3.450057624248732, time: 0.19597506523132324
Test Loss Energy: 10.501193619239853, Test Loss Force: 12.564799645254402, time: 10.668051958084106

Epoch 8, Batch 100/206, Loss: 1.406813621520996, Variance: 0.1140698492527008
Epoch 8, Batch 200/206, Loss: 0.8125895261764526, Variance: 0.1137375682592392

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.661518243891482, Training Loss Force: 3.4024622699558726, time: 3.523013114929199
Validation Loss Energy: 2.728179839991059, Validation Loss Force: 3.5319914085226127, time: 0.19687604904174805
Test Loss Energy: 10.635775737051963, Test Loss Force: 12.707277489737177, time: 10.779099702835083

Epoch 9, Batch 100/206, Loss: 0.9725692272186279, Variance: 0.10964719951152802
Epoch 9, Batch 200/206, Loss: 1.6003756523132324, Variance: 0.11014252156019211

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6410802844815766, Training Loss Force: 3.3982703146665725, time: 3.325404167175293
Validation Loss Energy: 2.611191753079345, Validation Loss Force: 3.408045356138947, time: 0.20070600509643555
Test Loss Energy: 10.384120473005847, Test Loss Force: 12.422830656894615, time: 10.679812908172607

Epoch 10, Batch 100/206, Loss: 0.7583277225494385, Variance: 0.11339682340621948
Epoch 10, Batch 200/206, Loss: 0.7535367608070374, Variance: 0.11064766347408295

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.654473100206146, Training Loss Force: 3.3925920774175866, time: 3.3235926628112793
Validation Loss Energy: 3.291165542099441, Validation Loss Force: 3.413958502310919, time: 0.19371294975280762
Test Loss Energy: 10.647079402177177, Test Loss Force: 12.60365376509925, time: 10.951547384262085

Epoch 11, Batch 100/206, Loss: 1.3221254348754883, Variance: 0.11476770788431168
Epoch 11, Batch 200/206, Loss: 0.7153737545013428, Variance: 0.11165552586317062

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6574212667211836, Training Loss Force: 3.3794370609912274, time: 3.367351531982422
Validation Loss Energy: 2.886037289953797, Validation Loss Force: 3.4336957840713858, time: 0.19959211349487305
Test Loss Energy: 10.33427469064409, Test Loss Force: 12.364897447214654, time: 10.815422058105469

Epoch 12, Batch 100/206, Loss: 1.0203725099563599, Variance: 0.11070813983678818
Epoch 12, Batch 200/206, Loss: 1.3592400550842285, Variance: 0.1131039410829544

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.663977487331645, Training Loss Force: 3.3933444818662672, time: 3.313415050506592
Validation Loss Energy: 2.1356016310580204, Validation Loss Force: 3.395112175470597, time: 0.1933300495147705
Test Loss Energy: 10.271661954794647, Test Loss Force: 12.336640937778455, time: 10.882153034210205

Epoch 13, Batch 100/206, Loss: 0.5753862261772156, Variance: 0.10933682322502136
Epoch 13, Batch 200/206, Loss: 0.6626154184341431, Variance: 0.11201721429824829

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6390212623487233, Training Loss Force: 3.3887973195669754, time: 3.336374521255493
Validation Loss Energy: 3.077307859614307, Validation Loss Force: 3.5926390801242403, time: 0.1907050609588623
Test Loss Energy: 10.27560612252164, Test Loss Force: 12.603471386244859, time: 10.692091464996338

Epoch 14, Batch 100/206, Loss: 1.1814501285552979, Variance: 0.11819981038570404
Epoch 14, Batch 200/206, Loss: 0.715273916721344, Variance: 0.11703117191791534

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6709217046052123, Training Loss Force: 3.390054314999611, time: 3.4802865982055664
Validation Loss Energy: 2.6562491887615516, Validation Loss Force: 3.472277165980715, time: 0.19264912605285645
Test Loss Energy: 10.242493476474836, Test Loss Force: 12.40784432388945, time: 10.934506177902222

Epoch 15, Batch 100/206, Loss: 0.8408455848693848, Variance: 0.11084991693496704
Epoch 15, Batch 200/206, Loss: 1.4560524225234985, Variance: 0.11893060803413391

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6778277514149447, Training Loss Force: 3.3984705266563067, time: 3.343759775161743
Validation Loss Energy: 2.286534014915117, Validation Loss Force: 3.44385033213499, time: 0.1946566104888916
Test Loss Energy: 10.178367159346923, Test Loss Force: 12.435508253603784, time: 10.712666988372803

Epoch 16, Batch 100/206, Loss: 0.6350488066673279, Variance: 0.11393900215625763
Epoch 16, Batch 200/206, Loss: 0.6885320544242859, Variance: 0.11331550031900406

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.676743298628367, Training Loss Force: 3.408962820037097, time: 3.2568414211273193
Validation Loss Energy: 2.899193473771794, Validation Loss Force: 3.4717562315138166, time: 0.1965484619140625
Test Loss Energy: 10.218665406349622, Test Loss Force: 12.43179139959799, time: 10.890648126602173

Epoch 17, Batch 100/206, Loss: 1.314577341079712, Variance: 0.11807294934988022
Epoch 17, Batch 200/206, Loss: 0.7156596183776855, Variance: 0.11508394777774811

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.662734159568989, Training Loss Force: 3.389100049080046, time: 3.382983446121216
Validation Loss Energy: 2.463005770468121, Validation Loss Force: 3.420932444294091, time: 0.19365453720092773
Test Loss Energy: 10.487205674063059, Test Loss Force: 12.38697828501309, time: 10.845889329910278

Epoch 18, Batch 100/206, Loss: 0.5832845568656921, Variance: 0.10905285179615021
Epoch 18, Batch 200/206, Loss: 1.4185956716537476, Variance: 0.1153506487607956

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.641058294822498, Training Loss Force: 3.376066151698054, time: 3.2998125553131104
Validation Loss Energy: 2.2580424956825, Validation Loss Force: 3.397890808075343, time: 0.20811700820922852
Test Loss Energy: 10.369123918204492, Test Loss Force: 12.546261825641409, time: 10.990296125411987

Epoch 19, Batch 100/206, Loss: 0.7512382864952087, Variance: 0.11365289241075516
Epoch 19, Batch 200/206, Loss: 0.5672162175178528, Variance: 0.11235901713371277

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6760206307793903, Training Loss Force: 3.3899926907730165, time: 3.3962249755859375
Validation Loss Energy: 3.249784040012783, Validation Loss Force: 3.427237933501188, time: 0.1887366771697998
Test Loss Energy: 10.205502362922964, Test Loss Force: 12.296049891341124, time: 10.714280843734741

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.060 MB of 0.061 MB uploadedwandb: / 0.060 MB of 0.061 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–„â–…â–â–†â–‡â–â–†â–ˆâ–„â–ˆâ–„â–ƒâ–ƒâ–‚â–â–‚â–†â–„â–‚
wandb:   test_error_force â–ƒâ–…â–„â–†â–‡â–„â–â–†â–ˆâ–„â–†â–ƒâ–‚â–†â–ƒâ–„â–„â–ƒâ–†â–‚
wandb:          test_loss â–„â–†â–ƒâ–ƒâ–ˆâ–‡â–„â–…â–†â–…â–‡â–„â–‚â–…â–ƒâ–â–ƒâ–„â–„â–
wandb: train_error_energy â–ˆâ–â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–
wandb: valid_error_energy â–â–ˆâ–„â–â–†â–…â–â–ˆâ–…â–„â–ˆâ–…â–â–‡â–„â–‚â–†â–ƒâ–‚â–ˆ
wandb:  valid_error_force â–‚â–‚â–â–‚â–„â–ƒâ–‚â–ƒâ–†â–â–‚â–‚â–â–ˆâ–„â–ƒâ–„â–‚â–â–‚
wandb:         valid_loss â–â–ˆâ–ƒâ–â–†â–…â–â–ˆâ–…â–„â–ˆâ–…â–â–‡â–„â–‚â–…â–ƒâ–‚â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 6573
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.2055
wandb:   test_error_force 12.29605
wandb:          test_loss 12.41308
wandb: train_error_energy 2.67602
wandb:  train_error_force 3.38999
wandb:         train_loss 0.96873
wandb: valid_error_energy 3.24978
wandb:  valid_error_force 3.42724
wandb:         valid_loss 1.19846
wandb: 
wandb: ğŸš€ View run al_71_66 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/nf4hn5em
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_010140-nf4hn5em/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.37650728225708, Uncertainty Bias: -0.1546364277601242
1.5258789e-05 0.01402092
1.9136714 4.802659
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 140 steps.
Found uncertainty sample 2 after 982 steps.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 2053 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 809 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 422 steps.
Found uncertainty sample 10 after 421 steps.
Found uncertainty sample 11 after 13 steps.
Found uncertainty sample 12 after 594 steps.
Found uncertainty sample 13 after 1686 steps.
Found uncertainty sample 14 after 927 steps.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 1443 steps.
Found uncertainty sample 18 after 728 steps.
Found uncertainty sample 19 after 392 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 24 steps.
Found uncertainty sample 22 after 2883 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 81 steps.
Found uncertainty sample 25 after 2844 steps.
Found uncertainty sample 26 after 181 steps.
Found uncertainty sample 27 after 340 steps.
Found uncertainty sample 28 after 141 steps.
Found uncertainty sample 29 after 148 steps.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 2817 steps.
Found uncertainty sample 33 after 1874 steps.
Found uncertainty sample 34 after 1745 steps.
Found uncertainty sample 35 after 2762 steps.
Found uncertainty sample 36 after 287 steps.
Found uncertainty sample 37 after 3005 steps.
Found uncertainty sample 38 after 1663 steps.
Found uncertainty sample 39 after 59 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 211 steps.
Found uncertainty sample 42 after 3897 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 58 steps.
Found uncertainty sample 45 after 40 steps.
Found uncertainty sample 46 after 217 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 395 steps.
Found uncertainty sample 49 after 1497 steps.
Found uncertainty sample 50 after 19 steps.
Found uncertainty sample 51 after 1651 steps.
Found uncertainty sample 52 after 1129 steps.
Found uncertainty sample 53 after 2483 steps.
Found uncertainty sample 54 after 402 steps.
Found uncertainty sample 55 after 161 steps.
Found uncertainty sample 56 after 498 steps.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 932 steps.
Found uncertainty sample 60 after 1817 steps.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 1373 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 3786 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 1344 steps.
Found uncertainty sample 68 after 1964 steps.
Found uncertainty sample 69 after 1688 steps.
Found uncertainty sample 70 after 1153 steps.
Found uncertainty sample 71 after 605 steps.
Found uncertainty sample 72 after 435 steps.
Found uncertainty sample 73 after 317 steps.
Found uncertainty sample 74 after 1065 steps.
Found uncertainty sample 75 after 3058 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 3893 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 1508 steps.
Found uncertainty sample 80 after 645 steps.
Found uncertainty sample 81 after 59 steps.
Found uncertainty sample 82 after 15 steps.
Found uncertainty sample 83 after 744 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 470 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 3820 steps.
Found uncertainty sample 88 after 979 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 142 steps.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 2148 steps.
Found uncertainty sample 94 after 652 steps.
Found uncertainty sample 95 after 2594 steps.
Found uncertainty sample 96 after 1069 steps.
Found uncertainty sample 97 after 584 steps.
Found uncertainty sample 98 after 2227 steps.
Found uncertainty sample 99 after 2025 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_012806-sj9bwa89
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_67
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sj9bwa89
Training model 67. Added 74 samples to the dataset.
Epoch 0, Batch 100/208, Loss: 0.7233515977859497, Variance: 0.1052924245595932
Epoch 0, Batch 200/208, Loss: 0.6189752817153931, Variance: 0.1079007238149643

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.8691439775992738, Training Loss Force: 3.561256784397209, time: 3.359302282333374
Validation Loss Energy: 2.660664773592586, Validation Loss Force: 3.400272254591041, time: 0.1906747817993164
Test Loss Energy: 10.371031584233817, Test Loss Force: 12.374978637744196, time: 10.536137819290161

Epoch 1, Batch 100/208, Loss: 0.7470039129257202, Variance: 0.11188480257987976
Epoch 1, Batch 200/208, Loss: 1.255232572555542, Variance: 0.11420686542987823

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6069784824368316, Training Loss Force: 3.3602214543656648, time: 3.4027836322784424
Validation Loss Energy: 3.3901345737152404, Validation Loss Force: 3.4238038742654027, time: 0.19254684448242188
Test Loss Energy: 10.395560406985142, Test Loss Force: 12.292322941446304, time: 11.666937112808228

Epoch 2, Batch 100/208, Loss: 1.3393634557724, Variance: 0.11845168471336365
Epoch 2, Batch 200/208, Loss: 0.7097794413566589, Variance: 0.11432351917028427

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6510033932604355, Training Loss Force: 3.371029783663782, time: 3.4001059532165527
Validation Loss Energy: 2.481830387066716, Validation Loss Force: 3.3790845616812875, time: 0.19018292427062988
Test Loss Energy: 10.200505023762855, Test Loss Force: 12.105336220712, time: 10.516451120376587

Epoch 3, Batch 100/208, Loss: 0.7959542274475098, Variance: 0.11525760591030121
Epoch 3, Batch 200/208, Loss: 1.093891978263855, Variance: 0.11146889626979828

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.602945122576341, Training Loss Force: 3.374885497972598, time: 3.41266131401062
Validation Loss Energy: 2.6739185421637868, Validation Loss Force: 3.426505741682091, time: 0.19844675064086914
Test Loss Energy: 10.451325588616417, Test Loss Force: 12.545652327727685, time: 10.724855422973633

Epoch 4, Batch 100/208, Loss: 0.8676801919937134, Variance: 0.1105877161026001
Epoch 4, Batch 200/208, Loss: 1.4777169227600098, Variance: 0.11498992145061493

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6075680075648724, Training Loss Force: 3.3884967104723986, time: 3.4243597984313965
Validation Loss Energy: 3.3465353426024285, Validation Loss Force: 3.3911735479501393, time: 0.18665409088134766
Test Loss Energy: 10.485640843906815, Test Loss Force: 12.376819125308257, time: 10.581084489822388

Epoch 5, Batch 100/208, Loss: 1.4050613641738892, Variance: 0.1140645369887352
Epoch 5, Batch 200/208, Loss: 0.7704474329948425, Variance: 0.10957153886556625

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6617558186861032, Training Loss Force: 3.3726689529468064, time: 3.471499443054199
Validation Loss Energy: 2.089098961284169, Validation Loss Force: 3.4239050217249405, time: 0.18864178657531738
Test Loss Energy: 10.1413211782522, Test Loss Force: 12.314468863920544, time: 10.544540643692017

Epoch 6, Batch 100/208, Loss: 0.48277759552001953, Variance: 0.11045315861701965
Epoch 6, Batch 200/208, Loss: 0.5970473289489746, Variance: 0.11016333848237991

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6507944193375064, Training Loss Force: 3.377023772652547, time: 3.5787534713745117
Validation Loss Energy: 2.8650197162715325, Validation Loss Force: 3.408908867086855, time: 0.18804526329040527
Test Loss Energy: 10.398093010876712, Test Loss Force: 12.340990228117423, time: 10.656193971633911

Epoch 7, Batch 100/208, Loss: 0.7321221828460693, Variance: 0.11249467730522156
Epoch 7, Batch 200/208, Loss: 1.260105013847351, Variance: 0.11756674945354462

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6309709052034433, Training Loss Force: 3.3914390423677676, time: 3.3256301879882812
Validation Loss Energy: 3.2722589275878584, Validation Loss Force: 3.4667487920929068, time: 0.1862635612487793
Test Loss Energy: 10.534411719528876, Test Loss Force: 12.706858727182759, time: 10.596101999282837

Epoch 8, Batch 100/208, Loss: 1.1978546380996704, Variance: 0.1186479702591896
Epoch 8, Batch 200/208, Loss: 0.7816663384437561, Variance: 0.11415881663560867

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6824751441757937, Training Loss Force: 3.40591869022928, time: 3.43277907371521
Validation Loss Energy: 1.9636266232397415, Validation Loss Force: 3.485591667561467, time: 0.20203447341918945
Test Loss Energy: 10.207682767179255, Test Loss Force: 12.599668234538237, time: 10.759002447128296

Epoch 9, Batch 100/208, Loss: 0.6264714002609253, Variance: 0.11252079904079437
Epoch 9, Batch 200/208, Loss: 0.813147246837616, Variance: 0.11413517594337463

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6601518557502035, Training Loss Force: 3.3942097455957647, time: 3.409177303314209
Validation Loss Energy: 2.595147439104091, Validation Loss Force: 3.4273193190740243, time: 0.21222925186157227
Test Loss Energy: 10.557995548414569, Test Loss Force: 12.576281483002184, time: 10.579142332077026

Epoch 10, Batch 100/208, Loss: 0.7342764139175415, Variance: 0.113521046936512
Epoch 10, Batch 200/208, Loss: 1.4567114114761353, Variance: 0.11779571324586868

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.650603685550362, Training Loss Force: 3.3834282982443944, time: 3.390591621398926
Validation Loss Energy: 3.286543264633699, Validation Loss Force: 3.457573765602238, time: 0.18725323677062988
Test Loss Energy: 10.4997569206045, Test Loss Force: 12.559593635959382, time: 10.766806602478027

Epoch 11, Batch 100/208, Loss: 1.3187482357025146, Variance: 0.11401757597923279
Epoch 11, Batch 200/208, Loss: 0.8286116719245911, Variance: 0.11325070261955261

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.650476926321681, Training Loss Force: 3.384493219857942, time: 3.3796770572662354
Validation Loss Energy: 2.319955373371289, Validation Loss Force: 3.4010097904696397, time: 0.21011900901794434
Test Loss Energy: 10.279239971065707, Test Loss Force: 12.03159384675005, time: 10.719306230545044

Epoch 12, Batch 100/208, Loss: 0.5589226484298706, Variance: 0.1148194968700409
Epoch 12, Batch 200/208, Loss: 0.9483000040054321, Variance: 0.11524444073438644

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6588710130434268, Training Loss Force: 3.386350534249343, time: 3.5203888416290283
Validation Loss Energy: 2.405148477268688, Validation Loss Force: 3.452050513104786, time: 0.21518468856811523
Test Loss Energy: 10.257941394458863, Test Loss Force: 12.280141889937713, time: 10.818763256072998

Epoch 13, Batch 100/208, Loss: 0.666696310043335, Variance: 0.11180871725082397
Epoch 13, Batch 200/208, Loss: 1.390129566192627, Variance: 0.11578364670276642

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6368481138598927, Training Loss Force: 3.3765330376885654, time: 3.4761505126953125
Validation Loss Energy: 3.3068342688928007, Validation Loss Force: 3.500497362859019, time: 0.1908280849456787
Test Loss Energy: 10.46103794577574, Test Loss Force: 12.623415845324311, time: 10.610904693603516

Epoch 14, Batch 100/208, Loss: 1.2848700284957886, Variance: 0.1198260486125946
Epoch 14, Batch 200/208, Loss: 0.7289408445358276, Variance: 0.11521077901124954

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6708810770168694, Training Loss Force: 3.3899317082064786, time: 3.289719820022583
Validation Loss Energy: 2.067847195327675, Validation Loss Force: 3.418751678346161, time: 0.20020246505737305
Test Loss Energy: 10.104038603612185, Test Loss Force: 12.22875420184948, time: 10.761707067489624

Epoch 15, Batch 100/208, Loss: 0.6743940114974976, Variance: 0.11595101654529572
Epoch 15, Batch 200/208, Loss: 0.866186261177063, Variance: 0.11460203677415848

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6614408863654546, Training Loss Force: 3.3770665668790305, time: 3.346092462539673
Validation Loss Energy: 2.879428525369947, Validation Loss Force: 3.4163390024827462, time: 0.19879508018493652
Test Loss Energy: 10.720473512268438, Test Loss Force: 12.382169121100006, time: 10.594706296920776

Epoch 16, Batch 100/208, Loss: 0.8790103793144226, Variance: 0.11424381285905838
Epoch 16, Batch 200/208, Loss: 1.703356385231018, Variance: 0.11816894263029099

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6604251280653313, Training Loss Force: 3.3823079148851147, time: 3.4297378063201904
Validation Loss Energy: 3.4354916664005795, Validation Loss Force: 3.451120145745813, time: 0.19166088104248047
Test Loss Energy: 10.542467273682423, Test Loss Force: 12.609892115187076, time: 10.813424587249756

Epoch 17, Batch 100/208, Loss: 1.45430326461792, Variance: 0.11785482615232468
Epoch 17, Batch 200/208, Loss: 0.5925216674804688, Variance: 0.11478577554225922

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.653060629217563, Training Loss Force: 3.3813346676576796, time: 3.381295680999756
Validation Loss Energy: 2.4423495888096975, Validation Loss Force: 3.4736956241075614, time: 0.1948716640472412
Test Loss Energy: 10.443780191626523, Test Loss Force: 12.365716734438047, time: 10.714476585388184

Epoch 18, Batch 100/208, Loss: 0.6066975593566895, Variance: 0.11297391355037689
Epoch 18, Batch 200/208, Loss: 0.8010045886039734, Variance: 0.11716645210981369

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.677939952291866, Training Loss Force: 3.386050747712718, time: 3.3034512996673584
Validation Loss Energy: 2.834469843453959, Validation Loss Force: 3.4166969431348253, time: 0.19058465957641602
Test Loss Energy: 10.397044064328336, Test Loss Force: 12.308709133584149, time: 11.788718223571777

Epoch 19, Batch 100/208, Loss: 0.691694974899292, Variance: 0.11549989879131317
Epoch 19, Batch 200/208, Loss: 1.3561069965362549, Variance: 0.11638029664754868

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.646140473073643, Training Loss Force: 3.3808714569552887, time: 3.377122163772583
Validation Loss Energy: 3.5088286402884803, Validation Loss Force: 3.454232281063873, time: 0.18830108642578125
Test Loss Energy: 10.611296661618937, Test Loss Force: 12.42988895172101, time: 10.670520782470703

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–„â–‚â–…â–…â–â–„â–†â–‚â–†â–…â–ƒâ–ƒâ–…â–â–ˆâ–†â–…â–„â–‡
wandb:   test_error_force â–…â–„â–‚â–†â–…â–„â–„â–ˆâ–‡â–‡â–†â–â–„â–‡â–ƒâ–…â–‡â–„â–„â–…
wandb:          test_loss â–„â–„â–â–…â–‡â–ƒâ–„â–ˆâ–‚â–…â–†â–â–â–‡â–â–†â–†â–„â–‚â–ˆ
wandb: train_error_energy â–ˆâ–â–‚â–â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚
wandb:  train_error_force â–ˆâ–â–â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–
wandb: valid_error_energy â–„â–‡â–ƒâ–„â–‡â–‚â–…â–‡â–â–„â–‡â–ƒâ–ƒâ–‡â–â–…â–ˆâ–ƒâ–…â–ˆ
wandb:  valid_error_force â–‚â–„â–â–„â–‚â–„â–ƒâ–†â–‡â–„â–†â–‚â–…â–ˆâ–ƒâ–ƒâ–…â–†â–ƒâ–…
wandb:         valid_loss â–ƒâ–‡â–‚â–„â–‡â–â–„â–‡â–â–ƒâ–‡â–‚â–ƒâ–‡â–â–„â–‡â–ƒâ–„â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6639
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.6113
wandb:   test_error_force 12.42989
wandb:          test_loss 13.56703
wandb: train_error_energy 2.64614
wandb:  train_error_force 3.38087
wandb:         train_loss 0.95674
wandb: valid_error_energy 3.50883
wandb:  valid_error_force 3.45423
wandb:         valid_loss 1.34423
wandb: 
wandb: ğŸš€ View run al_71_67 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/sj9bwa89
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_012806-sj9bwa89/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2139148712158203, Uncertainty Bias: -0.12260082364082336
3.8146973e-06 0.1466167
2.0410223 4.814745
(48745, 22, 3)
Found uncertainty sample 0 after 1036 steps.
Found uncertainty sample 1 after 2488 steps.
Found uncertainty sample 2 after 1667 steps.
Found uncertainty sample 3 after 94 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 151 steps.
Found uncertainty sample 6 after 525 steps.
Found uncertainty sample 7 after 1266 steps.
Found uncertainty sample 8 after 2527 steps.
Found uncertainty sample 9 after 2941 steps.
Found uncertainty sample 10 after 329 steps.
Found uncertainty sample 11 after 2244 steps.
Found uncertainty sample 12 after 1745 steps.
Found uncertainty sample 13 after 1322 steps.
Found uncertainty sample 14 after 148 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 3428 steps.
Found uncertainty sample 17 after 50 steps.
Found uncertainty sample 18 after 2704 steps.
Did not find any uncertainty samples for sample 19.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 1426 steps.
Found uncertainty sample 22 after 111 steps.
Found uncertainty sample 23 after 979 steps.
Found uncertainty sample 24 after 1460 steps.
Found uncertainty sample 25 after 30 steps.
Found uncertainty sample 26 after 2788 steps.
Found uncertainty sample 27 after 3697 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1649 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 73 steps.
Found uncertainty sample 32 after 1156 steps.
Found uncertainty sample 33 after 1023 steps.
Found uncertainty sample 34 after 129 steps.
Found uncertainty sample 35 after 1949 steps.
Found uncertainty sample 36 after 214 steps.
Found uncertainty sample 37 after 2592 steps.
Found uncertainty sample 38 after 808 steps.
Found uncertainty sample 39 after 806 steps.
Found uncertainty sample 40 after 989 steps.
Found uncertainty sample 41 after 52 steps.
Found uncertainty sample 42 after 1149 steps.
Found uncertainty sample 43 after 476 steps.
Found uncertainty sample 44 after 134 steps.
Found uncertainty sample 45 after 2436 steps.
Found uncertainty sample 46 after 1288 steps.
Found uncertainty sample 47 after 1929 steps.
Found uncertainty sample 48 after 32 steps.
Found uncertainty sample 49 after 70 steps.
Found uncertainty sample 50 after 45 steps.
Found uncertainty sample 51 after 419 steps.
Found uncertainty sample 52 after 2626 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 1660 steps.
Found uncertainty sample 55 after 337 steps.
Found uncertainty sample 56 after 709 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 2030 steps.
Found uncertainty sample 59 after 189 steps.
Found uncertainty sample 60 after 28 steps.
Found uncertainty sample 61 after 2752 steps.
Found uncertainty sample 62 after 3896 steps.
Found uncertainty sample 63 after 224 steps.
Found uncertainty sample 64 after 1868 steps.
Found uncertainty sample 65 after 2851 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 2116 steps.
Found uncertainty sample 68 after 845 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 2176 steps.
Found uncertainty sample 71 after 463 steps.
Found uncertainty sample 72 after 4 steps.
Found uncertainty sample 73 after 226 steps.
Found uncertainty sample 74 after 4 steps.
Found uncertainty sample 75 after 450 steps.
Found uncertainty sample 76 after 2569 steps.
Found uncertainty sample 77 after 3457 steps.
Found uncertainty sample 78 after 149 steps.
Found uncertainty sample 79 after 69 steps.
Found uncertainty sample 80 after 2861 steps.
Found uncertainty sample 81 after 29 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 401 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 3947 steps.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 1583 steps.
Found uncertainty sample 89 after 1446 steps.
Found uncertainty sample 90 after 1051 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 193 steps.
Found uncertainty sample 93 after 425 steps.
Found uncertainty sample 94 after 1878 steps.
Found uncertainty sample 95 after 365 steps.
Found uncertainty sample 96 after 794 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 1262 steps.
Found uncertainty sample 99 after 3991 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_015200-wt0a94s7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_68
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/wt0a94s7
Training model 68. Added 85 samples to the dataset.
Epoch 0, Batch 100/210, Loss: 2.5781896114349365, Variance: 0.09383731335401535
Epoch 0, Batch 200/210, Loss: 0.728705108165741, Variance: 0.10677459836006165

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.949326435002843, Training Loss Force: 3.6704677169378694, time: 3.4005138874053955
Validation Loss Energy: 3.2327947181822116, Validation Loss Force: 3.4077884516118333, time: 0.19330286979675293
Test Loss Energy: 10.526469934153408, Test Loss Force: 12.579724064578931, time: 11.60852575302124

Epoch 1, Batch 100/210, Loss: 1.303483009338379, Variance: 0.11379113793373108
Epoch 1, Batch 200/210, Loss: 0.8300243616104126, Variance: 0.11137673258781433

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.625268724711559, Training Loss Force: 3.343952844933885, time: 3.3559741973876953
Validation Loss Energy: 3.554317831444641, Validation Loss Force: 3.406495467548717, time: 0.1953730583190918
Test Loss Energy: 10.768811076064141, Test Loss Force: 12.781433039706176, time: 10.915820837020874

Epoch 2, Batch 100/210, Loss: 1.332448124885559, Variance: 0.11475898325443268
Epoch 2, Batch 200/210, Loss: 0.7737406492233276, Variance: 0.11432656645774841

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.646348246010549, Training Loss Force: 3.3722895921448135, time: 3.3711628913879395
Validation Loss Energy: 3.1765960317835082, Validation Loss Force: 3.4123290095010197, time: 0.1959395408630371
Test Loss Energy: 10.371265468443534, Test Loss Force: 12.464020956133341, time: 10.661508321762085

Epoch 3, Batch 100/210, Loss: 1.3161804676055908, Variance: 0.11913672089576721
Epoch 3, Batch 200/210, Loss: 0.7256883382797241, Variance: 0.11442314088344574

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6510026578150363, Training Loss Force: 3.371665348737169, time: 3.3954594135284424
Validation Loss Energy: 2.929925879621742, Validation Loss Force: 3.432575535598223, time: 0.18823599815368652
Test Loss Energy: 10.30217825528379, Test Loss Force: 12.3192618604215, time: 10.837086200714111

Epoch 4, Batch 100/210, Loss: 1.1976735591888428, Variance: 0.1173509731888771
Epoch 4, Batch 200/210, Loss: 0.3846755027770996, Variance: 0.11327847093343735

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6272552953305963, Training Loss Force: 3.3811184640733623, time: 3.439049243927002
Validation Loss Energy: 3.2667574409994207, Validation Loss Force: 3.413912743801431, time: 0.19976305961608887
Test Loss Energy: 10.441748637613957, Test Loss Force: 12.54405457984367, time: 10.685052394866943

Epoch 5, Batch 100/210, Loss: 1.269711971282959, Variance: 0.11630196869373322
Epoch 5, Batch 200/210, Loss: 0.6333191394805908, Variance: 0.11217153072357178

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6346350277097312, Training Loss Force: 3.3754073238807414, time: 3.382413387298584
Validation Loss Energy: 3.293374698363477, Validation Loss Force: 3.4019859720053875, time: 0.192854642868042
Test Loss Energy: 10.442469283571517, Test Loss Force: 12.325642728089175, time: 10.633634805679321

Epoch 6, Batch 100/210, Loss: 1.2253785133361816, Variance: 0.11898384988307953
Epoch 6, Batch 200/210, Loss: 0.4847690463066101, Variance: 0.1146114245057106

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6463661247524732, Training Loss Force: 3.3747178596199245, time: 3.5979835987091064
Validation Loss Energy: 3.1571535083049636, Validation Loss Force: 3.557714927602202, time: 0.19699645042419434
Test Loss Energy: 10.781677780665975, Test Loss Force: 12.826461175742782, time: 10.629881381988525

Epoch 7, Batch 100/210, Loss: 1.3642022609710693, Variance: 0.11832419037818909
Epoch 7, Batch 200/210, Loss: 0.8297502994537354, Variance: 0.11323966085910797

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6652745577287478, Training Loss Force: 3.377425761370467, time: 3.3701233863830566
Validation Loss Energy: 3.2588235166608674, Validation Loss Force: 3.4749560984753725, time: 0.19265079498291016
Test Loss Energy: 10.748487132801285, Test Loss Force: 12.616657368090971, time: 10.605918407440186

Epoch 8, Batch 100/210, Loss: 1.1851149797439575, Variance: 0.11577053368091583
Epoch 8, Batch 200/210, Loss: 0.7314029335975647, Variance: 0.11312153935432434

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6354127739249327, Training Loss Force: 3.382838057302339, time: 3.5923871994018555
Validation Loss Energy: 3.84225230605502, Validation Loss Force: 3.406860088275174, time: 0.19773554801940918
Test Loss Energy: 10.6630927738847, Test Loss Force: 12.249524956984295, time: 10.718499422073364

Epoch 9, Batch 100/210, Loss: 1.3644359111785889, Variance: 0.11483006179332733
Epoch 9, Batch 200/210, Loss: 0.5982286334037781, Variance: 0.11373516917228699

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6417363035741666, Training Loss Force: 3.3888462646438593, time: 3.3492555618286133
Validation Loss Energy: 3.12314007258977, Validation Loss Force: 3.40556548986075, time: 0.18900728225708008
Test Loss Energy: 10.220509032266257, Test Loss Force: 12.156618016214566, time: 10.673973798751831

Epoch 10, Batch 100/210, Loss: 1.3987082242965698, Variance: 0.11961138248443604
Epoch 10, Batch 200/210, Loss: 0.7121309638023376, Variance: 0.11406662315130234

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6450991966905835, Training Loss Force: 3.380740842649771, time: 3.3849916458129883
Validation Loss Energy: 3.241092984910789, Validation Loss Force: 3.498282695533621, time: 0.1965935230255127
Test Loss Energy: 10.488441658030787, Test Loss Force: 12.37161171704478, time: 11.004705667495728

Epoch 11, Batch 100/210, Loss: 1.3897502422332764, Variance: 0.11491608619689941
Epoch 11, Batch 200/210, Loss: 0.8341951966285706, Variance: 0.11401256173849106

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6525583898798533, Training Loss Force: 3.382699049195853, time: 3.3557355403900146
Validation Loss Energy: 3.465821430767403, Validation Loss Force: 3.3991668692833685, time: 0.19476747512817383
Test Loss Energy: 10.461647448352688, Test Loss Force: 12.212246695830352, time: 10.547869205474854

Epoch 12, Batch 100/210, Loss: 1.1827387809753418, Variance: 0.1203666478395462
Epoch 12, Batch 200/210, Loss: 0.7302663326263428, Variance: 0.11525867134332657

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6465935510656546, Training Loss Force: 3.365352348687714, time: 3.3959429264068604
Validation Loss Energy: 3.3196187065716716, Validation Loss Force: 3.3826034021943845, time: 0.19092178344726562
Test Loss Energy: 10.329610679034786, Test Loss Force: 12.237823798288424, time: 10.775770664215088

Epoch 13, Batch 100/210, Loss: 1.192285180091858, Variance: 0.11904542148113251
Epoch 13, Batch 200/210, Loss: 0.689893364906311, Variance: 0.11505377292633057

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6510573405857945, Training Loss Force: 3.39072843815924, time: 3.4228336811065674
Validation Loss Energy: 3.163449144402353, Validation Loss Force: 3.455820134743484, time: 0.19371938705444336
Test Loss Energy: 10.32215222327121, Test Loss Force: 12.284001340798234, time: 10.64763855934143

Epoch 14, Batch 100/210, Loss: 1.328438639640808, Variance: 0.11657994240522385
Epoch 14, Batch 200/210, Loss: 0.6064987778663635, Variance: 0.11331090331077576

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.630303414594613, Training Loss Force: 3.390446370428143, time: 3.6495938301086426
Validation Loss Energy: 3.5123360690078482, Validation Loss Force: 3.428011987119719, time: 0.22057580947875977
Test Loss Energy: 10.648588725928313, Test Loss Force: 12.57206620917808, time: 12.709094524383545

Epoch 15, Batch 100/210, Loss: 1.2175695896148682, Variance: 0.1197318434715271
Epoch 15, Batch 200/210, Loss: 0.8136845827102661, Variance: 0.11681961268186569

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6555314640749645, Training Loss Force: 3.3687520028896296, time: 3.8158960342407227
Validation Loss Energy: 3.4040707675514232, Validation Loss Force: 3.3869802425940274, time: 0.2278575897216797
Test Loss Energy: 10.503396481603097, Test Loss Force: 12.45136994448807, time: 12.359872817993164

Epoch 16, Batch 100/210, Loss: 1.3238130807876587, Variance: 0.11701464653015137
Epoch 16, Batch 200/210, Loss: 0.6738937497138977, Variance: 0.11395427584648132

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.670545321440316, Training Loss Force: 3.3692722226946024, time: 3.7336204051971436
Validation Loss Energy: 3.097261074779037, Validation Loss Force: 3.455884040788365, time: 0.2265031337738037
Test Loss Energy: 10.242916892930987, Test Loss Force: 12.114202010763034, time: 13.604804039001465

Epoch 17, Batch 100/210, Loss: 1.3932167291641235, Variance: 0.11760102212429047
Epoch 17, Batch 200/210, Loss: 0.6854935884475708, Variance: 0.11466269195079803

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6416675284972944, Training Loss Force: 3.3846769460564095, time: 3.8105247020721436
Validation Loss Energy: 3.2212498239714344, Validation Loss Force: 3.395733867726027, time: 0.2214353084564209
Test Loss Energy: 10.665446617586953, Test Loss Force: 12.53904466395714, time: 12.505516529083252

Epoch 18, Batch 100/210, Loss: 1.316375732421875, Variance: 0.11742640286684036
Epoch 18, Batch 200/210, Loss: 0.7603319883346558, Variance: 0.1167270690202713

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.624960407469144, Training Loss Force: 3.386682362702745, time: 3.9466702938079834
Validation Loss Energy: 3.241473415146607, Validation Loss Force: 3.433705470179233, time: 0.21871232986450195
Test Loss Energy: 10.493281935002333, Test Loss Force: 12.349849545607729, time: 12.481332778930664

Epoch 19, Batch 100/210, Loss: 1.1758596897125244, Variance: 0.11866603791713715
Epoch 19, Batch 200/210, Loss: 0.572068452835083, Variance: 0.11755256354808807

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6818961335123497, Training Loss Force: 3.3750708962491465, time: 3.784872055053711
Validation Loss Energy: 3.324753624078626, Validation Loss Force: 3.3631307627138534, time: 0.22461390495300293
Test Loss Energy: 10.620712372180376, Test Loss Force: 12.535229355214632, time: 12.53934121131897

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–ˆâ–ƒâ–‚â–„â–„â–ˆâ–ˆâ–‡â–â–„â–„â–‚â–‚â–†â–…â–â–‡â–„â–†
wandb:   test_error_force â–†â–ˆâ–„â–ƒâ–…â–ƒâ–ˆâ–†â–‚â–â–„â–‚â–‚â–ƒâ–…â–„â–â–…â–ƒâ–…
wandb:          test_loss â–ˆâ–‡â–‚â–‚â–†â–„â–‡â–…â–…â–â–ƒâ–‚â–â–‚â–…â–ƒâ–â–…â–‚â–ƒ
wandb: train_error_energy â–ˆâ–â–â–‚â–â–â–â–‚â–â–â–â–‚â–â–‚â–â–‚â–‚â–â–â–‚
wandb:  train_error_force â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–†â–ƒâ–â–„â–„â–ƒâ–„â–ˆâ–‚â–ƒâ–…â–„â–ƒâ–…â–…â–‚â–ƒâ–ƒâ–„
wandb:  valid_error_force â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ˆâ–…â–ƒâ–ƒâ–†â–‚â–‚â–„â–ƒâ–‚â–„â–‚â–„â–
wandb:         valid_loss â–„â–†â–‚â–â–„â–ƒâ–ƒâ–„â–ˆâ–‚â–„â–…â–ƒâ–ƒâ–…â–„â–‚â–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6715
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.62071
wandb:   test_error_force 12.53523
wandb:          test_loss 12.96105
wandb: train_error_energy 2.6819
wandb:  train_error_force 3.37507
wandb:         train_loss 0.96747
wandb: valid_error_energy 3.32475
wandb:  valid_error_force 3.36313
wandb:         valid_loss 1.21575
wandb: 
wandb: ğŸš€ View run al_71_68 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/wt0a94s7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_015200-wt0a94s7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.547670602798462, Uncertainty Bias: -0.17535902559757233
0.0001373291 0.005874634
1.8366404 4.829309
(48745, 22, 3)
Found uncertainty sample 0 after 1551 steps.
Found uncertainty sample 1 after 1206 steps.
Found uncertainty sample 2 after 2768 steps.
Found uncertainty sample 3 after 393 steps.
Found uncertainty sample 4 after 282 steps.
Found uncertainty sample 5 after 621 steps.
Found uncertainty sample 6 after 2894 steps.
Found uncertainty sample 7 after 1267 steps.
Found uncertainty sample 8 after 489 steps.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 1720 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 1785 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 267 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 978 steps.
Found uncertainty sample 18 after 1833 steps.
Found uncertainty sample 19 after 1804 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 3189 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 2013 steps.
Found uncertainty sample 24 after 373 steps.
Found uncertainty sample 25 after 280 steps.
Found uncertainty sample 26 after 6 steps.
Found uncertainty sample 27 after 1822 steps.
Found uncertainty sample 28 after 2278 steps.
Found uncertainty sample 29 after 485 steps.
Found uncertainty sample 30 after 3135 steps.
Found uncertainty sample 31 after 331 steps.
Found uncertainty sample 32 after 411 steps.
Found uncertainty sample 33 after 227 steps.
Found uncertainty sample 34 after 98 steps.
Found uncertainty sample 35 after 1319 steps.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 95 steps.
Found uncertainty sample 39 after 823 steps.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 383 steps.
Found uncertainty sample 43 after 1737 steps.
Found uncertainty sample 44 after 3363 steps.
Found uncertainty sample 45 after 1084 steps.
Found uncertainty sample 46 after 18 steps.
Found uncertainty sample 47 after 51 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 508 steps.
Found uncertainty sample 50 after 1104 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1488 steps.
Found uncertainty sample 53 after 1212 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 681 steps.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 19 steps.
Found uncertainty sample 58 after 2061 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 191 steps.
Found uncertainty sample 61 after 1570 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 974 steps.
Found uncertainty sample 64 after 37 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 84 steps.
Found uncertainty sample 67 after 1579 steps.
Found uncertainty sample 68 after 1690 steps.
Found uncertainty sample 69 after 11 steps.
Found uncertainty sample 70 after 295 steps.
Found uncertainty sample 71 after 41 steps.
Found uncertainty sample 72 after 507 steps.
Found uncertainty sample 73 after 3000 steps.
Found uncertainty sample 74 after 394 steps.
Found uncertainty sample 75 after 821 steps.
Found uncertainty sample 76 after 3194 steps.
Found uncertainty sample 77 after 265 steps.
Found uncertainty sample 78 after 614 steps.
Found uncertainty sample 79 after 3279 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 3508 steps.
Found uncertainty sample 82 after 1465 steps.
Did not find any uncertainty samples for sample 83.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 1345 steps.
Found uncertainty sample 86 after 1299 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 294 steps.
Found uncertainty sample 89 after 725 steps.
Found uncertainty sample 90 after 982 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 305 steps.
Found uncertainty sample 93 after 1172 steps.
Found uncertainty sample 94 after 1753 steps.
Found uncertainty sample 95 after 1909 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 854 steps.
Found uncertainty sample 98 after 2150 steps.
Found uncertainty sample 99 after 493 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_021729-4lfg0edo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_69
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/4lfg0edo
Training model 69. Added 77 samples to the dataset.
Epoch 0, Batch 100/212, Loss: 2.6554160118103027, Variance: 0.09468502551317215
Epoch 0, Batch 200/212, Loss: 0.6578170657157898, Variance: 0.088004931807518

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.4208008557353673, Training Loss Force: 3.576636779195388, time: 3.9197022914886475
Validation Loss Energy: 1.4536255175768118, Validation Loss Force: 3.440730013725817, time: 0.22380661964416504
Test Loss Energy: 10.380480939986622, Test Loss Force: 13.210171367717365, time: 12.428758144378662

Epoch 1, Batch 100/212, Loss: 0.28872597217559814, Variance: 0.08566123247146606
Epoch 1, Batch 200/212, Loss: 0.4654909372329712, Variance: 0.08339276909828186

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6413142749444634, Training Loss Force: 3.3653907610659917, time: 3.8852548599243164
Validation Loss Energy: 1.6475329630970426, Validation Loss Force: 3.3687509319570705, time: 0.22814202308654785
Test Loss Energy: 10.227401110987152, Test Loss Force: 13.211710522139212, time: 12.544811725616455

Epoch 2, Batch 100/212, Loss: 0.4196356534957886, Variance: 0.08089429140090942
Epoch 2, Batch 200/212, Loss: 0.7483481168746948, Variance: 0.08219342678785324

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.62988034874282, Training Loss Force: 3.3656459315046163, time: 3.7734744548797607
Validation Loss Energy: 1.3531683323914905, Validation Loss Force: 3.3863938990655127, time: 0.22838950157165527
Test Loss Energy: 10.030466731262438, Test Loss Force: 13.001791336895598, time: 12.451225519180298

Epoch 3, Batch 100/212, Loss: 0.4073562026023865, Variance: 0.08281062543392181
Epoch 3, Batch 200/212, Loss: 0.4763156771659851, Variance: 0.08182448893785477

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6448141113811838, Training Loss Force: 3.377773529863764, time: 3.779200315475464
Validation Loss Energy: 1.5432184246295275, Validation Loss Force: 3.4944394260749077, time: 0.22585511207580566
Test Loss Energy: 9.979320453834212, Test Loss Force: 12.970063604335127, time: 12.698753118515015

Epoch 4, Batch 100/212, Loss: 0.4899611473083496, Variance: 0.0794089064002037
Epoch 4, Batch 200/212, Loss: 0.5792893171310425, Variance: 0.08192694187164307

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6306843082981064, Training Loss Force: 3.371969946364523, time: 3.6912577152252197
Validation Loss Energy: 1.4516842601289701, Validation Loss Force: 3.4260750438865797, time: 0.1982283592224121
Test Loss Energy: 9.992563476287508, Test Loss Force: 12.69519882872989, time: 12.384852170944214

Epoch 5, Batch 100/212, Loss: 0.2983657121658325, Variance: 0.08291684091091156
Epoch 5, Batch 200/212, Loss: 0.41103988885879517, Variance: 0.07956822216510773

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6381910198233474, Training Loss Force: 3.399117860014415, time: 3.927504539489746
Validation Loss Energy: 1.3712791830428135, Validation Loss Force: 3.407310436622813, time: 0.2247905731201172
Test Loss Energy: 10.155345305416498, Test Loss Force: 12.858305283825885, time: 12.666126251220703

Epoch 6, Batch 100/212, Loss: 0.2882264256477356, Variance: 0.07860520482063293
Epoch 6, Batch 200/212, Loss: 0.3723095655441284, Variance: 0.07802695035934448

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6386003518841108, Training Loss Force: 3.3826902904259737, time: 4.064268350601196
Validation Loss Energy: 1.4474453910988725, Validation Loss Force: 3.3881285473795772, time: 0.23548650741577148
Test Loss Energy: 10.096436235963536, Test Loss Force: 12.855528364083142, time: 12.377978086471558

Epoch 7, Batch 100/212, Loss: 0.7236053943634033, Variance: 0.07988524436950684
Epoch 7, Batch 200/212, Loss: 0.5614392757415771, Variance: 0.07780532538890839

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.629399869163465, Training Loss Force: 3.388673029108657, time: 4.05873966217041
Validation Loss Energy: 1.5659392224426367, Validation Loss Force: 3.410048487775714, time: 0.22252583503723145
Test Loss Energy: 10.168173510406419, Test Loss Force: 13.03621501690457, time: 12.518953800201416

Epoch 8, Batch 100/212, Loss: 0.8314206600189209, Variance: 0.07724565267562866
Epoch 8, Batch 200/212, Loss: 0.47306621074676514, Variance: 0.07745614647865295

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6354185447557146, Training Loss Force: 3.401145495760496, time: 3.8496639728546143
Validation Loss Energy: 1.4611389869005666, Validation Loss Force: 3.425872581965645, time: 0.2157115936279297
Test Loss Energy: 9.912430106126264, Test Loss Force: 12.784785187298697, time: 11.185819387435913

Epoch 9, Batch 100/212, Loss: 0.08161085844039917, Variance: 0.07717233896255493
Epoch 9, Batch 200/212, Loss: 1.1196424961090088, Variance: 0.0785222202539444

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6380236001966715, Training Loss Force: 3.393857202711212, time: 3.960216522216797
Validation Loss Energy: 1.7784337244164001, Validation Loss Force: 3.4345189362365938, time: 0.22426414489746094
Test Loss Energy: 10.162092894012023, Test Loss Force: 12.88287384194459, time: 12.09149432182312

Epoch 10, Batch 100/212, Loss: 0.6704907417297363, Variance: 0.07996122539043427
Epoch 10, Batch 200/212, Loss: 0.49913841485977173, Variance: 0.07604146003723145

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.633613624369913, Training Loss Force: 3.3823187102383803, time: 3.4328370094299316
Validation Loss Energy: 1.5292397566537592, Validation Loss Force: 3.404403713591871, time: 0.17833828926086426
Test Loss Energy: 10.000215763793902, Test Loss Force: 12.785086944794674, time: 10.024991989135742

Epoch 11, Batch 100/212, Loss: 0.27274268865585327, Variance: 0.07745978981256485
Epoch 11, Batch 200/212, Loss: 0.5785065293312073, Variance: 0.07734005153179169

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.633767728749277, Training Loss Force: 3.391355947591092, time: 3.6155600547790527
Validation Loss Energy: 1.5031664800132043, Validation Loss Force: 3.5080230789858002, time: 0.18592596054077148
Test Loss Energy: 10.085620447097696, Test Loss Force: 12.750552396535559, time: 9.967491388320923

Epoch 12, Batch 100/212, Loss: 0.4286954998970032, Variance: 0.07669268548488617
Epoch 12, Batch 200/212, Loss: 0.6752108335494995, Variance: 0.07371962070465088

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.625510269292669, Training Loss Force: 3.381251921266111, time: 3.4199306964874268
Validation Loss Energy: 1.4673987732654616, Validation Loss Force: 3.4714738870367805, time: 0.1843411922454834
Test Loss Energy: 10.137465509269736, Test Loss Force: 12.828915586624115, time: 9.939183235168457

Epoch 13, Batch 100/212, Loss: 0.5554483532905579, Variance: 0.07700474560260773
Epoch 13, Batch 200/212, Loss: 0.5265213251113892, Variance: 0.08061791956424713

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6384643782349217, Training Loss Force: 3.3912891689041627, time: 3.5574262142181396
Validation Loss Energy: 1.4129214028657096, Validation Loss Force: 3.4409862502454933, time: 0.1828322410583496
Test Loss Energy: 10.419799783162901, Test Loss Force: 13.364496814913997, time: 10.198442697525024

Epoch 14, Batch 100/212, Loss: 0.6617239713668823, Variance: 0.07435469329357147
Epoch 14, Batch 200/212, Loss: 0.22950106859207153, Variance: 0.07494338601827621

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.634700246651062, Training Loss Force: 3.3864985651176864, time: 3.486849546432495
Validation Loss Energy: 1.4147583954859706, Validation Loss Force: 3.4380569819635274, time: 0.1975247859954834
Test Loss Energy: 9.834950476535203, Test Loss Force: 12.754290995174387, time: 11.094401597976685

Epoch 15, Batch 100/212, Loss: 0.4198542833328247, Variance: 0.07698247581720352
Epoch 15, Batch 200/212, Loss: 0.5030076503753662, Variance: 0.07778853178024292

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6458060315292764, Training Loss Force: 3.3963233391258463, time: 3.580472230911255
Validation Loss Energy: 1.5317347644338801, Validation Loss Force: 3.4225786929572215, time: 0.18941354751586914
Test Loss Energy: 10.117926046895095, Test Loss Force: 12.729154399903768, time: 10.210931301116943

Epoch 16, Batch 100/212, Loss: 0.3646273612976074, Variance: 0.07719767093658447
Epoch 16, Batch 200/212, Loss: 0.40810537338256836, Variance: 0.07789811491966248

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6335983100893365, Training Loss Force: 3.390579968724708, time: 3.442415475845337
Validation Loss Energy: 1.3576273377059809, Validation Loss Force: 3.4098965160836183, time: 0.18460679054260254
Test Loss Energy: 10.183794466798783, Test Loss Force: 12.916875746953732, time: 10.15784239768982

Epoch 17, Batch 100/212, Loss: 1.008504867553711, Variance: 0.07924621552228928
Epoch 17, Batch 200/212, Loss: 0.5400325059890747, Variance: 0.07826503366231918

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6553687200139553, Training Loss Force: 3.3869615930027295, time: 3.4495368003845215
Validation Loss Energy: 1.4681220304850782, Validation Loss Force: 3.3915024302786745, time: 0.18087291717529297
Test Loss Energy: 10.208979247206255, Test Loss Force: 13.133688671027484, time: 10.098119974136353

Epoch 18, Batch 100/212, Loss: 0.523961067199707, Variance: 0.07882510125637054
Epoch 18, Batch 200/212, Loss: 0.4898611903190613, Variance: 0.07800278067588806

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6450534728356097, Training Loss Force: 3.377994640325669, time: 3.419287919998169
Validation Loss Energy: 1.416132403284561, Validation Loss Force: 3.3896378545179617, time: 0.19107484817504883
Test Loss Energy: 10.084187175935064, Test Loss Force: 12.742399623400765, time: 11.165906429290771

Epoch 19, Batch 100/212, Loss: 0.7043275833129883, Variance: 0.07368561625480652
Epoch 19, Batch 200/212, Loss: 0.7867250442504883, Variance: 0.07775136828422546

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6430151353162852, Training Loss Force: 3.3838413775255205, time: 3.8920938968658447
Validation Loss Energy: 1.3017129163056973, Validation Loss Force: 3.463897068341009, time: 0.2331087589263916
Test Loss Energy: 9.917164705020634, Test Loss Force: 12.956690333255343, time: 12.639901161193848

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–†â–ƒâ–ƒâ–ƒâ–…â–„â–…â–‚â–…â–ƒâ–„â–…â–ˆâ–â–„â–…â–…â–„â–‚
wandb:   test_error_force â–†â–†â–„â–„â–â–ƒâ–ƒâ–…â–‚â–ƒâ–‚â–‚â–‚â–ˆâ–‚â–â–ƒâ–†â–â–„
wandb:          test_loss â–â–ƒâ–ƒâ–ƒâ–ƒâ–†â–„â–†â–ƒâ–…â–„â–…â–†â–ˆâ–‚â–†â–‡â–…â–…â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–†â–‚â–…â–ƒâ–‚â–ƒâ–…â–ƒâ–ˆâ–„â–„â–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–ƒâ–
wandb:  valid_error_force â–…â–â–‚â–‡â–„â–ƒâ–‚â–ƒâ–„â–„â–ƒâ–ˆâ–†â–…â–„â–„â–ƒâ–‚â–‚â–†
wandb:         valid_loss â–„â–…â–‚â–…â–ƒâ–‚â–ƒâ–…â–ƒâ–ˆâ–„â–…â–„â–ƒâ–ƒâ–„â–â–ƒâ–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 6784
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.91716
wandb:   test_error_force 12.95669
wandb:          test_loss 16.58061
wandb: train_error_energy 1.64302
wandb:  train_error_force 3.38384
wandb:         train_loss 0.52202
wandb: valid_error_energy 1.30171
wandb:  valid_error_force 3.4639
wandb:         valid_loss 0.40339
wandb: 
wandb: ğŸš€ View run al_71_69 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/4lfg0edo
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_021729-4lfg0edo/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 4.074199676513672, Uncertainty Bias: -0.07907918095588684
9.536743e-07 0.0010718107
2.1082106 5.375758
(48745, 22, 3)
Found uncertainty sample 0 after 2509 steps.
Found uncertainty sample 1 after 1264 steps.
Found uncertainty sample 2 after 492 steps.
Found uncertainty sample 3 after 2580 steps.
Found uncertainty sample 4 after 24 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 789 steps.
Found uncertainty sample 7 after 725 steps.
Found uncertainty sample 8 after 942 steps.
Found uncertainty sample 9 after 2402 steps.
Found uncertainty sample 10 after 105 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 2423 steps.
Found uncertainty sample 13 after 1969 steps.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 2170 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 776 steps.
Found uncertainty sample 19 after 356 steps.
Found uncertainty sample 20 after 341 steps.
Found uncertainty sample 21 after 41 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 429 steps.
Found uncertainty sample 24 after 1701 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 2717 steps.
Found uncertainty sample 27 after 129 steps.
Found uncertainty sample 28 after 478 steps.
Found uncertainty sample 29 after 2016 steps.
Found uncertainty sample 30 after 1160 steps.
Found uncertainty sample 31 after 179 steps.
Found uncertainty sample 32 after 9 steps.
Found uncertainty sample 33 after 494 steps.
Found uncertainty sample 34 after 695 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 2101 steps.
Found uncertainty sample 37 after 477 steps.
Found uncertainty sample 38 after 9 steps.
Found uncertainty sample 39 after 1017 steps.
Found uncertainty sample 40 after 342 steps.
Found uncertainty sample 41 after 254 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 223 steps.
Found uncertainty sample 44 after 2620 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 846 steps.
Found uncertainty sample 47 after 2039 steps.
Found uncertainty sample 48 after 1033 steps.
Found uncertainty sample 49 after 489 steps.
Found uncertainty sample 50 after 38 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1177 steps.
Found uncertainty sample 53 after 77 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 142 steps.
Found uncertainty sample 56 after 244 steps.
Found uncertainty sample 57 after 1702 steps.
Found uncertainty sample 58 after 3380 steps.
Found uncertainty sample 59 after 1207 steps.
Found uncertainty sample 60 after 226 steps.
Found uncertainty sample 61 after 1042 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 1514 steps.
Found uncertainty sample 64 after 2618 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 606 steps.
Found uncertainty sample 67 after 1796 steps.
Found uncertainty sample 68 after 3739 steps.
Found uncertainty sample 69 after 198 steps.
Found uncertainty sample 70 after 43 steps.
Found uncertainty sample 71 after 3102 steps.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 3862 steps.
Found uncertainty sample 75 after 77 steps.
Found uncertainty sample 76 after 19 steps.
Found uncertainty sample 77 after 447 steps.
Found uncertainty sample 78 after 1264 steps.
Found uncertainty sample 79 after 871 steps.
Found uncertainty sample 80 after 108 steps.
Found uncertainty sample 81 after 1129 steps.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 1242 steps.
Found uncertainty sample 84 after 1531 steps.
Found uncertainty sample 85 after 664 steps.
Found uncertainty sample 86 after 3043 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 16 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1298 steps.
Found uncertainty sample 91 after 1361 steps.
Found uncertainty sample 92 after 1677 steps.
Found uncertainty sample 93 after 1284 steps.
Found uncertainty sample 94 after 40 steps.
Found uncertainty sample 95 after 1595 steps.
Found uncertainty sample 96 after 2032 steps.
Found uncertainty sample 97 after 2469 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 3334 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_024124-ktfrcgmu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_70
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ktfrcgmu
Training model 70. Added 83 samples to the dataset.
Epoch 0, Batch 100/215, Loss: 0.5143750905990601, Variance: 0.0796990618109703
Epoch 0, Batch 200/215, Loss: 0.5010571479797363, Variance: 0.07660505175590515

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.023164335754089, Training Loss Force: 3.4680396059086025, time: 3.5240516662597656
Validation Loss Energy: 1.595484412073784, Validation Loss Force: 3.6071502613624946, time: 0.19537568092346191
Test Loss Energy: 10.002877786257347, Test Loss Force: 12.640104848847702, time: 10.624870777130127

Epoch 1, Batch 100/215, Loss: 0.6130481958389282, Variance: 0.07621409744024277
Epoch 1, Batch 200/215, Loss: 0.7200814485549927, Variance: 0.07765080034732819

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6453274446254489, Training Loss Force: 3.373185995418939, time: 3.5084359645843506
Validation Loss Energy: 1.7965771960976018, Validation Loss Force: 3.4476740067088616, time: 0.19450879096984863
Test Loss Energy: 10.256725173252192, Test Loss Force: 12.686649126275054, time: 10.777055501937866

Epoch 2, Batch 100/215, Loss: 0.24531793594360352, Variance: 0.07702823728322983
Epoch 2, Batch 200/215, Loss: 0.43233054876327515, Variance: 0.07515819370746613

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.643986912673231, Training Loss Force: 3.3828081485454056, time: 3.5991034507751465
Validation Loss Energy: 1.4532177268696884, Validation Loss Force: 3.4252824530616497, time: 0.19969391822814941
Test Loss Energy: 10.01468511266935, Test Loss Force: 12.891128963611628, time: 10.645655155181885

Epoch 3, Batch 100/215, Loss: 0.6532329320907593, Variance: 0.07625798881053925
Epoch 3, Batch 200/215, Loss: 0.4602530002593994, Variance: 0.07565813511610031

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.635411892534521, Training Loss Force: 3.365770646807843, time: 3.5245964527130127
Validation Loss Energy: 1.4873065082571355, Validation Loss Force: 3.5092744239186278, time: 0.19464492797851562
Test Loss Energy: 9.975583062971486, Test Loss Force: 13.102634014833837, time: 10.930101871490479

Epoch 4, Batch 100/215, Loss: 0.4749675989151001, Variance: 0.07657080888748169
Epoch 4, Batch 200/215, Loss: 0.5723872184753418, Variance: 0.07937045395374298

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6324076660505067, Training Loss Force: 3.389988947834886, time: 3.577319622039795
Validation Loss Energy: 2.046837971738383, Validation Loss Force: 3.4163503029419027, time: 0.20720672607421875
Test Loss Energy: 10.578506627024565, Test Loss Force: 12.906873346327643, time: 10.680761814117432

Epoch 5, Batch 100/215, Loss: 0.3617178201675415, Variance: 0.07938189804553986
Epoch 5, Batch 200/215, Loss: 0.5616952180862427, Variance: 0.07634930312633514

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6407571427988832, Training Loss Force: 3.3993796016892004, time: 3.5533084869384766
Validation Loss Energy: 2.0608632332329364, Validation Loss Force: 3.425408706467391, time: 0.20467424392700195
Test Loss Energy: 10.382739192174268, Test Loss Force: 12.717630238696962, time: 10.84175157546997

Epoch 6, Batch 100/215, Loss: 0.7327640056610107, Variance: 0.07807058095932007
Epoch 6, Batch 200/215, Loss: 0.6571691632270813, Variance: 0.07589834928512573

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6461860426116774, Training Loss Force: 3.37910765247172, time: 3.633211851119995
Validation Loss Energy: 1.5857474799933673, Validation Loss Force: 3.401354649309396, time: 0.19997096061706543
Test Loss Energy: 9.957783949998817, Test Loss Force: 12.586912458808516, time: 10.720705270767212

Epoch 7, Batch 100/215, Loss: 0.35137832164764404, Variance: 0.07730065286159515
Epoch 7, Batch 200/215, Loss: 0.30000752210617065, Variance: 0.07444106787443161

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6536310101029463, Training Loss Force: 3.383250674992, time: 3.5256130695343018
Validation Loss Energy: 1.6691733145468237, Validation Loss Force: 3.4662350387059644, time: 0.19513416290283203
Test Loss Energy: 10.087032453939798, Test Loss Force: 12.933661493809096, time: 10.747267723083496

Epoch 8, Batch 100/215, Loss: 0.6601959466934204, Variance: 0.07989981770515442
Epoch 8, Batch 200/215, Loss: 0.4504609704017639, Variance: 0.07428007572889328

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.654262583949021, Training Loss Force: 3.3879938032886012, time: 3.8002536296844482
Validation Loss Energy: 1.8636502364077647, Validation Loss Force: 3.3964092789129126, time: 0.20157551765441895
Test Loss Energy: 10.477430330680615, Test Loss Force: 12.663164185806293, time: 10.672930479049683

Epoch 9, Batch 100/215, Loss: 0.7890868186950684, Variance: 0.07838263362646103
Epoch 9, Batch 200/215, Loss: 0.715280294418335, Variance: 0.07671905308961868

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6555486070121512, Training Loss Force: 3.387161217460216, time: 3.5417089462280273
Validation Loss Energy: 1.81281598373818, Validation Loss Force: 3.414462680924586, time: 0.1953141689300537
Test Loss Energy: 10.124066996135776, Test Loss Force: 12.889920254865565, time: 10.67953896522522

Epoch 10, Batch 100/215, Loss: 0.16672587394714355, Variance: 0.0754464790225029
Epoch 10, Batch 200/215, Loss: 0.36749720573425293, Variance: 0.07131586968898773

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6337238462835233, Training Loss Force: 3.3923322278233012, time: 3.4635605812072754
Validation Loss Energy: 1.5704118105750287, Validation Loss Force: 3.393320301186029, time: 0.19531655311584473
Test Loss Energy: 10.344327892478054, Test Loss Force: 13.096824304017733, time: 10.886033773422241

Epoch 11, Batch 100/215, Loss: 0.3025563955307007, Variance: 0.07741875946521759
Epoch 11, Batch 200/215, Loss: 0.21184653043746948, Variance: 0.07414383441209793

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6301815970291669, Training Loss Force: 3.3857358977577454, time: 3.5849769115448
Validation Loss Energy: 1.5640392422877214, Validation Loss Force: 3.45476763335797, time: 0.20043301582336426
Test Loss Energy: 9.885943884017005, Test Loss Force: 12.877906324128737, time: 10.661563396453857

Epoch 12, Batch 100/215, Loss: 0.4527367353439331, Variance: 0.07576263695955276
Epoch 12, Batch 200/215, Loss: 0.6502678990364075, Variance: 0.08064500987529755

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6367400303595914, Training Loss Force: 3.382291988001429, time: 3.51422381401062
Validation Loss Energy: 1.5857532415575182, Validation Loss Force: 3.428393753370412, time: 0.20854806900024414
Test Loss Energy: 10.38745707339559, Test Loss Force: 12.697999792077162, time: 10.839102268218994

Epoch 13, Batch 100/215, Loss: 0.9058256149291992, Variance: 0.07696661353111267
Epoch 13, Batch 200/215, Loss: 0.607696533203125, Variance: 0.07992923259735107

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.636263885532479, Training Loss Force: 3.3689605583872346, time: 3.5329997539520264
Validation Loss Energy: 2.0197895155732373, Validation Loss Force: 3.4033015569868157, time: 0.19571709632873535
Test Loss Energy: 10.59453601225768, Test Loss Force: 13.098793683208882, time: 11.702319860458374

Epoch 14, Batch 100/215, Loss: 0.2152719497680664, Variance: 0.07571490854024887
Epoch 14, Batch 200/215, Loss: 0.7497909665107727, Variance: 0.0780959501862526

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6453964637753224, Training Loss Force: 3.375759442807313, time: 3.4916656017303467
Validation Loss Energy: 1.5040970143740366, Validation Loss Force: 3.4520211207775233, time: 0.2151165008544922
Test Loss Energy: 10.231636254826558, Test Loss Force: 13.076173911465652, time: 10.839768886566162

Epoch 15, Batch 100/215, Loss: 0.5900079011917114, Variance: 0.07815690338611603
Epoch 15, Batch 200/215, Loss: 0.5212673544883728, Variance: 0.07476186007261276

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.635024547903887, Training Loss Force: 3.3998707973970714, time: 3.5893659591674805
Validation Loss Energy: 1.5203868767499622, Validation Loss Force: 3.4927965731162525, time: 0.2007596492767334
Test Loss Energy: 9.936900200181803, Test Loss Force: 12.933533851927162, time: 10.664272785186768

Epoch 16, Batch 100/215, Loss: 0.636747419834137, Variance: 0.07887107133865356
Epoch 16, Batch 200/215, Loss: 0.413671612739563, Variance: 0.08153623342514038

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6465491862131605, Training Loss Force: 3.3764561321024322, time: 3.5407397747039795
Validation Loss Energy: 1.817094265207107, Validation Loss Force: 3.3963728725075537, time: 0.1946578025817871
Test Loss Energy: 10.42491510199614, Test Loss Force: 13.086747217128995, time: 10.873894214630127

Epoch 17, Batch 100/215, Loss: 0.7402138710021973, Variance: 0.08108995854854584
Epoch 17, Batch 200/215, Loss: 0.5557231307029724, Variance: 0.080499567091465

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6574955672498455, Training Loss Force: 3.386526439508698, time: 3.5481302738189697
Validation Loss Energy: 1.9930709437477838, Validation Loss Force: 3.3929582411547505, time: 0.2074577808380127
Test Loss Energy: 10.484725904546847, Test Loss Force: 12.946077422795012, time: 10.808644771575928

Epoch 18, Batch 100/215, Loss: 0.440546452999115, Variance: 0.07635751366615295
Epoch 18, Batch 200/215, Loss: 0.1738659143447876, Variance: 0.074724480509758

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6486484498164113, Training Loss Force: 3.3874583689176205, time: 3.4628114700317383
Validation Loss Energy: 1.4416666535901115, Validation Loss Force: 3.3904820489174607, time: 0.19860124588012695
Test Loss Energy: 10.00708058128282, Test Loss Force: 13.101312623841862, time: 10.849926948547363

Epoch 19, Batch 100/215, Loss: 0.2024770975112915, Variance: 0.07702913880348206
Epoch 19, Batch 200/215, Loss: 0.3222854733467102, Variance: 0.07619519531726837

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6425778649069762, Training Loss Force: 3.3638692842918, time: 3.5480895042419434
Validation Loss Energy: 1.4204753144730935, Validation Loss Force: 3.4344800804706295, time: 0.204254150390625
Test Loss Energy: 9.991320455508445, Test Loss Force: 12.651097677171656, time: 10.69429326057434

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–…â–‚â–‚â–ˆâ–†â–‚â–ƒâ–‡â–ƒâ–†â–â–†â–ˆâ–„â–‚â–†â–‡â–‚â–‚
wandb:   test_error_force â–‚â–‚â–…â–ˆâ–…â–ƒâ–â–†â–‚â–…â–ˆâ–…â–ƒâ–ˆâ–ˆâ–†â–ˆâ–†â–ˆâ–‚
wandb:          test_loss â–„â–ƒâ–ƒâ–ƒâ–…â–…â–‚â–„â–ƒâ–„â–ˆâ–„â–ƒâ–…â–„â–â–ƒâ–‚â–„â–‚
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–ƒâ–‚â–ƒâ–ƒâ–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–…â–â–‚â–ˆâ–ˆâ–ƒâ–„â–†â–…â–ƒâ–ƒâ–ƒâ–ˆâ–‚â–‚â–…â–‡â–â–
wandb:  valid_error_force â–ˆâ–ƒâ–‚â–…â–‚â–‚â–â–ƒâ–â–‚â–â–ƒâ–‚â–â–ƒâ–„â–â–â–â–‚
wandb:         valid_loss â–„â–…â–â–ƒâ–ˆâ–ˆâ–ƒâ–„â–†â–…â–ƒâ–ƒâ–‚â–‡â–‚â–ƒâ–„â–‡â–â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 6858
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.99132
wandb:   test_error_force 12.6511
wandb:          test_loss 16.37362
wandb: train_error_energy 1.64258
wandb:  train_error_force 3.36387
wandb:         train_loss 0.51648
wandb: valid_error_energy 1.42048
wandb:  valid_error_force 3.43448
wandb:         valid_loss 0.44958
wandb: 
wandb: ğŸš€ View run al_71_70 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ktfrcgmu
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_024124-ktfrcgmu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.830392599105835, Uncertainty Bias: -0.06328599154949188
1.5258789e-05 0.0038933903
2.0921173 5.3302736
(48745, 22, 3)
Found uncertainty sample 0 after 3222 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 2451 steps.
Found uncertainty sample 3 after 508 steps.
Found uncertainty sample 4 after 23 steps.
Found uncertainty sample 5 after 962 steps.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1298 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 110 steps.
Found uncertainty sample 11 after 927 steps.
Found uncertainty sample 12 after 595 steps.
Found uncertainty sample 13 after 142 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1482 steps.
Found uncertainty sample 16 after 3915 steps.
Found uncertainty sample 17 after 3235 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 1112 steps.
Found uncertainty sample 20 after 2163 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 3570 steps.
Found uncertainty sample 23 after 2639 steps.
Found uncertainty sample 24 after 2189 steps.
Found uncertainty sample 25 after 2579 steps.
Found uncertainty sample 26 after 604 steps.
Found uncertainty sample 27 after 1089 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 1621 steps.
Found uncertainty sample 30 after 3068 steps.
Found uncertainty sample 31 after 329 steps.
Found uncertainty sample 32 after 973 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 943 steps.
Found uncertainty sample 35 after 94 steps.
Found uncertainty sample 36 after 458 steps.
Found uncertainty sample 37 after 1291 steps.
Found uncertainty sample 38 after 1570 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 3909 steps.
Found uncertainty sample 41 after 1549 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 844 steps.
Found uncertainty sample 44 after 2266 steps.
Found uncertainty sample 45 after 1163 steps.
Found uncertainty sample 46 after 1076 steps.
Found uncertainty sample 47 after 3261 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 125 steps.
Found uncertainty sample 50 after 51 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1298 steps.
Found uncertainty sample 53 after 318 steps.
Found uncertainty sample 54 after 3090 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 2552 steps.
Found uncertainty sample 57 after 1350 steps.
Found uncertainty sample 58 after 93 steps.
Found uncertainty sample 59 after 486 steps.
Found uncertainty sample 60 after 466 steps.
Found uncertainty sample 61 after 1301 steps.
Found uncertainty sample 62 after 211 steps.
Found uncertainty sample 63 after 440 steps.
Found uncertainty sample 64 after 4 steps.
Found uncertainty sample 65 after 2521 steps.
Found uncertainty sample 66 after 38 steps.
Found uncertainty sample 67 after 652 steps.
Found uncertainty sample 68 after 1478 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 3452 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 1162 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 299 steps.
Found uncertainty sample 75 after 1044 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 2267 steps.
Found uncertainty sample 78 after 3025 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 936 steps.
Found uncertainty sample 82 after 2980 steps.
Found uncertainty sample 83 after 821 steps.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 222 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 211 steps.
Found uncertainty sample 89 after 1233 steps.
Found uncertainty sample 90 after 236 steps.
Found uncertainty sample 91 after 1332 steps.
Found uncertainty sample 92 after 203 steps.
Did not find any uncertainty samples for sample 93.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 3315 steps.
Found uncertainty sample 96 after 1388 steps.
Found uncertainty sample 97 after 113 steps.
Found uncertainty sample 98 after 1013 steps.
Found uncertainty sample 99 after 18 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_030714-barja2jx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_71
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/barja2jx
Training model 71. Added 79 samples to the dataset.
Epoch 0, Batch 100/217, Loss: 0.5437540411949158, Variance: 0.08249470591545105
Epoch 0, Batch 200/217, Loss: 0.5908130407333374, Variance: 0.08093270659446716

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.93654568120713, Training Loss Force: 3.7893805619498147, time: 3.4964120388031006
Validation Loss Energy: 1.404305693435969, Validation Loss Force: 3.375952107803662, time: 0.2080087661743164
Test Loss Energy: 9.695985889515578, Test Loss Force: 12.514613978843007, time: 10.454846620559692

Epoch 1, Batch 100/217, Loss: 0.5454659461975098, Variance: 0.07657107710838318
Epoch 1, Batch 200/217, Loss: 0.3849029541015625, Variance: 0.07471057772636414

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6324048082396134, Training Loss Force: 3.352502828511539, time: 3.5610382556915283
Validation Loss Energy: 1.7574953652827996, Validation Loss Force: 3.4215584229305804, time: 0.20391058921813965
Test Loss Energy: 10.301088729323729, Test Loss Force: 12.595460762294895, time: 10.698101043701172

Epoch 2, Batch 100/217, Loss: 0.33934855461120605, Variance: 0.07689277827739716
Epoch 2, Batch 200/217, Loss: 0.46616989374160767, Variance: 0.07643291354179382

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6443858115165981, Training Loss Force: 3.362227176963366, time: 3.5015671253204346
Validation Loss Energy: 1.7013837981874402, Validation Loss Force: 3.401873078748402, time: 0.20016717910766602
Test Loss Energy: 10.048398853492683, Test Loss Force: 12.726440125968487, time: 10.462984561920166

Epoch 3, Batch 100/217, Loss: 0.5999374389648438, Variance: 0.0779201090335846
Epoch 3, Batch 200/217, Loss: 0.6050203442573547, Variance: 0.07774662971496582

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6214972682881446, Training Loss Force: 3.3764102103221236, time: 3.496238946914673
Validation Loss Energy: 1.8060326465439225, Validation Loss Force: 3.42457419817574, time: 0.2026987075805664
Test Loss Energy: 9.898778995247872, Test Loss Force: 12.719970918106043, time: 10.496771574020386

Epoch 4, Batch 100/217, Loss: 0.747239351272583, Variance: 0.08173155784606934
Epoch 4, Batch 200/217, Loss: 0.6532420516014099, Variance: 0.07901275157928467

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6538947060211135, Training Loss Force: 3.3729718990804702, time: 3.663311004638672
Validation Loss Energy: 1.4419839849567018, Validation Loss Force: 3.4433605713485216, time: 0.19457650184631348
Test Loss Energy: 9.814906182403496, Test Loss Force: 12.64144605389624, time: 10.433565139770508

Epoch 5, Batch 100/217, Loss: 0.5954930782318115, Variance: 0.0762709304690361
Epoch 5, Batch 200/217, Loss: 0.5665755867958069, Variance: 0.07245602458715439

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6616740328134492, Training Loss Force: 3.3780858931271145, time: 3.5101468563079834
Validation Loss Energy: 1.9340315557199061, Validation Loss Force: 3.417745659144859, time: 0.19521141052246094
Test Loss Energy: 10.358711265415419, Test Loss Force: 12.543776175794449, time: 10.414246559143066

Epoch 6, Batch 100/217, Loss: 0.4024863839149475, Variance: 0.07632455229759216
Epoch 6, Batch 200/217, Loss: 0.7577385902404785, Variance: 0.07760967314243317

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6332771800342605, Training Loss Force: 3.371250638877071, time: 3.691624164581299
Validation Loss Energy: 1.934901023067468, Validation Loss Force: 3.4221075250444386, time: 0.1967329978942871
Test Loss Energy: 10.267249454397605, Test Loss Force: 12.811362179144615, time: 10.56795048713684

Epoch 7, Batch 100/217, Loss: 0.6435360312461853, Variance: 0.08262628316879272
Epoch 7, Batch 200/217, Loss: 0.6284542679786682, Variance: 0.07710880041122437

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6528253784927993, Training Loss Force: 3.373798025212739, time: 3.6423935890197754
Validation Loss Energy: 1.8169414306720708, Validation Loss Force: 3.366798680220149, time: 0.1976792812347412
Test Loss Energy: 10.002774467773635, Test Loss Force: 12.553664649092715, time: 10.497234582901001

Epoch 8, Batch 100/217, Loss: 0.3835330605506897, Variance: 0.07688148319721222
Epoch 8, Batch 200/217, Loss: 0.7161109447479248, Variance: 0.07579527795314789

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6402371667161206, Training Loss Force: 3.3799921021178965, time: 3.538424491882324
Validation Loss Energy: 1.4119178608586564, Validation Loss Force: 3.3876205277231426, time: 0.19403576850891113
Test Loss Energy: 10.044497153784786, Test Loss Force: 12.810832333831558, time: 10.642703294754028

Epoch 9, Batch 100/217, Loss: 0.6204732656478882, Variance: 0.07506518065929413
Epoch 9, Batch 200/217, Loss: 0.8185530304908752, Variance: 0.07766151428222656

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.633970369864567, Training Loss Force: 3.3981754113102483, time: 3.497502326965332
Validation Loss Energy: 2.0743539774426902, Validation Loss Force: 3.391827972570021, time: 0.1932051181793213
Test Loss Energy: 10.492801519908053, Test Loss Force: 12.756964018193125, time: 10.613867282867432

Epoch 10, Batch 100/217, Loss: 0.5494884848594666, Variance: 0.07702546566724777
Epoch 10, Batch 200/217, Loss: 0.3645184636116028, Variance: 0.07775585353374481

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.65315780146598, Training Loss Force: 3.3956576953817432, time: 3.5720043182373047
Validation Loss Energy: 2.0492661231074387, Validation Loss Force: 3.422403229670231, time: 0.2087700366973877
Test Loss Energy: 10.49594497404952, Test Loss Force: 13.045620221212904, time: 10.71708345413208

Epoch 11, Batch 100/217, Loss: 0.6249767541885376, Variance: 0.07711583375930786
Epoch 11, Batch 200/217, Loss: 0.5466492772102356, Variance: 0.0771288350224495

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6467503700654584, Training Loss Force: 3.375067034119508, time: 3.5738019943237305
Validation Loss Energy: 1.3301595398717667, Validation Loss Force: 3.5365918644120535, time: 0.2011559009552002
Test Loss Energy: 9.915785797772205, Test Loss Force: 12.804455192083207, time: 10.46665358543396

Epoch 12, Batch 100/217, Loss: 0.5163726806640625, Variance: 0.07801134139299393
Epoch 12, Batch 200/217, Loss: 0.4721221923828125, Variance: 0.07959964871406555

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6344470598695566, Training Loss Force: 3.3731119702867596, time: 3.4774351119995117
Validation Loss Energy: 1.4283931509658532, Validation Loss Force: 3.3809508631278815, time: 0.1944429874420166
Test Loss Energy: 9.787858055287389, Test Loss Force: 12.61181008228746, time: 10.584290504455566

Epoch 13, Batch 100/217, Loss: 0.7337748408317566, Variance: 0.08140280842781067
Epoch 13, Batch 200/217, Loss: 0.4972679018974304, Variance: 0.07525580376386642

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6490503283958098, Training Loss Force: 3.375804095671782, time: 3.487044095993042
Validation Loss Energy: 1.8647194379792233, Validation Loss Force: 3.4413269897060497, time: 0.2235245704650879
Test Loss Energy: 10.527478618389619, Test Loss Force: 12.777544723463178, time: 10.451255798339844

Epoch 14, Batch 100/217, Loss: 0.2581794261932373, Variance: 0.07460308074951172
Epoch 14, Batch 200/217, Loss: 0.5587082505226135, Variance: 0.07729218900203705

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.623285451344209, Training Loss Force: 3.3794767718332124, time: 3.587390422821045
Validation Loss Energy: 1.7998120221150677, Validation Loss Force: 3.4120910875185957, time: 0.20976543426513672
Test Loss Energy: 10.17576079251189, Test Loss Force: 12.741205575844125, time: 10.711695909500122

Epoch 15, Batch 100/217, Loss: 0.48585045337677, Variance: 0.07644881308078766
Epoch 15, Batch 200/217, Loss: 0.4471935033798218, Variance: 0.0763210877776146

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6347742496047457, Training Loss Force: 3.370181369324824, time: 3.545072078704834
Validation Loss Energy: 1.575248156590937, Validation Loss Force: 3.3880695400559055, time: 0.20155954360961914
Test Loss Energy: 10.220325720766226, Test Loss Force: 13.11564728003436, time: 10.539160966873169

Epoch 16, Batch 100/217, Loss: 0.6162864565849304, Variance: 0.07933437824249268
Epoch 16, Batch 200/217, Loss: 0.45870357751846313, Variance: 0.0770009234547615

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6464240320140997, Training Loss Force: 3.358790230468091, time: 3.4828579425811768
Validation Loss Energy: 1.5087377733788907, Validation Loss Force: 3.4800003896399336, time: 0.20508599281311035
Test Loss Energy: 9.824734663271132, Test Loss Force: 12.548058161760995, time: 10.671190977096558

Epoch 17, Batch 100/217, Loss: 0.5212804079055786, Variance: 0.07442182302474976
Epoch 17, Batch 200/217, Loss: 0.6385371088981628, Variance: 0.07784402370452881

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6369903042512737, Training Loss Force: 3.3679198768329006, time: 3.570833921432495
Validation Loss Energy: 2.0694155097897307, Validation Loss Force: 3.347272154806303, time: 0.2041034698486328
Test Loss Energy: 10.293435947515148, Test Loss Force: 12.709372034636091, time: 11.525937795639038

Epoch 18, Batch 100/217, Loss: 0.5940796136856079, Variance: 0.07704697549343109
Epoch 18, Batch 200/217, Loss: 0.6876914501190186, Variance: 0.07605074346065521

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6268989811420178, Training Loss Force: 3.370662484494292, time: 3.4884870052337646
Validation Loss Energy: 1.8065013929297438, Validation Loss Force: 3.438960907361421, time: 0.19797921180725098
Test Loss Energy: 10.525485993005956, Test Loss Force: 13.066357065352342, time: 10.712607622146606

Epoch 19, Batch 100/217, Loss: 0.4356819987297058, Variance: 0.07701148092746735
Epoch 19, Batch 200/217, Loss: 0.5815964937210083, Variance: 0.0764375627040863

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6461657245174475, Training Loss Force: 3.374707878022253, time: 3.5056300163269043
Validation Loss Energy: 1.6947901472718698, Validation Loss Force: 3.3940428625645653, time: 0.198577880859375
Test Loss Energy: 10.242994442415734, Test Loss Force: 13.036892206238656, time: 10.532625913619995

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.039 MB of 0.061 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–†â–„â–ƒâ–‚â–‡â–†â–„â–„â–ˆâ–ˆâ–ƒâ–‚â–ˆâ–…â–…â–‚â–†â–ˆâ–†
wandb:   test_error_force â–â–‚â–ƒâ–ƒâ–‚â–â–„â–â–„â–„â–‡â–„â–‚â–„â–„â–ˆâ–â–ƒâ–‡â–‡
wandb:          test_loss â–â–ƒâ–â–ƒâ–ƒâ–„â–ƒâ–…â–†â–†â–„â–‚â–‚â–ˆâ–„â–‡â–„â–ƒâ–…â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–…â–„â–…â–‚â–‡â–‡â–†â–‚â–ˆâ–ˆâ–â–‚â–†â–…â–ƒâ–ƒâ–ˆâ–…â–„
wandb:  valid_error_force â–‚â–„â–ƒâ–„â–…â–„â–„â–‚â–‚â–ƒâ–„â–ˆâ–‚â–„â–ƒâ–ƒâ–†â–â–„â–ƒ
wandb:         valid_loss â–â–„â–ƒâ–†â–‚â–‡â–†â–†â–â–ˆâ–ˆâ–â–â–†â–…â–ƒâ–ƒâ–‡â–…â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 6929
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.24299
wandb:   test_error_force 13.03689
wandb:          test_loss 16.38914
wandb: train_error_energy 1.64617
wandb:  train_error_force 3.37471
wandb:         train_loss 0.51793
wandb: valid_error_energy 1.69479
wandb:  valid_error_force 3.39404
wandb:         valid_loss 0.57984
wandb: 
wandb: ğŸš€ View run al_71_71 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/barja2jx
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_030714-barja2jx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.7395706176757812, Uncertainty Bias: -0.06146948039531708
3.4332275e-05 0.0027513504
2.0423338 5.2717376
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1601 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 993 steps.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 662 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 2354 steps.
Found uncertainty sample 9 after 1850 steps.
Found uncertainty sample 10 after 1420 steps.
Found uncertainty sample 11 after 2716 steps.
Found uncertainty sample 12 after 1524 steps.
Found uncertainty sample 13 after 2634 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1526 steps.
Found uncertainty sample 16 after 3722 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 634 steps.
Found uncertainty sample 19 after 64 steps.
Found uncertainty sample 20 after 884 steps.
Found uncertainty sample 21 after 33 steps.
Found uncertainty sample 22 after 736 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 488 steps.
Found uncertainty sample 25 after 133 steps.
Found uncertainty sample 26 after 421 steps.
Found uncertainty sample 27 after 31 steps.
Found uncertainty sample 28 after 2243 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 616 steps.
Found uncertainty sample 31 after 2154 steps.
Found uncertainty sample 32 after 3049 steps.
Found uncertainty sample 33 after 126 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 3401 steps.
Found uncertainty sample 36 after 1315 steps.
Found uncertainty sample 37 after 187 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 3446 steps.
Found uncertainty sample 40 after 1281 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 2870 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 10 steps.
Did not find any uncertainty samples for sample 45.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 1945 steps.
Found uncertainty sample 48 after 284 steps.
Found uncertainty sample 49 after 2077 steps.
Found uncertainty sample 50 after 1399 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 175 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 208 steps.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 1050 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 2407 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 2849 steps.
Found uncertainty sample 62 after 233 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 1881 steps.
Found uncertainty sample 65 after 551 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 430 steps.
Found uncertainty sample 69 after 360 steps.
Found uncertainty sample 70 after 2696 steps.
Found uncertainty sample 71 after 925 steps.
Found uncertainty sample 72 after 1576 steps.
Found uncertainty sample 73 after 1230 steps.
Found uncertainty sample 74 after 281 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 2763 steps.
Found uncertainty sample 77 after 1366 steps.
Found uncertainty sample 78 after 1763 steps.
Found uncertainty sample 79 after 129 steps.
Found uncertainty sample 80 after 328 steps.
Found uncertainty sample 81 after 2322 steps.
Found uncertainty sample 82 after 202 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 3709 steps.
Found uncertainty sample 85 after 447 steps.
Found uncertainty sample 86 after 1613 steps.
Found uncertainty sample 87 after 2318 steps.
Found uncertainty sample 88 after 904 steps.
Found uncertainty sample 89 after 1877 steps.
Found uncertainty sample 90 after 1964 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 3213 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 2110 steps.
Found uncertainty sample 95 after 1339 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 1211 steps.
Found uncertainty sample 99 after 420 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_033518-foos2j3m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_72
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/foos2j3m
Training model 72. Added 72 samples to the dataset.
Epoch 0, Batch 100/219, Loss: 0.4986444115638733, Variance: 0.08396144211292267
Epoch 0, Batch 200/219, Loss: 0.5049677491188049, Variance: 0.08401766419410706

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.182826068686266, Training Loss Force: 3.798658099995114, time: 3.5340094566345215
Validation Loss Energy: 1.8888875017399336, Validation Loss Force: 3.3826683102504225, time: 0.20464015007019043
Test Loss Energy: 9.851518520582653, Test Loss Force: 12.32680584911648, time: 10.543318748474121

Epoch 1, Batch 100/219, Loss: 0.8045415878295898, Variance: 0.08113009482622147
Epoch 1, Batch 200/219, Loss: 0.577972948551178, Variance: 0.0782993882894516

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6132748658449327, Training Loss Force: 3.3372964184551, time: 3.5462911128997803
Validation Loss Energy: 1.8427954338711734, Validation Loss Force: 3.3680043779780613, time: 0.19309210777282715
Test Loss Energy: 10.08347417767173, Test Loss Force: 12.463689899574666, time: 10.629276752471924

Epoch 2, Batch 100/219, Loss: 0.3229627013206482, Variance: 0.07495471090078354
Epoch 2, Batch 200/219, Loss: 0.17889612913131714, Variance: 0.07162965834140778

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6167502996007537, Training Loss Force: 3.349914990497222, time: 3.537290334701538
Validation Loss Energy: 1.3900907734963714, Validation Loss Force: 3.365898428863163, time: 0.2007906436920166
Test Loss Energy: 9.772923106393614, Test Loss Force: 12.646573562407962, time: 10.530173301696777

Epoch 3, Batch 100/219, Loss: 0.49524569511413574, Variance: 0.07382988184690475
Epoch 3, Batch 200/219, Loss: 0.2833902835845947, Variance: 0.07344867289066315

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6196877809680787, Training Loss Force: 3.351201875815112, time: 3.6515915393829346
Validation Loss Energy: 1.4507250014483133, Validation Loss Force: 3.4018001546031105, time: 0.19464969635009766
Test Loss Energy: 9.926102341428345, Test Loss Force: 12.793898342952135, time: 10.492265701293945

Epoch 4, Batch 100/219, Loss: 0.6654540300369263, Variance: 0.07552441209554672
Epoch 4, Batch 200/219, Loss: 0.46826446056365967, Variance: 0.07832145690917969

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6258407207125678, Training Loss Force: 3.3606423033384556, time: 3.862560987472534
Validation Loss Energy: 1.7160046996473506, Validation Loss Force: 3.4436067320893446, time: 0.20414352416992188
Test Loss Energy: 10.150047645034693, Test Loss Force: 13.14841840990166, time: 10.475756645202637

Epoch 5, Batch 100/219, Loss: 0.6358477473258972, Variance: 0.0770438089966774
Epoch 5, Batch 200/219, Loss: 0.6006993055343628, Variance: 0.0787421464920044

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6484292150518116, Training Loss Force: 3.3605770531446653, time: 3.5863893032073975
Validation Loss Energy: 1.8885951649849737, Validation Loss Force: 3.393065011010223, time: 0.19852113723754883
Test Loss Energy: 10.264173164731716, Test Loss Force: 12.698904598242281, time: 10.46195363998413

Epoch 6, Batch 100/219, Loss: 0.5673699378967285, Variance: 0.0727723240852356
Epoch 6, Batch 200/219, Loss: 0.6893543004989624, Variance: 0.07607617229223251

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6079363430770683, Training Loss Force: 3.361604722090828, time: 3.6961116790771484
Validation Loss Energy: 1.3046734696026667, Validation Loss Force: 3.387684545102597, time: 0.19332265853881836
Test Loss Energy: 10.028994051158394, Test Loss Force: 12.93936043430285, time: 10.566437005996704

Epoch 7, Batch 100/219, Loss: 0.45640987157821655, Variance: 0.07298346608877182
Epoch 7, Batch 200/219, Loss: 0.5641061663627625, Variance: 0.07688451558351517

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6544202682266862, Training Loss Force: 3.356395180171553, time: 3.5401241779327393
Validation Loss Energy: 1.5750858864508421, Validation Loss Force: 3.404193959256407, time: 0.2032184600830078
Test Loss Energy: 10.204685719749948, Test Loss Force: 13.064383978894783, time: 10.460102319717407

Epoch 8, Batch 100/219, Loss: 0.4915098547935486, Variance: 0.0746089369058609
Epoch 8, Batch 200/219, Loss: 0.5571548938751221, Variance: 0.07765702903270721

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6460628284435208, Training Loss Force: 3.372625650128196, time: 3.538332462310791
Validation Loss Energy: 1.9231768245269318, Validation Loss Force: 3.3975533529846897, time: 0.19366884231567383
Test Loss Energy: 10.287444617950845, Test Loss Force: 12.932455769529076, time: 10.68027663230896

Epoch 9, Batch 100/219, Loss: 0.6372019052505493, Variance: 0.07904554903507233
Epoch 9, Batch 200/219, Loss: 0.5923071503639221, Variance: 0.07665032148361206

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.64499471479894, Training Loss Force: 3.3766536658387514, time: 3.676537036895752
Validation Loss Energy: 1.973551432412035, Validation Loss Force: 3.479472663764085, time: 0.1986234188079834
Test Loss Energy: 10.665744322737636, Test Loss Force: 13.266328050671076, time: 10.563206434249878

Epoch 10, Batch 100/219, Loss: 0.3677904009819031, Variance: 0.07552506029605865
Epoch 10, Batch 200/219, Loss: 0.29073160886764526, Variance: 0.07356975227594376

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.626391399049455, Training Loss Force: 3.3757002534941174, time: 3.61218523979187
Validation Loss Energy: 1.2092210518085045, Validation Loss Force: 3.438383637600603, time: 0.21303033828735352
Test Loss Energy: 10.024488579980991, Test Loss Force: 12.938757628121575, time: 10.572969198226929

Epoch 11, Batch 100/219, Loss: 0.6293694972991943, Variance: 0.07529282569885254
Epoch 11, Batch 200/219, Loss: 0.6600449085235596, Variance: 0.07452626526355743

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.625624705967977, Training Loss Force: 3.360733523465237, time: 3.592329740524292
Validation Loss Energy: 1.443875615018645, Validation Loss Force: 3.4486813062577526, time: 0.19345784187316895
Test Loss Energy: 9.91452570491475, Test Loss Force: 12.680480099737458, time: 10.386771202087402

Epoch 12, Batch 100/219, Loss: 0.6405147314071655, Variance: 0.0779121071100235
Epoch 12, Batch 200/219, Loss: 0.3061445951461792, Variance: 0.07626804709434509

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.631349574955337, Training Loss Force: 3.37105244380802, time: 3.632615089416504
Validation Loss Energy: 1.8119766323926056, Validation Loss Force: 3.4031937294528998, time: 0.19251608848571777
Test Loss Energy: 10.198236843562396, Test Loss Force: 12.728595591280442, time: 10.646133422851562

Epoch 13, Batch 100/219, Loss: 0.5872721672058105, Variance: 0.07621721178293228
Epoch 13, Batch 200/219, Loss: 0.5654776096343994, Variance: 0.07762452960014343

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6379838260056092, Training Loss Force: 3.37156814351648, time: 3.517740249633789
Validation Loss Energy: 2.061462936910206, Validation Loss Force: 3.4956177911951576, time: 0.19769883155822754
Test Loss Energy: 10.617884772473694, Test Loss Force: 13.200305203302749, time: 10.488864660263062

Epoch 14, Batch 100/219, Loss: 0.488736629486084, Variance: 0.074070043861866
Epoch 14, Batch 200/219, Loss: 0.31356358528137207, Variance: 0.07393437623977661

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6543308403831782, Training Loss Force: 3.3792862018107286, time: 3.55794358253479
Validation Loss Energy: 1.3607311229310683, Validation Loss Force: 3.3967325945127995, time: 0.1909959316253662
Test Loss Energy: 9.911557881936488, Test Loss Force: 12.92775319768589, time: 10.597040891647339

Epoch 15, Batch 100/219, Loss: 0.584189772605896, Variance: 0.07474242150783539
Epoch 15, Batch 200/219, Loss: 0.465930700302124, Variance: 0.07404756546020508

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6468481764206837, Training Loss Force: 3.3693552982144137, time: 3.622105598449707
Validation Loss Energy: 1.4824322249854898, Validation Loss Force: 3.448884770189844, time: 0.20087218284606934
Test Loss Energy: 9.891830360895522, Test Loss Force: 12.820474434795017, time: 10.559784173965454

Epoch 16, Batch 100/219, Loss: 0.8849045634269714, Variance: 0.0790611207485199
Epoch 16, Batch 200/219, Loss: 0.9238331317901611, Variance: 0.07815524935722351

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6431570082902127, Training Loss Force: 3.3844820927895767, time: 3.5150132179260254
Validation Loss Energy: 1.954626781699632, Validation Loss Force: 3.4008684512292624, time: 0.20426416397094727
Test Loss Energy: 10.523803486286415, Test Loss Force: 12.811575363284414, time: 11.817945718765259

Epoch 17, Batch 100/219, Loss: 0.6682569980621338, Variance: 0.07967185974121094
Epoch 17, Batch 200/219, Loss: 0.5753054618835449, Variance: 0.0781475082039833

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6432356054149546, Training Loss Force: 3.356889399933087, time: 3.574253797531128
Validation Loss Energy: 2.0768026143731717, Validation Loss Force: 3.3838869784860846, time: 0.205369234085083
Test Loss Energy: 10.523819159396352, Test Loss Force: 12.911488906996084, time: 10.489222764968872

Epoch 18, Batch 100/219, Loss: 0.6600993275642395, Variance: 0.07560323178768158
Epoch 18, Batch 200/219, Loss: 0.34882956743240356, Variance: 0.07508200407028198

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.645249955407984, Training Loss Force: 3.374800373252375, time: 3.5849719047546387
Validation Loss Energy: 1.4369475150458908, Validation Loss Force: 3.3752150172633546, time: 0.19962501525878906
Test Loss Energy: 10.152959868343281, Test Loss Force: 12.898577253537756, time: 10.602953910827637

Epoch 19, Batch 100/219, Loss: 0.4496934413909912, Variance: 0.0751635804772377
Epoch 19, Batch 200/219, Loss: 0.6967390179634094, Variance: 0.07781466841697693

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6414561327342239, Training Loss Force: 3.357956156973523, time: 3.52664852142334
Validation Loss Energy: 1.4473667888903603, Validation Loss Force: 3.4173230887731747, time: 0.21466374397277832
Test Loss Energy: 10.1924967174373, Test Loss Force: 13.216976290703448, time: 10.548827648162842

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.039 MB of 0.061 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ƒâ–â–‚â–„â–…â–ƒâ–„â–…â–ˆâ–ƒâ–‚â–„â–ˆâ–‚â–‚â–‡â–‡â–„â–„
wandb:   test_error_force â–â–‚â–ƒâ–„â–‡â–„â–†â–†â–†â–ˆâ–†â–„â–„â–ˆâ–…â–…â–…â–…â–…â–ˆ
wandb:          test_loss â–â–„â–„â–…â–ƒâ–ƒâ–…â–ˆâ–„â–‡â–†â–…â–…â–ˆâ–„â–‚â–…â–…â–‡â–‡
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–†â–‚â–ƒâ–…â–†â–‚â–„â–‡â–‡â–â–ƒâ–†â–ˆâ–‚â–ƒâ–‡â–ˆâ–ƒâ–ƒ
wandb:  valid_error_force â–‚â–â–â–ƒâ–…â–‚â–‚â–ƒâ–ƒâ–‡â–…â–…â–ƒâ–ˆâ–ƒâ–…â–ƒâ–‚â–‚â–„
wandb:         valid_loss â–…â–…â–‚â–‚â–„â–†â–â–„â–†â–‡â–â–ƒâ–…â–ˆâ–‚â–ƒâ–†â–‡â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6993
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.1925
wandb:   test_error_force 13.21698
wandb:          test_loss 17.08629
wandb: train_error_energy 1.64146
wandb:  train_error_force 3.35796
wandb:         train_loss 0.51002
wandb: valid_error_energy 1.44737
wandb:  valid_error_force 3.41732
wandb:         valid_loss 0.44693
wandb: 
wandb: ğŸš€ View run al_71_72 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/foos2j3m
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_033518-foos2j3m/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.8585598468780518, Uncertainty Bias: -0.056551843881607056
1.5258789e-05 0.00039482117
2.1287951 5.3576794
(48745, 22, 3)
Found uncertainty sample 0 after 1537 steps.
Found uncertainty sample 1 after 2633 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 997 steps.
Found uncertainty sample 4 after 2777 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 391 steps.
Found uncertainty sample 7 after 1442 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 149 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 1613 steps.
Found uncertainty sample 12 after 3457 steps.
Did not find any uncertainty samples for sample 13.
Did not find any uncertainty samples for sample 14.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 37 steps.
Found uncertainty sample 17 after 82 steps.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 646 steps.
Found uncertainty sample 21 after 347 steps.
Found uncertainty sample 22 after 356 steps.
Found uncertainty sample 23 after 3491 steps.
Found uncertainty sample 24 after 2044 steps.
Found uncertainty sample 25 after 1384 steps.
Found uncertainty sample 26 after 2464 steps.
Found uncertainty sample 27 after 1251 steps.
Found uncertainty sample 28 after 1799 steps.
Found uncertainty sample 29 after 276 steps.
Found uncertainty sample 30 after 2574 steps.
Did not find any uncertainty samples for sample 31.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 2 steps.
Found uncertainty sample 34 after 663 steps.
Found uncertainty sample 35 after 2183 steps.
Found uncertainty sample 36 after 765 steps.
Found uncertainty sample 37 after 434 steps.
Found uncertainty sample 38 after 1401 steps.
Found uncertainty sample 39 after 492 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 13 steps.
Found uncertainty sample 42 after 3660 steps.
Found uncertainty sample 43 after 40 steps.
Found uncertainty sample 44 after 1433 steps.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 1766 steps.
Found uncertainty sample 47 after 585 steps.
Found uncertainty sample 48 after 195 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 53 steps.
Found uncertainty sample 51 after 1577 steps.
Found uncertainty sample 52 after 3746 steps.
Found uncertainty sample 53 after 165 steps.
Found uncertainty sample 54 after 32 steps.
Found uncertainty sample 55 after 1575 steps.
Found uncertainty sample 56 after 2340 steps.
Found uncertainty sample 57 after 471 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 319 steps.
Found uncertainty sample 60 after 2039 steps.
Found uncertainty sample 61 after 45 steps.
Found uncertainty sample 62 after 633 steps.
Found uncertainty sample 63 after 317 steps.
Found uncertainty sample 64 after 2666 steps.
Found uncertainty sample 65 after 2233 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 249 steps.
Found uncertainty sample 69 after 539 steps.
Found uncertainty sample 70 after 3256 steps.
Found uncertainty sample 71 after 596 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 3445 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 205 steps.
Found uncertainty sample 76 after 3208 steps.
Found uncertainty sample 77 after 52 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 1488 steps.
Found uncertainty sample 82 after 361 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 624 steps.
Found uncertainty sample 85 after 3560 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 330 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 87 steps.
Found uncertainty sample 90 after 947 steps.
Found uncertainty sample 91 after 701 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 3260 steps.
Found uncertainty sample 94 after 2711 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 753 steps.
Found uncertainty sample 97 after 2658 steps.
Found uncertainty sample 98 after 1293 steps.
Found uncertainty sample 99 after 1370 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_040115-kkq3n6i5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_73
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/kkq3n6i5
Training model 73. Added 77 samples to the dataset.
Epoch 0, Batch 100/221, Loss: 0.9697346687316895, Variance: 0.13226650655269623
Epoch 0, Batch 200/221, Loss: 0.749001145362854, Variance: 0.13820365071296692

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.61075023619697, Training Loss Force: 3.7264557235063824, time: 3.634618043899536
Validation Loss Energy: 5.992410041851485, Validation Loss Force: 3.526879704274122, time: 0.19042515754699707
Test Loss Energy: 11.091720765253754, Test Loss Force: 12.018539796955805, time: 9.993433952331543

Epoch 1, Batch 100/221, Loss: 1.1963205337524414, Variance: 0.14282697439193726
Epoch 1, Batch 200/221, Loss: 1.112995982170105, Variance: 0.1449773609638214

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.2631770983432276, Training Loss Force: 3.430661709654152, time: 3.602618932723999
Validation Loss Energy: 5.248897834710041, Validation Loss Force: 3.4313276420371572, time: 0.20012784004211426
Test Loss Energy: 11.427508896390549, Test Loss Force: 11.579334522189765, time: 10.15902853012085

Epoch 2, Batch 100/221, Loss: 1.6798166036605835, Variance: 0.14642377197742462
Epoch 2, Batch 200/221, Loss: 1.8076311349868774, Variance: 0.1535220891237259

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.243993635554568, Training Loss Force: 3.4439256139535908, time: 3.583651065826416
Validation Loss Energy: 1.7679038675073346, Validation Loss Force: 3.4204986847978236, time: 0.19748139381408691
Test Loss Energy: 9.784686332656655, Test Loss Force: 11.871058271322582, time: 9.92884373664856

Epoch 3, Batch 100/221, Loss: 1.7794866561889648, Variance: 0.153132826089859
Epoch 3, Batch 200/221, Loss: 1.661550521850586, Variance: 0.14634719491004944

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.197809811373066, Training Loss Force: 3.4424353427601893, time: 3.5675010681152344
Validation Loss Energy: 3.291732680414119, Validation Loss Force: 3.6312674452233606, time: 0.18994593620300293
Test Loss Energy: 10.060484145610454, Test Loss Force: 11.699873309663907, time: 9.96252989768982

Epoch 4, Batch 100/221, Loss: 0.9134536385536194, Variance: 0.1483781635761261
Epoch 4, Batch 200/221, Loss: 0.8497718572616577, Variance: 0.15205088257789612

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.175053586873651, Training Loss Force: 3.4241898198446807, time: 3.7598047256469727
Validation Loss Energy: 5.834285555915494, Validation Loss Force: 3.429638752232581, time: 0.23749136924743652
Test Loss Energy: 11.938229350030618, Test Loss Force: 11.778278636682359, time: 12.07432508468628

Epoch 5, Batch 100/221, Loss: 1.1509754657745361, Variance: 0.14762699604034424
Epoch 5, Batch 200/221, Loss: 1.321128010749817, Variance: 0.1511554718017578

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.199975758487824, Training Loss Force: 3.426476300488522, time: 4.030855178833008
Validation Loss Energy: 4.796815562431751, Validation Loss Force: 3.5373187678389355, time: 0.2264537811279297
Test Loss Energy: 11.018803703406018, Test Loss Force: 12.139643898256594, time: 11.454701900482178

Epoch 6, Batch 100/221, Loss: 2.075222969055176, Variance: 0.1555561125278473
Epoch 6, Batch 200/221, Loss: 2.2553703784942627, Variance: 0.14473824203014374

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.195158349802679, Training Loss Force: 3.4378501588507455, time: 3.9366092681884766
Validation Loss Energy: 2.19983182654848, Validation Loss Force: 3.4050391556670627, time: 0.20327067375183105
Test Loss Energy: 10.292193290467884, Test Loss Force: 12.113063460285042, time: 10.571486949920654

Epoch 7, Batch 100/221, Loss: 1.709900975227356, Variance: 0.14794158935546875
Epoch 7, Batch 200/221, Loss: 1.6520485877990723, Variance: 0.15525025129318237

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.132924064566922, Training Loss Force: 3.399298150880842, time: 3.655689239501953
Validation Loss Energy: 3.7574872835906676, Validation Loss Force: 3.3763879269895294, time: 0.194932222366333
Test Loss Energy: 10.992586656817394, Test Loss Force: 11.882655337438866, time: 10.570873737335205

Epoch 8, Batch 100/221, Loss: 0.9825329780578613, Variance: 0.15211093425750732
Epoch 8, Batch 200/221, Loss: 0.9602876901626587, Variance: 0.15314354002475739

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.164487032124608, Training Loss Force: 3.4169352550401433, time: 3.846318244934082
Validation Loss Energy: 5.361034016599275, Validation Loss Force: 3.40881401003069, time: 0.21291446685791016
Test Loss Energy: 11.038244743686551, Test Loss Force: 11.815661591972326, time: 10.691586256027222

Epoch 9, Batch 100/221, Loss: 1.226371169090271, Variance: 0.15570718050003052
Epoch 9, Batch 200/221, Loss: 1.103143334388733, Variance: 0.1555112600326538

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.188601609950425, Training Loss Force: 3.431733607688515, time: 3.6871190071105957
Validation Loss Energy: 4.763046673106072, Validation Loss Force: 3.5551532304128752, time: 0.21388888359069824
Test Loss Energy: 11.872886447964229, Test Loss Force: 12.155929891610382, time: 10.754502773284912

Epoch 10, Batch 100/221, Loss: 1.664259672164917, Variance: 0.14737561345100403
Epoch 10, Batch 200/221, Loss: 2.089150905609131, Variance: 0.16073265671730042

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.167248117947256, Training Loss Force: 3.417509216501682, time: 3.631364583969116
Validation Loss Energy: 1.8881915771579796, Validation Loss Force: 3.4063228989606236, time: 0.28761768341064453
Test Loss Energy: 9.953369719544117, Test Loss Force: 12.15719833164629, time: 10.705040454864502

Epoch 11, Batch 100/221, Loss: 1.8771440982818604, Variance: 0.15819299221038818
Epoch 11, Batch 200/221, Loss: 1.7233669757843018, Variance: 0.15067194402217865

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.1603043050642565, Training Loss Force: 3.4076394207141707, time: 3.697530508041382
Validation Loss Energy: 3.3228102234402552, Validation Loss Force: 3.407687693823301, time: 0.1960442066192627
Test Loss Energy: 10.535356242408465, Test Loss Force: 12.473038273121086, time: 10.700464725494385

Epoch 12, Batch 100/221, Loss: 0.9115225672721863, Variance: 0.15213166177272797
Epoch 12, Batch 200/221, Loss: 0.9353477954864502, Variance: 0.15662358701229095

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.122662416926225, Training Loss Force: 3.401573783204799, time: 3.5663466453552246
Validation Loss Energy: 6.0075223741164585, Validation Loss Force: 3.3943483089654913, time: 0.1967606544494629
Test Loss Energy: 12.30643240568642, Test Loss Force: 12.527066719352229, time: 10.907692909240723

Epoch 13, Batch 100/221, Loss: 1.113243818283081, Variance: 0.15063907206058502
Epoch 13, Batch 200/221, Loss: 1.2091331481933594, Variance: 0.15663842856884003

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.134490221606767, Training Loss Force: 3.420023816143986, time: 3.5995922088623047
Validation Loss Energy: 4.973028687771759, Validation Loss Force: 3.43524940236981, time: 0.19781899452209473
Test Loss Energy: 11.267170570852164, Test Loss Force: 12.494203500054693, time: 10.640718698501587

Epoch 14, Batch 100/221, Loss: 1.9396438598632812, Variance: 0.15838000178337097
Epoch 14, Batch 200/221, Loss: 1.822122573852539, Variance: 0.15018925070762634

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.141168216899612, Training Loss Force: 3.390634756374913, time: 3.56662917137146
Validation Loss Energy: 2.0959628490419657, Validation Loss Force: 3.387806456459993, time: 0.20507192611694336
Test Loss Energy: 10.365238250675104, Test Loss Force: 12.305360222271993, time: 10.825104475021362

Epoch 15, Batch 100/221, Loss: 1.2752289772033691, Variance: 0.14802445471286774
Epoch 15, Batch 200/221, Loss: 1.707932472229004, Variance: 0.15686240792274475

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.140797417356243, Training Loss Force: 3.4004245827049027, time: 3.645538330078125
Validation Loss Energy: 3.799503838081512, Validation Loss Force: 3.3914812767098113, time: 0.19617414474487305
Test Loss Energy: 10.86853157145533, Test Loss Force: 12.423966218243335, time: 10.63241195678711

Epoch 16, Batch 100/221, Loss: 0.9862169623374939, Variance: 0.1528632640838623
Epoch 16, Batch 200/221, Loss: 0.8840492367744446, Variance: 0.1535218060016632

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.137551269972902, Training Loss Force: 3.453568903834506, time: 3.6698882579803467
Validation Loss Energy: 5.588236483669007, Validation Loss Force: 3.5917236872653313, time: 0.20346546173095703
Test Loss Energy: 11.5940029582662, Test Loss Force: 12.679533944728007, time: 10.797196388244629

Epoch 17, Batch 100/221, Loss: 1.0933629274368286, Variance: 0.15491965413093567
Epoch 17, Batch 200/221, Loss: 0.9571771025657654, Variance: 0.15079978108406067

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.13420794060555, Training Loss Force: 3.3938089725561444, time: 3.621940851211548
Validation Loss Energy: 5.374078197836871, Validation Loss Force: 3.4556366553672397, time: 0.19457030296325684
Test Loss Energy: 11.942956436464375, Test Loss Force: 12.53099634247051, time: 10.546266317367554

Epoch 18, Batch 100/221, Loss: 1.8719491958618164, Variance: 0.14696526527404785
Epoch 18, Batch 200/221, Loss: 1.7669858932495117, Variance: 0.16027812659740448

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.123613174884782, Training Loss Force: 3.414263543675662, time: 3.567060947418213
Validation Loss Energy: 1.99926298302503, Validation Loss Force: 3.393501609538101, time: 0.19621753692626953
Test Loss Energy: 10.033134177060262, Test Loss Force: 12.044625578334195, time: 10.7339186668396

Epoch 19, Batch 100/221, Loss: 1.9465255737304688, Variance: 0.15650810301303864
Epoch 19, Batch 200/221, Loss: 1.7286111116409302, Variance: 0.15394741296768188

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.115189613184966, Training Loss Force: 3.4058425401521424, time: 3.628281354904175
Validation Loss Energy: 3.351784656383103, Validation Loss Force: 3.4124425707654424, time: 0.19800162315368652
Test Loss Energy: 10.771650678920174, Test Loss Force: 12.352841053121956, time: 10.683009624481201

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–†â–â–‚â–‡â–„â–‚â–„â–„â–‡â–â–ƒâ–ˆâ–…â–ƒâ–„â–†â–‡â–‚â–„
wandb:   test_error_force â–„â–â–ƒâ–‚â–‚â–…â–„â–ƒâ–ƒâ–…â–…â–‡â–‡â–‡â–†â–†â–ˆâ–‡â–„â–†
wandb:          test_loss â–ˆâ–…â–‚â–‚â–‡â–…â–ƒâ–ƒâ–ƒâ–†â–â–ƒâ–ˆâ–†â–ƒâ–ƒâ–†â–‡â–â–„
wandb: train_error_energy â–ˆâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‡â–â–„â–ˆâ–†â–‚â–„â–‡â–†â–â–„â–ˆâ–†â–‚â–„â–‡â–‡â–â–„
wandb:  valid_error_force â–…â–ƒâ–‚â–ˆâ–‚â–…â–‚â–â–‚â–†â–‚â–‚â–â–ƒâ–â–â–‡â–ƒâ–â–‚
wandb:         valid_loss â–ˆâ–…â–â–ƒâ–†â–…â–â–ƒâ–†â–…â–â–‚â–‡â–…â–â–ƒâ–†â–†â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7062
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.77165
wandb:   test_error_force 12.35284
wandb:          test_loss 9.93145
wandb: train_error_energy 4.11519
wandb:  train_error_force 3.40584
wandb:         train_loss 1.39992
wandb: valid_error_energy 3.35178
wandb:  valid_error_force 3.41244
wandb:         valid_loss 1.15542
wandb: 
wandb: ğŸš€ View run al_71_73 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/kkq3n6i5
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_040115-kkq3n6i5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.320797920227051, Uncertainty Bias: -0.28171175718307495
3.0517578e-05 0.021906137
1.7221922 5.014222
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 2641 steps.
Found uncertainty sample 2 after 78 steps.
Found uncertainty sample 3 after 2856 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 943 steps.
Found uncertainty sample 6 after 2034 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1492 steps.
Found uncertainty sample 9 after 65 steps.
Found uncertainty sample 10 after 2605 steps.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 1651 steps.
Found uncertainty sample 14 after 950 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 3641 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 653 steps.
Found uncertainty sample 19 after 2188 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 3880 steps.
Found uncertainty sample 22 after 3047 steps.
Found uncertainty sample 23 after 2466 steps.
Found uncertainty sample 24 after 1649 steps.
Found uncertainty sample 25 after 274 steps.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 2949 steps.
Found uncertainty sample 29 after 178 steps.
Found uncertainty sample 30 after 1277 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 18 steps.
Found uncertainty sample 33 after 544 steps.
Found uncertainty sample 34 after 1521 steps.
Found uncertainty sample 35 after 1816 steps.
Found uncertainty sample 36 after 1388 steps.
Found uncertainty sample 37 after 716 steps.
Found uncertainty sample 38 after 698 steps.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 227 steps.
Found uncertainty sample 42 after 1424 steps.
Found uncertainty sample 43 after 3508 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 2375 steps.
Did not find any uncertainty samples for sample 46.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 2 steps.
Did not find any uncertainty samples for sample 49.
Did not find any uncertainty samples for sample 50.
Found uncertainty sample 51 after 15 steps.
Found uncertainty sample 52 after 843 steps.
Found uncertainty sample 53 after 290 steps.
Found uncertainty sample 54 after 3287 steps.
Found uncertainty sample 55 after 116 steps.
Found uncertainty sample 56 after 2632 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 484 steps.
Found uncertainty sample 60 after 2807 steps.
Found uncertainty sample 61 after 622 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 2102 steps.
Did not find any uncertainty samples for sample 64.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 1139 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 2896 steps.
Found uncertainty sample 69 after 1430 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 1021 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 813 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 459 steps.
Found uncertainty sample 76 after 2568 steps.
Found uncertainty sample 77 after 1223 steps.
Found uncertainty sample 78 after 3 steps.
Found uncertainty sample 79 after 1901 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 1884 steps.
Found uncertainty sample 82 after 243 steps.
Found uncertainty sample 83 after 427 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 603 steps.
Found uncertainty sample 86 after 3437 steps.
Found uncertainty sample 87 after 591 steps.
Found uncertainty sample 88 after 2800 steps.
Found uncertainty sample 89 after 2439 steps.
Found uncertainty sample 90 after 157 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 2205 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 1776 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 652 steps.
Found uncertainty sample 97 after 3133 steps.
Found uncertainty sample 98 after 98 steps.
Found uncertainty sample 99 after 3196 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_043046-iuf8lw9h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_74
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/iuf8lw9h
Training model 74. Added 70 samples to the dataset.
Epoch 0, Batch 100/223, Loss: 1.1241129636764526, Variance: 0.12093792855739594
Epoch 0, Batch 200/223, Loss: 1.1970323324203491, Variance: 0.12171099334955215

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7883482916440605, Training Loss Force: 3.4594999315987884, time: 4.040102958679199
Validation Loss Energy: 3.120503335898266, Validation Loss Force: 3.3693879883835702, time: 0.2393815517425537
Test Loss Energy: 10.742183520834988, Test Loss Force: 12.333942015190368, time: 12.340931415557861

Epoch 1, Batch 100/223, Loss: 1.3562897443771362, Variance: 0.11492064595222473
Epoch 1, Batch 200/223, Loss: 0.7265045642852783, Variance: 0.11512315273284912

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.624026941350193, Training Loss Force: 3.3257209038411184, time: 4.00322961807251
Validation Loss Energy: 2.425356425936359, Validation Loss Force: 3.3650929232277695, time: 0.23251867294311523
Test Loss Energy: 10.447578605493309, Test Loss Force: 12.2587752395258, time: 12.428136825561523

Epoch 2, Batch 100/223, Loss: 0.6272169351577759, Variance: 0.1135050430893898
Epoch 2, Batch 200/223, Loss: 0.8052060604095459, Variance: 0.11146125197410583

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.637398235545353, Training Loss Force: 3.3343691348235462, time: 3.89367413520813
Validation Loss Energy: 2.3857279980355903, Validation Loss Force: 3.375521521342724, time: 0.2444443702697754
Test Loss Energy: 10.273930699245714, Test Loss Force: 12.44001104808821, time: 12.31149673461914

Epoch 3, Batch 100/223, Loss: 0.7522616386413574, Variance: 0.11683826893568039
Epoch 3, Batch 200/223, Loss: 1.3597214221954346, Variance: 0.10830759257078171

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6369468891806225, Training Loss Force: 3.3308170267412347, time: 3.9009904861450195
Validation Loss Energy: 3.5093815834451054, Validation Loss Force: 3.384655075704963, time: 0.24263453483581543
Test Loss Energy: 10.827348714728785, Test Loss Force: 12.694700987151876, time: 12.296129703521729

Epoch 4, Batch 100/223, Loss: 1.3473584651947021, Variance: 0.11886578053236008
Epoch 4, Batch 200/223, Loss: 0.5932918190956116, Variance: 0.11602847278118134

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6412518376512506, Training Loss Force: 3.34441355626934, time: 4.113589286804199
Validation Loss Energy: 1.8301371382634706, Validation Loss Force: 3.3719677217737165, time: 0.23582744598388672
Test Loss Energy: 10.107763608940852, Test Loss Force: 12.652332691187924, time: 12.06059217453003

Epoch 5, Batch 100/223, Loss: 0.6713390946388245, Variance: 0.11522425711154938
Epoch 5, Batch 200/223, Loss: 1.0916566848754883, Variance: 0.11951536685228348

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6327147650036515, Training Loss Force: 3.352466247138046, time: 3.9131414890289307
Validation Loss Energy: 2.715622691842894, Validation Loss Force: 3.360845787605629, time: 0.23474931716918945
Test Loss Energy: 10.841026112224469, Test Loss Force: 12.551734343090539, time: 11.748433589935303

Epoch 6, Batch 100/223, Loss: 0.8467309474945068, Variance: 0.11205232888460159
Epoch 6, Batch 200/223, Loss: 1.6720898151397705, Variance: 0.11630204319953918

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.650511721805732, Training Loss Force: 3.340555669529787, time: 3.591613292694092
Validation Loss Energy: 4.083620093512094, Validation Loss Force: 3.414040388889334, time: 0.23039960861206055
Test Loss Energy: 11.150403957259797, Test Loss Force: 12.43671458410347, time: 12.39096975326538

Epoch 7, Batch 100/223, Loss: 1.4447304010391235, Variance: 0.11404724419116974
Epoch 7, Batch 200/223, Loss: 0.7069500684738159, Variance: 0.11291944980621338

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6572131394705343, Training Loss Force: 3.3385675925240212, time: 4.0824949741363525
Validation Loss Energy: 2.299269354047311, Validation Loss Force: 3.3840192439587664, time: 0.22000432014465332
Test Loss Energy: 10.220467412954687, Test Loss Force: 12.079373912289542, time: 10.080701351165771

Epoch 8, Batch 100/223, Loss: 0.5302926898002625, Variance: 0.11181817948818207
Epoch 8, Batch 200/223, Loss: 0.8043417930603027, Variance: 0.11347788572311401

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6619468278957417, Training Loss Force: 3.337754849142869, time: 3.6428792476654053
Validation Loss Energy: 2.2020724348293776, Validation Loss Force: 3.4015487611373585, time: 0.190201997756958
Test Loss Energy: 10.317979007102128, Test Loss Force: 12.604723159903225, time: 10.933910369873047

Epoch 9, Batch 100/223, Loss: 0.7365350127220154, Variance: 0.11414682865142822
Epoch 9, Batch 200/223, Loss: 1.3550211191177368, Variance: 0.11614149808883667

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.645686442970597, Training Loss Force: 3.3422549905850363, time: 3.698227643966675
Validation Loss Energy: 3.1405155583866664, Validation Loss Force: 3.3582145063082, time: 0.22890830039978027
Test Loss Energy: 10.454064181286688, Test Loss Force: 12.555199639251786, time: 10.170711278915405

Epoch 10, Batch 100/223, Loss: 1.2183607816696167, Variance: 0.121877521276474
Epoch 10, Batch 200/223, Loss: 0.8371638655662537, Variance: 0.11263265460729599

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.651565084161968, Training Loss Force: 3.3422587769590333, time: 3.6676645278930664
Validation Loss Energy: 1.754647733076459, Validation Loss Force: 3.3525308763583936, time: 0.19994783401489258
Test Loss Energy: 9.949908895418703, Test Loss Force: 12.338903713297446, time: 10.039213418960571

Epoch 11, Batch 100/223, Loss: 0.7365349531173706, Variance: 0.11310921609401703
Epoch 11, Batch 200/223, Loss: 0.7909395694732666, Variance: 0.11546513438224792

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6360926544496017, Training Loss Force: 3.337732544284654, time: 3.635969877243042
Validation Loss Energy: 2.8102642042748736, Validation Loss Force: 3.347846362866364, time: 0.20491909980773926
Test Loss Energy: 10.529264126493356, Test Loss Force: 12.278390171585418, time: 10.176679611206055

Epoch 12, Batch 100/223, Loss: 0.5891286134719849, Variance: 0.10855616629123688
Epoch 12, Batch 200/223, Loss: 1.393182635307312, Variance: 0.11579497158527374

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6477914592215965, Training Loss Force: 3.332343034315193, time: 3.6130590438842773
Validation Loss Energy: 3.746877933796506, Validation Loss Force: 3.379886114939341, time: 0.1976757049560547
Test Loss Energy: 10.97500665028432, Test Loss Force: 12.53570172157706, time: 10.005422115325928

Epoch 13, Batch 100/223, Loss: 1.4429457187652588, Variance: 0.11100822687149048
Epoch 13, Batch 200/223, Loss: 0.7924426794052124, Variance: 0.116400808095932

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.671780965293148, Training Loss Force: 3.3404656213283603, time: 3.6401636600494385
Validation Loss Energy: 1.9941612988881057, Validation Loss Force: 3.3805733832582474, time: 0.19462966918945312
Test Loss Energy: 10.124089650098307, Test Loss Force: 12.4505543178988, time: 10.058990478515625

Epoch 14, Batch 100/223, Loss: 0.4639168381690979, Variance: 0.10653935372829437
Epoch 14, Batch 200/223, Loss: 0.7542263269424438, Variance: 0.11573232710361481

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6304700063895465, Training Loss Force: 3.3364550854033688, time: 3.5980546474456787
Validation Loss Energy: 2.7305245460976786, Validation Loss Force: 3.3685918905195917, time: 0.1919708251953125
Test Loss Energy: 10.072017031717202, Test Loss Force: 12.127767187672395, time: 9.929409265518188

Epoch 15, Batch 100/223, Loss: 0.8190542459487915, Variance: 0.11511826515197754
Epoch 15, Batch 200/223, Loss: 1.3670611381530762, Variance: 0.11402491480112076

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.633820848024622, Training Loss Force: 3.349504825381482, time: 3.6023597717285156
Validation Loss Energy: 3.3417572524018238, Validation Loss Force: 3.3202809246116614, time: 0.19010543823242188
Test Loss Energy: 10.562577832286669, Test Loss Force: 12.433430244512019, time: 10.394038677215576

Epoch 16, Batch 100/223, Loss: 1.1120508909225464, Variance: 0.11655985563993454
Epoch 16, Batch 200/223, Loss: 0.6649375557899475, Variance: 0.11095872521400452

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6499565746738942, Training Loss Force: 3.3550846984989007, time: 3.898148775100708
Validation Loss Energy: 1.7023851970039638, Validation Loss Force: 3.451218398506454, time: 0.23259425163269043
Test Loss Energy: 10.180909206275572, Test Loss Force: 12.698283572828482, time: 12.37839961051941

Epoch 17, Batch 100/223, Loss: 0.7211897373199463, Variance: 0.11451806128025055
Epoch 17, Batch 200/223, Loss: 0.9109314680099487, Variance: 0.11380965262651443

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6343966274997888, Training Loss Force: 3.353239532148618, time: 3.9687037467956543
Validation Loss Energy: 2.791321350359791, Validation Loss Force: 3.356793765822608, time: 0.2255551815032959
Test Loss Energy: 10.684008671996834, Test Loss Force: 12.455729419317393, time: 10.88363265991211

Epoch 18, Batch 100/223, Loss: 0.6601959466934204, Variance: 0.11204935610294342
Epoch 18, Batch 200/223, Loss: 1.4974595308303833, Variance: 0.1144976019859314

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6358431317042297, Training Loss Force: 3.3405703816696795, time: 3.739602565765381
Validation Loss Energy: 3.54769815607264, Validation Loss Force: 3.4013336627423927, time: 0.20396161079406738
Test Loss Energy: 10.729569782372613, Test Loss Force: 12.285358023101129, time: 10.51894474029541

Epoch 19, Batch 100/223, Loss: 1.3608121871948242, Variance: 0.11139233410358429
Epoch 19, Batch 200/223, Loss: 0.7297776937484741, Variance: 0.1119379848241806

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.629360271794285, Training Loss Force: 3.335186298636577, time: 3.627614974975586
Validation Loss Energy: 2.152445271599868, Validation Loss Force: 3.3483936279597684, time: 0.19960975646972656
Test Loss Energy: 10.378499184587865, Test Loss Force: 12.129218603455906, time: 10.725125312805176

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–ƒâ–†â–‚â–†â–ˆâ–ƒâ–ƒâ–„â–â–„â–‡â–‚â–‚â–…â–‚â–…â–†â–ƒ
wandb:   test_error_force â–„â–ƒâ–…â–ˆâ–‡â–†â–…â–â–‡â–†â–„â–ƒâ–†â–…â–‚â–…â–ˆâ–…â–ƒâ–‚
wandb:          test_loss â–â–‚â–„â–ˆâ–„â–†â–‡â–‚â–„â–†â–â–ƒâ–‡â–‚â–â–…â–„â–„â–„â–ƒ
wandb: train_error_energy â–ˆâ–â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–â–â–‚â–â–‚â–
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–‚â–â–
wandb: valid_error_energy â–…â–ƒâ–ƒâ–†â–â–„â–ˆâ–ƒâ–‚â–…â–â–„â–‡â–‚â–„â–†â–â–„â–†â–‚
wandb:  valid_error_force â–„â–ƒâ–„â–„â–„â–ƒâ–†â–„â–…â–ƒâ–ƒâ–‚â–„â–„â–„â–â–ˆâ–ƒâ–…â–ƒ
wandb:         valid_loss â–„â–ƒâ–‚â–†â–â–ƒâ–ˆâ–‚â–‚â–…â–â–ƒâ–‡â–‚â–ƒâ–…â–â–ƒâ–†â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7125
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.3785
wandb:   test_error_force 12.12922
wandb:          test_loss 12.43078
wandb: train_error_energy 2.62936
wandb:  train_error_force 3.33519
wandb:         train_loss 0.93214
wandb: valid_error_energy 2.15245
wandb:  valid_error_force 3.34839
wandb:         valid_loss 0.75059
wandb: 
wandb: ğŸš€ View run al_71_74 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/iuf8lw9h
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_043046-iuf8lw9h/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2436351776123047, Uncertainty Bias: -0.1442270129919052
0.000113487244 0.016399384
1.9175673 4.924462
(48745, 22, 3)
Found uncertainty sample 0 after 3689 steps.
Found uncertainty sample 1 after 741 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 1336 steps.
Found uncertainty sample 4 after 828 steps.
Found uncertainty sample 5 after 1945 steps.
Found uncertainty sample 6 after 98 steps.
Did not find any uncertainty samples for sample 7.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 49 steps.
Found uncertainty sample 10 after 545 steps.
Found uncertainty sample 11 after 1256 steps.
Found uncertainty sample 12 after 479 steps.
Found uncertainty sample 13 after 1835 steps.
Found uncertainty sample 14 after 237 steps.
Found uncertainty sample 15 after 616 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 41 steps.
Found uncertainty sample 18 after 418 steps.
Found uncertainty sample 19 after 64 steps.
Found uncertainty sample 20 after 1546 steps.
Found uncertainty sample 21 after 2677 steps.
Found uncertainty sample 22 after 3009 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 1730 steps.
Found uncertainty sample 25 after 1089 steps.
Found uncertainty sample 26 after 210 steps.
Found uncertainty sample 27 after 3809 steps.
Found uncertainty sample 28 after 1533 steps.
Found uncertainty sample 29 after 59 steps.
Found uncertainty sample 30 after 1345 steps.
Found uncertainty sample 31 after 813 steps.
Found uncertainty sample 32 after 1290 steps.
Found uncertainty sample 33 after 134 steps.
Found uncertainty sample 34 after 184 steps.
Found uncertainty sample 35 after 161 steps.
Found uncertainty sample 36 after 889 steps.
Found uncertainty sample 37 after 657 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 1078 steps.
Found uncertainty sample 42 after 1521 steps.
Found uncertainty sample 43 after 702 steps.
Found uncertainty sample 44 after 2552 steps.
Found uncertainty sample 45 after 2675 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 1164 steps.
Found uncertainty sample 48 after 1311 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 1028 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 35 steps.
Found uncertainty sample 53 after 21 steps.
Found uncertainty sample 54 after 3859 steps.
Found uncertainty sample 55 after 593 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 3 steps.
Found uncertainty sample 59 after 652 steps.
Found uncertainty sample 60 after 1571 steps.
Found uncertainty sample 61 after 1765 steps.
Found uncertainty sample 62 after 2141 steps.
Found uncertainty sample 63 after 25 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 1110 steps.
Found uncertainty sample 66 after 1008 steps.
Found uncertainty sample 67 after 260 steps.
Found uncertainty sample 68 after 1491 steps.
Did not find any uncertainty samples for sample 69.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 331 steps.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 3229 steps.
Found uncertainty sample 76 after 40 steps.
Did not find any uncertainty samples for sample 77.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 635 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 2491 steps.
Found uncertainty sample 82 after 1096 steps.
Found uncertainty sample 83 after 1257 steps.
Found uncertainty sample 84 after 387 steps.
Found uncertainty sample 85 after 669 steps.
Found uncertainty sample 86 after 34 steps.
Found uncertainty sample 87 after 1115 steps.
Found uncertainty sample 88 after 467 steps.
Found uncertainty sample 89 after 1375 steps.
Found uncertainty sample 90 after 525 steps.
Found uncertainty sample 91 after 2839 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 1361 steps.
Found uncertainty sample 94 after 3182 steps.
Did not find any uncertainty samples for sample 95.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 48 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_045632-vtvfbj5h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_75
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/vtvfbj5h
Training model 75. Added 75 samples to the dataset.
Epoch 0, Batch 100/225, Loss: 0.36169254779815674, Variance: 0.08652397245168686
Epoch 0, Batch 200/225, Loss: 0.31590980291366577, Variance: 0.08075239509344101

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.097831769325126, Training Loss Force: 3.474749831138733, time: 3.733272075653076
Validation Loss Energy: 1.9321491107602327, Validation Loss Force: 3.469963021214374, time: 0.21101927757263184
Test Loss Energy: 10.56488211519975, Test Loss Force: 12.825527797978564, time: 10.654656648635864

Epoch 1, Batch 100/225, Loss: 0.22931361198425293, Variance: 0.07834310084581375
Epoch 1, Batch 200/225, Loss: 0.15797168016433716, Variance: 0.07616904377937317

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.613405823856582, Training Loss Force: 3.3511113934586554, time: 3.6982603073120117
Validation Loss Energy: 1.7238651431677474, Validation Loss Force: 3.369130415849018, time: 0.20893621444702148
Test Loss Energy: 10.23272235797713, Test Loss Force: 12.667184186950067, time: 10.80752968788147

Epoch 2, Batch 100/225, Loss: 0.45741862058639526, Variance: 0.0809163749217987
Epoch 2, Batch 200/225, Loss: 0.6185423135757446, Variance: 0.07977893948554993

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6249322902162529, Training Loss Force: 3.3538647508886474, time: 3.7617545127868652
Validation Loss Energy: 1.361066741001268, Validation Loss Force: 3.437515728937063, time: 0.19879412651062012
Test Loss Energy: 9.531104817285273, Test Loss Force: 12.346193178334753, time: 10.562527894973755

Epoch 3, Batch 100/225, Loss: 0.3818432688713074, Variance: 0.07672993093729019
Epoch 3, Batch 200/225, Loss: 0.6204962730407715, Variance: 0.08143827319145203

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6319581284602156, Training Loss Force: 3.3598247172071134, time: 3.613063335418701
Validation Loss Energy: 1.5018090888996287, Validation Loss Force: 3.3884382244942763, time: 0.19926738739013672
Test Loss Energy: 9.697060605062916, Test Loss Force: 12.326377610882385, time: 10.762266874313354

Epoch 4, Batch 100/225, Loss: 0.8493479490280151, Variance: 0.07857899367809296
Epoch 4, Batch 200/225, Loss: 0.5628607273101807, Variance: 0.07771147042512894

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6189220739042154, Training Loss Force: 3.3671970076122753, time: 3.5872533321380615
Validation Loss Energy: 2.0749749460233966, Validation Loss Force: 3.443004634718756, time: 0.20406770706176758
Test Loss Energy: 10.535283043715614, Test Loss Force: 12.870495685106993, time: 10.767789840698242

Epoch 5, Batch 100/225, Loss: 0.531906247138977, Variance: 0.07897186279296875
Epoch 5, Batch 200/225, Loss: 0.42178821563720703, Variance: 0.07555261999368668

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6503950476442997, Training Loss Force: 3.360959496573919, time: 3.6306326389312744
Validation Loss Energy: 1.9778304097220427, Validation Loss Force: 3.464132986301119, time: 0.2050023078918457
Test Loss Energy: 10.253016125490383, Test Loss Force: 12.62503740365145, time: 10.667402744293213

Epoch 6, Batch 100/225, Loss: 0.7132061719894409, Variance: 0.07970094680786133
Epoch 6, Batch 200/225, Loss: 0.5754772424697876, Variance: 0.08017291128635406

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6329249836224906, Training Loss Force: 3.3467210219498345, time: 3.859586715698242
Validation Loss Energy: 1.4799464023076385, Validation Loss Force: 3.3596379188235432, time: 0.21352934837341309
Test Loss Energy: 10.012018273091188, Test Loss Force: 12.656701180037336, time: 10.648780822753906

Epoch 7, Batch 100/225, Loss: 0.5382202863693237, Variance: 0.07939368486404419
Epoch 7, Batch 200/225, Loss: 0.6321655511856079, Variance: 0.07793279737234116

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6268535830247655, Training Loss Force: 3.342024839604919, time: 3.6181390285491943
Validation Loss Energy: 1.3616320422954922, Validation Loss Force: 3.4134786503014447, time: 0.20419096946716309
Test Loss Energy: 9.969705175759287, Test Loss Force: 12.849196189844196, time: 10.546497344970703

Epoch 8, Batch 100/225, Loss: 0.37119394540786743, Variance: 0.07655255496501923
Epoch 8, Batch 200/225, Loss: 0.6034496426582336, Variance: 0.07824376970529556

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6397297824112633, Training Loss Force: 3.364141410959289, time: 3.8864083290100098
Validation Loss Energy: 1.8970975079857937, Validation Loss Force: 3.4020379652509614, time: 0.2006235122680664
Test Loss Energy: 10.235745263006734, Test Loss Force: 12.599394680246334, time: 10.635332345962524

Epoch 9, Batch 100/225, Loss: 0.7862124443054199, Variance: 0.0770459920167923
Epoch 9, Batch 200/225, Loss: 0.6710211634635925, Variance: 0.07719609886407852

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6310157523067925, Training Loss Force: 3.356627535635458, time: 3.6436777114868164
Validation Loss Energy: 1.610494149529771, Validation Loss Force: 3.3985988959593936, time: 0.22022795677185059
Test Loss Energy: 10.146556505262833, Test Loss Force: 12.490687104238974, time: 10.670961380004883

Epoch 10, Batch 100/225, Loss: 0.8930283784866333, Variance: 0.08246243745088577
Epoch 10, Batch 200/225, Loss: 0.535308301448822, Variance: 0.0776883214712143

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6347320981256355, Training Loss Force: 3.359393837565133, time: 3.917841672897339
Validation Loss Energy: 1.4592278176210685, Validation Loss Force: 3.3724519158315363, time: 0.20596694946289062
Test Loss Energy: 9.781208790858187, Test Loss Force: 12.370741437421165, time: 11.716980457305908

Epoch 11, Batch 100/225, Loss: 0.39122962951660156, Variance: 0.07804383337497711
Epoch 11, Batch 200/225, Loss: 0.7246209979057312, Variance: 0.07798172533512115

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6222424184812765, Training Loss Force: 3.3519669170168886, time: 3.7535901069641113
Validation Loss Energy: 1.2949803755103544, Validation Loss Force: 3.357526124643565, time: 0.19866156578063965
Test Loss Energy: 10.094350182803147, Test Loss Force: 12.88349291346578, time: 10.577164649963379

Epoch 12, Batch 100/225, Loss: 0.2195621132850647, Variance: 0.07415898889303207
Epoch 12, Batch 200/225, Loss: 0.31037259101867676, Variance: 0.07170960307121277

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6210222892886061, Training Loss Force: 3.3633333099912943, time: 3.7952864170074463
Validation Loss Energy: 1.775678221569187, Validation Loss Force: 3.4137525333509355, time: 0.262836217880249
Test Loss Energy: 10.262641168365134, Test Loss Force: 12.673783983854658, time: 10.53574800491333

Epoch 13, Batch 100/225, Loss: 0.44071078300476074, Variance: 0.07267636060714722
Epoch 13, Batch 200/225, Loss: 0.34523266553878784, Variance: 0.07817245274782181

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.635704937150093, Training Loss Force: 3.3489736548594333, time: 3.6177430152893066
Validation Loss Energy: 1.889331086512265, Validation Loss Force: 3.395969618684278, time: 0.2073981761932373
Test Loss Energy: 10.22347151584824, Test Loss Force: 12.633923739885876, time: 10.677438735961914

Epoch 14, Batch 100/225, Loss: 0.39184123277664185, Variance: 0.07874470949172974
Epoch 14, Batch 200/225, Loss: 0.47492516040802, Variance: 0.07627055048942566

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.641163458489773, Training Loss Force: 3.346362211898446, time: 3.6745049953460693
Validation Loss Energy: 1.5517377183190812, Validation Loss Force: 3.406649888601418, time: 0.20221853256225586
Test Loss Energy: 9.847186395586643, Test Loss Force: 12.55980364910401, time: 10.697450399398804

Epoch 15, Batch 100/225, Loss: 0.6928418278694153, Variance: 0.07363299280405045
Epoch 15, Batch 200/225, Loss: 0.4790842533111572, Variance: 0.0792064517736435

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6158513701393182, Training Loss Force: 3.359857571297364, time: 3.624434232711792
Validation Loss Energy: 1.2165400474669348, Validation Loss Force: 3.3836552355724607, time: 0.20414495468139648
Test Loss Energy: 10.120387508577444, Test Loss Force: 12.927669366832772, time: 10.575724840164185

Epoch 16, Batch 100/225, Loss: 0.6284250020980835, Variance: 0.07813575863838196
Epoch 16, Batch 200/225, Loss: 0.43490272760391235, Variance: 0.07635428011417389

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6460816818062152, Training Loss Force: 3.3477050166985025, time: 3.7437710762023926
Validation Loss Energy: 1.922655110718023, Validation Loss Force: 3.4212941146543643, time: 0.2277660369873047
Test Loss Energy: 10.33769791982386, Test Loss Force: 13.035619914376262, time: 10.832840204238892

Epoch 17, Batch 100/225, Loss: 0.3317286968231201, Variance: 0.07371576875448227
Epoch 17, Batch 200/225, Loss: 0.4167417287826538, Variance: 0.07772660255432129

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6388882459391365, Training Loss Force: 3.360260648964604, time: 3.591465711593628
Validation Loss Energy: 1.682823406364132, Validation Loss Force: 3.37334398409741, time: 0.20828866958618164
Test Loss Energy: 10.302676344012895, Test Loss Force: 12.567528864836023, time: 10.530519247055054

Epoch 18, Batch 100/225, Loss: 0.8465306758880615, Variance: 0.07534711062908173
Epoch 18, Batch 200/225, Loss: 0.6705841422080994, Variance: 0.08437235653400421

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.631484969143037, Training Loss Force: 3.350336274244555, time: 3.6415624618530273
Validation Loss Energy: 1.4456238461506103, Validation Loss Force: 3.469383636096669, time: 0.20351505279541016
Test Loss Energy: 9.992982173556733, Test Loss Force: 12.553969834061409, time: 10.864343404769897

Epoch 19, Batch 100/225, Loss: 0.39010703563690186, Variance: 0.08039065450429916
Epoch 19, Batch 200/225, Loss: 0.4467623829841614, Variance: 0.07995570451021194

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6382643462902393, Training Loss Force: 3.3347917065299133, time: 3.7233731746673584
Validation Loss Energy: 1.3228183329660301, Validation Loss Force: 3.362860776374416, time: 0.20910859107971191
Test Loss Energy: 10.030349479052905, Test Loss Force: 12.774138384495899, time: 10.596230506896973

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–†â–â–‚â–ˆâ–†â–„â–„â–†â–…â–ƒâ–…â–†â–†â–ƒâ–…â–†â–†â–„â–„
wandb:   test_error_force â–†â–„â–â–â–†â–„â–„â–†â–„â–ƒâ–â–†â–„â–„â–ƒâ–‡â–ˆâ–ƒâ–ƒâ–…
wandb:          test_loss â–„â–„â–â–‚â–‡â–„â–…â–…â–ƒâ–†â–…â–†â–…â–†â–„â–‡â–ˆâ–†â–„â–…
wandb: train_error_energy â–ˆâ–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–…â–‚â–ƒâ–ˆâ–‡â–ƒâ–‚â–‡â–„â–ƒâ–‚â–†â–†â–„â–â–‡â–…â–ƒâ–‚
wandb:  valid_error_force â–ˆâ–‚â–†â–ƒâ–†â–ˆâ–â–„â–„â–„â–‚â–â–…â–ƒâ–„â–ƒâ–…â–‚â–ˆâ–
wandb:         valid_loss â–‡â–…â–ƒâ–ƒâ–ˆâ–‡â–ƒâ–‚â–†â–„â–ƒâ–â–…â–†â–„â–â–‡â–„â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7192
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.03035
wandb:   test_error_force 12.77414
wandb:          test_loss 16.36184
wandb: train_error_energy 1.63826
wandb:  train_error_force 3.33479
wandb:         train_loss 0.49876
wandb: valid_error_energy 1.32282
wandb:  valid_error_force 3.36286
wandb:         valid_loss 0.37462
wandb: 
wandb: ğŸš€ View run al_71_75 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/vtvfbj5h
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_045632-vtvfbj5h/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.7463536262512207, Uncertainty Bias: -0.061244964599609375
6.1035156e-05 0.0010484457
2.0706267 5.2234006
(48745, 22, 3)
Found uncertainty sample 0 after 109 steps.
Found uncertainty sample 1 after 76 steps.
Found uncertainty sample 2 after 2029 steps.
Found uncertainty sample 3 after 2391 steps.
Found uncertainty sample 4 after 2782 steps.
Found uncertainty sample 5 after 227 steps.
Found uncertainty sample 6 after 3144 steps.
Found uncertainty sample 7 after 1022 steps.
Found uncertainty sample 8 after 895 steps.
Found uncertainty sample 9 after 44 steps.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 712 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 536 steps.
Found uncertainty sample 15 after 2399 steps.
Found uncertainty sample 16 after 1014 steps.
Found uncertainty sample 17 after 508 steps.
Found uncertainty sample 18 after 689 steps.
Found uncertainty sample 19 after 62 steps.
Found uncertainty sample 20 after 213 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 651 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 3805 steps.
Found uncertainty sample 27 after 3666 steps.
Found uncertainty sample 28 after 2260 steps.
Found uncertainty sample 29 after 723 steps.
Found uncertainty sample 30 after 30 steps.
Found uncertainty sample 31 after 501 steps.
Found uncertainty sample 32 after 1950 steps.
Found uncertainty sample 33 after 92 steps.
Found uncertainty sample 34 after 22 steps.
Found uncertainty sample 35 after 2588 steps.
Found uncertainty sample 36 after 1636 steps.
Found uncertainty sample 37 after 2364 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 2972 steps.
Found uncertainty sample 40 after 3657 steps.
Found uncertainty sample 41 after 516 steps.
Found uncertainty sample 42 after 1870 steps.
Found uncertainty sample 43 after 116 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 449 steps.
Found uncertainty sample 46 after 225 steps.
Found uncertainty sample 47 after 692 steps.
Found uncertainty sample 48 after 14 steps.
Found uncertainty sample 49 after 223 steps.
Found uncertainty sample 50 after 3674 steps.
Found uncertainty sample 51 after 1557 steps.
Found uncertainty sample 52 after 3792 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 3768 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 24 steps.
Found uncertainty sample 57 after 1377 steps.
Found uncertainty sample 58 after 1309 steps.
Found uncertainty sample 59 after 1566 steps.
Found uncertainty sample 60 after 828 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 339 steps.
Found uncertainty sample 63 after 2625 steps.
Found uncertainty sample 64 after 1281 steps.
Found uncertainty sample 65 after 408 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 208 steps.
Found uncertainty sample 68 after 2496 steps.
Found uncertainty sample 69 after 1021 steps.
Found uncertainty sample 70 after 1088 steps.
Did not find any uncertainty samples for sample 71.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 433 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 1872 steps.
Found uncertainty sample 76 after 362 steps.
Found uncertainty sample 77 after 2100 steps.
Found uncertainty sample 78 after 942 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 555 steps.
Found uncertainty sample 81 after 1882 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 408 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 2271 steps.
Found uncertainty sample 86 after 3088 steps.
Found uncertainty sample 87 after 544 steps.
Found uncertainty sample 88 after 2617 steps.
Found uncertainty sample 89 after 1862 steps.
Found uncertainty sample 90 after 2655 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 3430 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 1372 steps.
Found uncertainty sample 95 after 409 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 3398 steps.
Found uncertainty sample 98 after 2571 steps.
Found uncertainty sample 99 after 3735 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_052359-j3xx5h99
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_76
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j3xx5h99
Training model 76. Added 78 samples to the dataset.
Epoch 0, Batch 100/227, Loss: 0.9002059698104858, Variance: 0.10377915948629379
Epoch 0, Batch 200/227, Loss: 0.876179575920105, Variance: 0.10722241550683975

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.558415347624077, Training Loss Force: 3.6711663668451333, time: 4.060591697692871
Validation Loss Energy: 3.1495224140048728, Validation Loss Force: 3.380988605548116, time: 0.2301487922668457
Test Loss Energy: 10.193657506850512, Test Loss Force: 12.053441527802361, time: 12.148761510848999

Epoch 1, Batch 100/227, Loss: 1.4868671894073486, Variance: 0.10957753658294678
Epoch 1, Batch 200/227, Loss: 0.4981513023376465, Variance: 0.10601966083049774

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6371930548612426, Training Loss Force: 3.3329741500928645, time: 3.992551326751709
Validation Loss Energy: 2.4789162046193858, Validation Loss Force: 3.3614539527104443, time: 0.24065685272216797
Test Loss Energy: 9.872501704414297, Test Loss Force: 11.909756145043797, time: 12.425536632537842

Epoch 2, Batch 100/227, Loss: 0.9185317754745483, Variance: 0.1120658665895462
Epoch 2, Batch 200/227, Loss: 1.2323805093765259, Variance: 0.1047624945640564

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.649431540164703, Training Loss Force: 3.3245030820526034, time: 4.03076171875
Validation Loss Energy: 2.2546937743968924, Validation Loss Force: 3.3902343851599026, time: 0.2416212558746338
Test Loss Energy: 10.299353723981259, Test Loss Force: 12.080439632469707, time: 12.337068319320679

Epoch 3, Batch 100/227, Loss: 0.6036880612373352, Variance: 0.10566945374011993
Epoch 3, Batch 200/227, Loss: 0.7481632828712463, Variance: 0.10952700674533844

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6584189544122583, Training Loss Force: 3.328823996447759, time: 4.0758445262908936
Validation Loss Energy: 3.591726748966088, Validation Loss Force: 3.3734933394270405, time: 0.24553894996643066
Test Loss Energy: 10.849998820771374, Test Loss Force: 12.283145316663182, time: 12.422735691070557

Epoch 4, Batch 100/227, Loss: 1.001387596130371, Variance: 0.10341548174619675
Epoch 4, Batch 200/227, Loss: 0.5968163013458252, Variance: 0.1132538914680481

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6257574713374585, Training Loss Force: 3.3420824393544115, time: 4.179027557373047
Validation Loss Energy: 2.9932759753129226, Validation Loss Force: 3.3325468771842393, time: 0.23817205429077148
Test Loss Energy: 10.679741934786463, Test Loss Force: 12.271601630519957, time: 12.407765626907349

Epoch 5, Batch 100/227, Loss: 0.9156465530395508, Variance: 0.11143019795417786
Epoch 5, Batch 200/227, Loss: 1.2079862356185913, Variance: 0.11551527678966522

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6484660173203203, Training Loss Force: 3.3332184083367338, time: 4.125086307525635
Validation Loss Energy: 1.4923525529412214, Validation Loss Force: 3.4365286922804517, time: 0.23812127113342285
Test Loss Energy: 9.979867963455646, Test Loss Force: 12.329254458941948, time: 12.152270555496216

Epoch 6, Batch 100/227, Loss: 0.6089068651199341, Variance: 0.10994929075241089
Epoch 6, Batch 200/227, Loss: 1.0510575771331787, Variance: 0.1123918667435646

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6283302745182833, Training Loss Force: 3.3713210062147057, time: 3.554377794265747
Validation Loss Energy: 3.2747371795147484, Validation Loss Force: 3.413039379743816, time: 0.19555401802062988
Test Loss Energy: 10.690299907755525, Test Loss Force: 12.500322727978476, time: 12.37085485458374

Epoch 7, Batch 100/227, Loss: 1.2739989757537842, Variance: 0.11217404156923294
Epoch 7, Batch 200/227, Loss: 0.6651724576950073, Variance: 0.11322362720966339

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.646809560944415, Training Loss Force: 3.337325952720338, time: 4.0832178592681885
Validation Loss Energy: 2.2650598024818414, Validation Loss Force: 3.320377229017275, time: 0.19597792625427246
Test Loss Energy: 10.21794681221247, Test Loss Force: 12.574341477485387, time: 10.224912643432617

Epoch 8, Batch 100/227, Loss: 1.0668025016784668, Variance: 0.11441358923912048
Epoch 8, Batch 200/227, Loss: 1.1600464582443237, Variance: 0.10855785012245178

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6625898339445198, Training Loss Force: 3.333278846824651, time: 3.707195997238159
Validation Loss Energy: 2.1404935608594804, Validation Loss Force: 3.34879187172726, time: 0.20594143867492676
Test Loss Energy: 10.361867916492654, Test Loss Force: 12.197529864771793, time: 10.188709735870361

Epoch 9, Batch 100/227, Loss: 0.5936223268508911, Variance: 0.10932435095310211
Epoch 9, Batch 200/227, Loss: 0.5596657395362854, Variance: 0.107330821454525

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6231142689028912, Training Loss Force: 3.3361413558067663, time: 3.7371881008148193
Validation Loss Energy: 3.763187613726419, Validation Loss Force: 3.378848421052031, time: 0.2006988525390625
Test Loss Energy: 11.498857234286502, Test Loss Force: 12.342115903422387, time: 10.421983480453491

Epoch 10, Batch 100/227, Loss: 1.0954389572143555, Variance: 0.10860767960548401
Epoch 10, Batch 200/227, Loss: 0.7729470729827881, Variance: 0.1088886559009552

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6371113117425753, Training Loss Force: 3.331539937143023, time: 3.737285614013672
Validation Loss Energy: 2.4952582829100316, Validation Loss Force: 3.330896063803617, time: 0.2041919231414795
Test Loss Energy: 10.659268819517138, Test Loss Force: 12.123144454959002, time: 10.074131488800049

Epoch 11, Batch 100/227, Loss: 0.8348171710968018, Variance: 0.10603655874729156
Epoch 11, Batch 200/227, Loss: 1.2366098165512085, Variance: 0.11639636754989624

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6529057100100863, Training Loss Force: 3.3464557163120285, time: 3.7730300426483154
Validation Loss Energy: 1.6409758299214041, Validation Loss Force: 3.432997382295183, time: 0.19855713844299316
Test Loss Energy: 10.344928632849241, Test Loss Force: 12.816917566821862, time: 10.303217649459839

Epoch 12, Batch 100/227, Loss: 0.7373480796813965, Variance: 0.11245957016944885
Epoch 12, Batch 200/227, Loss: 0.9768852591514587, Variance: 0.1199745312333107

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6646410930158293, Training Loss Force: 3.3291868117884755, time: 3.7293150424957275
Validation Loss Energy: 3.3089417210244347, Validation Loss Force: 3.3354754774587683, time: 0.19698286056518555
Test Loss Energy: 10.4027684847009, Test Loss Force: 12.365033035237419, time: 10.167115926742554

Epoch 13, Batch 100/227, Loss: 1.434287667274475, Variance: 0.11216824501752853
Epoch 13, Batch 200/227, Loss: 0.695279598236084, Variance: 0.11267554014921188

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.653669407306259, Training Loss Force: 3.3380329429534, time: 3.7926652431488037
Validation Loss Energy: 1.9444073812472764, Validation Loss Force: 3.3668403193504375, time: 0.193572998046875
Test Loss Energy: 9.995731664191277, Test Loss Force: 12.204116138314644, time: 10.279174566268921

Epoch 14, Batch 100/227, Loss: 0.9934412240982056, Variance: 0.11378692090511322
Epoch 14, Batch 200/227, Loss: 1.0696535110473633, Variance: 0.10799956321716309

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6539528273635535, Training Loss Force: 3.3383024667884222, time: 3.7904787063598633
Validation Loss Energy: 2.047735217061899, Validation Loss Force: 3.3928283693753882, time: 0.2051987648010254
Test Loss Energy: 10.269257361180035, Test Loss Force: 12.266595659724477, time: 10.110689163208008

Epoch 15, Batch 100/227, Loss: 0.5803833603858948, Variance: 0.10959303379058838
Epoch 15, Batch 200/227, Loss: 0.7264938950538635, Variance: 0.10611617565155029

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.632481836766444, Training Loss Force: 3.3446463411644665, time: 3.74647855758667
Validation Loss Energy: 3.4866498470102023, Validation Loss Force: 3.370129142635791, time: 0.19513964653015137
Test Loss Energy: 11.108253891048543, Test Loss Force: 12.414538658419072, time: 10.878360748291016

Epoch 16, Batch 100/227, Loss: 1.1433764696121216, Variance: 0.11288917064666748
Epoch 16, Batch 200/227, Loss: 0.7843077182769775, Variance: 0.11247426271438599

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.65406901161166, Training Loss Force: 3.3360022287892575, time: 3.974228858947754
Validation Loss Energy: 2.6160446601129816, Validation Loss Force: 3.367769016690816, time: 0.24065947532653809
Test Loss Energy: 10.633298033345838, Test Loss Force: 12.602774816534431, time: 12.323888540267944

Epoch 17, Batch 100/227, Loss: 0.8327458500862122, Variance: 0.10969679057598114
Epoch 17, Batch 200/227, Loss: 1.3529694080352783, Variance: 0.11871310323476791

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.67523772688133, Training Loss Force: 3.3167049868739875, time: 4.015644073486328
Validation Loss Energy: 1.7315504017375858, Validation Loss Force: 3.4119077194769245, time: 0.23102307319641113
Test Loss Energy: 9.960470897416918, Test Loss Force: 12.45250939030587, time: 12.16382622718811

Epoch 18, Batch 100/227, Loss: 0.7266159057617188, Variance: 0.10927842557430267
Epoch 18, Batch 200/227, Loss: 0.9343246221542358, Variance: 0.11187805235385895

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6355785054071545, Training Loss Force: 3.3283396066961304, time: 3.5370755195617676
Validation Loss Energy: 3.498820831192042, Validation Loss Force: 3.3438418252787123, time: 0.2122950553894043
Test Loss Energy: 10.246536164223738, Test Loss Force: 12.276708327044279, time: 10.754772663116455

Epoch 19, Batch 100/227, Loss: 1.3703171014785767, Variance: 0.11378104984760284
Epoch 19, Batch 200/227, Loss: 0.5346570014953613, Variance: 0.10745660960674286

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.631131169378246, Training Loss Force: 3.3310699375075643, time: 3.7484066486358643
Validation Loss Energy: 2.2136255677985543, Validation Loss Force: 3.386660935038767, time: 0.21432161331176758
Test Loss Energy: 10.322354402192484, Test Loss Force: 12.788921195570074, time: 10.974637985229492

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–ƒâ–…â–„â–â–…â–‚â–ƒâ–ˆâ–„â–ƒâ–ƒâ–‚â–ƒâ–†â–„â–â–ƒâ–ƒ
wandb:   test_error_force â–‚â–â–‚â–„â–„â–„â–†â–†â–ƒâ–„â–ƒâ–ˆâ–…â–ƒâ–„â–…â–†â–…â–„â–ˆ
wandb:          test_loss â–†â–ƒâ–…â–„â–…â–ƒâ–ˆâ–„â–â–ˆâ–ƒâ–‚â–„â–‚â–â–‡â–†â–â–‚â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–‚â–â–‚â–â–â–â–â–‚â–â–â–â–‚â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–„â–ƒâ–‡â–†â–â–†â–ƒâ–ƒâ–ˆâ–„â–â–‡â–‚â–ƒâ–‡â–„â–‚â–‡â–ƒ
wandb:  valid_error_force â–…â–ƒâ–…â–„â–‚â–ˆâ–‡â–â–ƒâ–…â–‚â–ˆâ–‚â–„â–…â–„â–„â–‡â–‚â–…
wandb:         valid_loss â–†â–ƒâ–ƒâ–‡â–…â–â–†â–ƒâ–‚â–ˆâ–ƒâ–â–†â–‚â–‚â–‡â–„â–‚â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7262
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.32235
wandb:   test_error_force 12.78892
wandb:          test_loss 13.1738
wandb: train_error_energy 2.63113
wandb:  train_error_force 3.33107
wandb:         train_loss 0.9309
wandb: valid_error_energy 2.21363
wandb:  valid_error_force 3.38666
wandb:         valid_loss 0.78943
wandb: 
wandb: ğŸš€ View run al_71_76 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j3xx5h99
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_052359-j3xx5h99/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.242788076400757, Uncertainty Bias: -0.12316174805164337
7.05719e-05 0.029888153
1.9438194 4.9149857
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 27 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 626 steps.
Found uncertainty sample 4 after 254 steps.
Found uncertainty sample 5 after 2434 steps.
Found uncertainty sample 6 after 140 steps.
Found uncertainty sample 7 after 277 steps.
Found uncertainty sample 8 after 1159 steps.
Found uncertainty sample 9 after 136 steps.
Found uncertainty sample 10 after 1061 steps.
Found uncertainty sample 11 after 2244 steps.
Found uncertainty sample 12 after 289 steps.
Found uncertainty sample 13 after 1327 steps.
Found uncertainty sample 14 after 2485 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 2105 steps.
Found uncertainty sample 17 after 191 steps.
Found uncertainty sample 18 after 3655 steps.
Found uncertainty sample 19 after 2599 steps.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 3215 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 50 steps.
Found uncertainty sample 26 after 630 steps.
Found uncertainty sample 27 after 3420 steps.
Found uncertainty sample 28 after 1293 steps.
Found uncertainty sample 29 after 2286 steps.
Found uncertainty sample 30 after 1116 steps.
Found uncertainty sample 31 after 2402 steps.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 1144 steps.
Found uncertainty sample 35 after 2358 steps.
Found uncertainty sample 36 after 147 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 1060 steps.
Found uncertainty sample 39 after 629 steps.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 922 steps.
Found uncertainty sample 43 after 889 steps.
Found uncertainty sample 44 after 3955 steps.
Found uncertainty sample 45 after 496 steps.
Found uncertainty sample 46 after 12 steps.
Found uncertainty sample 47 after 2578 steps.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 1063 steps.
Found uncertainty sample 51 after 455 steps.
Found uncertainty sample 52 after 915 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 921 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 2495 steps.
Found uncertainty sample 57 after 1321 steps.
Found uncertainty sample 58 after 764 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 644 steps.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 584 steps.
Found uncertainty sample 65 after 653 steps.
Found uncertainty sample 66 after 2863 steps.
Found uncertainty sample 67 after 2603 steps.
Found uncertainty sample 68 after 1288 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 321 steps.
Found uncertainty sample 71 after 1932 steps.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 15 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 1814 steps.
Found uncertainty sample 77 after 439 steps.
Found uncertainty sample 78 after 853 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 2895 steps.
Found uncertainty sample 81 after 162 steps.
Found uncertainty sample 82 after 3374 steps.
Found uncertainty sample 83 after 484 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 564 steps.
Found uncertainty sample 86 after 44 steps.
Found uncertainty sample 87 after 193 steps.
Found uncertainty sample 88 after 1617 steps.
Found uncertainty sample 89 after 1183 steps.
Found uncertainty sample 90 after 3185 steps.
Found uncertainty sample 91 after 626 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 507 steps.
Found uncertainty sample 94 after 1094 steps.
Found uncertainty sample 95 after 167 steps.
Found uncertainty sample 96 after 3389 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 3798 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_055121-87b0pzm0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_77
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/87b0pzm0
Training model 77. Added 74 samples to the dataset.
Epoch 0, Batch 100/229, Loss: 0.5990184545516968, Variance: 0.1123591959476471
Epoch 0, Batch 200/229, Loss: 0.9832837581634521, Variance: 0.11649250239133835

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.969944093022609, Training Loss Force: 3.3811216075792156, time: 3.8184945583343506
Validation Loss Energy: 2.885468314618665, Validation Loss Force: 3.381513826396073, time: 0.2160944938659668
Test Loss Energy: 10.760961977012117, Test Loss Force: 12.34304748118053, time: 10.70752477645874

Epoch 1, Batch 100/229, Loss: 0.7355652451515198, Variance: 0.10688953846693039
Epoch 1, Batch 200/229, Loss: 1.5384767055511475, Variance: 0.1137101799249649

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6254560464988117, Training Loss Force: 3.327138182128241, time: 3.7171831130981445
Validation Loss Energy: 3.6070506725869786, Validation Loss Force: 3.4076667072562974, time: 0.2117598056793213
Test Loss Energy: 10.83684313591879, Test Loss Force: 12.27618187656134, time: 11.986640453338623

Epoch 2, Batch 100/229, Loss: 1.0036035776138306, Variance: 0.11127772927284241
Epoch 2, Batch 200/229, Loss: 0.7463293671607971, Variance: 0.11490108072757721

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.648518819999488, Training Loss Force: 3.3271071426327112, time: 3.7318058013916016
Validation Loss Energy: 2.116412338065682, Validation Loss Force: 3.3858851557343543, time: 0.24277758598327637
Test Loss Energy: 10.244073247614192, Test Loss Force: 12.462974285240321, time: 10.736781120300293

Epoch 3, Batch 100/229, Loss: 0.6969888210296631, Variance: 0.1120600774884224
Epoch 3, Batch 200/229, Loss: 0.6896679997444153, Variance: 0.10996688902378082

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6317215682958346, Training Loss Force: 3.343739697442903, time: 3.7840092182159424
Validation Loss Energy: 2.3334805756313752, Validation Loss Force: 3.3883574512615904, time: 0.21213603019714355
Test Loss Energy: 10.663197090299244, Test Loss Force: 13.076857759889585, time: 10.909975528717041

Epoch 4, Batch 100/229, Loss: 0.8564811944961548, Variance: 0.11262962222099304
Epoch 4, Batch 200/229, Loss: 1.2244764566421509, Variance: 0.10805165767669678

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.660151025553593, Training Loss Force: 3.335001207249434, time: 3.7245099544525146
Validation Loss Energy: 3.312896325056112, Validation Loss Force: 3.353698330690493, time: 0.20653986930847168
Test Loss Energy: 10.667239073900689, Test Loss Force: 12.556955890261877, time: 10.816861152648926

Epoch 5, Batch 100/229, Loss: 1.1663093566894531, Variance: 0.11433674395084381
Epoch 5, Batch 200/229, Loss: 0.5963430404663086, Variance: 0.11183032393455505

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6782777102662254, Training Loss Force: 3.3266937082594015, time: 3.7503774166107178
Validation Loss Energy: 2.0325747882614054, Validation Loss Force: 3.338554713410517, time: 0.22239923477172852
Test Loss Energy: 9.970711505065262, Test Loss Force: 12.139318833946136, time: 11.0016610622406

Epoch 6, Batch 100/229, Loss: 0.6477202773094177, Variance: 0.11376936733722687
Epoch 6, Batch 200/229, Loss: 1.0508638620376587, Variance: 0.11171819269657135

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.645578686873114, Training Loss Force: 3.341209044124701, time: 3.786224842071533
Validation Loss Energy: 2.863335279849875, Validation Loss Force: 3.3338634641768237, time: 0.20600128173828125
Test Loss Energy: 10.678522570563992, Test Loss Force: 12.057442544944188, time: 10.838986158370972

Epoch 7, Batch 100/229, Loss: 0.8989375829696655, Variance: 0.11183619499206543
Epoch 7, Batch 200/229, Loss: 1.2724461555480957, Variance: 0.11743131279945374

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.646048917436304, Training Loss Force: 3.343909155469434, time: 3.751553773880005
Validation Loss Energy: 3.6265464139913997, Validation Loss Force: 3.3645230572160534, time: 0.21859502792358398
Test Loss Energy: 10.890033884501657, Test Loss Force: 12.30377833402704, time: 10.870133638381958

Epoch 8, Batch 100/229, Loss: 1.1093971729278564, Variance: 0.11051832139492035
Epoch 8, Batch 200/229, Loss: 0.7906467914581299, Variance: 0.11295269429683685

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.64459189840995, Training Loss Force: 3.3232286788083347, time: 3.7415900230407715
Validation Loss Energy: 2.2314315843891026, Validation Loss Force: 3.336482220334794, time: 0.20667171478271484
Test Loss Energy: 10.652954398187902, Test Loss Force: 12.45432932236008, time: 10.836249351501465

Epoch 9, Batch 100/229, Loss: 0.672452449798584, Variance: 0.11079937219619751
Epoch 9, Batch 200/229, Loss: 0.8112056255340576, Variance: 0.10900133848190308

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.633295106974719, Training Loss Force: 3.3375875074346766, time: 3.7798030376434326
Validation Loss Energy: 2.1352999700325745, Validation Loss Force: 3.424594182731587, time: 0.208632230758667
Test Loss Energy: 9.91971491726184, Test Loss Force: 12.2592785883763, time: 10.923210620880127

Epoch 10, Batch 100/229, Loss: 0.896654486656189, Variance: 0.11440761387348175
Epoch 10, Batch 200/229, Loss: 1.4814574718475342, Variance: 0.11203716695308685

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6657018022425145, Training Loss Force: 3.3404011304537615, time: 3.7245776653289795
Validation Loss Energy: 3.4117202877233392, Validation Loss Force: 3.30542109915817, time: 0.21433138847351074
Test Loss Energy: 10.808697093695113, Test Loss Force: 12.529278427859346, time: 10.714138984680176

Epoch 11, Batch 100/229, Loss: 1.382097601890564, Variance: 0.11584382504224777
Epoch 11, Batch 200/229, Loss: 0.616139829158783, Variance: 0.11234892904758453

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.633189538463045, Training Loss Force: 3.327210308456789, time: 3.7473278045654297
Validation Loss Energy: 1.830302470104322, Validation Loss Force: 3.379307564830934, time: 0.2088031768798828
Test Loss Energy: 10.313258248669015, Test Loss Force: 12.623107778480593, time: 10.707730293273926

Epoch 12, Batch 100/229, Loss: 0.6551938056945801, Variance: 0.11033985018730164
Epoch 12, Batch 200/229, Loss: 0.9525034427642822, Variance: 0.11782130599021912

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.617383419701787, Training Loss Force: 3.3408290315150757, time: 3.892646551132202
Validation Loss Energy: 2.765420864976476, Validation Loss Force: 3.3193329107228733, time: 0.2099597454071045
Test Loss Energy: 10.442783328673768, Test Loss Force: 12.314511234011851, time: 10.751550912857056

Epoch 13, Batch 100/229, Loss: 0.9674423933029175, Variance: 0.10991761833429337
Epoch 13, Batch 200/229, Loss: 1.3322399854660034, Variance: 0.11439293622970581

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6397007528122716, Training Loss Force: 3.3215126513231428, time: 3.734171152114868
Validation Loss Energy: 4.079965647489127, Validation Loss Force: 3.343622162166255, time: 0.21261286735534668
Test Loss Energy: 11.316937843104162, Test Loss Force: 12.491160454542543, time: 10.868858814239502

Epoch 14, Batch 100/229, Loss: 1.0640650987625122, Variance: 0.10856401920318604
Epoch 14, Batch 200/229, Loss: 0.81389981508255, Variance: 0.11342626810073853

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6297017204292765, Training Loss Force: 3.352455808901028, time: 3.9335038661956787
Validation Loss Energy: 1.90300536576555, Validation Loss Force: 3.3905055961796426, time: 0.2145986557006836
Test Loss Energy: 10.483266407388887, Test Loss Force: 12.307932076555211, time: 10.857182264328003

Epoch 15, Batch 100/229, Loss: 0.6996849775314331, Variance: 0.11074163764715195
Epoch 15, Batch 200/229, Loss: 0.6894627809524536, Variance: 0.10840891301631927

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6283056879538704, Training Loss Force: 3.338020694214232, time: 3.8379528522491455
Validation Loss Energy: 2.2753896468305777, Validation Loss Force: 3.3174318172222286, time: 0.2084364891052246
Test Loss Energy: 10.277655345505805, Test Loss Force: 12.403255394917181, time: 10.80523157119751

Epoch 16, Batch 100/229, Loss: 0.997989296913147, Variance: 0.11763035506010056
Epoch 16, Batch 200/229, Loss: 1.3132219314575195, Variance: 0.11145340651273727

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.644950295582396, Training Loss Force: 3.3388563851318604, time: 3.9338760375976562
Validation Loss Energy: 3.277113114969084, Validation Loss Force: 3.3630472617841303, time: 0.20739459991455078
Test Loss Energy: 10.376397854691936, Test Loss Force: 12.267763555897446, time: 10.927858829498291

Epoch 17, Batch 100/229, Loss: 1.2612740993499756, Variance: 0.11394809186458588
Epoch 17, Batch 200/229, Loss: 0.7235779762268066, Variance: 0.110928975045681

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.65717693100687, Training Loss Force: 3.3416875465063383, time: 3.762108087539673
Validation Loss Energy: 1.9474713643625292, Validation Loss Force: 3.41696492545195, time: 0.21173906326293945
Test Loss Energy: 10.115074016535942, Test Loss Force: 12.212300989796132, time: 11.925697803497314

Epoch 18, Batch 100/229, Loss: 0.8897740840911865, Variance: 0.11631342768669128
Epoch 18, Batch 200/229, Loss: 0.7456515431404114, Variance: 0.11288988590240479

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6525827215217674, Training Loss Force: 3.32752387912139, time: 3.9055604934692383
Validation Loss Energy: 2.6428696677901886, Validation Loss Force: 3.3241545327415074, time: 0.20972275733947754
Test Loss Energy: 10.689927334476298, Test Loss Force: 12.192022317152517, time: 10.958868265151978

Epoch 19, Batch 100/229, Loss: 0.646786093711853, Variance: 0.11099459230899811
Epoch 19, Batch 200/229, Loss: 1.142580509185791, Variance: 0.11550906300544739

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6682801971869, Training Loss Force: 3.3187270291922473, time: 3.7010915279388428
Validation Loss Energy: 3.5831495334649492, Validation Loss Force: 3.3632876331271593, time: 0.22499608993530273
Test Loss Energy: 11.02150814725628, Test Loss Force: 12.266765671697712, time: 10.788272857666016

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–†â–ƒâ–…â–…â–â–…â–†â–…â–â–…â–ƒâ–„â–ˆâ–„â–ƒâ–ƒâ–‚â–…â–‡
wandb:   test_error_force â–ƒâ–ƒâ–„â–ˆâ–„â–‚â–â–ƒâ–„â–‚â–„â–…â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚
wandb:          test_loss â–„â–„â–ƒâ–ˆâ–‡â–‚â–ƒâ–…â–„â–â–…â–„â–ƒâ–ˆâ–ƒâ–„â–ƒâ–â–…â–ƒ
wandb: train_error_energy â–ˆâ–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–„â–ƒâ–‚â–„â–„â–‚â–ƒâ–ƒâ–‚â–ƒâ–â–…â–ƒâ–ƒâ–„â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–‡â–‚â–ƒâ–†â–‚â–„â–‡â–‚â–‚â–†â–â–„â–ˆâ–â–‚â–†â–â–„â–†
wandb:  valid_error_force â–…â–‡â–†â–†â–„â–ƒâ–ƒâ–„â–ƒâ–ˆâ–â–…â–‚â–ƒâ–†â–‚â–„â–ˆâ–‚â–„
wandb:         valid_loss â–„â–†â–‚â–‚â–…â–â–ƒâ–†â–‚â–‚â–…â–â–ƒâ–ˆâ–â–‚â–…â–â–ƒâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 7328
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.02151
wandb:   test_error_force 12.26677
wandb:          test_loss 12.70847
wandb: train_error_energy 2.66828
wandb:  train_error_force 3.31873
wandb:         train_loss 0.94181
wandb: valid_error_energy 3.58315
wandb:  valid_error_force 3.36329
wandb:         valid_loss 1.29421
wandb: 
wandb: ğŸš€ View run al_71_77 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/87b0pzm0
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_055121-87b0pzm0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.412604331970215, Uncertainty Bias: -0.17343443632125854
7.6293945e-06 0.008969784
1.936761 4.894568
(48745, 22, 3)
Found uncertainty sample 0 after 2314 steps.
Found uncertainty sample 1 after 1651 steps.
Found uncertainty sample 2 after 1178 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 1851 steps.
Found uncertainty sample 5 after 445 steps.
Found uncertainty sample 6 after 2562 steps.
Found uncertainty sample 7 after 1775 steps.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 2006 steps.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 605 steps.
Found uncertainty sample 14 after 473 steps.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 1424 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 842 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 1814 steps.
Found uncertainty sample 22 after 1283 steps.
Found uncertainty sample 23 after 1645 steps.
Found uncertainty sample 24 after 3466 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 3726 steps.
Found uncertainty sample 27 after 3542 steps.
Found uncertainty sample 28 after 705 steps.
Found uncertainty sample 29 after 3 steps.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 808 steps.
Found uncertainty sample 33 after 3923 steps.
Found uncertainty sample 34 after 2354 steps.
Did not find any uncertainty samples for sample 35.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 2519 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 3153 steps.
Found uncertainty sample 40 after 1422 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 1961 steps.
Found uncertainty sample 43 after 632 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 21 steps.
Found uncertainty sample 46 after 316 steps.
Found uncertainty sample 47 after 1253 steps.
Found uncertainty sample 48 after 872 steps.
Found uncertainty sample 49 after 1174 steps.
Found uncertainty sample 50 after 593 steps.
Found uncertainty sample 51 after 2077 steps.
Found uncertainty sample 52 after 662 steps.
Found uncertainty sample 53 after 629 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 24 steps.
Found uncertainty sample 56 after 207 steps.
Found uncertainty sample 57 after 3996 steps.
Found uncertainty sample 58 after 1293 steps.
Found uncertainty sample 59 after 3613 steps.
Found uncertainty sample 60 after 1999 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 945 steps.
Found uncertainty sample 63 after 1410 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 1834 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 2250 steps.
Found uncertainty sample 69 after 1 steps.
Did not find any uncertainty samples for sample 70.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 2985 steps.
Found uncertainty sample 73 after 2257 steps.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 899 steps.
Found uncertainty sample 78 after 32 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 10 steps.
Found uncertainty sample 82 after 179 steps.
Found uncertainty sample 83 after 3860 steps.
Found uncertainty sample 84 after 1813 steps.
Found uncertainty sample 85 after 3560 steps.
Did not find any uncertainty samples for sample 86.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 3285 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 3122 steps.
Found uncertainty sample 92 after 60 steps.
Found uncertainty sample 93 after 100 steps.
Did not find any uncertainty samples for sample 94.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 2999 steps.
Found uncertainty sample 97 after 1836 steps.
Found uncertainty sample 98 after 2234 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_062229-hnjckxij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_78
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hnjckxij
Training model 78. Added 66 samples to the dataset.
Epoch 0, Batch 100/231, Loss: 0.483401894569397, Variance: 0.09352569282054901
Epoch 0, Batch 200/231, Loss: 0.46959853172302246, Variance: 0.08490578830242157

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.194864134052085, Training Loss Force: 3.536821968153027, time: 3.6845595836639404
Validation Loss Energy: 1.6726559713237072, Validation Loss Force: 3.33445231273323, time: 0.21815896034240723
Test Loss Energy: 9.924202369725531, Test Loss Force: 12.422475584710776, time: 11.806788206100464

Epoch 1, Batch 100/231, Loss: 0.4105224609375, Variance: 0.07951082289218903
Epoch 1, Batch 200/231, Loss: 0.30631858110427856, Variance: 0.0780957043170929

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6090659333613755, Training Loss Force: 3.3267462700228845, time: 3.6621739864349365
Validation Loss Energy: 1.5269136712544678, Validation Loss Force: 3.336190913424634, time: 0.208115816116333
Test Loss Energy: 9.964726461532557, Test Loss Force: 12.592910466115582, time: 10.947338819503784

Epoch 2, Batch 100/231, Loss: 0.3775082230567932, Variance: 0.08153581619262695
Epoch 2, Batch 200/231, Loss: 0.5377210378646851, Variance: 0.07986839860677719

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6158064662680711, Training Loss Force: 3.3254779311447087, time: 3.7260587215423584
Validation Loss Energy: 1.7960909423501819, Validation Loss Force: 3.4048665314116824, time: 0.22129511833190918
Test Loss Energy: 10.268175161151456, Test Loss Force: 12.644066783474093, time: 10.733134984970093

Epoch 3, Batch 100/231, Loss: 0.28617221117019653, Variance: 0.07930894196033478
Epoch 3, Batch 200/231, Loss: 0.6626694202423096, Variance: 0.07983455061912537

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6157521258390757, Training Loss Force: 3.338713845939919, time: 3.7627780437469482
Validation Loss Energy: 1.874666124395104, Validation Loss Force: 3.3608045381463456, time: 0.21834683418273926
Test Loss Energy: 10.160609567758028, Test Loss Force: 12.439058947462819, time: 10.904910802841187

Epoch 4, Batch 100/231, Loss: 0.4012036919593811, Variance: 0.07801772654056549
Epoch 4, Batch 200/231, Loss: 0.455346941947937, Variance: 0.07884418964385986

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.627364689540678, Training Loss Force: 3.3218381475476235, time: 3.8278658390045166
Validation Loss Energy: 1.3929101823884655, Validation Loss Force: 3.3357655040213388, time: 0.2161874771118164
Test Loss Energy: 9.851184558832816, Test Loss Force: 12.649109076153925, time: 10.687096357345581

Epoch 5, Batch 100/231, Loss: 0.6924154162406921, Variance: 0.07731775939464569
Epoch 5, Batch 200/231, Loss: 0.7075191736221313, Variance: 0.0782923549413681

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6105260930333467, Training Loss Force: 3.3498797869053503, time: 3.6866495609283447
Validation Loss Energy: 1.5643404023075889, Validation Loss Force: 3.369823974571297, time: 0.21663188934326172
Test Loss Energy: 9.992281971191545, Test Loss Force: 12.813861170487415, time: 10.854390621185303

Epoch 6, Batch 100/231, Loss: 0.5956095457077026, Variance: 0.07766007632017136
Epoch 6, Batch 200/231, Loss: 0.3370829224586487, Variance: 0.07748159766197205

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6356749951031548, Training Loss Force: 3.3445405268389172, time: 3.8132216930389404
Validation Loss Energy: 1.5717447080825524, Validation Loss Force: 3.3618347169499807, time: 0.21787571907043457
Test Loss Energy: 9.986204301957361, Test Loss Force: 12.255311957879126, time: 10.793612480163574

Epoch 7, Batch 100/231, Loss: 0.4695689082145691, Variance: 0.07790809124708176
Epoch 7, Batch 200/231, Loss: 0.45789217948913574, Variance: 0.07677586376667023

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6210677053792169, Training Loss Force: 3.3276524216302694, time: 3.6879777908325195
Validation Loss Energy: 1.802708474531653, Validation Loss Force: 3.390163075365919, time: 0.21338224411010742
Test Loss Energy: 10.120906614475992, Test Loss Force: 12.502247545361074, time: 10.914552450180054

Epoch 8, Batch 100/231, Loss: 0.3797842264175415, Variance: 0.07672323286533356
Epoch 8, Batch 200/231, Loss: 0.46544384956359863, Variance: 0.07875493913888931

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6357297637605832, Training Loss Force: 3.3245858125131953, time: 3.709078311920166
Validation Loss Energy: 1.3010819276134242, Validation Loss Force: 3.4301496109206253, time: 0.21825265884399414
Test Loss Energy: 9.95957306277794, Test Loss Force: 12.890613317828398, time: 10.706734895706177

Epoch 9, Batch 100/231, Loss: 0.5206080079078674, Variance: 0.07884754240512848
Epoch 9, Batch 200/231, Loss: 0.4639361500740051, Variance: 0.0769050121307373

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6253357109321414, Training Loss Force: 3.3401185807552367, time: 3.8620169162750244
Validation Loss Energy: 1.6592227870881757, Validation Loss Force: 3.386648576901168, time: 0.22260570526123047
Test Loss Energy: 10.047145550295467, Test Loss Force: 12.74135098462545, time: 10.830016613006592

Epoch 10, Batch 100/231, Loss: 0.7083756923675537, Variance: 0.07799707353115082
Epoch 10, Batch 200/231, Loss: 0.6034696698188782, Variance: 0.08157994598150253

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6238602444432748, Training Loss Force: 3.3461003733668613, time: 4.005078077316284
Validation Loss Energy: 1.8393752636233658, Validation Loss Force: 3.3671525571603143, time: 0.21721649169921875
Test Loss Energy: 10.38184992388547, Test Loss Force: 12.704287679638016, time: 10.808234930038452

Epoch 11, Batch 100/231, Loss: 0.7162240743637085, Variance: 0.0839461013674736
Epoch 11, Batch 200/231, Loss: 0.6574602723121643, Variance: 0.07792987674474716

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6161370166873295, Training Loss Force: 3.334381635522048, time: 3.811398506164551
Validation Loss Energy: 1.9517748909458754, Validation Loss Force: 3.4187669868499433, time: 0.2063448429107666
Test Loss Energy: 10.661385985735986, Test Loss Force: 13.286356671692849, time: 10.688422918319702

Epoch 12, Batch 100/231, Loss: 0.14312171936035156, Variance: 0.07486622780561447
Epoch 12, Batch 200/231, Loss: 0.46055907011032104, Variance: 0.07636359333992004

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6113203900244597, Training Loss Force: 3.326117692374773, time: 3.9909682273864746
Validation Loss Energy: 1.549525277685143, Validation Loss Force: 3.315786069564676, time: 0.21035218238830566
Test Loss Energy: 10.159529773955198, Test Loss Force: 12.727331954369562, time: 10.696342706680298

Epoch 13, Batch 100/231, Loss: 0.23397672176361084, Variance: 0.0766240656375885
Epoch 13, Batch 200/231, Loss: 0.3438013792037964, Variance: 0.07665302604436874

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6255603040937197, Training Loss Force: 3.3463866217834695, time: 3.788968563079834
Validation Loss Energy: 1.504478223951865, Validation Loss Force: 3.4419295880822167, time: 0.21297955513000488
Test Loss Energy: 10.267492007548523, Test Loss Force: 13.091467667789045, time: 10.720144748687744

Epoch 14, Batch 100/231, Loss: 0.35703063011169434, Variance: 0.0788663849234581
Epoch 14, Batch 200/231, Loss: 0.7486226558685303, Variance: 0.07669125497341156

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6427749336032043, Training Loss Force: 3.355660110208824, time: 3.90594220161438
Validation Loss Energy: 1.5975339702274767, Validation Loss Force: 3.507607355690353, time: 0.21017694473266602
Test Loss Energy: 10.160501553403535, Test Loss Force: 12.413339733125751, time: 10.747557878494263

Epoch 15, Batch 100/231, Loss: 0.41977065801620483, Variance: 0.07654688507318497
Epoch 15, Batch 200/231, Loss: 0.3470614552497864, Variance: 0.07724982500076294

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6120701941536544, Training Loss Force: 3.3513835545462705, time: 3.79384708404541
Validation Loss Energy: 2.1183019084380255, Validation Loss Force: 3.3267362701539676, time: 0.21060395240783691
Test Loss Energy: 10.256133003894412, Test Loss Force: 12.698193614493526, time: 10.725712537765503

Epoch 16, Batch 100/231, Loss: 0.34005671739578247, Variance: 0.07601994276046753
Epoch 16, Batch 200/231, Loss: 0.393821120262146, Variance: 0.07423446327447891

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6257798399666656, Training Loss Force: 3.328879950362856, time: 4.065133094787598
Validation Loss Energy: 1.4929168522242888, Validation Loss Force: 3.3801897243198136, time: 0.2223517894744873
Test Loss Energy: 10.03245050117577, Test Loss Force: 12.865767302295795, time: 10.897933006286621

Epoch 17, Batch 100/231, Loss: 0.5842316150665283, Variance: 0.0761411115527153
Epoch 17, Batch 200/231, Loss: 0.8341852426528931, Variance: 0.07556521147489548

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.633933213182968, Training Loss Force: 3.3423016252061695, time: 3.6918036937713623
Validation Loss Energy: 1.4878824420030776, Validation Loss Force: 3.3726854400983286, time: 0.21344375610351562
Test Loss Energy: 9.781359487740277, Test Loss Force: 12.57162984293726, time: 12.073554992675781

Epoch 18, Batch 100/231, Loss: 0.3601149916648865, Variance: 0.07840517163276672
Epoch 18, Batch 200/231, Loss: 0.7550201416015625, Variance: 0.0786280557513237

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.622128129593206, Training Loss Force: 3.33345303096048, time: 3.931809663772583
Validation Loss Energy: 1.7513085587304826, Validation Loss Force: 3.410710928484849, time: 0.22262024879455566
Test Loss Energy: 10.59678574472743, Test Loss Force: 13.101118605569916, time: 10.713489294052124

Epoch 19, Batch 100/231, Loss: 0.49562084674835205, Variance: 0.07801249623298645
Epoch 19, Batch 200/231, Loss: 0.6402359008789062, Variance: 0.07582437247037888

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6187277936477689, Training Loss Force: 3.3521851133469545, time: 3.8248214721679688
Validation Loss Energy: 1.6189872795946783, Validation Loss Force: 3.3512940605873376, time: 0.21202993392944336
Test Loss Energy: 10.1862124951738, Test Loss Force: 12.648952428992667, time: 10.651827096939087

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–…â–„â–‚â–ƒâ–ƒâ–„â–‚â–ƒâ–†â–ˆâ–„â–…â–„â–…â–ƒâ–â–‡â–„
wandb:   test_error_force â–‚â–ƒâ–„â–‚â–„â–…â–â–ƒâ–…â–„â–„â–ˆâ–„â–‡â–‚â–„â–…â–ƒâ–‡â–„
wandb:          test_loss â–â–„â–…â–…â–…â–…â–‚â–ƒâ–…â–†â–†â–‡â–‡â–ˆâ–„â–‡â–…â–…â–‡â–…
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–‚â–â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚â–‚â–â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–ƒâ–…â–†â–‚â–ƒâ–ƒâ–…â–â–„â–†â–‡â–ƒâ–ƒâ–„â–ˆâ–ƒâ–ƒâ–…â–„
wandb:  valid_error_force â–‚â–‚â–„â–ƒâ–‚â–ƒâ–ƒâ–„â–…â–„â–ƒâ–…â–â–†â–ˆâ–â–ƒâ–ƒâ–„â–‚
wandb:         valid_loss â–„â–‚â–…â–…â–â–ƒâ–‚â–…â–â–„â–…â–†â–‚â–ƒâ–ƒâ–ˆâ–‚â–‚â–…â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7387
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.18621
wandb:   test_error_force 12.64895
wandb:          test_loss 16.66145
wandb: train_error_energy 1.61873
wandb:  train_error_force 3.35219
wandb:         train_loss 0.49754
wandb: valid_error_energy 1.61899
wandb:  valid_error_force 3.35129
wandb:         valid_loss 0.47979
wandb: 
wandb: ğŸš€ View run al_71_78 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/hnjckxij
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_062229-hnjckxij/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.687777042388916, Uncertainty Bias: -0.06163252890110016
0.00012207031 0.0024662018
2.0839756 5.1120567
(48745, 22, 3)
Found uncertainty sample 0 after 553 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 3768 steps.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1020 steps.
Found uncertainty sample 9 after 78 steps.
Found uncertainty sample 10 after 408 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 73 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 2381 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 7 steps.
Found uncertainty sample 17 after 62 steps.
Found uncertainty sample 18 after 567 steps.
Found uncertainty sample 19 after 2900 steps.
Found uncertainty sample 20 after 3692 steps.
Found uncertainty sample 21 after 2229 steps.
Found uncertainty sample 22 after 1325 steps.
Found uncertainty sample 23 after 65 steps.
Found uncertainty sample 24 after 2727 steps.
Found uncertainty sample 25 after 2284 steps.
Found uncertainty sample 26 after 68 steps.
Did not find any uncertainty samples for sample 27.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 3185 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 3904 steps.
Found uncertainty sample 36 after 1046 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 200 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 633 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 304 steps.
Found uncertainty sample 43 after 2025 steps.
Found uncertainty sample 44 after 1764 steps.
Found uncertainty sample 45 after 2901 steps.
Found uncertainty sample 46 after 543 steps.
Found uncertainty sample 47 after 2738 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 1227 steps.
Did not find any uncertainty samples for sample 50.
Found uncertainty sample 51 after 3807 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 468 steps.
Found uncertainty sample 55 after 1997 steps.
Found uncertainty sample 56 after 1079 steps.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 684 steps.
Found uncertainty sample 61 after 2688 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 418 steps.
Found uncertainty sample 65 after 824 steps.
Found uncertainty sample 66 after 3835 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 1123 steps.
Found uncertainty sample 69 after 1197 steps.
Found uncertainty sample 70 after 365 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 3003 steps.
Found uncertainty sample 73 after 847 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 240 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 285 steps.
Found uncertainty sample 78 after 2287 steps.
Found uncertainty sample 79 after 145 steps.
Found uncertainty sample 80 after 3481 steps.
Found uncertainty sample 81 after 49 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 29 steps.
Found uncertainty sample 84 after 2045 steps.
Found uncertainty sample 85 after 1060 steps.
Found uncertainty sample 86 after 553 steps.
Found uncertainty sample 87 after 759 steps.
Found uncertainty sample 88 after 2418 steps.
Found uncertainty sample 89 after 1158 steps.
Found uncertainty sample 90 after 2046 steps.
Found uncertainty sample 91 after 1297 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 1673 steps.
Found uncertainty sample 94 after 2722 steps.
Found uncertainty sample 95 after 1182 steps.
Found uncertainty sample 96 after 3811 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 3607 steps.
Found uncertainty sample 99 after 218 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_065326-tejge5uv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_79
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/tejge5uv
Training model 79. Added 65 samples to the dataset.
Epoch 0, Batch 100/233, Loss: 0.5553414225578308, Variance: 0.07554734498262405
Epoch 0, Batch 200/233, Loss: 0.35882729291915894, Variance: 0.07282568514347076

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.9493366900786595, Training Loss Force: 3.4503192506996663, time: 4.456154108047485
Validation Loss Energy: 1.5944051347234722, Validation Loss Force: 3.374834865363969, time: 0.2516977787017822
Test Loss Energy: 10.154484240331772, Test Loss Force: 12.582008127734593, time: 12.747365474700928

Epoch 1, Batch 100/233, Loss: 0.4886605143547058, Variance: 0.07555137574672699
Epoch 1, Batch 200/233, Loss: 0.2052707076072693, Variance: 0.07341177016496658

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.634844947609074, Training Loss Force: 3.330756237614421, time: 3.880012035369873
Validation Loss Energy: 1.6433229018876991, Validation Loss Force: 3.3692990877270583, time: 0.2165696620941162
Test Loss Energy: 10.491577497120627, Test Loss Force: 13.234566574991451, time: 10.867484092712402

Epoch 2, Batch 100/233, Loss: 0.5781715512275696, Variance: 0.07852934300899506
Epoch 2, Batch 200/233, Loss: 0.784109354019165, Variance: 0.077908456325531

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6302011276202095, Training Loss Force: 3.335592748123577, time: 3.797532320022583
Validation Loss Energy: 1.9628394599752435, Validation Loss Force: 3.4029088242806855, time: 0.21678900718688965
Test Loss Energy: 10.005680801425022, Test Loss Force: 12.43688146238737, time: 10.696173429489136

Epoch 3, Batch 100/233, Loss: 0.21849435567855835, Variance: 0.07457226514816284
Epoch 3, Batch 200/233, Loss: 0.5086157917976379, Variance: 0.07621528953313828

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6276781000817568, Training Loss Force: 3.3422339328094606, time: 3.817714214324951
Validation Loss Energy: 1.5126844169886011, Validation Loss Force: 3.3587678391358304, time: 0.2113802433013916
Test Loss Energy: 9.985627087572379, Test Loss Force: 12.986086782811528, time: 10.852193355560303

Epoch 4, Batch 100/233, Loss: 0.2758411169052124, Variance: 0.07478620111942291
Epoch 4, Batch 200/233, Loss: 0.4794691801071167, Variance: 0.08048364520072937

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6119476093589258, Training Loss Force: 3.3231318960189866, time: 3.7585573196411133
Validation Loss Energy: 1.816421553883616, Validation Loss Force: 3.365128605958404, time: 0.20874571800231934
Test Loss Energy: 10.091321276198864, Test Loss Force: 12.53257547941083, time: 10.609214782714844

Epoch 5, Batch 100/233, Loss: 0.20902633666992188, Variance: 0.07466751337051392
Epoch 5, Batch 200/233, Loss: 0.10528558492660522, Variance: 0.0747147649526596

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6202768708203037, Training Loss Force: 3.3433104769676807, time: 3.7168962955474854
Validation Loss Energy: 1.9241050765290184, Validation Loss Force: 3.3491690175931756, time: 0.21060633659362793
Test Loss Energy: 10.290236688521967, Test Loss Force: 12.513815559565751, time: 10.897439241409302

Epoch 6, Batch 100/233, Loss: 0.4131782054901123, Variance: 0.07594460994005203
Epoch 6, Batch 200/233, Loss: 0.5791823863983154, Variance: 0.07884161174297333

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.63328148719105, Training Loss Force: 3.34073905926669, time: 3.8098700046539307
Validation Loss Energy: 1.5692035799555815, Validation Loss Force: 3.3009149993193163, time: 0.21654462814331055
Test Loss Energy: 9.947263959023335, Test Loss Force: 12.577558438092762, time: 10.605992078781128

Epoch 7, Batch 100/233, Loss: 0.6289986371994019, Variance: 0.07436202466487885
Epoch 7, Batch 200/233, Loss: 0.3309447169303894, Variance: 0.07857578247785568

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6119593522893885, Training Loss Force: 3.335367276977925, time: 3.707918405532837
Validation Loss Energy: 1.506415362020507, Validation Loss Force: 3.3709966616283253, time: 0.21602511405944824
Test Loss Energy: 9.935624914526914, Test Loss Force: 12.544850578905502, time: 10.834076404571533

Epoch 8, Batch 100/233, Loss: 0.40757691860198975, Variance: 0.07661355286836624
Epoch 8, Batch 200/233, Loss: 0.36848241090774536, Variance: 0.0779731348156929

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6359384495040319, Training Loss Force: 3.3294694565116436, time: 3.834383010864258
Validation Loss Energy: 2.140712555964501, Validation Loss Force: 3.3707580159711776, time: 0.20961260795593262
Test Loss Energy: 10.174490799620992, Test Loss Force: 12.36793629582143, time: 10.740828275680542

Epoch 9, Batch 100/233, Loss: 0.480110764503479, Variance: 0.07762886583805084
Epoch 9, Batch 200/233, Loss: 0.4493812322616577, Variance: 0.07826651632785797

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.618861609620326, Training Loss Force: 3.344696117437281, time: 3.9320623874664307
Validation Loss Energy: 1.8177428725955516, Validation Loss Force: 3.361272811360232, time: 0.21623921394348145
Test Loss Energy: 10.389254239569794, Test Loss Force: 12.690986730564182, time: 10.801010131835938

Epoch 10, Batch 100/233, Loss: 0.6069156527519226, Variance: 0.07583105564117432
Epoch 10, Batch 200/233, Loss: 0.33378535509109497, Variance: 0.0766516849398613

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6208444697638589, Training Loss Force: 3.3380981439449693, time: 3.8141701221466064
Validation Loss Energy: 1.5190026540043022, Validation Loss Force: 3.3591946402212876, time: 0.24238252639770508
Test Loss Energy: 9.809856619750922, Test Loss Force: 12.818713567189018, time: 10.640284299850464

Epoch 11, Batch 100/233, Loss: 0.3716738224029541, Variance: 0.07649820297956467
Epoch 11, Batch 200/233, Loss: 0.4050794243812561, Variance: 0.07568569481372833

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6280339167759952, Training Loss Force: 3.3459832320659464, time: 3.8530502319335938
Validation Loss Energy: 1.3210756685627127, Validation Loss Force: 3.4138415453420206, time: 0.20822834968566895
Test Loss Energy: 9.692812562734344, Test Loss Force: 12.399180418976313, time: 10.86803126335144

Epoch 12, Batch 100/233, Loss: 0.43217313289642334, Variance: 0.07711567729711533
Epoch 12, Batch 200/233, Loss: 0.3770906925201416, Variance: 0.08020932227373123

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6368915310272125, Training Loss Force: 3.347153668062145, time: 3.786332845687866
Validation Loss Energy: 1.984083905683565, Validation Loss Force: 3.3405968121478207, time: 0.20772910118103027
Test Loss Energy: 10.430017135292275, Test Loss Force: 12.617430060338323, time: 10.785154819488525

Epoch 13, Batch 100/233, Loss: 0.095278799533844, Variance: 0.07635897397994995
Epoch 13, Batch 200/233, Loss: 0.31652551889419556, Variance: 0.07562662661075592

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6172779345709323, Training Loss Force: 3.332217204284669, time: 3.8054139614105225
Validation Loss Energy: 1.4877267962700742, Validation Loss Force: 3.31522143627789, time: 0.20979666709899902
Test Loss Energy: 10.170028398019124, Test Loss Force: 12.77002469909785, time: 10.768393278121948

Epoch 14, Batch 100/233, Loss: 0.5889366269111633, Variance: 0.07621762156486511
Epoch 14, Batch 200/233, Loss: 0.5554829239845276, Variance: 0.0764332264661789

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6308299352538633, Training Loss Force: 3.336097836288144, time: 3.942715883255005
Validation Loss Energy: 1.4754987252859473, Validation Loss Force: 3.351849174768135, time: 0.20950555801391602
Test Loss Energy: 9.78709870453588, Test Loss Force: 12.432033006113587, time: 10.808202743530273

Epoch 15, Batch 100/233, Loss: 0.6210200786590576, Variance: 0.08000163733959198
Epoch 15, Batch 200/233, Loss: 0.396864652633667, Variance: 0.07545185089111328

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6253444194426763, Training Loss Force: 3.3150459536387524, time: 3.755295991897583
Validation Loss Energy: 1.511541344927981, Validation Loss Force: 3.3134194305471825, time: 0.21241259574890137
Test Loss Energy: 9.860309223333418, Test Loss Force: 12.573050360170802, time: 10.845771789550781

Epoch 16, Batch 100/233, Loss: 0.5711394548416138, Variance: 0.07601505517959595
Epoch 16, Batch 200/233, Loss: 0.5952098965644836, Variance: 0.07671701908111572

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6163626272437335, Training Loss Force: 3.3106397039488535, time: 3.98132061958313
Validation Loss Energy: 2.168754895862495, Validation Loss Force: 3.331385212249934, time: 0.2119615077972412
Test Loss Energy: 10.294387950212746, Test Loss Force: 12.507566931723003, time: 10.816466331481934

Epoch 17, Batch 100/233, Loss: 0.62006676197052, Variance: 0.07584033161401749
Epoch 17, Batch 200/233, Loss: 0.30080753564834595, Variance: 0.07713797688484192

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.616521806057563, Training Loss Force: 3.3237962193814163, time: 3.8289923667907715
Validation Loss Energy: 1.8559822903640222, Validation Loss Force: 3.310312082615761, time: 0.2127397060394287
Test Loss Energy: 10.285390409975747, Test Loss Force: 12.84339571688313, time: 10.810516595840454

Epoch 18, Batch 100/233, Loss: 0.5956113338470459, Variance: 0.07500562816858292
Epoch 18, Batch 200/233, Loss: 0.606440544128418, Variance: 0.07862338423728943

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6252694602160704, Training Loss Force: 3.3274503500360644, time: 4.058649063110352
Validation Loss Energy: 1.5503640723578527, Validation Loss Force: 3.4105544477425482, time: 0.21315956115722656
Test Loss Energy: 9.934952131943467, Test Loss Force: 12.599917633308134, time: 10.776163578033447

Epoch 19, Batch 100/233, Loss: 0.5566553473472595, Variance: 0.07820501923561096
Epoch 19, Batch 200/233, Loss: 0.5793710350990295, Variance: 0.07501645386219025

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.629046756772841, Training Loss Force: 3.3133974030258138, time: 3.8280134201049805
Validation Loss Energy: 1.3327839740929581, Validation Loss Force: 3.356474959272926, time: 0.22507214546203613
Test Loss Energy: 10.095397114836947, Test Loss Force: 12.935947571479334, time: 10.779790878295898

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–ˆâ–„â–„â–„â–†â–ƒâ–ƒâ–…â–‡â–‚â–â–‡â–…â–‚â–‚â–†â–†â–ƒâ–…
wandb:   test_error_force â–ƒâ–ˆâ–‚â–†â–‚â–‚â–ƒâ–‚â–â–„â–…â–â–ƒâ–„â–‚â–ƒâ–‚â–…â–ƒâ–†
wandb:          test_loss â–„â–ˆâ–…â–ƒâ–„â–…â–„â–„â–„â–„â–„â–ƒâ–…â–…â–…â–â–„â–†â–†â–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–â–â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–„â–†â–ƒâ–…â–†â–ƒâ–ƒâ–ˆâ–…â–ƒâ–â–†â–‚â–‚â–ƒâ–ˆâ–…â–ƒâ–
wandb:  valid_error_force â–†â–…â–‡â–…â–…â–„â–â–…â–…â–…â–…â–ˆâ–ƒâ–‚â–„â–‚â–ƒâ–‚â–ˆâ–„
wandb:         valid_loss â–ƒâ–ƒâ–‡â–‚â–…â–†â–‚â–‚â–ˆâ–…â–‚â–â–†â–â–‚â–‚â–ˆâ–…â–ƒâ–
wandb: 
wandb: Run summary:
wandb:       dataset_size 7445
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.0954
wandb:   test_error_force 12.93595
wandb:          test_loss 16.76925
wandb: train_error_energy 1.62905
wandb:  train_error_force 3.3134
wandb:         train_loss 0.48334
wandb: valid_error_energy 1.33278
wandb:  valid_error_force 3.35647
wandb:         valid_loss 0.36979
wandb: 
wandb: ğŸš€ View run al_71_79 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/tejge5uv
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_065326-tejge5uv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.720128059387207, Uncertainty Bias: -0.06118996441364288
7.6293945e-06 0.00116539
2.053558 5.2508206
(48745, 22, 3)
Found uncertainty sample 0 after 3565 steps.
Found uncertainty sample 1 after 2843 steps.
Found uncertainty sample 2 after 170 steps.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 545 steps.
Found uncertainty sample 6 after 3794 steps.
Found uncertainty sample 7 after 1227 steps.
Found uncertainty sample 8 after 3493 steps.
Found uncertainty sample 9 after 2892 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 257 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 2876 steps.
Found uncertainty sample 14 after 3636 steps.
Found uncertainty sample 15 after 2294 steps.
Found uncertainty sample 16 after 1619 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 1878 steps.
Found uncertainty sample 19 after 569 steps.
Found uncertainty sample 20 after 2543 steps.
Found uncertainty sample 21 after 3124 steps.
Found uncertainty sample 22 after 3542 steps.
Found uncertainty sample 23 after 1816 steps.
Found uncertainty sample 24 after 1447 steps.
Found uncertainty sample 25 after 734 steps.
Found uncertainty sample 26 after 1390 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 2551 steps.
Found uncertainty sample 29 after 594 steps.
Found uncertainty sample 30 after 20 steps.
Found uncertainty sample 31 after 212 steps.
Found uncertainty sample 32 after 427 steps.
Found uncertainty sample 33 after 662 steps.
Found uncertainty sample 34 after 1807 steps.
Found uncertainty sample 35 after 123 steps.
Found uncertainty sample 36 after 637 steps.
Found uncertainty sample 37 after 947 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 631 steps.
Found uncertainty sample 40 after 2290 steps.
Found uncertainty sample 41 after 2450 steps.
Found uncertainty sample 42 after 3544 steps.
Found uncertainty sample 43 after 623 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 2950 steps.
Found uncertainty sample 46 after 3103 steps.
Found uncertainty sample 47 after 33 steps.
Found uncertainty sample 48 after 3822 steps.
Found uncertainty sample 49 after 1200 steps.
Found uncertainty sample 50 after 3501 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 3305 steps.
Found uncertainty sample 53 after 2418 steps.
Found uncertainty sample 54 after 3396 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 166 steps.
Found uncertainty sample 57 after 742 steps.
Found uncertainty sample 58 after 2911 steps.
Found uncertainty sample 59 after 795 steps.
Found uncertainty sample 60 after 6 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 1608 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 3017 steps.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 1485 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 1904 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 320 steps.
Found uncertainty sample 73 after 2403 steps.
Found uncertainty sample 74 after 2515 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 3752 steps.
Found uncertainty sample 77 after 3583 steps.
Found uncertainty sample 78 after 3722 steps.
Found uncertainty sample 79 after 273 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 2887 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 558 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 1132 steps.
Found uncertainty sample 86 after 523 steps.
Found uncertainty sample 87 after 657 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 898 steps.
Found uncertainty sample 90 after 1369 steps.
Found uncertainty sample 91 after 1661 steps.
Found uncertainty sample 92 after 1156 steps.
Did not find any uncertainty samples for sample 93.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 2261 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 894 steps.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_072342-n1u07eui
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_80
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/n1u07eui
Training model 80. Added 75 samples to the dataset.
Epoch 0, Batch 100/235, Loss: 0.5476943850517273, Variance: 0.0909501165151596
Epoch 0, Batch 200/235, Loss: 0.4913299083709717, Variance: 0.07793857157230377

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9962907258062463, Training Loss Force: 3.6664958869774127, time: 3.7995495796203613
Validation Loss Energy: 1.633635569359538, Validation Loss Force: 3.3178851159360514, time: 0.22120308876037598
Test Loss Energy: 9.516000779737437, Test Loss Force: 12.151234738963245, time: 10.696853876113892

Epoch 1, Batch 100/235, Loss: 0.7086262702941895, Variance: 0.07364719361066818
Epoch 1, Batch 200/235, Loss: 0.5755869150161743, Variance: 0.07787046581506729

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6070924608210757, Training Loss Force: 3.3234745357546265, time: 3.869961977005005
Validation Loss Energy: 1.6192457149721708, Validation Loss Force: 3.320634578882681, time: 0.21239638328552246
Test Loss Energy: 10.112724714817947, Test Loss Force: 12.369262802374724, time: 10.894925117492676

Epoch 2, Batch 100/235, Loss: 0.6103301644325256, Variance: 0.07864809036254883
Epoch 2, Batch 200/235, Loss: 0.5091675519943237, Variance: 0.07616844773292542

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6158364055303969, Training Loss Force: 3.3090481381625048, time: 3.830319404602051
Validation Loss Energy: 1.9981938927881444, Validation Loss Force: 3.3356614061956176, time: 0.21453046798706055
Test Loss Energy: 10.16445153617608, Test Loss Force: 12.703932761340708, time: 10.80150556564331

Epoch 3, Batch 100/235, Loss: 0.208055317401886, Variance: 0.07524760812520981
Epoch 3, Batch 200/235, Loss: 0.46200740337371826, Variance: 0.07684006541967392

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6052018718558305, Training Loss Force: 3.3203726095800254, time: 3.8237721920013428
Validation Loss Energy: 1.3294862440417676, Validation Loss Force: 3.354974558900378, time: 0.22064781188964844
Test Loss Energy: 10.043439948659291, Test Loss Force: 13.170857598133354, time: 10.977812051773071

Epoch 4, Batch 100/235, Loss: 0.5821916460990906, Variance: 0.07528670132160187
Epoch 4, Batch 200/235, Loss: 0.45292091369628906, Variance: 0.07779036462306976

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6175644481510532, Training Loss Force: 3.304788191247365, time: 3.7373740673065186
Validation Loss Energy: 1.5016167584248334, Validation Loss Force: 3.4056987016773013, time: 0.21640706062316895
Test Loss Energy: 9.853615919377221, Test Loss Force: 12.61341006852101, time: 10.752431392669678

Epoch 5, Batch 100/235, Loss: 0.6470013856887817, Variance: 0.07897500693798065
Epoch 5, Batch 200/235, Loss: 0.35233646631240845, Variance: 0.07527966052293777

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.608218116364371, Training Loss Force: 3.33499489727381, time: 3.743751287460327
Validation Loss Energy: 1.7440399141326752, Validation Loss Force: 3.3503611325331866, time: 0.21194767951965332
Test Loss Energy: 10.277739115736956, Test Loss Force: 12.820171865897331, time: 12.038341283798218

Epoch 6, Batch 100/235, Loss: 0.4724714756011963, Variance: 0.07432106882333755
Epoch 6, Batch 200/235, Loss: 0.4100889563560486, Variance: 0.07600554823875427

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6514640617946645, Training Loss Force: 3.325192090895391, time: 3.8040692806243896
Validation Loss Energy: 1.9631522822916287, Validation Loss Force: 3.3309835727779666, time: 0.22431397438049316
Test Loss Energy: 10.308624911774725, Test Loss Force: 12.729381228649238, time: 10.804129362106323

Epoch 7, Batch 100/235, Loss: 0.2858034372329712, Variance: 0.07782351225614548
Epoch 7, Batch 200/235, Loss: 0.31121671199798584, Variance: 0.07090452313423157

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.6149717348078736, Training Loss Force: 3.307916609365162, time: 3.81136417388916
Validation Loss Energy: 1.5406923266699062, Validation Loss Force: 3.335412239421325, time: 0.21989893913269043
Test Loss Energy: 9.955364960998795, Test Loss Force: 12.651343144840192, time: 10.838997840881348

Epoch 8, Batch 100/235, Loss: 0.7808915376663208, Variance: 0.07731392234563828
Epoch 8, Batch 200/235, Loss: 0.40864527225494385, Variance: 0.07543987780809402

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.617629731191408, Training Loss Force: 3.31281245577477, time: 3.842944860458374
Validation Loss Energy: 1.5248908055646435, Validation Loss Force: 3.3511757119362766, time: 0.2134697437286377
Test Loss Energy: 10.149606979297479, Test Loss Force: 13.068812758621423, time: 10.732895374298096

Epoch 9, Batch 100/235, Loss: 0.5471161603927612, Variance: 0.0761648491024971
Epoch 9, Batch 200/235, Loss: 0.5729512572288513, Variance: 0.0815730094909668

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6102836822977171, Training Loss Force: 3.3320249921095937, time: 3.7651312351226807
Validation Loss Energy: 1.8763472207795724, Validation Loss Force: 3.3189422933736297, time: 0.21427202224731445
Test Loss Energy: 10.178773373154288, Test Loss Force: 12.705535321676209, time: 10.996489524841309

Epoch 10, Batch 100/235, Loss: 0.9194016456604004, Variance: 0.07729780673980713
Epoch 10, Batch 200/235, Loss: 0.4948742389678955, Variance: 0.0760093480348587

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6313987175545077, Training Loss Force: 3.327301404446332, time: 3.8894760608673096
Validation Loss Energy: 1.9249630207482782, Validation Loss Force: 3.3210059910180885, time: 0.21689772605895996
Test Loss Energy: 10.003275065319508, Test Loss Force: 12.596002881224543, time: 10.77169418334961

Epoch 11, Batch 100/235, Loss: 0.327553391456604, Variance: 0.07627208530902863
Epoch 11, Batch 200/235, Loss: 0.4697039723396301, Variance: 0.08058801293373108

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6356059198341466, Training Loss Force: 3.312621768550896, time: 3.8694403171539307
Validation Loss Energy: 1.2811082268872693, Validation Loss Force: 3.3393681794678773, time: 0.21410202980041504
Test Loss Energy: 9.713261958493497, Test Loss Force: 12.670684396700747, time: 10.75816559791565

Epoch 12, Batch 100/235, Loss: 0.5748826861381531, Variance: 0.07492431253194809
Epoch 12, Batch 200/235, Loss: 0.31235671043395996, Variance: 0.07244754582643509

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6033689487510936, Training Loss Force: 3.3199741445646795, time: 3.9837934970855713
Validation Loss Energy: 1.6480867690966083, Validation Loss Force: 3.3051000102497423, time: 0.24068093299865723
Test Loss Energy: 9.666653013749265, Test Loss Force: 12.42422341859399, time: 10.883294105529785

Epoch 13, Batch 100/235, Loss: 0.39127254486083984, Variance: 0.07715868204832077
Epoch 13, Batch 200/235, Loss: 0.4031875729560852, Variance: 0.07507801055908203

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.624163275532061, Training Loss Force: 3.3230220322539132, time: 3.923374891281128
Validation Loss Energy: 1.9182462894622963, Validation Loss Force: 3.3597375632663464, time: 0.23722267150878906
Test Loss Energy: 10.292441742602458, Test Loss Force: 12.789395175409455, time: 10.829792499542236

Epoch 14, Batch 100/235, Loss: 0.713127613067627, Variance: 0.07685312628746033
Epoch 14, Batch 200/235, Loss: 0.6413993835449219, Variance: 0.0778123065829277

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6316010297659742, Training Loss Force: 3.3262622454434183, time: 4.078343391418457
Validation Loss Energy: 1.9461234923549418, Validation Loss Force: 3.3443628154128975, time: 0.22622966766357422
Test Loss Energy: 10.328851421547839, Test Loss Force: 12.630350368748166, time: 10.85418963432312

Epoch 15, Batch 100/235, Loss: 0.4536208510398865, Variance: 0.07726994156837463
Epoch 15, Batch 200/235, Loss: 0.07991313934326172, Variance: 0.07133352011442184

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6123536961656937, Training Loss Force: 3.328996569493828, time: 3.8304355144500732
Validation Loss Energy: 1.5714458025877878, Validation Loss Force: 3.411422746077094, time: 0.2131495475769043
Test Loss Energy: 9.573491122636483, Test Loss Force: 12.406941401668513, time: 10.916166305541992

Epoch 16, Batch 100/235, Loss: 0.5929528474807739, Variance: 0.07371225953102112
Epoch 16, Batch 200/235, Loss: 0.540064811706543, Variance: 0.07481275498867035

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6353653736721496, Training Loss Force: 3.3176840378801704, time: 4.099544525146484
Validation Loss Energy: 1.323699490443753, Validation Loss Force: 3.4176082166145347, time: 0.2189319133758545
Test Loss Energy: 9.557171843159898, Test Loss Force: 12.400675962888158, time: 10.866532325744629

Epoch 17, Batch 100/235, Loss: 0.6242192983627319, Variance: 0.0782252624630928
Epoch 17, Batch 200/235, Loss: 0.684630274772644, Variance: 0.0776369646191597

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.623375385868695, Training Loss Force: 3.3188037879534575, time: 3.8279225826263428
Validation Loss Energy: 1.6754297887081728, Validation Loss Force: 3.3731681868066996, time: 0.2146914005279541
Test Loss Energy: 10.647474326309107, Test Loss Force: 12.979493472457342, time: 10.947248697280884

Epoch 18, Batch 100/235, Loss: 0.5695919394493103, Variance: 0.0772133618593216
Epoch 18, Batch 200/235, Loss: 0.5845012068748474, Variance: 0.07445605844259262

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.6157904698331202, Training Loss Force: 3.3128377945760485, time: 3.9687933921813965
Validation Loss Energy: 1.8921808897427783, Validation Loss Force: 3.328197258087686, time: 0.21543550491333008
Test Loss Energy: 10.639918385760282, Test Loss Force: 12.946901191380695, time: 10.795434713363647

Epoch 19, Batch 100/235, Loss: 0.29845792055130005, Variance: 0.07608173042535782
Epoch 19, Batch 200/235, Loss: 0.4243180751800537, Variance: 0.07716716825962067

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6338363482333642, Training Loss Force: 3.304961350145578, time: 3.928800344467163
Validation Loss Energy: 1.2985355389278808, Validation Loss Force: 3.3712844058321503, time: 0.21880769729614258
Test Loss Energy: 9.863813821181827, Test Loss Force: 12.71649804419899, time: 10.944368362426758

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.039 MB of 0.051 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–…â–…â–„â–ƒâ–†â–†â–„â–…â–…â–„â–‚â–‚â–†â–†â–â–â–ˆâ–ˆâ–ƒ
wandb:   test_error_force â–â–‚â–…â–ˆâ–„â–†â–…â–„â–‡â–…â–„â–…â–ƒâ–…â–„â–ƒâ–ƒâ–‡â–†â–…
wandb:          test_loss â–â–…â–†â–†â–ƒâ–†â–…â–†â–‡â–‡â–„â–‚â–ƒâ–„â–„â–†â–‚â–ˆâ–ˆâ–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–‚â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–„â–ˆâ–â–ƒâ–†â–ˆâ–„â–ƒâ–‡â–‡â–â–…â–‡â–‡â–„â–â–…â–‡â–
wandb:  valid_error_force â–‚â–‚â–ƒâ–„â–‡â–„â–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–â–„â–ƒâ–ˆâ–ˆâ–…â–‚â–…
wandb:         valid_loss â–„â–ƒâ–ˆâ–â–„â–…â–‡â–„â–„â–‡â–‡â–â–„â–‡â–‡â–„â–‚â–…â–†â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 7512
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.86381
wandb:   test_error_force 12.7165
wandb:          test_loss 17.08037
wandb: train_error_energy 1.63384
wandb:  train_error_force 3.30496
wandb:         train_loss 0.48327
wandb: valid_error_energy 1.29854
wandb:  valid_error_force 3.37128
wandb:         valid_loss 0.33928
wandb: 
wandb: ğŸš€ View run al_71_80 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/n1u07eui
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_072342-n1u07eui/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.937936305999756, Uncertainty Bias: -0.0635208785533905
0.00019359589 0.0054397583
2.0666964 5.310001
(48745, 22, 3)
Found uncertainty sample 0 after 3118 steps.
Found uncertainty sample 1 after 1190 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 762 steps.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 2115 steps.
Found uncertainty sample 7 after 1140 steps.
Found uncertainty sample 8 after 1678 steps.
Found uncertainty sample 9 after 364 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 1287 steps.
Found uncertainty sample 12 after 808 steps.
Found uncertainty sample 13 after 3789 steps.
Found uncertainty sample 14 after 913 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 2760 steps.
Found uncertainty sample 17 after 636 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 938 steps.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 2923 steps.
Found uncertainty sample 23 after 1269 steps.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 294 steps.
Found uncertainty sample 27 after 748 steps.
Found uncertainty sample 28 after 13 steps.
Found uncertainty sample 29 after 306 steps.
Found uncertainty sample 30 after 166 steps.
Found uncertainty sample 31 after 1030 steps.
Found uncertainty sample 32 after 309 steps.
Found uncertainty sample 33 after 1515 steps.
Found uncertainty sample 34 after 826 steps.
Found uncertainty sample 35 after 2884 steps.
Found uncertainty sample 36 after 3671 steps.
Found uncertainty sample 37 after 1988 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 1544 steps.
Found uncertainty sample 41 after 563 steps.
Found uncertainty sample 42 after 1460 steps.
Found uncertainty sample 43 after 689 steps.
Found uncertainty sample 44 after 3182 steps.
Found uncertainty sample 45 after 382 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 248 steps.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 1628 steps.
Found uncertainty sample 51 after 324 steps.
Found uncertainty sample 52 after 6 steps.
Found uncertainty sample 53 after 164 steps.
Found uncertainty sample 54 after 208 steps.
Found uncertainty sample 55 after 71 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 1390 steps.
Found uncertainty sample 59 after 585 steps.
Found uncertainty sample 60 after 1919 steps.
Found uncertainty sample 61 after 454 steps.
Found uncertainty sample 62 after 2209 steps.
Found uncertainty sample 63 after 320 steps.
Found uncertainty sample 64 after 3974 steps.
Found uncertainty sample 65 after 1502 steps.
Found uncertainty sample 66 after 2481 steps.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 583 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 2200 steps.
Found uncertainty sample 72 after 3857 steps.
Found uncertainty sample 73 after 393 steps.
Found uncertainty sample 74 after 1787 steps.
Found uncertainty sample 75 after 1161 steps.
Found uncertainty sample 76 after 924 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 751 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 2864 steps.
Found uncertainty sample 81 after 2991 steps.
Found uncertainty sample 82 after 3913 steps.
Found uncertainty sample 83 after 2054 steps.
Found uncertainty sample 84 after 3541 steps.
Found uncertainty sample 85 after 552 steps.
Found uncertainty sample 86 after 1850 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 3965 steps.
Found uncertainty sample 89 after 1584 steps.
Found uncertainty sample 90 after 3304 steps.
Found uncertainty sample 91 after 2132 steps.
Found uncertainty sample 92 after 307 steps.
Found uncertainty sample 93 after 644 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 836 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 133 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 3802 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_075228-krf1ju7i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_81
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/krf1ju7i
Training model 81. Added 74 samples to the dataset.
Epoch 0, Batch 100/237, Loss: 0.4195109009742737, Variance: 0.07978012412786484
Epoch 0, Batch 200/237, Loss: 0.47430121898651123, Variance: 0.07772994041442871

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.8084743993101116, Training Loss Force: 3.6997971089875636, time: 3.9110798835754395
Validation Loss Energy: 1.7885234757585398, Validation Loss Force: 3.3234949867312693, time: 0.2110593318939209
Test Loss Energy: 9.651060974034891, Test Loss Force: 11.97392795196268, time: 10.089505672454834

Epoch 1, Batch 100/237, Loss: 0.44305551052093506, Variance: 0.07700496912002563
Epoch 1, Batch 200/237, Loss: 0.47954195737838745, Variance: 0.07591134309768677

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6061737939271188, Training Loss Force: 3.2872816247169747, time: 3.80179762840271
Validation Loss Energy: 1.4366042983910865, Validation Loss Force: 3.339615175217998, time: 0.20990657806396484
Test Loss Energy: 9.457085711041412, Test Loss Force: 12.193891073205846, time: 10.360016584396362

Epoch 2, Batch 100/237, Loss: 0.4195548892021179, Variance: 0.07565076649188995
Epoch 2, Batch 200/237, Loss: 0.43313586711883545, Variance: 0.07599541544914246

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6161923035913637, Training Loss Force: 3.2923030441430496, time: 3.9091131687164307
Validation Loss Energy: 1.9283703318529084, Validation Loss Force: 3.321332535320592, time: 0.20067930221557617
Test Loss Energy: 10.228625105795214, Test Loss Force: 12.520982759423223, time: 10.353083610534668

Epoch 3, Batch 100/237, Loss: 0.2815856337547302, Variance: 0.07453738152980804
Epoch 3, Batch 200/237, Loss: 0.35697752237319946, Variance: 0.07650914788246155

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6198534550560792, Training Loss Force: 3.3006689081038805, time: 3.914780855178833
Validation Loss Energy: 1.5946691089959695, Validation Loss Force: 3.3467562303951883, time: 0.2117924690246582
Test Loss Energy: 10.00108252734347, Test Loss Force: 12.522778130254572, time: 10.260377883911133

Epoch 4, Batch 100/237, Loss: 0.3697112798690796, Variance: 0.0784096047282219
Epoch 4, Batch 200/237, Loss: 0.3132970929145813, Variance: 0.07782803475856781

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6144347299854749, Training Loss Force: 3.3065122745101565, time: 4.0745909214019775
Validation Loss Energy: 1.5460982046938114, Validation Loss Force: 3.3591995389424985, time: 0.20614266395568848
Test Loss Energy: 9.84527850572237, Test Loss Force: 12.594418680018407, time: 10.247449398040771

Epoch 5, Batch 100/237, Loss: 0.46170228719711304, Variance: 0.08093041181564331
Epoch 5, Batch 200/237, Loss: 0.4965081214904785, Variance: 0.07602489739656448

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6154089389732398, Training Loss Force: 3.297365339799218, time: 3.7977383136749268
Validation Loss Energy: 1.7335558992340274, Validation Loss Force: 3.3243342824095574, time: 0.2065443992614746
Test Loss Energy: 10.080989735600873, Test Loss Force: 12.983430272283803, time: 10.11802077293396

Epoch 6, Batch 100/237, Loss: 0.43141376972198486, Variance: 0.07440651953220367
Epoch 6, Batch 200/237, Loss: 0.3460286259651184, Variance: 0.07829219102859497

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6218507907183257, Training Loss Force: 3.306553631780767, time: 4.078890562057495
Validation Loss Energy: 1.8063994939618384, Validation Loss Force: 3.409043192249136, time: 0.20760512351989746
Test Loss Energy: 10.59599095810493, Test Loss Force: 13.027615543815292, time: 10.281754493713379

Epoch 7, Batch 100/237, Loss: 0.3156278729438782, Variance: 0.07676123082637787
Epoch 7, Batch 200/237, Loss: 0.5329403281211853, Variance: 0.07803602516651154

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.624659132977356, Training Loss Force: 3.3018405348987514, time: 3.887084484100342
Validation Loss Energy: 1.6963548155266612, Validation Loss Force: 3.3138745662814593, time: 0.20392847061157227
Test Loss Energy: 10.165900410500225, Test Loss Force: 12.433177506870742, time: 10.986560106277466

Epoch 8, Batch 100/237, Loss: 0.5602859854698181, Variance: 0.07695132493972778
Epoch 8, Batch 200/237, Loss: 0.5541091561317444, Variance: 0.07637288421392441

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.631049923492871, Training Loss Force: 3.3130514913927795, time: 4.391081809997559
Validation Loss Energy: 1.6874886589750382, Validation Loss Force: 3.287309858835842, time: 0.25546932220458984
Test Loss Energy: 9.967191280739202, Test Loss Force: 12.812635011941476, time: 12.450749635696411

Epoch 9, Batch 100/237, Loss: 0.6397973299026489, Variance: 0.07698652148246765
Epoch 9, Batch 200/237, Loss: 0.6188309192657471, Variance: 0.07673737406730652

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.6191680888202362, Training Loss Force: 3.324389279104888, time: 4.269500494003296
Validation Loss Energy: 1.4654105326278293, Validation Loss Force: 3.3349090628188143, time: 0.2234816551208496
Test Loss Energy: 9.900162211319147, Test Loss Force: 12.824020731229869, time: 11.95318341255188

Epoch 10, Batch 100/237, Loss: 0.5941756963729858, Variance: 0.07346343994140625
Epoch 10, Batch 200/237, Loss: 0.3347914218902588, Variance: 0.07527047395706177

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6155269352997024, Training Loss Force: 3.31779707354226, time: 4.01080584526062
Validation Loss Energy: 1.9927649406739922, Validation Loss Force: 3.3820445583053917, time: 0.22135543823242188
Test Loss Energy: 10.546789120923513, Test Loss Force: 13.020536870681726, time: 10.763915777206421

Epoch 11, Batch 100/237, Loss: 0.5605742335319519, Variance: 0.07665866613388062
Epoch 11, Batch 200/237, Loss: 0.12403863668441772, Variance: 0.07573606073856354

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.614924742096823, Training Loss Force: 3.343973783994111, time: 3.8089466094970703
Validation Loss Energy: 1.755244542054832, Validation Loss Force: 3.3546993955032978, time: 0.22144842147827148
Test Loss Energy: 10.082368796735837, Test Loss Force: 12.886551151775356, time: 10.686699867248535

Epoch 12, Batch 100/237, Loss: 0.6786969304084778, Variance: 0.07652880251407623
Epoch 12, Batch 200/237, Loss: 0.5467644333839417, Variance: 0.07398276031017303

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6142085782283377, Training Loss Force: 3.307908013572509, time: 4.032943964004517
Validation Loss Energy: 1.832737579884188, Validation Loss Force: 3.307126633481404, time: 0.2140216827392578
Test Loss Energy: 10.129097975824028, Test Loss Force: 12.90477563509679, time: 10.767719984054565

Epoch 13, Batch 100/237, Loss: 0.6174706220626831, Variance: 0.07975052297115326
Epoch 13, Batch 200/237, Loss: 0.35824519395828247, Variance: 0.07332076877355576

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.632113929216042, Training Loss Force: 3.3199092670234505, time: 3.877368450164795
Validation Loss Energy: 1.386427058870205, Validation Loss Force: 3.3344940631980915, time: 0.2352466583251953
Test Loss Energy: 10.162301946531644, Test Loss Force: 12.917907248365465, time: 10.801332235336304

Epoch 14, Batch 100/237, Loss: 0.5308107137680054, Variance: 0.07816694676876068
Epoch 14, Batch 200/237, Loss: 0.41667240858078003, Variance: 0.07893107831478119

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6308433630973294, Training Loss Force: 3.313153325415517, time: 4.011590480804443
Validation Loss Energy: 2.042638654315331, Validation Loss Force: 3.3433816238213665, time: 0.2136821746826172
Test Loss Energy: 10.510438655649645, Test Loss Force: 12.86577420387351, time: 10.782246828079224

Epoch 15, Batch 100/237, Loss: 0.29579442739486694, Variance: 0.07418131828308105
Epoch 15, Batch 200/237, Loss: 0.40099406242370605, Variance: 0.07634919881820679

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6307536603151498, Training Loss Force: 3.315276018169727, time: 3.824329137802124
Validation Loss Energy: 1.7231902329864404, Validation Loss Force: 3.45043410210721, time: 0.21276140213012695
Test Loss Energy: 10.159213098792247, Test Loss Force: 12.748678501604765, time: 10.790763854980469

Epoch 16, Batch 100/237, Loss: 0.6482945680618286, Variance: 0.07307139784097672
Epoch 16, Batch 200/237, Loss: 0.4371325373649597, Variance: 0.07877963781356812

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.6369506798180302, Training Loss Force: 3.33007783032468, time: 4.049424886703491
Validation Loss Energy: 1.5905527358297755, Validation Loss Force: 3.319225194523531, time: 0.2145555019378662
Test Loss Energy: 9.903356382730374, Test Loss Force: 12.743154632686837, time: 10.69404673576355

Epoch 17, Batch 100/237, Loss: 0.49170148372650146, Variance: 0.07796639204025269
Epoch 17, Batch 200/237, Loss: 0.19554322957992554, Variance: 0.07960355281829834

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.6353310553852372, Training Loss Force: 3.303876715522342, time: 3.8303184509277344
Validation Loss Energy: 1.312571739044228, Validation Loss Force: 3.32318530961657, time: 0.22036981582641602
Test Loss Energy: 10.004617818559804, Test Loss Force: 12.96251289592833, time: 10.711303949356079

Epoch 18, Batch 100/237, Loss: 0.3482819199562073, Variance: 0.07668943703174591
Epoch 18, Batch 200/237, Loss: 0.37785768508911133, Variance: 0.07429930567741394

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.63775595526802, Training Loss Force: 3.3047161340444737, time: 4.1073713302612305
Validation Loss Energy: 1.7885310465095012, Validation Loss Force: 3.397841598194093, time: 0.219679594039917
Test Loss Energy: 10.23560345423757, Test Loss Force: 12.793727067244056, time: 10.670906066894531

Epoch 19, Batch 100/237, Loss: 0.43288135528564453, Variance: 0.07731534540653229
Epoch 19, Batch 200/237, Loss: 0.41317832469940186, Variance: 0.07743656635284424

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.6236406161179144, Training Loss Force: 3.316480172511017, time: 3.7880055904388428
Validation Loss Energy: 1.8624830604120974, Validation Loss Force: 3.3339853198669327, time: 0.21373677253723145
Test Loss Energy: 10.188765482035777, Test Loss Force: 12.49017876573431, time: 10.768764019012451

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–†â–„â–ƒâ–…â–ˆâ–…â–„â–„â–ˆâ–…â–…â–…â–‡â–…â–„â–„â–†â–…
wandb:   test_error_force â–â–‚â–…â–…â–…â–ˆâ–ˆâ–„â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–†â–†â–ˆâ–†â–„
wandb:          test_loss â–ƒâ–â–„â–ƒâ–„â–…â–‡â–ƒâ–ƒâ–…â–ˆâ–„â–…â–…â–…â–†â–„â–…â–„â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–‚â–‡â–„â–ƒâ–…â–†â–…â–…â–‚â–ˆâ–…â–†â–‚â–ˆâ–…â–„â–â–†â–†
wandb:  valid_error_force â–ƒâ–ƒâ–‚â–„â–„â–ƒâ–†â–‚â–â–ƒâ–…â–„â–‚â–ƒâ–ƒâ–ˆâ–‚â–ƒâ–†â–ƒ
wandb:         valid_loss â–…â–‚â–†â–ƒâ–ƒâ–…â–†â–„â–„â–‚â–ˆâ–…â–†â–â–ˆâ–…â–ƒâ–â–…â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 7578
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.18877
wandb:   test_error_force 12.49018
wandb:          test_loss 16.78944
wandb: train_error_energy 1.62364
wandb:  train_error_force 3.31648
wandb:         train_loss 0.48132
wandb: valid_error_energy 1.86248
wandb:  valid_error_force 3.33399
wandb:         valid_loss 0.62056
wandb: 
wandb: ğŸš€ View run al_71_81 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/krf1ju7i
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_075228-krf1ju7i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.7432363033294678, Uncertainty Bias: -0.0617838054895401
0.00017547607 0.01768589
2.0862384 5.1875987
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 2238 steps.
Found uncertainty sample 4 after 619 steps.
Found uncertainty sample 5 after 2985 steps.
Found uncertainty sample 6 after 1353 steps.
Found uncertainty sample 7 after 2804 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 211 steps.
Found uncertainty sample 10 after 1374 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 305 steps.
Found uncertainty sample 13 after 812 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 342 steps.
Found uncertainty sample 16 after 186 steps.
Found uncertainty sample 17 after 8 steps.
Found uncertainty sample 18 after 1035 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 2671 steps.
Found uncertainty sample 21 after 3686 steps.
Found uncertainty sample 22 after 379 steps.
Found uncertainty sample 23 after 2553 steps.
Found uncertainty sample 24 after 2122 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 640 steps.
Found uncertainty sample 27 after 973 steps.
Found uncertainty sample 28 after 9 steps.
Found uncertainty sample 29 after 72 steps.
Found uncertainty sample 30 after 212 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 1145 steps.
Found uncertainty sample 33 after 36 steps.
Found uncertainty sample 34 after 1802 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 463 steps.
Found uncertainty sample 37 after 204 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 196 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 703 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1905 steps.
Did not find any uncertainty samples for sample 45.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 3530 steps.
Found uncertainty sample 48 after 2873 steps.
Found uncertainty sample 49 after 1351 steps.
Found uncertainty sample 50 after 348 steps.
Found uncertainty sample 51 after 1880 steps.
Found uncertainty sample 52 after 1829 steps.
Found uncertainty sample 53 after 22 steps.
Found uncertainty sample 54 after 1074 steps.
Found uncertainty sample 55 after 2418 steps.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 2977 steps.
Found uncertainty sample 58 after 1552 steps.
Found uncertainty sample 59 after 142 steps.
Found uncertainty sample 60 after 3899 steps.
Found uncertainty sample 61 after 3432 steps.
Found uncertainty sample 62 after 8 steps.
Found uncertainty sample 63 after 1386 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 2671 steps.
Found uncertainty sample 66 after 2908 steps.
Found uncertainty sample 67 after 1917 steps.
Found uncertainty sample 68 after 215 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 1887 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 705 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 830 steps.
Found uncertainty sample 75 after 1812 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 2166 steps.
Found uncertainty sample 78 after 2200 steps.
Found uncertainty sample 79 after 1560 steps.
Found uncertainty sample 80 after 1963 steps.
Found uncertainty sample 81 after 2157 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 953 steps.
Found uncertainty sample 84 after 12 steps.
Found uncertainty sample 85 after 15 steps.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 1068 steps.
Found uncertainty sample 89 after 244 steps.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 1470 steps.
Found uncertainty sample 93 after 2945 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 965 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 421 steps.
Found uncertainty sample 99 after 3643 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_081958-j7gctrab
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_82
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j7gctrab
Training model 82. Added 74 samples to the dataset.
Epoch 0, Batch 100/239, Loss: 1.3054757118225098, Variance: 0.1112426295876503
Epoch 0, Batch 200/239, Loss: 1.5210306644439697, Variance: 0.1356138288974762

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.070120988029327, Training Loss Force: 3.644500714258084, time: 3.9884390830993652
Validation Loss Energy: 3.909553053400537, Validation Loss Force: 3.4021672803574594, time: 0.22650623321533203
Test Loss Energy: 11.299447524438808, Test Loss Force: 12.33866537570481, time: 10.58667540550232

Epoch 1, Batch 100/239, Loss: 0.9589484333992004, Variance: 0.142976775765419
Epoch 1, Batch 200/239, Loss: 0.7072465419769287, Variance: 0.14599637687206268

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.206075963614667, Training Loss Force: 3.369205135555445, time: 3.792290449142456
Validation Loss Energy: 1.911242760316359, Validation Loss Force: 3.315852257968303, time: 0.21216535568237305
Test Loss Energy: 9.437184589673114, Test Loss Force: 11.481364765872954, time: 10.800986289978027

Epoch 2, Batch 100/239, Loss: 1.6204264163970947, Variance: 0.15081946551799774
Epoch 2, Batch 200/239, Loss: 1.4924757480621338, Variance: 0.14349544048309326

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.2007649010711505, Training Loss Force: 3.3665935674649563, time: 3.9329206943511963
Validation Loss Energy: 5.281122068170419, Validation Loss Force: 3.3658678118555976, time: 0.2219550609588623
Test Loss Energy: 10.544695853749117, Test Loss Force: 11.470769762208858, time: 10.663255453109741

Epoch 3, Batch 100/239, Loss: 1.8746280670166016, Variance: 0.1562516689300537
Epoch 3, Batch 200/239, Loss: 2.175868511199951, Variance: 0.14785918593406677

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.176462390332299, Training Loss Force: 3.4011540987096023, time: 3.9090983867645264
Validation Loss Energy: 6.009730361367478, Validation Loss Force: 3.326754807360366, time: 0.21724414825439453
Test Loss Energy: 11.056103508674186, Test Loss Force: 11.754381839900125, time: 10.745654106140137

Epoch 4, Batch 100/239, Loss: 1.2371351718902588, Variance: 0.15329097211360931
Epoch 4, Batch 200/239, Loss: 0.9960545301437378, Variance: 0.14902141690254211

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.173954882334892, Training Loss Force: 3.3706146433608795, time: 3.8434743881225586
Validation Loss Energy: 3.118724320756876, Validation Loss Force: 3.4146574050927, time: 0.21915006637573242
Test Loss Energy: 9.669222811548856, Test Loss Force: 11.354110211672435, time: 10.596031188964844

Epoch 5, Batch 100/239, Loss: 0.8067411184310913, Variance: 0.15015438199043274
Epoch 5, Batch 200/239, Loss: 0.9888988137245178, Variance: 0.15378665924072266

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.17239076854025, Training Loss Force: 3.374585943851821, time: 3.8477065563201904
Validation Loss Energy: 2.071334862160342, Validation Loss Force: 3.334345876881476, time: 0.22379493713378906
Test Loss Energy: 10.014641171070142, Test Loss Force: 11.768180224851118, time: 10.81512451171875

Epoch 6, Batch 100/239, Loss: 1.6986558437347412, Variance: 0.1517484188079834
Epoch 6, Batch 200/239, Loss: 1.7277021408081055, Variance: 0.16020314395427704

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.161018177624515, Training Loss Force: 3.3672712260729605, time: 3.8695836067199707
Validation Loss Energy: 5.312111935220345, Validation Loss Force: 3.369149342726422, time: 0.2209179401397705
Test Loss Energy: 11.553814576919581, Test Loss Force: 11.549494767509215, time: 10.650639295578003

Epoch 7, Batch 100/239, Loss: 1.7802971601486206, Variance: 0.1533733308315277
Epoch 7, Batch 200/239, Loss: 1.9646785259246826, Variance: 0.16086745262145996

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.2022787503340275, Training Loss Force: 3.3418621097715664, time: 3.737006425857544
Validation Loss Energy: 6.027170476883979, Validation Loss Force: 3.3971212635070165, time: 0.225449800491333
Test Loss Energy: 11.95109314944129, Test Loss Force: 11.608325504055093, time: 10.685159921646118

Epoch 8, Batch 100/239, Loss: 1.0637706518173218, Variance: 0.1514580249786377
Epoch 8, Batch 200/239, Loss: 1.1784321069717407, Variance: 0.15723533928394318

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.1412600520754825, Training Loss Force: 3.3980890448725254, time: 4.071174383163452
Validation Loss Energy: 3.554561988163151, Validation Loss Force: 3.357766996992172, time: 0.236236572265625
Test Loss Energy: 10.84928006808446, Test Loss Force: 11.905294783341818, time: 12.231853723526001

Epoch 9, Batch 100/239, Loss: 1.0065370798110962, Variance: 0.15996986627578735
Epoch 9, Batch 200/239, Loss: 0.8021705150604248, Variance: 0.15287581086158752

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.149249045102501, Training Loss Force: 3.3657554827248375, time: 4.193583011627197
Validation Loss Energy: 1.9430871758810704, Validation Loss Force: 3.420386069877271, time: 0.25707530975341797
Test Loss Energy: 9.934530296321675, Test Loss Force: 11.98624808425195, time: 12.489359617233276

Epoch 10, Batch 100/239, Loss: 1.616018533706665, Variance: 0.15749184787273407
Epoch 10, Batch 200/239, Loss: 1.5720810890197754, Variance: 0.152883380651474

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.14783315722045, Training Loss Force: 3.3576469280123336, time: 4.225057363510132
Validation Loss Energy: 4.799701799901267, Validation Loss Force: 3.337393438147217, time: 0.2582669258117676
Test Loss Energy: 10.651993623958083, Test Loss Force: 11.886342029684746, time: 12.220669031143188

Epoch 11, Batch 100/239, Loss: 1.9901058673858643, Variance: 0.1614183485507965
Epoch 11, Batch 200/239, Loss: 1.663611650466919, Variance: 0.15161195397377014

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.1379906941728315, Training Loss Force: 3.3946783777758567, time: 4.0256335735321045
Validation Loss Energy: 5.536116762019981, Validation Loss Force: 3.402962097873848, time: 0.24688959121704102
Test Loss Energy: 11.141941172715141, Test Loss Force: 11.935783580111114, time: 12.136274099349976

Epoch 12, Batch 100/239, Loss: 1.1542705297470093, Variance: 0.15808364748954773
Epoch 12, Batch 200/239, Loss: 0.9522081017494202, Variance: 0.15254122018814087

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.1144947297448375, Training Loss Force: 3.3594742376691324, time: 4.096828460693359
Validation Loss Energy: 3.172941585760541, Validation Loss Force: 3.3475065140823923, time: 0.2504909038543701
Test Loss Energy: 10.562430553550842, Test Loss Force: 12.19286823607209, time: 12.0922532081604

Epoch 13, Batch 100/239, Loss: 0.9614217281341553, Variance: 0.15797282755374908
Epoch 13, Batch 200/239, Loss: 0.9149353504180908, Variance: 0.1602548062801361

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.158173701799901, Training Loss Force: 3.362769706557072, time: 4.278683185577393
Validation Loss Energy: 2.0903532009158936, Validation Loss Force: 3.4716674421918783, time: 0.24929118156433105
Test Loss Energy: 10.070134533130402, Test Loss Force: 11.84634651479271, time: 12.417250156402588

Epoch 14, Batch 100/239, Loss: 1.6757209300994873, Variance: 0.15295863151550293
Epoch 14, Batch 200/239, Loss: 1.5345349311828613, Variance: 0.15466420352458954

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.113687452072978, Training Loss Force: 3.3731276584600485, time: 4.014377117156982
Validation Loss Energy: 5.5282490817523415, Validation Loss Force: 3.3653036239071032, time: 0.2543034553527832
Test Loss Energy: 11.846716075626528, Test Loss Force: 12.009710436043365, time: 12.555634498596191

Epoch 15, Batch 100/239, Loss: 0.5854218602180481, Variance: 0.10914680361747742
Epoch 15, Batch 200/239, Loss: 1.0361497402191162, Variance: 0.1421162486076355

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 3.7923747651130473, Training Loss Force: 3.7076658857856644, time: 4.426795244216919
Validation Loss Energy: 1.8179117393090944, Validation Loss Force: 3.3113238912148564, time: 0.2475605010986328
Test Loss Energy: 9.819504950787238, Test Loss Force: 12.024743592522762, time: 13.640938997268677

Epoch 16, Batch 100/239, Loss: 1.7972368001937866, Variance: 0.15554291009902954
Epoch 16, Batch 200/239, Loss: 1.6197519302368164, Variance: 0.1551872044801712

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.106405298026287, Training Loss Force: 3.3359727381892728, time: 4.135422229766846
Validation Loss Energy: 5.386179931914328, Validation Loss Force: 3.312021513649048, time: 0.22766542434692383
Test Loss Energy: 10.827642777898255, Test Loss Force: 11.544872878840122, time: 12.206625938415527

Epoch 17, Batch 100/239, Loss: 1.8184545040130615, Variance: 0.15794777870178223
Epoch 17, Batch 200/239, Loss: 1.6757681369781494, Variance: 0.15181726217269897

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.0777919941434035, Training Loss Force: 3.324288339662314, time: 4.453716993331909
Validation Loss Energy: 5.566498185628661, Validation Loss Force: 3.346832717246815, time: 0.2724294662475586
Test Loss Energy: 11.149448387192638, Test Loss Force: 11.87219328241038, time: 12.295255422592163

Epoch 18, Batch 100/239, Loss: 1.1759690046310425, Variance: 0.1590975821018219
Epoch 18, Batch 200/239, Loss: 0.9073784947395325, Variance: 0.1534610539674759

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 4.110859600969033, Training Loss Force: 3.32510764752391, time: 4.230046033859253
Validation Loss Energy: 3.426949409955352, Validation Loss Force: 3.5403898206995708, time: 0.267444372177124
Test Loss Energy: 10.592653569629622, Test Loss Force: 12.20000389385411, time: 12.32140851020813

Epoch 19, Batch 100/239, Loss: 0.9991142749786377, Variance: 0.15329870581626892
Epoch 19, Batch 200/239, Loss: 0.9642125964164734, Variance: 0.15887132287025452

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 4.099452003016379, Training Loss Force: 3.3809754834410617, time: 4.246148109436035
Validation Loss Energy: 2.4232406793790395, Validation Loss Force: 3.3465600588776265, time: 0.251143217086792
Test Loss Energy: 10.374143286454558, Test Loss Force: 11.889372992772657, time: 12.320469617843628

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–â–„â–†â–‚â–ƒâ–‡â–ˆâ–…â–‚â–„â–†â–„â–ƒâ–ˆâ–‚â–…â–†â–„â–„
wandb:   test_error_force â–ˆâ–‚â–‚â–„â–â–„â–‚â–ƒâ–…â–…â–…â–…â–‡â–„â–†â–†â–‚â–…â–‡â–…
wandb:          test_loss â–ˆâ–â–ƒâ–„â–â–ƒâ–„â–…â–„â–ƒâ–ƒâ–„â–„â–‚â–†â–„â–„â–…â–„â–„
wandb: train_error_energy â–†â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–†â–‡â–†â–â–†â–†â–†â–†
wandb:  train_error_force â–‡â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–â–â–â–‚
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–ƒâ–â–â–â–
wandb: valid_error_energy â–„â–â–‡â–ˆâ–ƒâ–â–‡â–ˆâ–„â–â–†â–‡â–ƒâ–â–‡â–â–‡â–‡â–„â–‚
wandb:  valid_error_force â–„â–â–ƒâ–â–„â–‚â–ƒâ–„â–‚â–„â–‚â–„â–‚â–†â–ƒâ–â–â–‚â–ˆâ–‚
wandb:         valid_loss â–„â–â–†â–ˆâ–ƒâ–â–†â–ˆâ–ƒâ–â–…â–‡â–ƒâ–‚â–‡â–â–‡â–‡â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7644
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.37414
wandb:   test_error_force 11.88937
wandb:          test_loss 9.8839
wandb: train_error_energy 4.09945
wandb:  train_error_force 3.38098
wandb:         train_loss 1.38573
wandb: valid_error_energy 2.42324
wandb:  valid_error_force 3.34656
wandb:         valid_loss 0.95365
wandb: 
wandb: ğŸš€ View run al_71_82 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/j7gctrab
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_081958-j7gctrab/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2811191082000732, Uncertainty Bias: -0.2925247550010681
0.00010108948 0.0040779114
1.8963974 5.0237126
(48745, 22, 3)
Found uncertainty sample 0 after 2263 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 18 steps.
Found uncertainty sample 3 after 1638 steps.
Found uncertainty sample 4 after 1938 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 1299 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1618 steps.
Found uncertainty sample 9 after 403 steps.
Found uncertainty sample 10 after 829 steps.
Found uncertainty sample 11 after 1767 steps.
Found uncertainty sample 12 after 514 steps.
Found uncertainty sample 13 after 1380 steps.
Found uncertainty sample 14 after 36 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 2778 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 164 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 2031 steps.
Found uncertainty sample 22 after 3944 steps.
Found uncertainty sample 23 after 3477 steps.
Found uncertainty sample 24 after 659 steps.
Found uncertainty sample 25 after 369 steps.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 1208 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 3013 steps.
Found uncertainty sample 31 after 426 steps.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 1619 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 3921 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 615 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 785 steps.
Found uncertainty sample 41 after 1415 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 1840 steps.
Did not find any uncertainty samples for sample 44.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 729 steps.
Found uncertainty sample 47 after 1434 steps.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Did not find any uncertainty samples for sample 50.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 468 steps.
Found uncertainty sample 55 after 257 steps.
Found uncertainty sample 56 after 2985 steps.
Found uncertainty sample 57 after 1429 steps.
Found uncertainty sample 58 after 507 steps.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 3166 steps.
Found uncertainty sample 62 after 1772 steps.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 325 steps.
Found uncertainty sample 66 after 1109 steps.
Found uncertainty sample 67 after 834 steps.
Found uncertainty sample 68 after 865 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 815 steps.
Did not find any uncertainty samples for sample 71.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1714 steps.
Found uncertainty sample 74 after 1033 steps.
Found uncertainty sample 75 after 1026 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 1124 steps.
Found uncertainty sample 78 after 834 steps.
Found uncertainty sample 79 after 1070 steps.
Found uncertainty sample 80 after 632 steps.
Found uncertainty sample 81 after 298 steps.
Found uncertainty sample 82 after 354 steps.
Found uncertainty sample 83 after 3291 steps.
Found uncertainty sample 84 after 1571 steps.
Found uncertainty sample 85 after 749 steps.
Found uncertainty sample 86 after 116 steps.
Found uncertainty sample 87 after 1933 steps.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1727 steps.
Found uncertainty sample 91 after 1185 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 464 steps.
Found uncertainty sample 94 after 462 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 40 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 2527 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_084852-uaq2yjoh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_83
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/uaq2yjoh
Training model 83. Added 67 samples to the dataset.
Epoch 0, Batch 100/241, Loss: 0.8899080753326416, Variance: 0.15892133116722107
Epoch 0, Batch 200/241, Loss: 0.9190762042999268, Variance: 0.15734541416168213

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.388702954719967, Training Loss Force: 3.518046034203853, time: 3.9955341815948486
Validation Loss Energy: 5.880482055613694, Validation Loss Force: 3.3312773290487323, time: 0.2211754322052002
Test Loss Energy: 11.66111694507154, Test Loss Force: 12.097404456613903, time: 10.679040431976318

Epoch 1, Batch 100/241, Loss: 1.077333927154541, Variance: 0.15738695859909058
Epoch 1, Batch 200/241, Loss: 1.2417455911636353, Variance: 0.1532570719718933

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.136673443329567, Training Loss Force: 3.339469608959601, time: 4.0221030712127686
Validation Loss Energy: 4.850320063499302, Validation Loss Force: 3.4328827633523167, time: 0.224257230758667
Test Loss Energy: 10.871745953297518, Test Loss Force: 11.817397222187376, time: 10.841413259506226

Epoch 2, Batch 100/241, Loss: 1.836430311203003, Variance: 0.1593470424413681
Epoch 2, Batch 200/241, Loss: 1.5752851963043213, Variance: 0.1541222631931305

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.122021335851383, Training Loss Force: 3.3828778825700283, time: 3.8548338413238525
Validation Loss Energy: 1.877207475191221, Validation Loss Force: 3.4823864008731227, time: 0.2233901023864746
Test Loss Energy: 9.693503965587942, Test Loss Force: 12.028627889063062, time: 10.749092102050781

Epoch 3, Batch 100/241, Loss: 1.6464893817901611, Variance: 0.15900203585624695
Epoch 3, Batch 200/241, Loss: 1.8392088413238525, Variance: 0.14960455894470215

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.105619222124651, Training Loss Force: 3.3643880123057857, time: 4.019590139389038
Validation Loss Energy: 3.821010226206374, Validation Loss Force: 3.380490327666816, time: 0.22119760513305664
Test Loss Energy: 11.143169490335731, Test Loss Force: 12.026199331118802, time: 10.809899091720581

Epoch 4, Batch 100/241, Loss: 0.8838897943496704, Variance: 0.15712833404541016
Epoch 4, Batch 200/241, Loss: 0.9958170652389526, Variance: 0.1566169261932373

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.10675641285561, Training Loss Force: 3.3487553623027737, time: 3.8586578369140625
Validation Loss Energy: 5.8386248208967615, Validation Loss Force: 3.4581520373742127, time: 0.22254490852355957
Test Loss Energy: 12.017988109081914, Test Loss Force: 12.096041693461455, time: 10.653694868087769

Epoch 5, Batch 100/241, Loss: 1.1343412399291992, Variance: 0.15588971972465515
Epoch 5, Batch 200/241, Loss: 1.2084792852401733, Variance: 0.15636278688907623

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.080951619075789, Training Loss Force: 3.388023939308894, time: 3.910325765609741
Validation Loss Energy: 5.461014133139447, Validation Loss Force: 3.2957258618753285, time: 0.22649836540222168
Test Loss Energy: 12.150487577890217, Test Loss Force: 12.050093274005382, time: 10.797350645065308

Epoch 6, Batch 100/241, Loss: 1.7687809467315674, Variance: 0.15293121337890625
Epoch 6, Batch 200/241, Loss: 1.6935656070709229, Variance: 0.15933799743652344

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.113306778760188, Training Loss Force: 3.3641481865483613, time: 4.007835149765015
Validation Loss Energy: 2.579064492032019, Validation Loss Force: 3.467572445263693, time: 0.22442245483398438
Test Loss Energy: 10.377753184658655, Test Loss Force: 11.95576959321694, time: 10.667690992355347

Epoch 7, Batch 100/241, Loss: 1.5561188459396362, Variance: 0.15310712158679962
Epoch 7, Batch 200/241, Loss: 1.7359569072723389, Variance: 0.1563301980495453

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.114162097608583, Training Loss Force: 3.3641041719725373, time: 3.86933970451355
Validation Loss Energy: 3.4139482595302066, Validation Loss Force: 3.321216524695563, time: 0.22187471389770508
Test Loss Energy: 10.58920701031923, Test Loss Force: 12.093230713500038, time: 10.900140285491943

Epoch 8, Batch 100/241, Loss: 0.8159996867179871, Variance: 0.15383872389793396
Epoch 8, Batch 200/241, Loss: 0.9211925864219666, Variance: 0.15551191568374634

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.112481656065431, Training Loss Force: 3.3621431007819536, time: 4.075462818145752
Validation Loss Energy: 5.574108015025316, Validation Loss Force: 3.339194163530335, time: 0.22878599166870117
Test Loss Energy: 11.520012831653906, Test Loss Force: 12.242347372117651, time: 10.666252374649048

Epoch 9, Batch 100/241, Loss: 1.1935499906539917, Variance: 0.1573055386543274
Epoch 9, Batch 200/241, Loss: 1.1503288745880127, Variance: 0.15132732689380646

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.060291868936415, Training Loss Force: 3.407350150265549, time: 3.8523056507110596
Validation Loss Energy: 4.377214817811957, Validation Loss Force: 3.375315552382741, time: 0.22113370895385742
Test Loss Energy: 10.846189345727327, Test Loss Force: 12.062925237557222, time: 10.805742740631104

Epoch 10, Batch 100/241, Loss: 1.8871248960494995, Variance: 0.16234710812568665
Epoch 10, Batch 200/241, Loss: 1.6151336431503296, Variance: 0.15159723162651062

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.09447404697995, Training Loss Force: 3.3492213219749947, time: 3.9949755668640137
Validation Loss Energy: 1.8370600333449385, Validation Loss Force: 3.3180107013343543, time: 0.22243452072143555
Test Loss Energy: 10.104368345140514, Test Loss Force: 12.119345626064911, time: 11.876973867416382

Epoch 11, Batch 100/241, Loss: 1.9039545059204102, Variance: 0.15634073317050934
Epoch 11, Batch 200/241, Loss: 1.7333370447158813, Variance: 0.15302158892154694

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.122536662936345, Training Loss Force: 3.3535771092840707, time: 4.314726114273071
Validation Loss Energy: 3.6717500319679077, Validation Loss Force: 3.2969068990253296, time: 0.24909591674804688
Test Loss Energy: 11.06083820083312, Test Loss Force: 11.893915224177862, time: 12.1961829662323

Epoch 12, Batch 100/241, Loss: 0.9283090829849243, Variance: 0.1556980162858963
Epoch 12, Batch 200/241, Loss: 0.8751810789108276, Variance: 0.15596900880336761

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.103715173638131, Training Loss Force: 3.3428806286373356, time: 4.260104656219482
Validation Loss Energy: 5.751244883317206, Validation Loss Force: 3.3379205516829145, time: 0.2646613121032715
Test Loss Energy: 12.232117956216953, Test Loss Force: 12.06573745793734, time: 12.13676142692566

Epoch 13, Batch 100/241, Loss: 1.1897703409194946, Variance: 0.1566138118505478
Epoch 13, Batch 200/241, Loss: 1.2022159099578857, Variance: 0.15875448286533356

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.079772040362497, Training Loss Force: 3.3981096911528366, time: 4.494020462036133
Validation Loss Energy: 5.464435778570345, Validation Loss Force: 3.3649033204500114, time: 0.2626829147338867
Test Loss Energy: 11.860895306423556, Test Loss Force: 12.066314625690044, time: 12.569632053375244

Epoch 14, Batch 100/241, Loss: 1.6851880550384521, Variance: 0.15292659401893616
Epoch 14, Batch 200/241, Loss: 1.7457443475723267, Variance: 0.1594405174255371

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.079978147451265, Training Loss Force: 3.3258314240514926, time: 4.416752099990845
Validation Loss Energy: 2.372036573258069, Validation Loss Force: 3.3533389426889655, time: 0.27039623260498047
Test Loss Energy: 10.691498217502733, Test Loss Force: 12.12090954222843, time: 12.512726068496704

Epoch 15, Batch 100/241, Loss: 1.8360579013824463, Variance: 0.15499836206436157
Epoch 15, Batch 200/241, Loss: 1.4681615829467773, Variance: 0.16142824292182922

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.058041772270479, Training Loss Force: 3.353747244218513, time: 4.587019920349121
Validation Loss Energy: 4.201810204140813, Validation Loss Force: 3.386629037683613, time: 0.2573237419128418
Test Loss Energy: 10.850866983289396, Test Loss Force: 12.040643610479515, time: 12.572438716888428

Epoch 16, Batch 100/241, Loss: 0.9451622366905212, Variance: 0.15649068355560303
Epoch 16, Batch 200/241, Loss: 0.9257857799530029, Variance: 0.1574142873287201

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.0799408258849565, Training Loss Force: 3.377004517257309, time: 4.440486669540405
Validation Loss Energy: 5.645155059416495, Validation Loss Force: 3.3370044351492814, time: 0.26349401473999023
Test Loss Energy: 11.268114468859174, Test Loss Force: 11.821582376220615, time: 12.488005638122559

Epoch 17, Batch 100/241, Loss: 1.1808645725250244, Variance: 0.1567411720752716
Epoch 17, Batch 200/241, Loss: 0.7359166145324707, Variance: 0.11967335641384125

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 3.316149469306584, Training Loss Force: 3.451053081679601, time: 4.787298202514648
Validation Loss Energy: 2.0606159591463835, Validation Loss Force: 3.3259440133327565, time: 0.25324440002441406
Test Loss Energy: 10.394542303027992, Test Loss Force: 12.68438609427851, time: 12.603123426437378

Epoch 18, Batch 100/241, Loss: 0.7649106979370117, Variance: 0.11885618418455124
Epoch 18, Batch 200/241, Loss: 1.1863861083984375, Variance: 0.11045178025960922

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6164668673768885, Training Loss Force: 3.2765715752907423, time: 4.404423236846924
Validation Loss Energy: 3.1719333955350666, Validation Loss Force: 3.2887898849815373, time: 0.2635946273803711
Test Loss Energy: 10.609491637905236, Test Loss Force: 12.563605490443512, time: 12.741539716720581

Epoch 19, Batch 100/241, Loss: 1.230150580406189, Variance: 0.11734604835510254
Epoch 19, Batch 200/241, Loss: 0.6130638122558594, Variance: 0.11081169545650482

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5904010591611213, Training Loss Force: 3.2791728812558003, time: 4.411471366882324
Validation Loss Energy: 1.7431151135023506, Validation Loss Force: 3.325269938160746, time: 0.25879478454589844
Test Loss Energy: 10.100895014237103, Test Loss Force: 12.31149754818437, time: 13.584388494491577

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.039 MB of 0.061 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–â–…â–‡â–ˆâ–ƒâ–ƒâ–†â–„â–‚â–…â–ˆâ–‡â–„â–„â–…â–ƒâ–„â–‚
wandb:   test_error_force â–ƒâ–â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–â–ˆâ–‡â–…
wandb:          test_loss â–ƒâ–â–â–‚â–ƒâ–ƒâ–â–‚â–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‡â–ˆâ–ˆ
wandb: train_error_energy â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–„â–â–
wandb:  train_error_force â–ˆâ–ƒâ–„â–„â–ƒâ–„â–„â–„â–ƒâ–…â–ƒâ–ƒâ–ƒâ–…â–‚â–ƒâ–„â–†â–â–
wandb:         train_loss â–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–„â–â–
wandb: valid_error_energy â–ˆâ–†â–â–…â–ˆâ–‡â–‚â–„â–‡â–…â–â–„â–ˆâ–‡â–‚â–…â–ˆâ–‚â–ƒâ–
wandb:  valid_error_force â–ƒâ–†â–ˆâ–„â–‡â–â–‡â–‚â–ƒâ–„â–‚â–â–ƒâ–„â–ƒâ–…â–ƒâ–‚â–â–‚
wandb:         valid_loss â–ˆâ–†â–ƒâ–„â–ˆâ–‡â–ƒâ–„â–‡â–…â–‚â–„â–‡â–‡â–ƒâ–…â–ˆâ–‚â–„â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 7704
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.1009
wandb:   test_error_force 12.3115
wandb:          test_loss 13.35572
wandb: train_error_energy 2.5904
wandb:  train_error_force 3.27917
wandb:         train_loss 0.90168
wandb: valid_error_energy 1.74312
wandb:  valid_error_force 3.32527
wandb:         valid_loss 0.62405
wandb: 
wandb: ğŸš€ View run al_71_83 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/uaq2yjoh
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_084852-uaq2yjoh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.112481117248535, Uncertainty Bias: -0.12074150145053864
2.2888184e-05 0.0021333694
1.9063035 4.8077474
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 803 steps.
Found uncertainty sample 2 after 342 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 1159 steps.
Found uncertainty sample 5 after 2625 steps.
Found uncertainty sample 6 after 1374 steps.
Found uncertainty sample 7 after 804 steps.
Found uncertainty sample 8 after 1947 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 521 steps.
Found uncertainty sample 11 after 209 steps.
Found uncertainty sample 12 after 3866 steps.
Found uncertainty sample 13 after 448 steps.
Found uncertainty sample 14 after 555 steps.
Found uncertainty sample 15 after 584 steps.
Found uncertainty sample 16 after 31 steps.
Found uncertainty sample 17 after 137 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 1438 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 3055 steps.
Found uncertainty sample 22 after 2608 steps.
Found uncertainty sample 23 after 604 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 273 steps.
Found uncertainty sample 26 after 50 steps.
Found uncertainty sample 27 after 2520 steps.
Found uncertainty sample 28 after 6 steps.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 1138 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 2948 steps.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 1467 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 1090 steps.
Found uncertainty sample 43 after 1600 steps.
Found uncertainty sample 44 after 3025 steps.
Found uncertainty sample 45 after 2180 steps.
Found uncertainty sample 46 after 1602 steps.
Did not find any uncertainty samples for sample 47.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 2623 steps.
Found uncertainty sample 50 after 4 steps.
Found uncertainty sample 51 after 223 steps.
Found uncertainty sample 52 after 1730 steps.
Found uncertainty sample 53 after 300 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 2543 steps.
Found uncertainty sample 56 after 449 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 195 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 1057 steps.
Found uncertainty sample 61 after 1306 steps.
Found uncertainty sample 62 after 1769 steps.
Found uncertainty sample 63 after 2003 steps.
Found uncertainty sample 64 after 37 steps.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 1106 steps.
Found uncertainty sample 70 after 1480 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 1500 steps.
Found uncertainty sample 73 after 714 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 282 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 911 steps.
Found uncertainty sample 78 after 520 steps.
Found uncertainty sample 79 after 1882 steps.
Found uncertainty sample 80 after 30 steps.
Found uncertainty sample 81 after 2364 steps.
Found uncertainty sample 82 after 7 steps.
Found uncertainty sample 83 after 3165 steps.
Found uncertainty sample 84 after 1467 steps.
Found uncertainty sample 85 after 134 steps.
Found uncertainty sample 86 after 55 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 2090 steps.
Found uncertainty sample 89 after 3417 steps.
Found uncertainty sample 90 after 1249 steps.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 1127 steps.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 119 steps.
Found uncertainty sample 98 after 838 steps.
Found uncertainty sample 99 after 861 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_091814-3stppbqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_84
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/3stppbqc
Training model 84. Added 66 samples to the dataset.
Epoch 0, Batch 100/243, Loss: 0.8975993394851685, Variance: 0.1478901207447052
Epoch 0, Batch 200/243, Loss: 0.7879559397697449, Variance: 0.14778798818588257

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.4231837496430195, Training Loss Force: 3.4464200380828975, time: 4.124475479125977
Validation Loss Energy: 1.8513908247413482, Validation Loss Force: 3.3445779138078637, time: 0.21772313117980957
Test Loss Energy: 10.147802566466671, Test Loss Force: 11.64389821281579, time: 10.6852867603302

Epoch 1, Batch 100/243, Loss: 1.972886085510254, Variance: 0.14647474884986877
Epoch 1, Batch 200/243, Loss: 1.5512313842773438, Variance: 0.1603032797574997

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.101092922150035, Training Loss Force: 3.355986617887721, time: 3.9654436111450195
Validation Loss Energy: 4.533501169948316, Validation Loss Force: 3.433031339339937, time: 0.22538542747497559
Test Loss Energy: 10.710597872174148, Test Loss Force: 11.632891562683492, time: 10.83189582824707

Epoch 2, Batch 100/243, Loss: 1.8081884384155273, Variance: 0.155302956700325
Epoch 2, Batch 200/243, Loss: 1.9386552572250366, Variance: 0.15296481549739838

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.102322465436825, Training Loss Force: 3.3577346265796244, time: 4.065800189971924
Validation Loss Energy: 5.881170214609184, Validation Loss Force: 3.299321244907338, time: 0.21859312057495117
Test Loss Energy: 11.778068740118437, Test Loss Force: 11.30677022896663, time: 10.798628330230713

Epoch 3, Batch 100/243, Loss: 0.994181215763092, Variance: 0.15158145129680634
Epoch 3, Batch 200/243, Loss: 1.1185609102249146, Variance: 0.15918195247650146

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.090841324472245, Training Loss Force: 3.385188094783781, time: 4.009393930435181
Validation Loss Energy: 3.7538067296103406, Validation Loss Force: 3.298912034684409, time: 0.22767376899719238
Test Loss Energy: 10.396766406395624, Test Loss Force: 11.647722251121461, time: 10.847261905670166

Epoch 4, Batch 100/243, Loss: 1.026029348373413, Variance: 0.15466493368148804
Epoch 4, Batch 200/243, Loss: 0.8226908445358276, Variance: 0.1540224850177765

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.08935616754377, Training Loss Force: 3.35093590894937, time: 3.9481589794158936
Validation Loss Energy: 1.8881759346504783, Validation Loss Force: 3.3280216834661216, time: 0.22095608711242676
Test Loss Energy: 9.91543413440817, Test Loss Force: 11.658040692711472, time: 10.56159257888794

Epoch 5, Batch 100/243, Loss: 1.8595225811004639, Variance: 0.09491732716560364
Epoch 5, Batch 200/243, Loss: 1.2696120738983154, Variance: 0.11805661022663116

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.9562649508321543, Training Loss Force: 3.527062507166839, time: 3.9919066429138184
Validation Loss Energy: 2.2022073723318103, Validation Loss Force: 3.303085132815528, time: 0.2273707389831543
Test Loss Energy: 9.918097802991165, Test Loss Force: 11.57378523401499, time: 11.988384485244751

Epoch 6, Batch 100/243, Loss: 0.9537287950515747, Variance: 0.1139644980430603
Epoch 6, Batch 200/243, Loss: 1.1647461652755737, Variance: 0.11256617307662964

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.606787726197302, Training Loss Force: 3.279702543630417, time: 3.965465545654297
Validation Loss Energy: 2.622243268130033, Validation Loss Force: 3.3902424012491563, time: 0.22502422332763672
Test Loss Energy: 10.686153388567815, Test Loss Force: 12.115325107302809, time: 10.610509872436523

Epoch 7, Batch 100/243, Loss: 0.6556602716445923, Variance: 0.11229744553565979
Epoch 7, Batch 200/243, Loss: 1.3286089897155762, Variance: 0.11691600829362869

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.6083726260744773, Training Loss Force: 3.2783666066432358, time: 4.0123724937438965
Validation Loss Energy: 2.161981133441512, Validation Loss Force: 3.3077823941161615, time: 0.2264842987060547
Test Loss Energy: 10.012983136972126, Test Loss Force: 11.977307295501983, time: 10.811581373214722

Epoch 8, Batch 100/243, Loss: 0.8364317417144775, Variance: 0.11563434451818466
Epoch 8, Batch 200/243, Loss: 1.2227948904037476, Variance: 0.11094745993614197

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5860036662439896, Training Loss Force: 3.276251373462002, time: 3.9563422203063965
Validation Loss Energy: 2.585098937221235, Validation Loss Force: 3.358035201978442, time: 0.21819758415222168
Test Loss Energy: 10.306242664349584, Test Loss Force: 12.102001728101396, time: 10.624104976654053

Epoch 9, Batch 100/243, Loss: 0.7872605323791504, Variance: 0.11148898303508759
Epoch 9, Batch 200/243, Loss: 1.1966325044631958, Variance: 0.11513417214155197

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6078750310910412, Training Loss Force: 3.268775003979311, time: 3.9221537113189697
Validation Loss Energy: 2.0753435747944797, Validation Loss Force: 3.3225926756846835, time: 0.2188575267791748
Test Loss Energy: 10.37168037917147, Test Loss Force: 12.409235630661628, time: 10.884894132614136

Epoch 10, Batch 100/243, Loss: 1.0580674409866333, Variance: 0.11802402138710022
Epoch 10, Batch 200/243, Loss: 1.093106746673584, Variance: 0.11162163317203522

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.610331179898029, Training Loss Force: 3.2869077909777604, time: 4.032031536102295
Validation Loss Energy: 2.586587090790757, Validation Loss Force: 3.3304755531215324, time: 0.2348184585571289
Test Loss Energy: 10.446283599046415, Test Loss Force: 11.866611053191692, time: 10.64621090888977

Epoch 11, Batch 100/243, Loss: 0.6799415946006775, Variance: 0.11263160407543182
Epoch 11, Batch 200/243, Loss: 1.2482366561889648, Variance: 0.11392048001289368

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6096407703863975, Training Loss Force: 3.2809043348276865, time: 3.975052833557129
Validation Loss Energy: 2.0322518066652373, Validation Loss Force: 3.318949697122732, time: 0.22925519943237305
Test Loss Energy: 9.714709361569152, Test Loss Force: 11.724305277165785, time: 10.802992582321167

Epoch 12, Batch 100/243, Loss: 0.9953687787055969, Variance: 0.11493846774101257
Epoch 12, Batch 200/243, Loss: 1.2944923639297485, Variance: 0.11317172646522522

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6159531859326166, Training Loss Force: 3.287007987511758, time: 3.969991445541382
Validation Loss Energy: 2.538115636095899, Validation Loss Force: 3.3540537663146797, time: 0.22783422470092773
Test Loss Energy: 10.353092271305707, Test Loss Force: 11.959884602971796, time: 10.74833369255066

Epoch 13, Batch 100/243, Loss: 0.7978439331054688, Variance: 0.11192309856414795
Epoch 13, Batch 200/243, Loss: 1.462375521659851, Variance: 0.11421079933643341

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6119411135601105, Training Loss Force: 3.27876139079196, time: 3.833681344985962
Validation Loss Energy: 2.1455395777099233, Validation Loss Force: 3.328722089264638, time: 0.22426462173461914
Test Loss Energy: 10.169548029557127, Test Loss Force: 12.039348813877893, time: 10.759316444396973

Epoch 14, Batch 100/243, Loss: 0.8785677552223206, Variance: 0.11882902681827545
Epoch 14, Batch 200/243, Loss: 1.1517037153244019, Variance: 0.10859431326389313

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.613527776921941, Training Loss Force: 3.287169566506983, time: 4.032751798629761
Validation Loss Energy: 2.8591071267724253, Validation Loss Force: 3.244909033066413, time: 0.2199399471282959
Test Loss Energy: 10.64692327263425, Test Loss Force: 12.112614797111489, time: 10.611629247665405

Epoch 15, Batch 100/243, Loss: 0.9584833979606628, Variance: 0.11278393864631653
Epoch 15, Batch 200/243, Loss: 1.2332639694213867, Variance: 0.11761996150016785

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6155025301549433, Training Loss Force: 3.29097368010037, time: 3.917041540145874
Validation Loss Energy: 2.175058381047278, Validation Loss Force: 3.2969193684975013, time: 0.2231137752532959
Test Loss Energy: 10.155872968338766, Test Loss Force: 12.201897290405176, time: 10.713272094726562

Epoch 16, Batch 100/243, Loss: 0.7589033246040344, Variance: 0.1117914617061615
Epoch 16, Batch 200/243, Loss: 0.9688625335693359, Variance: 0.10792750865221024

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6090962242848588, Training Loss Force: 3.288625215191953, time: 3.999774217605591
Validation Loss Energy: 2.6611246410462135, Validation Loss Force: 3.3041571734185977, time: 0.2259385585784912
Test Loss Energy: 11.025274492102822, Test Loss Force: 12.058276432735035, time: 10.749743938446045

Epoch 17, Batch 100/243, Loss: 0.6758474111557007, Variance: 0.1096210926771164
Epoch 17, Batch 200/243, Loss: 1.254380702972412, Variance: 0.11475028097629547

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6074312239931894, Training Loss Force: 3.2866685206099535, time: 3.9981095790863037
Validation Loss Energy: 1.9747348632553545, Validation Loss Force: 3.302053952000737, time: 0.21918058395385742
Test Loss Energy: 9.993473360708121, Test Loss Force: 11.935147543765778, time: 10.53914189338684

Epoch 18, Batch 100/243, Loss: 0.9896379113197327, Variance: 0.11188577860593796
Epoch 18, Batch 200/243, Loss: 1.244102120399475, Variance: 0.10682783275842667

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.618403787253969, Training Loss Force: 3.28394098484711, time: 4.06259822845459
Validation Loss Energy: 2.551636663309499, Validation Loss Force: 3.307803158014019, time: 0.22338247299194336
Test Loss Energy: 10.460111119847122, Test Loss Force: 11.97644250449794, time: 10.535981893539429

Epoch 19, Batch 100/243, Loss: 0.658728301525116, Variance: 0.10780784487724304
Epoch 19, Batch 200/243, Loss: 1.3395665884017944, Variance: 0.11773823946714401

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.629745404922944, Training Loss Force: 3.2834742070902037, time: 4.021247863769531
Validation Loss Energy: 1.8613280755521702, Validation Loss Force: 3.404765049948426, time: 0.22178912162780762
Test Loss Energy: 10.01090369626931, Test Loss Force: 11.981701087027536, time: 10.60626220703125

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–ˆâ–ƒâ–‚â–‚â–„â–‚â–ƒâ–ƒâ–ƒâ–â–ƒâ–ƒâ–„â–‚â–…â–‚â–„â–‚
wandb:   test_error_force â–ƒâ–ƒâ–â–ƒâ–ƒâ–ƒâ–†â–…â–†â–ˆâ–…â–„â–…â–†â–†â–‡â–†â–…â–…â–…
wandb:          test_loss â–‚â–‚â–‚â–â–â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–†â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–†
wandb: train_error_energy â–ˆâ–‡â–‡â–‡â–‡â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–†â–ƒâ–ƒâ–„â–ƒâ–ˆâ–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–
wandb:         train_loss â–ˆâ–…â–…â–…â–…â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–†â–ˆâ–„â–â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–
wandb:  valid_error_force â–…â–ˆâ–ƒâ–ƒâ–„â–ƒâ–†â–ƒâ–…â–„â–„â–„â–…â–„â–â–ƒâ–ƒâ–ƒâ–ƒâ–‡
wandb:         valid_loss â–‚â–†â–ˆâ–„â–‚â–â–ƒâ–â–‚â–â–‚â–â–‚â–â–ƒâ–â–‚â–â–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 7763
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.0109
wandb:   test_error_force 11.9817
wandb:          test_loss 12.54541
wandb: train_error_energy 2.62975
wandb:  train_error_force 3.28347
wandb:         train_loss 0.91517
wandb: valid_error_energy 1.86133
wandb:  valid_error_force 3.40477
wandb:         valid_loss 0.68238
wandb: 
wandb: ğŸš€ View run al_71_84 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/3stppbqc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_091814-3stppbqc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.0029356479644775, Uncertainty Bias: -0.10112537443637848
0.00012397766 0.0068683624
2.0895567 4.85199
(48745, 22, 3)
Found uncertainty sample 0 after 838 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 1038 steps.
Found uncertainty sample 4 after 1359 steps.
Found uncertainty sample 5 after 2469 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 808 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 56 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 3165 steps.
Did not find any uncertainty samples for sample 12.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 3214 steps.
Found uncertainty sample 15 after 13 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 95 steps.
Found uncertainty sample 18 after 791 steps.
Found uncertainty sample 19 after 6 steps.
Found uncertainty sample 20 after 461 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 2314 steps.
Found uncertainty sample 23 after 2081 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 1924 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 2327 steps.
Found uncertainty sample 28 after 748 steps.
Found uncertainty sample 29 after 2285 steps.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 107 steps.
Found uncertainty sample 33 after 2203 steps.
Found uncertainty sample 34 after 353 steps.
Found uncertainty sample 35 after 2826 steps.
Found uncertainty sample 36 after 3231 steps.
Found uncertainty sample 37 after 933 steps.
Found uncertainty sample 38 after 1104 steps.
Found uncertainty sample 39 after 1811 steps.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 3028 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1877 steps.
Found uncertainty sample 45 after 777 steps.
Found uncertainty sample 46 after 466 steps.
Found uncertainty sample 47 after 2358 steps.
Found uncertainty sample 48 after 3354 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 435 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 3943 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 3159 steps.
Found uncertainty sample 55 after 2669 steps.
Found uncertainty sample 56 after 392 steps.
Found uncertainty sample 57 after 550 steps.
Did not find any uncertainty samples for sample 58.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 623 steps.
Found uncertainty sample 61 after 1563 steps.
Found uncertainty sample 62 after 226 steps.
Found uncertainty sample 63 after 1240 steps.
Found uncertainty sample 64 after 127 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 1418 steps.
Found uncertainty sample 68 after 195 steps.
Found uncertainty sample 69 after 1671 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 1051 steps.
Found uncertainty sample 72 after 2484 steps.
Found uncertainty sample 73 after 1433 steps.
Found uncertainty sample 74 after 2390 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 1907 steps.
Found uncertainty sample 77 after 1221 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 3338 steps.
Found uncertainty sample 80 after 1187 steps.
Found uncertainty sample 81 after 2643 steps.
Found uncertainty sample 82 after 2009 steps.
Found uncertainty sample 83 after 1367 steps.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 3179 steps.
Found uncertainty sample 88 after 3311 steps.
Found uncertainty sample 89 after 188 steps.
Found uncertainty sample 90 after 122 steps.
Found uncertainty sample 91 after 781 steps.
Found uncertainty sample 92 after 383 steps.
Found uncertainty sample 93 after 1072 steps.
Found uncertainty sample 94 after 1846 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1884 steps.
Found uncertainty sample 98 after 154 steps.
Found uncertainty sample 99 after 468 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_094630-clhte9t4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_85
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/clhte9t4
Training model 85. Added 73 samples to the dataset.
Epoch 0, Batch 100/245, Loss: 1.140058159828186, Variance: 0.07439872622489929
Epoch 0, Batch 200/245, Loss: 0.8689327239990234, Variance: 0.09945306181907654

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.4014680980154757, Training Loss Force: 3.955007703573469, time: 4.01334547996521
Validation Loss Energy: 1.7702745424030981, Validation Loss Force: 3.450428145707556, time: 0.23043251037597656
Test Loss Energy: 10.41530520994977, Test Loss Force: 12.44271371991176, time: 10.641742944717407

Epoch 1, Batch 100/245, Loss: 0.4010680913925171, Variance: 0.07851892709732056
Epoch 1, Batch 200/245, Loss: 0.640810489654541, Variance: 0.07955871522426605

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5936095284357157, Training Loss Force: 3.257089305732626, time: 4.038330554962158
Validation Loss Energy: 1.6563959187796928, Validation Loss Force: 3.2489723553085894, time: 0.21911144256591797
Test Loss Energy: 10.062055103248538, Test Loss Force: 12.30706676326961, time: 10.823981046676636

Epoch 2, Batch 100/245, Loss: 0.6126013398170471, Variance: 0.07552330195903778
Epoch 2, Batch 200/245, Loss: 0.5851520299911499, Variance: 0.07985363900661469

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5556277738599489, Training Loss Force: 3.2550234286570983, time: 3.9983556270599365
Validation Loss Energy: 1.658605464714734, Validation Loss Force: 3.2878787228364006, time: 0.24077343940734863
Test Loss Energy: 9.640199126464253, Test Loss Force: 12.070934886892498, time: 10.69105577468872

Epoch 3, Batch 100/245, Loss: 0.3136703372001648, Variance: 0.07739658653736115
Epoch 3, Batch 200/245, Loss: 0.5948429107666016, Variance: 0.07943370938301086

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5862252347094217, Training Loss Force: 3.2696941271177207, time: 3.938044548034668
Validation Loss Energy: 1.4571042776436418, Validation Loss Force: 3.391293288283362, time: 0.22240304946899414
Test Loss Energy: 9.651425246767946, Test Loss Force: 12.165861695624956, time: 10.880932092666626

Epoch 4, Batch 100/245, Loss: 0.17632120847702026, Variance: 0.07632669806480408
Epoch 4, Batch 200/245, Loss: 0.32079052925109863, Variance: 0.07463771849870682

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5701575856893148, Training Loss Force: 3.2842567819644706, time: 3.985766887664795
Validation Loss Energy: 1.9702520880095362, Validation Loss Force: 3.3060861005624833, time: 0.22664570808410645
Test Loss Energy: 10.454375796847057, Test Loss Force: 12.612790833786832, time: 10.659536838531494

Epoch 5, Batch 100/245, Loss: 0.5109760165214539, Variance: 0.07391925156116486
Epoch 5, Batch 200/245, Loss: 0.18825900554656982, Variance: 0.0748913437128067

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6146938685525327, Training Loss Force: 3.2797571184000014, time: 3.976989269256592
Validation Loss Energy: 1.7472703702681525, Validation Loss Force: 3.283805714486376, time: 0.2248859405517578
Test Loss Energy: 10.172710574954792, Test Loss Force: 12.241975904023503, time: 10.842906951904297

Epoch 6, Batch 100/245, Loss: 0.42983734607696533, Variance: 0.07746447622776031
Epoch 6, Batch 200/245, Loss: 0.45691341161727905, Variance: 0.08000852167606354

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.6095803802243132, Training Loss Force: 3.2812301429643376, time: 3.920626401901245
Validation Loss Energy: 1.4198175498160228, Validation Loss Force: 3.3568887342133458, time: 0.2230391502380371
Test Loss Energy: 9.724881834790457, Test Loss Force: 12.04485614698867, time: 10.728208303451538

Epoch 7, Batch 100/245, Loss: 0.5374770164489746, Variance: 0.08152960985898972
Epoch 7, Batch 200/245, Loss: 0.798005223274231, Variance: 0.0815649926662445

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5922415185032261, Training Loss Force: 3.283482755838763, time: 3.9139177799224854
Validation Loss Energy: 1.3958172672567692, Validation Loss Force: 3.3863572699910076, time: 0.22205162048339844
Test Loss Energy: 10.001244812347606, Test Loss Force: 12.411357049744584, time: 10.89286470413208

Epoch 8, Batch 100/245, Loss: 0.18326932191848755, Variance: 0.07438203692436218
Epoch 8, Batch 200/245, Loss: 0.5076152086257935, Variance: 0.07926496863365173

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.6047003877612083, Training Loss Force: 3.298244835899167, time: 3.9501752853393555
Validation Loss Energy: 1.8466755829071813, Validation Loss Force: 3.3910105541353204, time: 0.23023152351379395
Test Loss Energy: 10.301578587455086, Test Loss Force: 12.407331415421163, time: 10.63610553741455

Epoch 9, Batch 100/245, Loss: 0.3479354977607727, Variance: 0.07682012766599655
Epoch 9, Batch 200/245, Loss: 0.2736596465110779, Variance: 0.07789293676614761

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.611442019706074, Training Loss Force: 3.2724653764225717, time: 3.8778672218322754
Validation Loss Energy: 1.5670354783153257, Validation Loss Force: 3.268455738575412, time: 0.22278237342834473
Test Loss Energy: 10.26471130126449, Test Loss Force: 12.302731989559993, time: 12.065713167190552

Epoch 10, Batch 100/245, Loss: 0.6699636578559875, Variance: 0.08139146864414215
Epoch 10, Batch 200/245, Loss: 0.30854249000549316, Variance: 0.07874689996242523

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6077512258988087, Training Loss Force: 3.295306863752942, time: 3.9974656105041504
Validation Loss Energy: 1.5663958055263616, Validation Loss Force: 3.3518293015468617, time: 0.22701573371887207
Test Loss Energy: 9.975853059220283, Test Loss Force: 12.375374140116417, time: 10.76715874671936

Epoch 11, Batch 100/245, Loss: 0.5785342454910278, Variance: 0.0779603123664856
Epoch 11, Batch 200/245, Loss: 0.6757305264472961, Variance: 0.08143305778503418

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.6209961251943854, Training Loss Force: 3.2834559528355687, time: 3.9713637828826904
Validation Loss Energy: 1.4750289768889693, Validation Loss Force: 3.3340878590086325, time: 0.2269744873046875
Test Loss Energy: 9.822574865820195, Test Loss Force: 12.382243928606583, time: 11.025744915008545

Epoch 12, Batch 100/245, Loss: 0.5623186826705933, Variance: 0.07764676213264465
Epoch 12, Batch 200/245, Loss: 0.3776758313179016, Variance: 0.0746750682592392

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.6240205275056692, Training Loss Force: 3.279667518153387, time: 4.10953426361084
Validation Loss Energy: 1.9749432815759431, Validation Loss Force: 3.3129223773461853, time: 0.2324695587158203
Test Loss Energy: 10.123686901069487, Test Loss Force: 12.36515554290512, time: 10.696645259857178

Epoch 13, Batch 100/245, Loss: 0.1441827416419983, Variance: 0.07835310697555542
Epoch 13, Batch 200/245, Loss: 0.2672731280326843, Variance: 0.07374109327793121

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.6168873041476528, Training Loss Force: 3.3049306287842097, time: 3.9580142498016357
Validation Loss Energy: 1.9209471827014708, Validation Loss Force: 3.3555415858804833, time: 0.23888206481933594
Test Loss Energy: 10.180974636030262, Test Loss Force: 12.331700149586572, time: 10.900172710418701

Epoch 14, Batch 100/245, Loss: 0.8257638812065125, Variance: 0.07748788595199585
Epoch 14, Batch 200/245, Loss: 0.5130777955055237, Variance: 0.08095528930425644

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.6011051617336682, Training Loss Force: 3.2884800831266734, time: 3.946300745010376
Validation Loss Energy: 1.4154867462623002, Validation Loss Force: 3.3282225867030353, time: 0.22861433029174805
Test Loss Energy: 9.642091968091169, Test Loss Force: 12.27679564599208, time: 10.817574262619019

Epoch 15, Batch 100/245, Loss: 0.4354783892631531, Variance: 0.08194123208522797
Epoch 15, Batch 200/245, Loss: 0.6672900915145874, Variance: 0.08090415596961975

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.6162862228754522, Training Loss Force: 3.3002385863484878, time: 3.891044855117798
Validation Loss Energy: 1.4882242113084336, Validation Loss Force: 3.338380029465536, time: 0.24654769897460938
Test Loss Energy: 10.010563835535814, Test Loss Force: 12.61656945691319, time: 11.018361568450928

Epoch 16, Batch 100/245, Loss: 0.6013013124465942, Variance: 0.07804811745882034
Epoch 16, Batch 200/245, Loss: 0.233850359916687, Variance: 0.07550552487373352

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.609613791502446, Training Loss Force: 3.2855222661608514, time: 3.985313653945923
Validation Loss Energy: 1.7223157555830957, Validation Loss Force: 3.359559984622469, time: 0.21975040435791016
Test Loss Energy: 10.119486931960294, Test Loss Force: 12.297809140806939, time: 10.73639965057373

Epoch 17, Batch 100/245, Loss: 0.2776831388473511, Variance: 0.073796346783638
Epoch 17, Batch 200/245, Loss: 0.37674760818481445, Variance: 0.0776425451040268

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.5953110138159083, Training Loss Force: 3.2980561860544686, time: 3.9704084396362305
Validation Loss Energy: 1.8203478789337606, Validation Loss Force: 3.261774383849989, time: 0.22750067710876465
Test Loss Energy: 9.969767252652911, Test Loss Force: 12.342849312635492, time: 10.972748041152954

Epoch 18, Batch 100/245, Loss: 0.8142815828323364, Variance: 0.08248305320739746
Epoch 18, Batch 200/245, Loss: 0.8060445189476013, Variance: 0.07576380670070648

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.604278065279359, Training Loss Force: 3.283101300870447, time: 4.058635711669922
Validation Loss Energy: 1.5118404452582483, Validation Loss Force: 3.3206394776038914, time: 0.22364068031311035
Test Loss Energy: 9.73274024630917, Test Loss Force: 12.128860416235636, time: 10.738842248916626

Epoch 19, Batch 100/245, Loss: 0.36869382858276367, Variance: 0.07951285690069199
Epoch 19, Batch 200/245, Loss: 0.3069419264793396, Variance: 0.07909595221281052

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.612429675973666, Training Loss Force: 3.2784229494062114, time: 4.052267551422119
Validation Loss Energy: 1.5669083620666442, Validation Loss Force: 3.32439184238378, time: 0.2227766513824463
Test Loss Energy: 9.66949695340751, Test Loss Force: 12.141996273041967, time: 10.869498491287231

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.060 MB of 0.061 MB uploadedwandb: - 0.060 MB of 0.061 MB uploadedwandb: \ 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–…â–â–â–ˆâ–†â–‚â–„â–‡â–†â–„â–ƒâ–…â–†â–â–„â–…â–„â–‚â–
wandb:   test_error_force â–†â–„â–â–‚â–ˆâ–ƒâ–â–…â–…â–„â–…â–…â–…â–…â–„â–ˆâ–„â–…â–‚â–‚
wandb:          test_loss â–ƒâ–ƒâ–ƒâ–„â–ˆâ–…â–‚â–‡â–‡â–„â–…â–„â–„â–„â–ƒâ–„â–ƒâ–ƒâ–‚â–
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–†â–„â–„â–‚â–ˆâ–…â–â–â–†â–ƒâ–ƒâ–‚â–ˆâ–‡â–â–‚â–…â–†â–‚â–ƒ
wandb:  valid_error_force â–ˆâ–â–‚â–†â–ƒâ–‚â–…â–†â–†â–‚â–…â–„â–ƒâ–…â–„â–„â–…â–â–ƒâ–„
wandb:         valid_loss â–†â–ƒâ–„â–‚â–ˆâ–„â–â–â–‡â–‚â–ƒâ–‚â–ˆâ–ˆâ–â–‚â–…â–…â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7828
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.6695
wandb:   test_error_force 12.142
wandb:          test_loss 15.81993
wandb: train_error_energy 1.61243
wandb:  train_error_force 3.27842
wandb:         train_loss 0.46071
wandb: valid_error_energy 1.56691
wandb:  valid_error_force 3.32439
wandb:         valid_loss 0.46484
wandb: 
wandb: ğŸš€ View run al_71_85 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/clhte9t4
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_094630-clhte9t4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.656996488571167, Uncertainty Bias: -0.0591411292552948
7.6293945e-06 0.007464409
1.9985126 5.200133
(48745, 22, 3)
Found uncertainty sample 0 after 2008 steps.
Found uncertainty sample 1 after 472 steps.
Found uncertainty sample 2 after 3605 steps.
Found uncertainty sample 3 after 3653 steps.
Found uncertainty sample 4 after 469 steps.
Found uncertainty sample 5 after 1288 steps.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 3029 steps.
Found uncertainty sample 8 after 3100 steps.
Found uncertainty sample 9 after 1132 steps.
Found uncertainty sample 10 after 3218 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 122 steps.
Found uncertainty sample 14 after 1916 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 264 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 286 steps.
Did not find any uncertainty samples for sample 19.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 17 steps.
Found uncertainty sample 22 after 810 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 75 steps.
Found uncertainty sample 25 after 522 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 13 steps.
Found uncertainty sample 28 after 1279 steps.
Found uncertainty sample 29 after 105 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 885 steps.
Found uncertainty sample 32 after 256 steps.
Found uncertainty sample 33 after 226 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 300 steps.
Found uncertainty sample 37 after 11 steps.
Found uncertainty sample 38 after 2212 steps.
Found uncertainty sample 39 after 2384 steps.
Found uncertainty sample 40 after 448 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 1710 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 199 steps.
Found uncertainty sample 45 after 3221 steps.
Did not find any uncertainty samples for sample 46.
Did not find any uncertainty samples for sample 47.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 1379 steps.
Found uncertainty sample 50 after 2142 steps.
Found uncertainty sample 51 after 584 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 3754 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 1092 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 3541 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 65 steps.
Found uncertainty sample 63 after 1060 steps.
Found uncertainty sample 64 after 674 steps.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 718 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 1249 steps.
Did not find any uncertainty samples for sample 70.
Did not find any uncertainty samples for sample 71.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 1464 steps.
Found uncertainty sample 74 after 3366 steps.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 3070 steps.
Found uncertainty sample 78 after 563 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 3 steps.
Found uncertainty sample 82 after 1021 steps.
Did not find any uncertainty samples for sample 83.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 2869 steps.
Found uncertainty sample 86 after 2507 steps.
Found uncertainty sample 87 after 2901 steps.
Found uncertainty sample 88 after 2072 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1853 steps.
Found uncertainty sample 91 after 52 steps.
Found uncertainty sample 92 after 469 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 20 steps.
Found uncertainty sample 95 after 2389 steps.
Found uncertainty sample 96 after 2576 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 241 steps.
Found uncertainty sample 99 after 553 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_101636-i7d40nnc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_86
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/i7d40nnc
Training model 86. Added 64 samples to the dataset.
Epoch 0, Batch 100/247, Loss: 0.6084847450256348, Variance: 0.10595434904098511
Epoch 0, Batch 200/247, Loss: 1.406550645828247, Variance: 0.10773033648729324

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.589288371493968, Training Loss Force: 3.6695338066346093, time: 4.5436999797821045
Validation Loss Energy: 3.752663546215147, Validation Loss Force: 3.320847673255334, time: 0.26271939277648926
Test Loss Energy: 10.901703734283481, Test Loss Force: 11.567826845494597, time: 12.133360147476196

Epoch 1, Batch 100/247, Loss: 0.9545547962188721, Variance: 0.10568161308765411
Epoch 1, Batch 200/247, Loss: 0.6572982668876648, Variance: 0.1096036285161972

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6142290318986148, Training Loss Force: 3.2757184831169726, time: 4.642594814300537
Validation Loss Energy: 2.1665363318270434, Validation Loss Force: 3.259474935247279, time: 0.26403331756591797
Test Loss Energy: 9.99271133792686, Test Loss Force: 11.84686877769674, time: 12.478314876556396

Epoch 2, Batch 100/247, Loss: 0.6445557475090027, Variance: 0.1069536954164505
Epoch 2, Batch 200/247, Loss: 0.6470546722412109, Variance: 0.10921291261911392

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.62645728937942, Training Loss Force: 3.265675028251175, time: 4.467702627182007
Validation Loss Energy: 1.920778956161722, Validation Loss Force: 3.318719624227702, time: 0.26285457611083984
Test Loss Energy: 9.831943673632528, Test Loss Force: 12.070499989568555, time: 12.300305366516113

Epoch 3, Batch 100/247, Loss: 0.8705583810806274, Variance: 0.11373186856508255
Epoch 3, Batch 200/247, Loss: 1.3745455741882324, Variance: 0.10859496891498566

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.624429358079212, Training Loss Force: 3.277618891256411, time: 4.5857603549957275
Validation Loss Energy: 3.50508390192684, Validation Loss Force: 3.330418104481883, time: 0.2668919563293457
Test Loss Energy: 10.334032767028978, Test Loss Force: 12.01064468116803, time: 11.536097526550293

Epoch 4, Batch 100/247, Loss: 1.3340662717819214, Variance: 0.11370626837015152
Epoch 4, Batch 200/247, Loss: 0.779067873954773, Variance: 0.1128489151597023

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.631673892431511, Training Loss Force: 3.272836259101051, time: 4.7467567920684814
Validation Loss Energy: 1.5912796670903, Validation Loss Force: 3.2959890011385284, time: 0.27367401123046875
Test Loss Energy: 9.77725685885626, Test Loss Force: 11.925467605368814, time: 10.978858947753906

Epoch 5, Batch 100/247, Loss: 0.5653199553489685, Variance: 0.10800734162330627
Epoch 5, Batch 200/247, Loss: 0.7788794040679932, Variance: 0.1155560314655304

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.623481919343927, Training Loss Force: 3.272660770572213, time: 4.131493330001831
Validation Loss Energy: 2.5957336712980323, Validation Loss Force: 3.2834757188121113, time: 0.3051772117614746
Test Loss Energy: 10.346599015175498, Test Loss Force: 11.929638410648923, time: 9.952857255935669

Epoch 6, Batch 100/247, Loss: 0.6338435411453247, Variance: 0.11126621067523956
Epoch 6, Batch 200/247, Loss: 1.307815432548523, Variance: 0.11080291122198105

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.625988295871414, Training Loss Force: 3.282796962179069, time: 3.924959182739258
Validation Loss Energy: 3.7160094214311092, Validation Loss Force: 3.340753014553689, time: 0.20962071418762207
Test Loss Energy: 10.839208087430585, Test Loss Force: 12.218925408545552, time: 9.882498502731323

Epoch 7, Batch 100/247, Loss: 1.0259939432144165, Variance: 0.1067127212882042
Epoch 7, Batch 200/247, Loss: 0.7366510629653931, Variance: 0.11180134117603302

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.631811382966419, Training Loss Force: 3.2756850367688632, time: 4.026496171951294
Validation Loss Energy: 2.2981934724014748, Validation Loss Force: 3.256638854002883, time: 0.21124839782714844
Test Loss Energy: 10.244211987979213, Test Loss Force: 12.225763039414142, time: 10.122649431228638

Epoch 8, Batch 100/247, Loss: 0.7824076414108276, Variance: 0.11161139607429504
Epoch 8, Batch 200/247, Loss: 0.6317161321640015, Variance: 0.10809646546840668

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.640302385842324, Training Loss Force: 3.276417646205296, time: 3.989842176437378
Validation Loss Energy: 2.024228926002886, Validation Loss Force: 3.261329323894566, time: 0.21459007263183594
Test Loss Energy: 10.091604837778457, Test Loss Force: 12.260576131619326, time: 9.95424222946167

Epoch 9, Batch 100/247, Loss: 0.7325419783592224, Variance: 0.10860653221607208
Epoch 9, Batch 200/247, Loss: 1.0309336185455322, Variance: 0.10592179000377655

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.6169366229039968, Training Loss Force: 3.2875472442036537, time: 4.02459192276001
Validation Loss Energy: 3.260733015921771, Validation Loss Force: 3.3016478034785646, time: 0.2105879783630371
Test Loss Energy: 10.294819522654995, Test Loss Force: 12.17726571808398, time: 10.257322549819946

Epoch 10, Batch 100/247, Loss: 1.232679843902588, Variance: 0.11075074225664139
Epoch 10, Batch 200/247, Loss: 0.7391693592071533, Variance: 0.11419712007045746

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.618024122335059, Training Loss Force: 3.2745881793739575, time: 4.006788730621338
Validation Loss Energy: 1.7233347174285043, Validation Loss Force: 3.343740677819175, time: 0.20751500129699707
Test Loss Energy: 9.842351481327515, Test Loss Force: 12.096472743622149, time: 9.836873769760132

Epoch 11, Batch 100/247, Loss: 0.6912442445755005, Variance: 0.11243028938770294
Epoch 11, Batch 200/247, Loss: 0.9181838035583496, Variance: 0.11018875241279602

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.613205881596988, Training Loss Force: 3.2724121600200373, time: 4.0218963623046875
Validation Loss Energy: 2.72402778409423, Validation Loss Force: 3.306880918078873, time: 0.2165517807006836
Test Loss Energy: 10.566361626933086, Test Loss Force: 11.884787071582569, time: 10.195637702941895

Epoch 12, Batch 100/247, Loss: 0.9153923988342285, Variance: 0.11050718277692795
Epoch 12, Batch 200/247, Loss: 1.2585039138793945, Variance: 0.11564543098211288

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.621938695051921, Training Loss Force: 3.280159029655087, time: 4.090441703796387
Validation Loss Energy: 3.729815242482351, Validation Loss Force: 3.273856300373458, time: 0.24651598930358887
Test Loss Energy: 10.682314689072726, Test Loss Force: 11.88341167574497, time: 10.075801849365234

Epoch 13, Batch 100/247, Loss: 1.3049730062484741, Variance: 0.10765513777732849
Epoch 13, Batch 200/247, Loss: 0.8185106515884399, Variance: 0.1122794821858406

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6345692367986473, Training Loss Force: 3.274418905748326, time: 4.2385642528533936
Validation Loss Energy: 2.1150723151454884, Validation Loss Force: 3.3654377820675245, time: 0.24875497817993164
Test Loss Energy: 10.168319474111902, Test Loss Force: 12.013865316399222, time: 12.297047138214111

Epoch 14, Batch 100/247, Loss: 0.6029233932495117, Variance: 0.10794354975223541
Epoch 14, Batch 200/247, Loss: 0.7664610147476196, Variance: 0.11267867684364319

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.590600028955074, Training Loss Force: 3.2704985711292527, time: 4.506375551223755
Validation Loss Energy: 2.2381195635216167, Validation Loss Force: 3.30001474796233, time: 0.25392603874206543
Test Loss Energy: 10.23132983876562, Test Loss Force: 12.39602236893062, time: 11.08632206916809

Epoch 15, Batch 100/247, Loss: 0.8159381747245789, Variance: 0.11043748259544373
Epoch 15, Batch 200/247, Loss: 1.2658209800720215, Variance: 0.10764363408088684

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.636517893375432, Training Loss Force: 3.2842525639084172, time: 4.0452351570129395
Validation Loss Energy: 2.9392048926035583, Validation Loss Force: 3.322121118100895, time: 0.23693060874938965
Test Loss Energy: 10.319993834115186, Test Loss Force: 12.472612254583607, time: 10.670236587524414

Epoch 16, Batch 100/247, Loss: 1.3545876741409302, Variance: 0.11407484859228134
Epoch 16, Batch 200/247, Loss: 0.782318651676178, Variance: 0.11278751492500305

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6196281600950035, Training Loss Force: 3.286920185351358, time: 4.029019355773926
Validation Loss Energy: 1.641752555569687, Validation Loss Force: 3.3131927533566286, time: 0.23066401481628418
Test Loss Energy: 9.935740735125394, Test Loss Force: 12.360572960586413, time: 10.527465343475342

Epoch 17, Batch 100/247, Loss: 0.5625187158584595, Variance: 0.11193886399269104
Epoch 17, Batch 200/247, Loss: 0.7499645948410034, Variance: 0.11662229895591736

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.604616765917466, Training Loss Force: 3.26510776282577, time: 4.0561301708221436
Validation Loss Energy: 2.4630173492637093, Validation Loss Force: 3.2803047431722394, time: 0.22184419631958008
Test Loss Energy: 10.567101615961164, Test Loss Force: 11.735319486390978, time: 10.687077760696411

Epoch 18, Batch 100/247, Loss: 0.6046428680419922, Variance: 0.10790664702653885
Epoch 18, Batch 200/247, Loss: 1.3783419132232666, Variance: 0.11410316824913025

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5891287578987625, Training Loss Force: 3.2684344452748983, time: 3.9586098194122314
Validation Loss Energy: 3.8838030366925835, Validation Loss Force: 3.29453101923828, time: 0.2183222770690918
Test Loss Energy: 10.992474328976828, Test Loss Force: 11.851913737743097, time: 10.693915843963623

Epoch 19, Batch 100/247, Loss: 1.1726526021957397, Variance: 0.11020314693450928
Epoch 19, Batch 200/247, Loss: 0.6706101894378662, Variance: 0.116453155875206

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.639960821739657, Training Loss Force: 3.265641490852119, time: 4.049826383590698
Validation Loss Energy: 2.3154406451030303, Validation Loss Force: 3.393285230795546, time: 0.22008061408996582
Test Loss Energy: 10.335497575603842, Test Loss Force: 12.127073411235651, time: 11.891127824783325

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‡â–‚â–â–„â–â–„â–‡â–„â–ƒâ–„â–â–†â–†â–ƒâ–„â–„â–‚â–†â–ˆâ–„
wandb:   test_error_force â–â–ƒâ–…â–„â–„â–„â–†â–†â–†â–†â–…â–ƒâ–ƒâ–„â–‡â–ˆâ–‡â–‚â–ƒâ–…
wandb:          test_loss â–†â–…â–…â–„â–â–â–…â–„â–„â–†â–‚â–„â–†â–ƒâ–†â–…â–…â–„â–ˆâ–…
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–ƒâ–‚â–‡â–â–„â–‡â–ƒâ–‚â–†â–â–„â–ˆâ–ƒâ–ƒâ–…â–â–„â–ˆâ–ƒ
wandb:  valid_error_force â–„â–â–„â–…â–ƒâ–‚â–…â–â–â–ƒâ–…â–„â–‚â–‡â–ƒâ–„â–„â–‚â–ƒâ–ˆ
wandb:         valid_loss â–‡â–‚â–‚â–†â–â–ƒâ–‡â–ƒâ–‚â–†â–‚â–„â–‡â–‚â–ƒâ–„â–â–ƒâ–ˆâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7885
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.3355
wandb:   test_error_force 12.12707
wandb:          test_loss 12.72871
wandb: train_error_energy 2.63996
wandb:  train_error_force 3.26564
wandb:         train_loss 0.91323
wandb: valid_error_energy 2.31544
wandb:  valid_error_force 3.39329
wandb:         valid_loss 0.82019
wandb: 
wandb: ğŸš€ View run al_71_86 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/i7d40nnc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_101636-i7d40nnc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.070841073989868, Uncertainty Bias: -0.11773835122585297
8.106232e-05 0.0016651154
2.057647 4.9291635
(48745, 22, 3)
Found uncertainty sample 0 after 1173 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 114 steps.
Found uncertainty sample 4 after 2277 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 773 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1676 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 1117 steps.
Found uncertainty sample 11 after 2950 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 10 steps.
Found uncertainty sample 14 after 2085 steps.
Found uncertainty sample 15 after 3188 steps.
Found uncertainty sample 16 after 3740 steps.
Found uncertainty sample 17 after 2232 steps.
Found uncertainty sample 18 after 1617 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 516 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 91 steps.
Found uncertainty sample 23 after 1205 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 245 steps.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 3673 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 2939 steps.
Found uncertainty sample 31 after 2226 steps.
Found uncertainty sample 32 after 553 steps.
Found uncertainty sample 33 after 3074 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 2072 steps.
Found uncertainty sample 36 after 1791 steps.
Found uncertainty sample 37 after 533 steps.
Found uncertainty sample 38 after 3722 steps.
Found uncertainty sample 39 after 612 steps.
Found uncertainty sample 40 after 1174 steps.
Found uncertainty sample 41 after 14 steps.
Found uncertainty sample 42 after 3254 steps.
Found uncertainty sample 43 after 3501 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 3325 steps.
Found uncertainty sample 46 after 87 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 1849 steps.
Found uncertainty sample 49 after 3457 steps.
Found uncertainty sample 50 after 950 steps.
Found uncertainty sample 51 after 1412 steps.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 357 steps.
Found uncertainty sample 54 after 3688 steps.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 1356 steps.
Found uncertainty sample 58 after 1183 steps.
Found uncertainty sample 59 after 1775 steps.
Found uncertainty sample 60 after 1013 steps.
Found uncertainty sample 61 after 3015 steps.
Found uncertainty sample 62 after 723 steps.
Found uncertainty sample 63 after 216 steps.
Did not find any uncertainty samples for sample 64.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 830 steps.
Found uncertainty sample 67 after 1617 steps.
Found uncertainty sample 68 after 3261 steps.
Found uncertainty sample 69 after 1606 steps.
Found uncertainty sample 70 after 2533 steps.
Found uncertainty sample 71 after 375 steps.
Found uncertainty sample 72 after 436 steps.
Found uncertainty sample 73 after 39 steps.
Found uncertainty sample 74 after 1932 steps.
Found uncertainty sample 75 after 1841 steps.
Found uncertainty sample 76 after 772 steps.
Found uncertainty sample 77 after 575 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 446 steps.
Found uncertainty sample 80 after 1225 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 3261 steps.
Found uncertainty sample 83 after 2526 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 2639 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 1410 steps.
Found uncertainty sample 88 after 1491 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1326 steps.
Found uncertainty sample 92 after 281 steps.
Found uncertainty sample 93 after 493 steps.
Found uncertainty sample 94 after 108 steps.
Found uncertainty sample 95 after 3792 steps.
Found uncertainty sample 96 after 1073 steps.
Found uncertainty sample 97 after 3032 steps.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_104621-4kz8zbgs
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_87
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/4kz8zbgs
Training model 87. Added 73 samples to the dataset.
Epoch 0, Batch 100/249, Loss: 1.2449666261672974, Variance: 0.11386136710643768
Epoch 0, Batch 200/249, Loss: 0.5401660203933716, Variance: 0.10715198516845703

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.8210014745870464, Training Loss Force: 3.358360612367382, time: 4.081105709075928
Validation Loss Energy: 3.767022477424874, Validation Loss Force: 3.26509616216894, time: 0.22960972785949707
Test Loss Energy: 10.936384400134614, Test Loss Force: 11.818407296065587, time: 10.579785585403442

Epoch 1, Batch 100/249, Loss: 1.3525885343551636, Variance: 0.10488221049308777
Epoch 1, Batch 200/249, Loss: 0.5881625413894653, Variance: 0.11133743822574615

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.570580166676257, Training Loss Force: 3.261286497642626, time: 4.021493434906006
Validation Loss Energy: 3.167623968219353, Validation Loss Force: 3.3395718104018286, time: 0.2490372657775879
Test Loss Energy: 10.232783777355579, Test Loss Force: 12.11013713071119, time: 10.886468410491943

Epoch 2, Batch 100/249, Loss: 1.470999002456665, Variance: 0.1167600154876709
Epoch 2, Batch 200/249, Loss: 0.47154128551483154, Variance: 0.10718131810426712

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.621629481970663, Training Loss Force: 3.267479181453153, time: 4.008714914321899
Validation Loss Energy: 3.707867969448552, Validation Loss Force: 3.3612717536817884, time: 0.23517560958862305
Test Loss Energy: 10.947112100619995, Test Loss Force: 12.091130474799504, time: 10.764838457107544

Epoch 3, Batch 100/249, Loss: 1.0788209438323975, Variance: 0.11031011492013931
Epoch 3, Batch 200/249, Loss: 0.6996126174926758, Variance: 0.11109209060668945

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6295107289596396, Training Loss Force: 3.2630137082259507, time: 4.021597623825073
Validation Loss Energy: 3.2278949949606437, Validation Loss Force: 3.2878688697266933, time: 0.2344212532043457
Test Loss Energy: 10.14306719813965, Test Loss Force: 11.876608300950641, time: 10.77930474281311

Epoch 4, Batch 100/249, Loss: 1.323129653930664, Variance: 0.11231158673763275
Epoch 4, Batch 200/249, Loss: 0.631680428981781, Variance: 0.11076043546199799

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.576181415748972, Training Loss Force: 3.2792711840900046, time: 4.075313329696655
Validation Loss Energy: 3.4703141147902157, Validation Loss Force: 3.3081297579838087, time: 0.23002314567565918
Test Loss Energy: 11.157419541579229, Test Loss Force: 11.958524390605165, time: 10.731390953063965

Epoch 5, Batch 100/249, Loss: 1.1360561847686768, Variance: 0.10703035444021225
Epoch 5, Batch 200/249, Loss: 0.6953883171081543, Variance: 0.11372855305671692

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6310858243231667, Training Loss Force: 3.278111153851276, time: 3.986452579498291
Validation Loss Energy: 3.2204318488638686, Validation Loss Force: 3.3344916695047724, time: 0.23751592636108398
Test Loss Energy: 10.273778724639097, Test Loss Force: 12.170979555917556, time: 11.937989234924316

Epoch 6, Batch 100/249, Loss: 1.3600258827209473, Variance: 0.11587284505367279
Epoch 6, Batch 200/249, Loss: 0.5937714576721191, Variance: 0.11180303990840912

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6109970995572653, Training Loss Force: 3.2704212418092773, time: 3.9302170276641846
Validation Loss Energy: 3.9368134349253197, Validation Loss Force: 3.364034187106169, time: 0.22789502143859863
Test Loss Energy: 11.055455296640055, Test Loss Force: 12.014367937786124, time: 10.587973833084106

Epoch 7, Batch 100/249, Loss: 1.2463924884796143, Variance: 0.11088399589061737
Epoch 7, Batch 200/249, Loss: 0.6656122207641602, Variance: 0.11161422729492188

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.612371053141732, Training Loss Force: 3.282162563517709, time: 4.07253360748291
Validation Loss Energy: 3.4481218500286093, Validation Loss Force: 3.281908851699504, time: 0.22925233840942383
Test Loss Energy: 10.382905786001249, Test Loss Force: 12.046808412132373, time: 10.751010656356812

Epoch 8, Batch 100/249, Loss: 1.230190396308899, Variance: 0.11255192011594772
Epoch 8, Batch 200/249, Loss: 0.5657758116722107, Variance: 0.10885748267173767

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.604296040624499, Training Loss Force: 3.2722112951699556, time: 3.9802205562591553
Validation Loss Energy: 3.7717121123072586, Validation Loss Force: 3.3086705100046947, time: 0.23592400550842285
Test Loss Energy: 11.12295608423382, Test Loss Force: 11.927452844199028, time: 10.599955797195435

Epoch 9, Batch 100/249, Loss: 1.2310701608657837, Variance: 0.10767485201358795
Epoch 9, Batch 200/249, Loss: 0.751002311706543, Variance: 0.11245009303092957

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.614553222844248, Training Loss Force: 3.281170090469912, time: 3.978933095932007
Validation Loss Energy: 3.29777575403274, Validation Loss Force: 3.268837616160678, time: 0.23330116271972656
Test Loss Energy: 10.26924013987834, Test Loss Force: 12.260387597303724, time: 10.761492252349854

Epoch 10, Batch 100/249, Loss: 1.2147713899612427, Variance: 0.11561164259910583
Epoch 10, Batch 200/249, Loss: 0.6999044418334961, Variance: 0.10849955677986145

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6073251212453807, Training Loss Force: 3.267201894874467, time: 3.9550914764404297
Validation Loss Energy: 3.878083779679428, Validation Loss Force: 3.244659587955687, time: 0.22668886184692383
Test Loss Energy: 11.090870024556239, Test Loss Force: 11.899109803900597, time: 10.748847007751465

Epoch 11, Batch 100/249, Loss: 1.4681200981140137, Variance: 0.11066924035549164
Epoch 11, Batch 200/249, Loss: 0.7698822021484375, Variance: 0.11440365016460419

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6085058664215826, Training Loss Force: 3.273051767462415, time: 3.995802402496338
Validation Loss Energy: 2.870503611997442, Validation Loss Force: 3.3762851651786834, time: 0.22364282608032227
Test Loss Energy: 10.12895507119624, Test Loss Force: 12.008225953308783, time: 10.873769521713257

Epoch 12, Batch 100/249, Loss: 1.3347699642181396, Variance: 0.1145794540643692
Epoch 12, Batch 200/249, Loss: 0.620624840259552, Variance: 0.11111121624708176

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5971905487950875, Training Loss Force: 3.278349875743544, time: 4.14324164390564
Validation Loss Energy: 3.933058787786685, Validation Loss Force: 3.351071948114275, time: 0.23381876945495605
Test Loss Energy: 11.11730017134655, Test Loss Force: 12.216351954775066, time: 10.751720428466797

Epoch 13, Batch 100/249, Loss: 1.140951156616211, Variance: 0.10950461775064468
Epoch 13, Batch 200/249, Loss: 0.6332528591156006, Variance: 0.11114505678415298

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.6016124902042606, Training Loss Force: 3.265791616708846, time: 4.133303642272949
Validation Loss Energy: 3.1494378553967066, Validation Loss Force: 3.2932005710913614, time: 0.22768950462341309
Test Loss Energy: 10.240428295923813, Test Loss Force: 11.839545084564538, time: 10.944799184799194

Epoch 14, Batch 100/249, Loss: 1.258317232131958, Variance: 0.11428375542163849
Epoch 14, Batch 200/249, Loss: 0.8257734775543213, Variance: 0.11200481653213501

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6145407381697643, Training Loss Force: 3.2636892855204422, time: 4.046900510787964
Validation Loss Energy: 3.388566370587758, Validation Loss Force: 3.2699979450801075, time: 0.23142361640930176
Test Loss Energy: 10.687220820152724, Test Loss Force: 12.200429758505056, time: 10.73586893081665

Epoch 15, Batch 100/249, Loss: 1.0564098358154297, Variance: 0.10935752093791962
Epoch 15, Batch 200/249, Loss: 0.6897987723350525, Variance: 0.11349934339523315

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.6193883129927595, Training Loss Force: 3.266804248915506, time: 4.021329164505005
Validation Loss Energy: 3.400276930643105, Validation Loss Force: 3.2908639924085676, time: 0.22350573539733887
Test Loss Energy: 10.222521961345443, Test Loss Force: 12.086867747965371, time: 10.879910469055176

Epoch 16, Batch 100/249, Loss: 1.1455039978027344, Variance: 0.11418528854846954
Epoch 16, Batch 200/249, Loss: 0.5728833675384521, Variance: 0.10912466794252396

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.6221403015351132, Training Loss Force: 3.2630459514336843, time: 4.082031488418579
Validation Loss Energy: 3.7086970223461275, Validation Loss Force: 3.269376698162969, time: 0.22777318954467773
Test Loss Energy: 10.827372818955283, Test Loss Force: 11.776281539262865, time: 10.691402196884155

Epoch 17, Batch 100/249, Loss: 1.090470790863037, Variance: 0.10956402122974396
Epoch 17, Batch 200/249, Loss: 0.7172693014144897, Variance: 0.11209122836589813

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.6379087588431847, Training Loss Force: 3.2697874655049755, time: 4.127973794937134
Validation Loss Energy: 3.233998690254243, Validation Loss Force: 3.2333073582234237, time: 0.2545912265777588
Test Loss Energy: 10.0152908100521, Test Loss Force: 11.869599580903099, time: 10.816757678985596

Epoch 18, Batch 100/249, Loss: 1.3332300186157227, Variance: 0.11507460474967957
Epoch 18, Batch 200/249, Loss: 0.6847881078720093, Variance: 0.11071601510047913

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.620576617741752, Training Loss Force: 3.2746961227671156, time: 3.950448989868164
Validation Loss Energy: 3.9207041015594553, Validation Loss Force: 3.3121804994192403, time: 0.22774577140808105
Test Loss Energy: 11.501151253434344, Test Loss Force: 12.059557109081709, time: 10.702278852462769

Epoch 19, Batch 100/249, Loss: 1.1657137870788574, Variance: 0.108786441385746
Epoch 19, Batch 200/249, Loss: 0.789720892906189, Variance: 0.11459170281887054

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.629366559630187, Training Loss Force: 3.2661113436690443, time: 4.06889271736145
Validation Loss Energy: 3.2965605371688387, Validation Loss Force: 3.2634893259446383, time: 0.2568702697753906
Test Loss Energy: 10.407694164033462, Test Loss Force: 12.099925579637679, time: 10.897014617919922

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–…â–‚â–…â–‚â–†â–‚â–†â–ƒâ–†â–‚â–†â–‚â–†â–‚â–„â–‚â–…â–â–ˆâ–ƒ
wandb:   test_error_force â–‚â–†â–†â–‚â–„â–‡â–„â–…â–ƒâ–ˆâ–ƒâ–„â–‡â–‚â–‡â–…â–â–‚â–…â–†
wandb:          test_loss â–…â–ƒâ–„â–‚â–‡â–ƒâ–†â–ƒâ–„â–„â–†â–‚â–‡â–ƒâ–„â–‚â–ƒâ–â–ˆâ–„
wandb: train_error_energy â–ˆâ–â–‚â–ƒâ–â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒ
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‡â–ƒâ–†â–ƒâ–…â–ƒâ–ˆâ–…â–‡â–„â–ˆâ–â–ˆâ–ƒâ–„â–„â–‡â–ƒâ–ˆâ–„
wandb:  valid_error_force â–ƒâ–†â–‡â–„â–…â–†â–‡â–ƒâ–…â–ƒâ–‚â–ˆâ–‡â–„â–ƒâ–„â–ƒâ–â–…â–‚
wandb:         valid_loss â–†â–ƒâ–†â–ƒâ–„â–ƒâ–ˆâ–„â–†â–ƒâ–‡â–â–ˆâ–‚â–„â–„â–…â–‚â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7950
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.40769
wandb:   test_error_force 12.09993
wandb:          test_loss 12.87299
wandb: train_error_energy 2.62937
wandb:  train_error_force 3.26611
wandb:         train_loss 0.90286
wandb: valid_error_energy 3.29656
wandb:  valid_error_force 3.26349
wandb:         valid_loss 1.15677
wandb: 
wandb: ğŸš€ View run al_71_87 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/4kz8zbgs
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_104621-4kz8zbgs/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2933542728424072, Uncertainty Bias: -0.14342056214809418
1.8119812e-05 0.2540095
1.8688797 4.856667
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 2498 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 504 steps.
Found uncertainty sample 5 after 1552 steps.
Found uncertainty sample 6 after 1954 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 1822 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 2098 steps.
Did not find any uncertainty samples for sample 13.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 1082 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 2855 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 2091 steps.
Found uncertainty sample 21 after 720 steps.
Found uncertainty sample 22 after 2455 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 51 steps.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 1135 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 1682 steps.
Found uncertainty sample 31 after 3751 steps.
Found uncertainty sample 32 after 6 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 3822 steps.
Did not find any uncertainty samples for sample 35.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 728 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 3642 steps.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 3450 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 1197 steps.
Found uncertainty sample 49 after 324 steps.
Found uncertainty sample 50 after 51 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 3307 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 442 steps.
Found uncertainty sample 56 after 202 steps.
Found uncertainty sample 57 after 2061 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 1120 steps.
Found uncertainty sample 60 after 3779 steps.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 3701 steps.
Found uncertainty sample 64 after 1125 steps.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 836 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 112 steps.
Found uncertainty sample 70 after 2561 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 1013 steps.
Found uncertainty sample 73 after 3774 steps.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 236 steps.
Found uncertainty sample 78 after 834 steps.
Found uncertainty sample 79 after 292 steps.
Found uncertainty sample 80 after 1772 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 3166 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 1735 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 1035 steps.
Found uncertainty sample 87 after 1321 steps.
Found uncertainty sample 88 after 124 steps.
Found uncertainty sample 89 after 2521 steps.
Found uncertainty sample 90 after 26 steps.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Did not find any uncertainty samples for sample 93.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 700 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 1991 steps.
Found uncertainty sample 99 after 495 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_112127-fi0unjpr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_88
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/fi0unjpr
Training model 88. Added 50 samples to the dataset.
Epoch 0, Batch 100/250, Loss: 0.5628682971000671, Variance: 0.09447000175714493
Epoch 0, Batch 200/250, Loss: 1.1075925827026367, Variance: 0.12652944028377533

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.603985438739133, Training Loss Force: 3.499526897685586, time: 4.296993732452393
Validation Loss Energy: 2.4048000000553107, Validation Loss Force: 3.2408454324878035, time: 0.2539489269256592
Test Loss Energy: 10.262211237379207, Test Loss Force: 11.69019961999271, time: 11.51640009880066

Epoch 1, Batch 100/250, Loss: 1.304503083229065, Variance: 0.13603636622428894
Epoch 1, Batch 200/250, Loss: 1.5154880285263062, Variance: 0.14773456752300262

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.9887476486821813, Training Loss Force: 3.294312560765902, time: 4.159510850906372
Validation Loss Energy: 5.732320232604803, Validation Loss Force: 3.3310860005850933, time: 0.25761938095092773
Test Loss Energy: 11.282913036981391, Test Loss Force: 11.609620142106957, time: 12.508842468261719

Epoch 2, Batch 100/250, Loss: 1.0492091178894043, Variance: 0.14939334988594055
Epoch 2, Batch 200/250, Loss: 0.976551353931427, Variance: 0.14617782831192017

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.046946647726263, Training Loss Force: 3.2906172665334523, time: 4.3587329387664795
Validation Loss Energy: 2.1462920324212997, Validation Loss Force: 3.347400467901644, time: 0.22540688514709473
Test Loss Energy: 9.803223075063936, Test Loss Force: 11.514111740218247, time: 10.19490122795105

Epoch 3, Batch 100/250, Loss: 1.5426890850067139, Variance: 0.15416651964187622
Epoch 3, Batch 200/250, Loss: 1.5262259244918823, Variance: 0.14923852682113647

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 4.067557760704213, Training Loss Force: 3.328057533643954, time: 4.094587326049805
Validation Loss Energy: 6.206577123114886, Validation Loss Force: 3.2664316760389283, time: 0.24841666221618652
Test Loss Energy: 12.504356159219428, Test Loss Force: 11.895856106381556, time: 10.289313554763794

Epoch 4, Batch 100/250, Loss: 1.0513430833816528, Variance: 0.1512780785560608
Epoch 4, Batch 200/250, Loss: 1.1965956687927246, Variance: 0.1553279459476471

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 4.107077278899017, Training Loss Force: 3.3164043389499955, time: 4.078453779220581
Validation Loss Energy: 2.227673764215139, Validation Loss Force: 3.2789519167761556, time: 0.23787426948547363
Test Loss Energy: 10.058891849040188, Test Loss Force: 11.63159876194353, time: 10.255338907241821

Epoch 5, Batch 100/250, Loss: 1.5706835985183716, Variance: 0.15249407291412354
Epoch 5, Batch 200/250, Loss: 1.7474464178085327, Variance: 0.1556476354598999

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.076123179639339, Training Loss Force: 3.313461979058263, time: 4.103694915771484
Validation Loss Energy: 5.542667131620313, Validation Loss Force: 3.339836007343472, time: 0.2166736125946045
Test Loss Energy: 11.010220047986756, Test Loss Force: 11.542277773701159, time: 10.336007833480835

Epoch 6, Batch 100/250, Loss: 1.1433430910110474, Variance: 0.15541206300258636
Epoch 6, Batch 200/250, Loss: 1.3455188274383545, Variance: 0.14964473247528076

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 4.033797843163762, Training Loss Force: 3.373184499102992, time: 4.05744481086731
Validation Loss Energy: 1.7315558571316607, Validation Loss Force: 3.3051211638186055, time: 0.21793603897094727
Test Loss Energy: 9.6240310148558, Test Loss Force: 11.580319615154577, time: 10.182840824127197

Epoch 7, Batch 100/250, Loss: 1.6725572347640991, Variance: 0.1592182070016861
Epoch 7, Batch 200/250, Loss: 1.4419121742248535, Variance: 0.151351198554039

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.041491378791092, Training Loss Force: 3.3238248799777272, time: 4.1277220249176025
Validation Loss Energy: 5.393377934725589, Validation Loss Force: 3.34905423164845, time: 0.22163724899291992
Test Loss Energy: 11.671416444628656, Test Loss Force: 11.959489245159645, time: 10.412580490112305

Epoch 8, Batch 100/250, Loss: 0.9302862882614136, Variance: 0.1530359983444214
Epoch 8, Batch 200/250, Loss: 1.2072398662567139, Variance: 0.15983903408050537

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 4.116516882204892, Training Loss Force: 3.352221893080753, time: 4.142850160598755
Validation Loss Energy: 2.5211662833770214, Validation Loss Force: 3.3485932508490928, time: 0.23410725593566895
Test Loss Energy: 10.344226882292299, Test Loss Force: 11.927631590398665, time: 10.218546628952026

Epoch 9, Batch 100/250, Loss: 1.5098756551742554, Variance: 0.15187978744506836
Epoch 9, Batch 200/250, Loss: 1.5072513818740845, Variance: 0.15332502126693726

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 4.031509563645412, Training Loss Force: 3.3548285997205602, time: 4.119197607040405
Validation Loss Energy: 5.00260293572645, Validation Loss Force: 3.336792676791504, time: 0.22306394577026367
Test Loss Energy: 10.996744386405668, Test Loss Force: 11.752541123837883, time: 10.33289122581482

Epoch 10, Batch 100/250, Loss: 1.3203020095825195, Variance: 0.15676291286945343
Epoch 10, Batch 200/250, Loss: 1.272376537322998, Variance: 0.15011417865753174

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 4.019472161801583, Training Loss Force: 3.3436552365505148, time: 4.0911030769348145
Validation Loss Energy: 2.0830755382176767, Validation Loss Force: 3.34096677693378, time: 0.2135000228881836
Test Loss Energy: 9.807272683043468, Test Loss Force: 11.709354697131129, time: 11.700393438339233

Epoch 11, Batch 100/250, Loss: 1.5558488368988037, Variance: 0.16168801486492157
Epoch 11, Batch 200/250, Loss: 1.4898560047149658, Variance: 0.14899194240570068

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 4.028673891666907, Training Loss Force: 3.337305553307349, time: 4.673648357391357
Validation Loss Energy: 5.702464864853379, Validation Loss Force: 3.378629537281584, time: 0.2710850238800049
Test Loss Energy: 11.82574324882469, Test Loss Force: 11.77964465476808, time: 13.167077779769897

Epoch 12, Batch 100/250, Loss: 1.3275245428085327, Variance: 0.15368598699569702
Epoch 12, Batch 200/250, Loss: 0.7959946393966675, Variance: 0.11601987481117249

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 3.369476810660542, Training Loss Force: 3.4739292365513346, time: 4.092596530914307
Validation Loss Energy: 2.5838723085636053, Validation Loss Force: 3.2566170880938694, time: 0.23128008842468262
Test Loss Energy: 10.676398199657596, Test Loss Force: 12.394371105839932, time: 10.690544366836548

Epoch 13, Batch 100/250, Loss: 0.5936367511749268, Variance: 0.11413487046957016
Epoch 13, Batch 200/250, Loss: 0.6831531524658203, Variance: 0.11241523921489716

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5685771003557734, Training Loss Force: 3.249488347856463, time: 4.1000189781188965
Validation Loss Energy: 2.382795835054727, Validation Loss Force: 3.2835740829073248, time: 0.2462477684020996
Test Loss Energy: 10.145671634398214, Test Loss Force: 12.019673306484123, time: 10.808050155639648

Epoch 14, Batch 100/250, Loss: 0.9485419988632202, Variance: 0.11402818560600281
Epoch 14, Batch 200/250, Loss: 1.1782883405685425, Variance: 0.11541707813739777

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5983839467189473, Training Loss Force: 3.252386825003592, time: 3.9855430126190186
Validation Loss Energy: 3.324092519384367, Validation Loss Force: 3.372676756001637, time: 0.23072504997253418
Test Loss Energy: 10.246008123579985, Test Loss Force: 12.043351808228467, time: 10.70911979675293

Epoch 15, Batch 100/250, Loss: 1.2576545476913452, Variance: 0.11354934424161911
Epoch 15, Batch 200/250, Loss: 0.46439242362976074, Variance: 0.11277194321155548

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.591612080103073, Training Loss Force: 3.2470675592653864, time: 4.0978477001190186
Validation Loss Energy: 2.4049543654407253, Validation Loss Force: 3.293085896481209, time: 0.23560833930969238
Test Loss Energy: 10.575524455293174, Test Loss Force: 12.036475132150272, time: 10.861873149871826

Epoch 16, Batch 100/250, Loss: 0.45618510246276855, Variance: 0.10787318646907806
Epoch 16, Batch 200/250, Loss: 0.8169911503791809, Variance: 0.10908877849578857

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.580043179040996, Training Loss Force: 3.2515611295113995, time: 3.9965038299560547
Validation Loss Energy: 2.5783639192316437, Validation Loss Force: 3.3208192272719415, time: 0.23009228706359863
Test Loss Energy: 10.533930758836904, Test Loss Force: 12.159179499478297, time: 10.574387311935425

Epoch 17, Batch 100/250, Loss: 0.8600989580154419, Variance: 0.11147484183311462
Epoch 17, Batch 200/250, Loss: 1.3109415769577026, Variance: 0.11592858284711838

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5857932222902673, Training Loss Force: 3.2468133183065957, time: 4.126274347305298
Validation Loss Energy: 3.6503636625127003, Validation Loss Force: 3.240372204885421, time: 0.233856201171875
Test Loss Energy: 10.698171912584405, Test Loss Force: 12.455061066800726, time: 10.772987127304077

Epoch 18, Batch 100/250, Loss: 1.1404345035552979, Variance: 0.11633255332708359
Epoch 18, Batch 200/250, Loss: 0.6191624402999878, Variance: 0.11163304001092911

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.593611975911811, Training Loss Force: 3.256087094555438, time: 4.1314308643341064
Validation Loss Energy: 2.14122191163581, Validation Loss Force: 3.2289347485375535, time: 0.22948813438415527
Test Loss Energy: 10.330813041127286, Test Loss Force: 12.069668745828084, time: 10.568207263946533

Epoch 19, Batch 100/250, Loss: 0.7778900861740112, Variance: 0.11123019456863403
Epoch 19, Batch 200/250, Loss: 0.7621383666992188, Variance: 0.10941299796104431

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.607595176986051, Training Loss Force: 3.2742407720542737, time: 4.065279483795166
Validation Loss Energy: 2.7493769963395143, Validation Loss Force: 3.2723586276979475, time: 0.22388195991516113
Test Loss Energy: 10.40858329708864, Test Loss Force: 12.066809524771068, time: 10.783855438232422

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–…â–â–ˆâ–‚â–„â–â–†â–ƒâ–„â–â–†â–„â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒ
wandb:   test_error_force â–‚â–‚â–â–„â–‚â–â–â–„â–„â–ƒâ–‚â–ƒâ–ˆâ–…â–…â–…â–†â–ˆâ–…â–…
wandb:          test_loss â–ƒâ–ƒâ–â–„â–â–‚â–â–ƒâ–â–‚â–â–ƒâ–‡â–‡â–†â–ˆâ–ˆâ–ˆâ–‡â–‡
wandb: train_error_energy â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–…â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–‚â–ƒâ–ƒâ–ƒâ–…â–ƒâ–„â–„â–„â–„â–‡â–â–â–â–â–â–â–‚
wandb:         train_loss â–ˆâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–„â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–‡â–‚â–ˆâ–‚â–‡â–â–‡â–‚â–†â–‚â–‡â–‚â–‚â–ƒâ–‚â–‚â–„â–‚â–ƒ
wandb:  valid_error_force â–‚â–†â–‡â–ƒâ–ƒâ–†â–…â–‡â–‡â–†â–†â–ˆâ–‚â–„â–ˆâ–„â–…â–‚â–â–ƒ
wandb:         valid_loss â–‚â–ˆâ–‚â–ˆâ–‚â–‡â–‚â–‡â–‚â–†â–‚â–‡â–‚â–‚â–„â–‚â–‚â–…â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7995
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.40858
wandb:   test_error_force 12.06681
wandb:          test_loss 12.99739
wandb: train_error_energy 2.6076
wandb:  train_error_force 3.27424
wandb:         train_loss 0.90268
wandb: valid_error_energy 2.74938
wandb:  valid_error_force 3.27236
wandb:         valid_loss 0.94053
wandb: 
wandb: ğŸš€ View run al_71_88 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/fi0unjpr
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_112127-fi0unjpr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1990299224853516, Uncertainty Bias: -0.13981430232524872
1.2397766e-05 0.0025596619
1.9078383 4.76884
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 2417 steps.
Found uncertainty sample 3 after 1803 steps.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 857 steps.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 100 steps.
Found uncertainty sample 9 after 1046 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 3353 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 157 steps.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 1452 steps.
Found uncertainty sample 18 after 1180 steps.
Found uncertainty sample 19 after 3088 steps.
Found uncertainty sample 20 after 1614 steps.
Found uncertainty sample 21 after 1772 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 98 steps.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 2619 steps.
Found uncertainty sample 27 after 470 steps.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 1362 steps.
Found uncertainty sample 31 after 280 steps.
Found uncertainty sample 32 after 2568 steps.
Found uncertainty sample 33 after 904 steps.
Found uncertainty sample 34 after 1191 steps.
Found uncertainty sample 35 after 816 steps.
Found uncertainty sample 36 after 243 steps.
Found uncertainty sample 37 after 2505 steps.
Found uncertainty sample 38 after 5 steps.
Found uncertainty sample 39 after 621 steps.
Found uncertainty sample 40 after 3116 steps.
Found uncertainty sample 41 after 1591 steps.
Found uncertainty sample 42 after 2452 steps.
Found uncertainty sample 43 after 2176 steps.
Did not find any uncertainty samples for sample 44.
Did not find any uncertainty samples for sample 45.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 3908 steps.
Found uncertainty sample 48 after 2716 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 3164 steps.
Found uncertainty sample 51 after 1297 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 1568 steps.
Found uncertainty sample 55 after 1131 steps.
Found uncertainty sample 56 after 381 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 1619 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 2103 steps.
Found uncertainty sample 61 after 2962 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 257 steps.
Found uncertainty sample 65 after 3 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 1331 steps.
Found uncertainty sample 69 after 1020 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 1060 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 1909 steps.
Found uncertainty sample 74 after 1699 steps.
Found uncertainty sample 75 after 443 steps.
Found uncertainty sample 76 after 1338 steps.
Did not find any uncertainty samples for sample 77.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 2751 steps.
Found uncertainty sample 80 after 787 steps.
Found uncertainty sample 81 after 136 steps.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 676 steps.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 3586 steps.
Found uncertainty sample 87 after 3877 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 3725 steps.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 1151 steps.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 3833 steps.
Found uncertainty sample 97 after 1322 steps.
Found uncertainty sample 98 after 1554 steps.
Found uncertainty sample 99 after 424 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_115350-95o4ne7k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_89
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/95o4ne7k
Training model 89. Added 61 samples to the dataset.
Epoch 0, Batch 100/252, Loss: 1.7956194877624512, Variance: 0.09350892901420593
Epoch 0, Batch 200/252, Loss: 1.3220840692520142, Variance: 0.08026441186666489

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.6441017509582863, Training Loss Force: 3.7408410162728836, time: 4.072211980819702
Validation Loss Energy: 1.1910068715400137, Validation Loss Force: 3.3728324351350123, time: 0.24250364303588867
Test Loss Energy: 9.385993307429658, Test Loss Force: 11.73688302949624, time: 10.891937494277954

Epoch 1, Batch 100/252, Loss: 0.49750345945358276, Variance: 0.0962878093123436
Epoch 1, Batch 200/252, Loss: 0.6865625381469727, Variance: 0.10411074757575989

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.771953519346491, Training Loss Force: 3.4976411901422417, time: 4.108813762664795
Validation Loss Energy: 2.297255890562174, Validation Loss Force: 3.2217523548334213, time: 0.2396535873413086
Test Loss Energy: 10.219143429207842, Test Loss Force: 11.798921381540215, time: 11.081437826156616

Epoch 2, Batch 100/252, Loss: 0.4590566158294678, Variance: 0.10864980518817902
Epoch 2, Batch 200/252, Loss: 0.7028130292892456, Variance: 0.10820086300373077

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.535683963991875, Training Loss Force: 3.230734357924119, time: 4.095923900604248
Validation Loss Energy: 2.4020921317051402, Validation Loss Force: 3.258677000362847, time: 0.24716901779174805
Test Loss Energy: 10.241083992439183, Test Loss Force: 11.667322358778677, time: 10.845435857772827

Epoch 3, Batch 100/252, Loss: 0.7233452796936035, Variance: 0.11077333986759186
Epoch 3, Batch 200/252, Loss: 0.7414528131484985, Variance: 0.11005595326423645

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5478364348247244, Training Loss Force: 3.2320590431814744, time: 4.1359148025512695
Validation Loss Energy: 2.1663835251256507, Validation Loss Force: 3.3628178902969106, time: 0.24291133880615234
Test Loss Energy: 9.960255624150895, Test Loss Force: 11.988268572424024, time: 10.955950736999512

Epoch 4, Batch 100/252, Loss: 0.5458763241767883, Variance: 0.10700280219316483
Epoch 4, Batch 200/252, Loss: 0.6174887418746948, Variance: 0.11170761287212372

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5744935826593878, Training Loss Force: 3.233066971859063, time: 4.162108421325684
Validation Loss Energy: 2.301187114333529, Validation Loss Force: 3.2868588090803934, time: 0.23509001731872559
Test Loss Energy: 10.077053653911555, Test Loss Force: 11.861591871470754, time: 10.850035667419434

Epoch 5, Batch 100/252, Loss: 0.7641232013702393, Variance: 0.11061269044876099
Epoch 5, Batch 200/252, Loss: 0.853211522102356, Variance: 0.11222274601459503

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5758813773892637, Training Loss Force: 3.230003337768861, time: 4.087833642959595
Validation Loss Energy: 2.07501549407487, Validation Loss Force: 3.2556072835170866, time: 0.23714685440063477
Test Loss Energy: 10.19245481366881, Test Loss Force: 12.14321524555327, time: 11.091926574707031

Epoch 6, Batch 100/252, Loss: 0.6141821146011353, Variance: 0.11162334680557251
Epoch 6, Batch 200/252, Loss: 0.5137849450111389, Variance: 0.10914343595504761

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.61155233660212, Training Loss Force: 3.238413483372759, time: 4.105915784835815
Validation Loss Energy: 2.1777551938743533, Validation Loss Force: 3.3276263677978988, time: 0.2500324249267578
Test Loss Energy: 10.10877322115947, Test Loss Force: 12.016596247385582, time: 10.898709535598755

Epoch 7, Batch 100/252, Loss: 0.41408634185791016, Variance: 0.10882896184921265
Epoch 7, Batch 200/252, Loss: 0.7053579688072205, Variance: 0.10926178097724915

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.585599274350331, Training Loss Force: 3.246879031180822, time: 4.045815944671631
Validation Loss Energy: 1.9436400967707812, Validation Loss Force: 3.2935050377480457, time: 0.23466825485229492
Test Loss Energy: 10.039655487171242, Test Loss Force: 11.920619834589571, time: 11.387225151062012

Epoch 8, Batch 100/252, Loss: 0.6033021807670593, Variance: 0.11469577252864838
Epoch 8, Batch 200/252, Loss: 0.7722010016441345, Variance: 0.11057133972644806

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.58840411476011, Training Loss Force: 3.240997239828863, time: 4.390022039413452
Validation Loss Energy: 2.374754862524269, Validation Loss Force: 3.1869915635302926, time: 0.25179052352905273
Test Loss Energy: 10.164461016518983, Test Loss Force: 11.604768090484239, time: 11.931314945220947

Epoch 9, Batch 100/252, Loss: 0.5302561521530151, Variance: 0.11414451897144318
Epoch 9, Batch 200/252, Loss: 0.8594126105308533, Variance: 0.1118553876876831

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.613966248190933, Training Loss Force: 3.2442035880530593, time: 4.394125461578369
Validation Loss Energy: 2.0804885012795435, Validation Loss Force: 3.175448573140713, time: 0.2631814479827881
Test Loss Energy: 10.252138324079697, Test Loss Force: 11.81234443724412, time: 12.233461380004883

Epoch 10, Batch 100/252, Loss: 0.5150934457778931, Variance: 0.11211957782506943
Epoch 10, Batch 200/252, Loss: 0.9492635726928711, Variance: 0.11364827305078506

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.598681771027769, Training Loss Force: 3.2532450015692698, time: 4.455633640289307
Validation Loss Energy: 1.9567632142206084, Validation Loss Force: 3.2730793297891148, time: 0.2757565975189209
Test Loss Energy: 10.25646805217642, Test Loss Force: 11.867137582949509, time: 12.315263271331787

Epoch 11, Batch 100/252, Loss: 0.5089613795280457, Variance: 0.11213670670986176
Epoch 11, Batch 200/252, Loss: 0.6448396444320679, Variance: 0.10835964977741241

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.582910298770306, Training Loss Force: 3.2878195485108703, time: 4.469178199768066
Validation Loss Energy: 1.6483522408191118, Validation Loss Force: 3.4733482713733714, time: 0.2699406147003174
Test Loss Energy: 9.895850996238327, Test Loss Force: 12.137979125793846, time: 12.337435245513916

Epoch 12, Batch 100/252, Loss: 0.518458366394043, Variance: 0.1122729629278183
Epoch 12, Batch 200/252, Loss: 0.5660015344619751, Variance: 0.1071213036775589

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.584008229521432, Training Loss Force: 3.2725634780105546, time: 4.464252233505249
Validation Loss Energy: 2.2072752886927547, Validation Loss Force: 3.270659316977342, time: 0.27622127532958984
Test Loss Energy: 10.354803839263308, Test Loss Force: 11.967066655211042, time: 12.021513223648071

Epoch 13, Batch 100/252, Loss: 0.4009367823600769, Variance: 0.10771505534648895
Epoch 13, Batch 200/252, Loss: 0.76019686460495, Variance: 0.11351019144058228

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.599304903121982, Training Loss Force: 3.2551050815338933, time: 4.629876375198364
Validation Loss Energy: 1.8671841738889552, Validation Loss Force: 3.272244454093373, time: 0.2693521976470947
Test Loss Energy: 10.025395531231075, Test Loss Force: 11.766403152527174, time: 11.932096004486084

Epoch 14, Batch 100/252, Loss: 0.6722720861434937, Variance: 0.11258559674024582
Epoch 14, Batch 200/252, Loss: 0.9260997176170349, Variance: 0.1109747588634491

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5603616462971917, Training Loss Force: 3.265445216378159, time: 4.448147535324097
Validation Loss Energy: 1.9633168793242983, Validation Loss Force: 3.308640527604195, time: 0.27471303939819336
Test Loss Energy: 10.410714205840957, Test Loss Force: 12.188585326786365, time: 12.167459964752197

Epoch 15, Batch 100/252, Loss: 0.5927696824073792, Variance: 0.11098532378673553
Epoch 15, Batch 200/252, Loss: 0.9716288447380066, Variance: 0.11101555079221725

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5896815345150075, Training Loss Force: 3.2580720577753937, time: 4.420736789703369
Validation Loss Energy: 2.5594327331798086, Validation Loss Force: 3.261254841065253, time: 0.2699882984161377
Test Loss Energy: 10.37619169337379, Test Loss Force: 11.857976992938955, time: 12.048406600952148

Epoch 16, Batch 100/252, Loss: 0.5527152419090271, Variance: 0.110533207654953
Epoch 16, Batch 200/252, Loss: 0.5374640822410583, Variance: 0.11087065935134888

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.580738731620565, Training Loss Force: 3.2522036981444162, time: 4.381433486938477
Validation Loss Energy: 2.0307994805612952, Validation Loss Force: 3.3406296224464733, time: 0.2535219192504883
Test Loss Energy: 10.163569085526888, Test Loss Force: 12.034364457145056, time: 12.542505025863647

Epoch 17, Batch 100/252, Loss: 0.7611410617828369, Variance: 0.1105394959449768
Epoch 17, Batch 200/252, Loss: 0.6700580716133118, Variance: 0.11455433070659637

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.636131299160116, Training Loss Force: 3.259381158843358, time: 4.384905576705933
Validation Loss Energy: 2.6375847492130804, Validation Loss Force: 3.3324879923285993, time: 0.2604341506958008
Test Loss Energy: 10.23148881754097, Test Loss Force: 12.247982314467762, time: 12.012969732284546

Epoch 18, Batch 100/252, Loss: 0.6885812282562256, Variance: 0.11550761759281158
Epoch 18, Batch 200/252, Loss: 0.7387248277664185, Variance: 0.11289382725954056

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6007496719941607, Training Loss Force: 3.2506385788078824, time: 4.544565677642822
Validation Loss Energy: 2.5559488071887926, Validation Loss Force: 3.320387037593153, time: 0.26965951919555664
Test Loss Energy: 10.490364297685819, Test Loss Force: 12.097956652779667, time: 12.307600259780884

Epoch 19, Batch 100/252, Loss: 0.510085940361023, Variance: 0.11286037415266037
Epoch 19, Batch 200/252, Loss: 1.0442914962768555, Variance: 0.11252814531326294

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.6025733937100464, Training Loss Force: 3.252172470563915, time: 4.563706636428833
Validation Loss Energy: 2.1560454865523444, Validation Loss Force: 3.1800578689952474, time: 0.24595046043395996
Test Loss Energy: 10.159203968191456, Test Loss Force: 12.021753689159034, time: 12.100192308425903

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–†â–†â–…â–…â–†â–†â–…â–†â–†â–‡â–„â–‡â–…â–‡â–‡â–†â–†â–ˆâ–†
wandb:   test_error_force â–‚â–ƒâ–‚â–…â–„â–‡â–…â–„â–â–ƒâ–„â–‡â–…â–ƒâ–‡â–„â–†â–ˆâ–†â–†
wandb:          test_loss â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–‚â–‚â–ƒâ–‚â–â–ƒâ–‚â–‚â–‚â–ƒâ–‚
wandb: train_error_energy â–„â–ˆâ–â–â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–„â–ƒâ–ƒ
wandb:  train_error_force â–ˆâ–…â–â–â–â–â–â–â–â–â–â–‚â–‚â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–†â–‡â–†â–†â–…â–†â–…â–‡â–…â–…â–ƒâ–†â–„â–…â–ˆâ–…â–ˆâ–ˆâ–†
wandb:  valid_error_force â–†â–‚â–ƒâ–…â–„â–ƒâ–…â–„â–â–â–ƒâ–ˆâ–ƒâ–ƒâ–„â–ƒâ–…â–…â–„â–
wandb:         valid_loss â–â–†â–†â–†â–†â–…â–†â–…â–†â–…â–…â–„â–†â–…â–…â–‡â–…â–ˆâ–‡â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 8049
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.1592
wandb:   test_error_force 12.02175
wandb:          test_loss 12.45358
wandb: train_error_energy 2.60257
wandb:  train_error_force 3.25217
wandb:         train_loss 0.89483
wandb: valid_error_energy 2.15605
wandb:  valid_error_force 3.18006
wandb:         valid_loss 0.68807
wandb: 
wandb: ğŸš€ View run al_71_89 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/95o4ne7k
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_115350-95o4ne7k/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.199554204940796, Uncertainty Bias: -0.14584940671920776
1.5258789e-05 0.00730896
1.9216516 4.7814946
(48745, 22, 3)
Found uncertainty sample 0 after 3859 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 3828 steps.
Found uncertainty sample 3 after 2457 steps.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 1827 steps.
Found uncertainty sample 11 after 291 steps.
Found uncertainty sample 12 after 336 steps.
Found uncertainty sample 13 after 566 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 3417 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 2688 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 862 steps.
Found uncertainty sample 20 after 116 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 426 steps.
Found uncertainty sample 23 after 884 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 1696 steps.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 556 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 2667 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 2538 steps.
Found uncertainty sample 33 after 910 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 47 steps.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 1901 steps.
Found uncertainty sample 39 after 1163 steps.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 68 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 3472 steps.
Found uncertainty sample 45 after 3441 steps.
Found uncertainty sample 46 after 1100 steps.
Found uncertainty sample 47 after 1180 steps.
Found uncertainty sample 48 after 438 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 466 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 2914 steps.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 726 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 3496 steps.
Found uncertainty sample 59 after 2054 steps.
Found uncertainty sample 60 after 1170 steps.
Found uncertainty sample 61 after 635 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 1658 steps.
Found uncertainty sample 66 after 1405 steps.
Found uncertainty sample 67 after 3108 steps.
Found uncertainty sample 68 after 1165 steps.
Found uncertainty sample 69 after 1127 steps.
Found uncertainty sample 70 after 462 steps.
Found uncertainty sample 71 after 1933 steps.
Found uncertainty sample 72 after 1389 steps.
Found uncertainty sample 73 after 1850 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 562 steps.
Found uncertainty sample 76 after 3962 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 429 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 3 steps.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 2443 steps.
Found uncertainty sample 93 after 3397 steps.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 1963 steps.
Found uncertainty sample 97 after 215 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 1360 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_122859-5ts5b8bu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_90
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5ts5b8bu
Training model 90. Added 52 samples to the dataset.
Epoch 0, Batch 100/253, Loss: 1.0752949714660645, Variance: 0.13581043481826782
Epoch 0, Batch 200/253, Loss: 1.065416932106018, Variance: 0.1463359296321869

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.277793379039034, Training Loss Force: 3.4322225541752744, time: 4.152972221374512
Validation Loss Energy: 4.967811171145123, Validation Loss Force: 3.3141513662967625, time: 0.24565696716308594
Test Loss Energy: 10.725689659421537, Test Loss Force: 11.662506885517859, time: 10.76462173461914

Epoch 1, Batch 100/253, Loss: 1.8044097423553467, Variance: 0.15392526984214783
Epoch 1, Batch 200/253, Loss: 1.9459803104400635, Variance: 0.14684881269931793

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.036465819887581, Training Loss Force: 3.3211309475740807, time: 4.09042501449585
Validation Loss Energy: 2.0750104840190864, Validation Loss Force: 3.297289923356697, time: 0.23327159881591797
Test Loss Energy: 10.320829587197311, Test Loss Force: 11.460919373495198, time: 11.027469635009766

Epoch 2, Batch 100/253, Loss: 1.6203055381774902, Variance: 0.1482185423374176
Epoch 2, Batch 200/253, Loss: 1.6179051399230957, Variance: 0.15560759603977203

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.054837771455298, Training Loss Force: 3.3130238684907094, time: 4.0884177684783936
Validation Loss Energy: 3.4908808391192623, Validation Loss Force: 3.34949995961131, time: 0.23848867416381836
Test Loss Energy: 10.393867310074384, Test Loss Force: 11.423662339355962, time: 10.906131505966187

Epoch 3, Batch 100/253, Loss: 0.9074989557266235, Variance: 0.15485328435897827
Epoch 3, Batch 200/253, Loss: 0.7474585175514221, Variance: 0.1500726342201233

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.8129443114311403, Training Loss Force: 3.396524987438398, time: 4.137694358825684
Validation Loss Energy: 2.774433599060134, Validation Loss Force: 3.3901723495358467, time: 0.24306511878967285
Test Loss Energy: 10.21885181423317, Test Loss Force: 11.56514724266089, time: 10.98481273651123

Epoch 4, Batch 100/253, Loss: 1.0267201662063599, Variance: 0.116964191198349
Epoch 4, Batch 200/253, Loss: 1.2472296953201294, Variance: 0.11621665209531784

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5683853510826533, Training Loss Force: 3.2452452436741717, time: 4.149061441421509
Validation Loss Energy: 3.5683218725695873, Validation Loss Force: 3.2158285984766457, time: 0.23749685287475586
Test Loss Energy: 10.70907906341688, Test Loss Force: 11.732496675819647, time: 10.914788484573364

Epoch 5, Batch 100/253, Loss: 1.0569508075714111, Variance: 0.10966188460588455
Epoch 5, Batch 200/253, Loss: 0.8187891244888306, Variance: 0.11842843145132065

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6009469172976716, Training Loss Force: 3.250793266353878, time: 4.108551502227783
Validation Loss Energy: 1.8469757242157059, Validation Loss Force: 3.24537096020964, time: 0.23719525337219238
Test Loss Energy: 9.92131425064344, Test Loss Force: 11.753215165492941, time: 11.070956468582153

Epoch 6, Batch 100/253, Loss: 0.6042149066925049, Variance: 0.11442065238952637
Epoch 6, Batch 200/253, Loss: 0.8597922325134277, Variance: 0.1109832376241684

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5957611975610395, Training Loss Force: 3.2491654404821695, time: 4.110508680343628
Validation Loss Energy: 2.01566648459958, Validation Loss Force: 3.2686575213554514, time: 0.23528289794921875
Test Loss Energy: 10.0818170016077, Test Loss Force: 12.17853929699735, time: 10.830159664154053

Epoch 7, Batch 100/253, Loss: 0.7438676357269287, Variance: 0.11542001366615295
Epoch 7, Batch 200/253, Loss: 0.8977341055870056, Variance: 0.11084040254354477

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.588038565559265, Training Loss Force: 3.246412185779224, time: 4.112988233566284
Validation Loss Energy: 3.379479599013078, Validation Loss Force: 3.308069514846378, time: 0.24025297164916992
Test Loss Energy: 10.139592897284773, Test Loss Force: 11.713340880544086, time: 10.97044825553894

Epoch 8, Batch 100/253, Loss: 1.3456557989120483, Variance: 0.11533127725124359
Epoch 8, Batch 200/253, Loss: 0.63519287109375, Variance: 0.11203517764806747

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.61273433759352, Training Loss Force: 3.2504629529580256, time: 4.03574275970459
Validation Loss Energy: 1.6815214376009777, Validation Loss Force: 3.4271017156511663, time: 0.2343597412109375
Test Loss Energy: 9.787958146817195, Test Loss Force: 11.786254319063518, time: 10.76242470741272

Epoch 9, Batch 100/253, Loss: 0.6572860479354858, Variance: 0.11481548845767975
Epoch 9, Batch 200/253, Loss: 0.7789453268051147, Variance: 0.11665096133947372

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5914831661548443, Training Loss Force: 3.251640349071439, time: 4.129805326461792
Validation Loss Energy: 2.820235083693459, Validation Loss Force: 3.219110118214013, time: 0.24737882614135742
Test Loss Energy: 10.550851823247788, Test Loss Force: 11.806273870106466, time: 11.002386331558228

Epoch 10, Batch 100/253, Loss: 0.6340035200119019, Variance: 0.10968922823667526
Epoch 10, Batch 200/253, Loss: 1.3407044410705566, Variance: 0.11718154698610306

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5707638421715053, Training Loss Force: 3.2403017677256596, time: 4.1085615158081055
Validation Loss Energy: 3.9127947821615265, Validation Loss Force: 3.3364668672971822, time: 0.24452924728393555
Test Loss Energy: 11.027237617904975, Test Loss Force: 11.64524203859857, time: 10.93701958656311

Epoch 11, Batch 100/253, Loss: 1.1396260261535645, Variance: 0.1098342090845108
Epoch 11, Batch 200/253, Loss: 0.6192361116409302, Variance: 0.11749948561191559

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6140178923223982, Training Loss Force: 3.2506386424346236, time: 4.081519365310669
Validation Loss Energy: 2.0159407239197042, Validation Loss Force: 3.3241324216953165, time: 0.23458600044250488
Test Loss Energy: 10.095163211974315, Test Loss Force: 11.587949402584968, time: 12.27559232711792

Epoch 12, Batch 100/253, Loss: 0.5164022445678711, Variance: 0.11319661140441895
Epoch 12, Batch 200/253, Loss: 0.80848228931427, Variance: 0.11186307668685913

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.6082312079234415, Training Loss Force: 3.2558842003236763, time: 4.156725168228149
Validation Loss Energy: 2.0789889581832073, Validation Loss Force: 3.36516749512474, time: 0.26807522773742676
Test Loss Energy: 9.79036257255559, Test Loss Force: 11.843928271909197, time: 10.783952236175537

Epoch 13, Batch 100/253, Loss: 0.8504286408424377, Variance: 0.11328274756669998
Epoch 13, Batch 200/253, Loss: 1.1079074144363403, Variance: 0.10911595821380615

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.566303751014885, Training Loss Force: 3.2417538777173136, time: 4.059953451156616
Validation Loss Energy: 3.3149818776772433, Validation Loss Force: 3.258583479321557, time: 0.24819588661193848
Test Loss Energy: 10.271828917016652, Test Loss Force: 12.009325099508596, time: 11.06403660774231

Epoch 14, Batch 100/253, Loss: 1.3310847282409668, Variance: 0.11758331954479218
Epoch 14, Batch 200/253, Loss: 0.6067308187484741, Variance: 0.11194650828838348

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5864262891228753, Training Loss Force: 3.2602834517567767, time: 4.097400188446045
Validation Loss Energy: 1.5234671815136116, Validation Loss Force: 3.3668966991781857, time: 0.2351367473602295
Test Loss Energy: 9.763552755052752, Test Loss Force: 12.022503792729053, time: 10.795048475265503

Epoch 15, Batch 100/253, Loss: 0.6105098724365234, Variance: 0.11453744024038315
Epoch 15, Batch 200/253, Loss: 0.9155003428459167, Variance: 0.11652062088251114

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.59850981493699, Training Loss Force: 3.2441548742319783, time: 4.112839937210083
Validation Loss Energy: 2.588160927447632, Validation Loss Force: 3.243568052668892, time: 0.24103879928588867
Test Loss Energy: 10.426838674296809, Test Loss Force: 11.964357827926287, time: 11.229026556015015

Epoch 16, Batch 100/253, Loss: 0.5964682102203369, Variance: 0.11024737358093262
Epoch 16, Batch 200/253, Loss: 1.383390188217163, Variance: 0.11357758194208145

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5972629038045176, Training Loss Force: 3.259592983779056, time: 4.514646768569946
Validation Loss Energy: 3.6045027140839556, Validation Loss Force: 3.1850868961898553, time: 0.26693177223205566
Test Loss Energy: 10.73707246885554, Test Loss Force: 11.816109896077307, time: 12.246155023574829

Epoch 17, Batch 100/253, Loss: 1.1118310689926147, Variance: 0.11120223999023438
Epoch 17, Batch 200/253, Loss: 0.6560488343238831, Variance: 0.11673850566148758

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.615236564518579, Training Loss Force: 3.2618130444504074, time: 4.522154331207275
Validation Loss Energy: 1.9665712111589477, Validation Loss Force: 3.3390708938911495, time: 0.27607202529907227
Test Loss Energy: 10.135148262582296, Test Loss Force: 11.993614497217562, time: 12.402209281921387

Epoch 18, Batch 100/253, Loss: 0.5257893800735474, Variance: 0.1097843199968338
Epoch 18, Batch 200/253, Loss: 0.8197662234306335, Variance: 0.11310707032680511

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5692428169593016, Training Loss Force: 3.2564078187018812, time: 4.4023377895355225
Validation Loss Energy: 2.025968918376452, Validation Loss Force: 3.3064080912809524, time: 0.2663557529449463
Test Loss Energy: 10.095343291857214, Test Loss Force: 11.832369724057743, time: 12.360008478164673

Epoch 19, Batch 100/253, Loss: 0.9448140859603882, Variance: 0.11677681654691696
Epoch 19, Batch 200/253, Loss: 1.0567190647125244, Variance: 0.10570108145475388

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5921459435080445, Training Loss Force: 3.2411058480144845, time: 4.792654752731323
Validation Loss Energy: 3.2760978772651415, Validation Loss Force: 3.291116109549045, time: 0.27915239334106445
Test Loss Energy: 10.21186557560872, Test Loss Force: 12.053100809034884, time: 12.36974310874939

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–†â–„â–„â–„â–†â–‚â–ƒâ–ƒâ–â–…â–ˆâ–ƒâ–â–„â–â–…â–†â–ƒâ–ƒâ–ƒ
wandb:   test_error_force â–ƒâ–â–â–‚â–„â–„â–ˆâ–„â–„â–…â–ƒâ–ƒâ–…â–†â–‡â–†â–…â–†â–…â–‡
wandb:          test_loss â–‚â–â–â–„â–‡â–†â–‡â–‡â–†â–‡â–ˆâ–†â–†â–‡â–†â–‡â–‡â–‡â–‡â–ˆ
wandb: train_error_energy â–ˆâ–‡â–‡â–†â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–„â–„â–‡â–â–â–â–â–â–â–â–â–‚â–â–‚â–â–‚â–‚â–‚â–
wandb:         train_loss â–ˆâ–…â–…â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‚â–…â–„â–…â–‚â–‚â–…â–â–„â–†â–‚â–‚â–…â–â–ƒâ–…â–‚â–‚â–…
wandb:  valid_error_force â–…â–„â–†â–‡â–‚â–ƒâ–ƒâ–…â–ˆâ–‚â–…â–…â–†â–ƒâ–†â–ƒâ–â–…â–…â–„
wandb:         valid_loss â–ˆâ–ƒâ–…â–„â–†â–â–‚â–…â–â–„â–‡â–‚â–‚â–…â–â–ƒâ–†â–‚â–‚â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 8095
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.21187
wandb:   test_error_force 12.0531
wandb:          test_loss 12.95823
wandb: train_error_energy 2.59215
wandb:  train_error_force 3.24111
wandb:         train_loss 0.88744
wandb: valid_error_energy 3.2761
wandb:  valid_error_force 3.29112
wandb:         valid_loss 1.21925
wandb: 
wandb: ğŸš€ View run al_71_90 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/5ts5b8bu
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_122859-5ts5b8bu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1818652153015137, Uncertainty Bias: -0.11846287548542023
0.0001373291 0.01194191
2.0051696 4.8003993
(48745, 22, 3)
Found uncertainty sample 0 after 564 steps.
Found uncertainty sample 1 after 373 steps.
Found uncertainty sample 2 after 15 steps.
Found uncertainty sample 3 after 634 steps.
Found uncertainty sample 4 after 1446 steps.
Found uncertainty sample 5 after 1040 steps.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1359 steps.
Found uncertainty sample 9 after 845 steps.
Found uncertainty sample 10 after 887 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 2986 steps.
Found uncertainty sample 13 after 388 steps.
Found uncertainty sample 14 after 3341 steps.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 3500 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 487 steps.
Found uncertainty sample 22 after 2775 steps.
Found uncertainty sample 23 after 1765 steps.
Found uncertainty sample 24 after 1599 steps.
Found uncertainty sample 25 after 1477 steps.
Found uncertainty sample 26 after 3487 steps.
Found uncertainty sample 27 after 923 steps.
Found uncertainty sample 28 after 666 steps.
Found uncertainty sample 29 after 3583 steps.
Found uncertainty sample 30 after 192 steps.
Found uncertainty sample 31 after 1450 steps.
Found uncertainty sample 32 after 1603 steps.
Found uncertainty sample 33 after 457 steps.
Found uncertainty sample 34 after 2418 steps.
Found uncertainty sample 35 after 88 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 2343 steps.
Found uncertainty sample 38 after 1947 steps.
Found uncertainty sample 39 after 213 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 1626 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 3525 steps.
Found uncertainty sample 45 after 2205 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 30 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 3764 steps.
Found uncertainty sample 50 after 583 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 1133 steps.
Found uncertainty sample 54 after 922 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 1903 steps.
Found uncertainty sample 57 after 811 steps.
Did not find any uncertainty samples for sample 58.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Did not find any uncertainty samples for sample 60.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 487 steps.
Found uncertainty sample 63 after 137 steps.
Found uncertainty sample 64 after 837 steps.
Found uncertainty sample 65 after 3990 steps.
Found uncertainty sample 66 after 1878 steps.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 1058 steps.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 3487 steps.
Found uncertainty sample 73 after 335 steps.
Found uncertainty sample 74 after 1203 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 1491 steps.
Found uncertainty sample 77 after 1847 steps.
Did not find any uncertainty samples for sample 78.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 93 steps.
Found uncertainty sample 81 after 3034 steps.
Found uncertainty sample 82 after 1838 steps.
Found uncertainty sample 83 after 1040 steps.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 622 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 1029 steps.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Did not find any uncertainty samples for sample 93.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 1658 steps.
Found uncertainty sample 97 after 776 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 1576 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_130024-cyhs2m60
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_91
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/cyhs2m60
Training model 91. Added 63 samples to the dataset.
Epoch 0, Batch 100/255, Loss: 1.2662338018417358, Variance: 0.0928245484828949
Epoch 0, Batch 200/255, Loss: 0.5325677394866943, Variance: 0.081692636013031

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.624756016965204, Training Loss Force: 3.8872385832272807, time: 4.631643533706665
Validation Loss Energy: 3.1187350088758805, Validation Loss Force: 3.1891664176123986, time: 0.32036876678466797
Test Loss Energy: 10.129990797492702, Test Loss Force: 12.142323314561175, time: 12.135229349136353

Epoch 1, Batch 100/255, Loss: 1.360562801361084, Variance: 0.11091333627700806
Epoch 1, Batch 200/255, Loss: 0.6680055856704712, Variance: 0.10732395946979523

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5316652279144494, Training Loss Force: 3.212073157587997, time: 4.62042236328125
Validation Loss Energy: 3.784328579515899, Validation Loss Force: 3.246906753842933, time: 0.2689957618713379
Test Loss Energy: 10.969381062369951, Test Loss Force: 11.98311213790786, time: 12.580100297927856

Epoch 2, Batch 100/255, Loss: 1.134141445159912, Variance: 0.10691157728433609
Epoch 2, Batch 200/255, Loss: 0.6568474769592285, Variance: 0.11046132445335388

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.563903750621509, Training Loss Force: 3.2186288323898813, time: 4.7028648853302
Validation Loss Energy: 3.471181700583494, Validation Loss Force: 3.2091440261820643, time: 0.2677290439605713
Test Loss Energy: 10.388139555493666, Test Loss Force: 12.10423941078951, time: 12.429629564285278

Epoch 3, Batch 100/255, Loss: 1.412722110748291, Variance: 0.11280485987663269
Epoch 3, Batch 200/255, Loss: 0.5176001191139221, Variance: 0.11002865433692932

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.552166404122815, Training Loss Force: 3.221340524318572, time: 4.525976896286011
Validation Loss Energy: 3.4703186349738777, Validation Loss Force: 3.2821309196387376, time: 0.27960681915283203
Test Loss Energy: 10.95772223645956, Test Loss Force: 11.667920469089164, time: 12.36003303527832

Epoch 4, Batch 100/255, Loss: 1.0681157112121582, Variance: 0.10963036119937897
Epoch 4, Batch 200/255, Loss: 0.8201740980148315, Variance: 0.11553244292736053

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.580668772582113, Training Loss Force: 3.2350995905250497, time: 4.539716482162476
Validation Loss Energy: 3.123891091085155, Validation Loss Force: 3.2398529070367452, time: 0.2524223327636719
Test Loss Energy: 9.851623191402538, Test Loss Force: 11.62948817553965, time: 12.227097988128662

Epoch 5, Batch 100/255, Loss: 1.2684316635131836, Variance: 0.11633679270744324
Epoch 5, Batch 200/255, Loss: 0.5685079097747803, Variance: 0.11258537322282791

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5855423819521377, Training Loss Force: 3.224357913041827, time: 4.6578967571258545
Validation Loss Energy: 3.5022023182424733, Validation Loss Force: 3.233620932048177, time: 0.28326916694641113
Test Loss Energy: 10.971354866964647, Test Loss Force: 12.129819483723939, time: 12.224939823150635

Epoch 6, Batch 100/255, Loss: 1.4213199615478516, Variance: 0.11057175695896149
Epoch 6, Batch 200/255, Loss: 0.7062798738479614, Variance: 0.11485321819782257

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5696591018666126, Training Loss Force: 3.242794178713799, time: 4.630580902099609
Validation Loss Energy: 3.1061213806988506, Validation Loss Force: 3.267389108832588, time: 0.2797410488128662
Test Loss Energy: 10.11188335653554, Test Loss Force: 11.841407992929724, time: 12.442500829696655

Epoch 7, Batch 100/255, Loss: 1.2963939905166626, Variance: 0.11369653046131134
Epoch 7, Batch 200/255, Loss: 0.5383329391479492, Variance: 0.11321831494569778

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5991907628190236, Training Loss Force: 3.235324722129768, time: 4.510539293289185
Validation Loss Energy: 3.9570425260283963, Validation Loss Force: 3.237661530904011, time: 0.26737308502197266
Test Loss Energy: 11.1120175918447, Test Loss Force: 11.821310290845592, time: 12.348544120788574

Epoch 8, Batch 100/255, Loss: 1.1008304357528687, Variance: 0.11159437894821167
Epoch 8, Batch 200/255, Loss: 0.6230027675628662, Variance: 0.1125558540225029

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.591077055848655, Training Loss Force: 3.237361316136499, time: 4.751973628997803
Validation Loss Energy: 3.1077833386702265, Validation Loss Force: 3.205108058844465, time: 0.2734222412109375
Test Loss Energy: 10.176612004195453, Test Loss Force: 11.931362466731612, time: 12.397411346435547

Epoch 9, Batch 100/255, Loss: 1.5331573486328125, Variance: 0.11662667244672775
Epoch 9, Batch 200/255, Loss: 0.7779167890548706, Variance: 0.11169251799583435

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5769153261104423, Training Loss Force: 3.2472894583602825, time: 4.637066841125488
Validation Loss Energy: 3.7297524163828273, Validation Loss Force: 3.276446577147664, time: 0.2555043697357178
Test Loss Energy: 10.921150421733344, Test Loss Force: 11.6296856679166, time: 12.32290267944336

Epoch 10, Batch 100/255, Loss: 1.549544095993042, Variance: 0.11016689240932465
Epoch 10, Batch 200/255, Loss: 0.5442112684249878, Variance: 0.1145753264427185

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5693158543219217, Training Loss Force: 3.248467989390876, time: 4.639365911483765
Validation Loss Energy: 3.1459864613679107, Validation Loss Force: 3.198754016630099, time: 0.22093725204467773
Test Loss Energy: 10.355941499064716, Test Loss Force: 11.797259808052003, time: 11.541350603103638

Epoch 11, Batch 100/255, Loss: 1.153985619544983, Variance: 0.11201415956020355
Epoch 11, Batch 200/255, Loss: 0.4965522289276123, Variance: 0.11260783672332764

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5826449605973134, Training Loss Force: 3.251168219534432, time: 4.711416482925415
Validation Loss Energy: 3.687239888636344, Validation Loss Force: 3.2521691828363033, time: 0.26514768600463867
Test Loss Energy: 10.698213457284323, Test Loss Force: 11.628884763138714, time: 10.753139734268188

Epoch 12, Batch 100/255, Loss: 1.1574329137802124, Variance: 0.10919933021068573
Epoch 12, Batch 200/255, Loss: 0.7191005349159241, Variance: 0.11593998968601227

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5861687995961296, Training Loss Force: 3.2407245213951406, time: 4.0991270542144775
Validation Loss Energy: 3.314509462817243, Validation Loss Force: 3.209774046263553, time: 0.22458600997924805
Test Loss Energy: 10.106417325636814, Test Loss Force: 11.728420604020155, time: 10.317276954650879

Epoch 13, Batch 100/255, Loss: 1.517233967781067, Variance: 0.11549109220504761
Epoch 13, Batch 200/255, Loss: 0.6093822121620178, Variance: 0.11160431802272797

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5729246256637848, Training Loss Force: 3.2545839956686082, time: 4.177123546600342
Validation Loss Energy: 3.611766671496241, Validation Loss Force: 3.2779435372819075, time: 0.22787714004516602
Test Loss Energy: 10.629942710094923, Test Loss Force: 11.78958513705031, time: 10.002977132797241

Epoch 14, Batch 100/255, Loss: 0.8719543814659119, Variance: 0.11048510670661926
Epoch 14, Batch 200/255, Loss: 0.6028497219085693, Variance: 0.11495906859636307

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.6093822728693365, Training Loss Force: 3.2356482307095815, time: 4.044788122177124
Validation Loss Energy: 3.394029691684583, Validation Loss Force: 3.3099608220380587, time: 0.22614216804504395
Test Loss Energy: 10.237238881005336, Test Loss Force: 11.820751211755027, time: 10.28907561302185

Epoch 15, Batch 100/255, Loss: 1.2235783338546753, Variance: 0.11472870409488678
Epoch 15, Batch 200/255, Loss: 0.6097762584686279, Variance: 0.11255091428756714

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5794604623911916, Training Loss Force: 3.2578348901028935, time: 4.174986124038696
Validation Loss Energy: 3.9580222702704786, Validation Loss Force: 3.2965776492927032, time: 0.2229022979736328
Test Loss Energy: 11.145526178607177, Test Loss Force: 11.799267463020108, time: 10.035200595855713

Epoch 16, Batch 100/255, Loss: 1.0128109455108643, Variance: 0.11192776262760162
Epoch 16, Batch 200/255, Loss: 0.7163036465644836, Variance: 0.11645679175853729

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.591093680501732, Training Loss Force: 3.245410941982913, time: 4.170453786849976
Validation Loss Energy: 3.346572350214482, Validation Loss Force: 3.3201744776264506, time: 0.23051118850708008
Test Loss Energy: 10.005771146807918, Test Loss Force: 11.79711166195573, time: 10.38485860824585

Epoch 17, Batch 100/255, Loss: 1.5270204544067383, Variance: 0.11476708948612213
Epoch 17, Batch 200/255, Loss: 0.5316084623336792, Variance: 0.11263949424028397

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5878275782013866, Training Loss Force: 3.2417976295002915, time: 4.181082487106323
Validation Loss Energy: 3.5185455210123058, Validation Loss Force: 3.381579369059177, time: 0.24894022941589355
Test Loss Energy: 10.883829226237484, Test Loss Force: 12.040124187481453, time: 10.116078853607178

Epoch 18, Batch 100/255, Loss: 1.218790888786316, Variance: 0.11277806013822556
Epoch 18, Batch 200/255, Loss: 0.6735485792160034, Variance: 0.11438488215208054

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.588860646028038, Training Loss Force: 3.252360628528698, time: 4.191739797592163
Validation Loss Energy: 3.480139813925656, Validation Loss Force: 3.288925267822261, time: 0.23293495178222656
Test Loss Energy: 10.317577214035083, Test Loss Force: 12.01213272194572, time: 10.31002950668335

Epoch 19, Batch 100/255, Loss: 1.2150804996490479, Variance: 0.11646169424057007
Epoch 19, Batch 200/255, Loss: 0.4982038736343384, Variance: 0.11440188437700272

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5616007447700415, Training Loss Force: 3.2518875605058697, time: 4.179915904998779
Validation Loss Energy: 3.8605133581787285, Validation Loss Force: 3.3156322942524987, time: 0.2215108871459961
Test Loss Energy: 11.255382746060437, Test Loss Force: 12.103979962762871, time: 10.450532674789429

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‡â–„â–‡â–â–‡â–‚â–‡â–ƒâ–†â–„â–…â–‚â–…â–ƒâ–‡â–‚â–†â–ƒâ–ˆ
wandb:   test_error_force â–ˆâ–†â–‡â–‚â–â–ˆâ–„â–„â–…â–â–ƒâ–â–‚â–ƒâ–„â–ƒâ–ƒâ–‡â–†â–‡
wandb:          test_loss â–ˆâ–…â–…â–…â–â–†â–ƒâ–†â–ƒâ–„â–ƒâ–…â–‚â–„â–‚â–†â–‚â–†â–ƒâ–†
wandb: train_error_energy â–ˆâ–â–ƒâ–ƒâ–…â–…â–„â–†â–…â–„â–„â–…â–…â–„â–‡â–…â–…â–…â–…â–ƒ
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–‡â–„â–„â–â–„â–â–ˆâ–â–†â–â–†â–ƒâ–…â–ƒâ–ˆâ–ƒâ–„â–„â–‡
wandb:  valid_error_force â–â–ƒâ–‚â–„â–ƒâ–ƒâ–„â–ƒâ–‚â–„â–â–ƒâ–‚â–„â–…â–…â–†â–ˆâ–…â–†
wandb:         valid_loss â–ƒâ–†â–„â–„â–â–„â–‚â–ˆâ–â–…â–â–†â–‚â–…â–ƒâ–ˆâ–ƒâ–…â–„â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 8151
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.25538
wandb:   test_error_force 12.10398
wandb:          test_loss 13.42486
wandb: train_error_energy 2.5616
wandb:  train_error_force 3.25189
wandb:         train_loss 0.8773
wandb: valid_error_energy 3.86051
wandb:  valid_error_force 3.31563
wandb:         valid_loss 1.45117
wandb: 
wandb: ğŸš€ View run al_71_91 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/cyhs2m60
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_130024-cyhs2m60/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2563562393188477, Uncertainty Bias: -0.15664097666740417
7.05719e-05 0.07757759
1.9195505 4.812822
(48745, 22, 3)
Found uncertainty sample 0 after 188 steps.
Found uncertainty sample 1 after 3181 steps.
Found uncertainty sample 2 after 3077 steps.
Found uncertainty sample 3 after 3774 steps.
Found uncertainty sample 4 after 696 steps.
Found uncertainty sample 5 after 644 steps.
Found uncertainty sample 6 after 3985 steps.
Found uncertainty sample 7 after 1424 steps.
Found uncertainty sample 8 after 526 steps.
Found uncertainty sample 9 after 2942 steps.
Found uncertainty sample 10 after 520 steps.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 1097 steps.
Found uncertainty sample 14 after 1859 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 1843 steps.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 973 steps.
Found uncertainty sample 20 after 3357 steps.
Found uncertainty sample 21 after 203 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 3639 steps.
Found uncertainty sample 24 after 1259 steps.
Did not find any uncertainty samples for sample 25.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 1486 steps.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 3499 steps.
Found uncertainty sample 33 after 1642 steps.
Found uncertainty sample 34 after 2360 steps.
Found uncertainty sample 35 after 3412 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 1421 steps.
Found uncertainty sample 38 after 4 steps.
Found uncertainty sample 39 after 748 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 1334 steps.
Found uncertainty sample 42 after 3481 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 17 steps.
Found uncertainty sample 45 after 3 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 3594 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 1 steps.
Did not find any uncertainty samples for sample 50.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1310 steps.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 120 steps.
Found uncertainty sample 56 after 2829 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 2527 steps.
Found uncertainty sample 59 after 2069 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 3736 steps.
Found uncertainty sample 62 after 2095 steps.
Found uncertainty sample 63 after 1689 steps.
Found uncertainty sample 64 after 1434 steps.
Found uncertainty sample 65 after 586 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 2866 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 765 steps.
Found uncertainty sample 72 after 2222 steps.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 729 steps.
Did not find any uncertainty samples for sample 78.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Did not find any uncertainty samples for sample 82.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 3125 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 261 steps.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 3040 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 167 steps.
Found uncertainty sample 93 after 1389 steps.
Found uncertainty sample 94 after 1365 steps.
Found uncertainty sample 95 after 1887 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 3068 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_133531-13cy746w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_92
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/13cy746w
Training model 92. Added 55 samples to the dataset.
Epoch 0, Batch 100/257, Loss: 0.73405522108078, Variance: 0.14371135830879211
Epoch 0, Batch 200/257, Loss: 0.8379330635070801, Variance: 0.14967107772827148

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.347729104669238, Training Loss Force: 3.4694655179097915, time: 4.148581504821777
Validation Loss Energy: 5.840396866493884, Validation Loss Force: 3.4443515381155585, time: 0.2370450496673584
Test Loss Energy: 10.929338751983058, Test Loss Force: 11.468531076926682, time: 10.7192702293396

Epoch 1, Batch 100/257, Loss: 1.1638329029083252, Variance: 0.15148690342903137
Epoch 1, Batch 200/257, Loss: 1.0983824729919434, Variance: 0.1513199359178543

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.051439274903463, Training Loss Force: 3.3108464889991955, time: 4.183920383453369
Validation Loss Energy: 5.12434205853754, Validation Loss Force: 3.298364658256432, time: 0.23608016967773438
Test Loss Energy: 10.610311769172936, Test Loss Force: 11.247908780399912, time: 10.8588387966156

Epoch 2, Batch 100/257, Loss: 0.193836510181427, Variance: 0.08675599843263626
Epoch 2, Batch 200/257, Loss: 0.28150415420532227, Variance: 0.08268795907497406

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.9480572322281597, Training Loss Force: 3.3723363700391804, time: 4.09898567199707
Validation Loss Energy: 1.3792613876189321, Validation Loss Force: 3.2873468998482673, time: 0.23372220993041992
Test Loss Energy: 9.64220204825575, Test Loss Force: 12.184275608601794, time: 10.709607124328613

Epoch 3, Batch 100/257, Loss: 0.44621866941452026, Variance: 0.08100028336048126
Epoch 3, Batch 200/257, Loss: 0.3856867551803589, Variance: 0.07806740701198578

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6007877154328962, Training Loss Force: 3.2475656029074087, time: 4.201484203338623
Validation Loss Energy: 1.752894870426258, Validation Loss Force: 3.2098685915829135, time: 0.23758506774902344
Test Loss Energy: 9.82566909492492, Test Loss Force: 11.83504636574155, time: 10.776286840438843

Epoch 4, Batch 100/257, Loss: 0.2447054386138916, Variance: 0.07924392074346542
Epoch 4, Batch 200/257, Loss: 0.22488808631896973, Variance: 0.07798938453197479

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5821404498271359, Training Loss Force: 3.2440561491551176, time: 4.152724742889404
Validation Loss Energy: 1.6700166349699246, Validation Loss Force: 3.276453079086725, time: 0.23955559730529785
Test Loss Energy: 9.970793755084026, Test Loss Force: 11.96206839739498, time: 10.708549976348877

Epoch 5, Batch 100/257, Loss: 0.41065263748168945, Variance: 0.07961426675319672
Epoch 5, Batch 200/257, Loss: 0.4640992283821106, Variance: 0.07902815192937851

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5981803060873865, Training Loss Force: 3.259812638999586, time: 4.172590732574463
Validation Loss Energy: 1.4215798146709107, Validation Loss Force: 3.3017132014067236, time: 0.23640775680541992
Test Loss Energy: 9.74003991448975, Test Loss Force: 12.017914672564427, time: 10.898338794708252

Epoch 6, Batch 100/257, Loss: 0.573841392993927, Variance: 0.07798364758491516
Epoch 6, Batch 200/257, Loss: 0.43792223930358887, Variance: 0.07935357093811035

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.5958018214178964, Training Loss Force: 3.2463815376303065, time: 4.171929836273193
Validation Loss Energy: 1.388086567480976, Validation Loss Force: 3.2235578453362628, time: 0.2364969253540039
Test Loss Energy: 9.436144992897253, Test Loss Force: 11.964683946126312, time: 10.667033433914185

Epoch 7, Batch 100/257, Loss: 0.06614327430725098, Variance: 0.07979722321033478
Epoch 7, Batch 200/257, Loss: 0.7640354633331299, Variance: 0.07692952454090118

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.590346545825178, Training Loss Force: 3.273298186007504, time: 4.271609306335449
Validation Loss Energy: 1.5361006272446291, Validation Loss Force: 3.400742142156235, time: 0.235182523727417
Test Loss Energy: 9.97002438305514, Test Loss Force: 11.98836172507029, time: 10.795000076293945

Epoch 8, Batch 100/257, Loss: 0.2574818730354309, Variance: 0.07589031755924225
Epoch 8, Batch 200/257, Loss: 0.18255430459976196, Variance: 0.0754404067993164

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.5908987884837313, Training Loss Force: 3.2586530621300347, time: 4.115105628967285
Validation Loss Energy: 1.5092041316381217, Validation Loss Force: 3.3197526531964043, time: 0.25242066383361816
Test Loss Energy: 9.818377545424614, Test Loss Force: 12.043098347786822, time: 11.879514455795288

Epoch 9, Batch 100/257, Loss: 0.26536303758621216, Variance: 0.07461675256490707
Epoch 9, Batch 200/257, Loss: 0.6694338917732239, Variance: 0.07940594851970673

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.5968251960329312, Training Loss Force: 3.2645383202911042, time: 4.138875961303711
Validation Loss Energy: 1.5739635114873358, Validation Loss Force: 3.3190045739337455, time: 0.24416112899780273
Test Loss Energy: 9.964655160773576, Test Loss Force: 12.359916275466668, time: 10.847559690475464

Epoch 10, Batch 100/257, Loss: 0.3987821936607361, Variance: 0.0781366303563118
Epoch 10, Batch 200/257, Loss: 0.3588612675666809, Variance: 0.07624490559101105

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.597422653307757, Training Loss Force: 3.2658782929591164, time: 4.157062530517578
Validation Loss Energy: 1.5606817868048644, Validation Loss Force: 3.3499019219535398, time: 0.24685263633728027
Test Loss Energy: 9.562758269283952, Test Loss Force: 12.255301745409398, time: 10.772978782653809

Epoch 11, Batch 100/257, Loss: 0.3584945797920227, Variance: 0.07740825414657593
Epoch 11, Batch 200/257, Loss: 0.2312808632850647, Variance: 0.07664814591407776

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.60225795280034, Training Loss Force: 3.247512575593252, time: 4.1121251583099365
Validation Loss Energy: 1.7542749292588895, Validation Loss Force: 3.280222021584528, time: 0.25069665908813477
Test Loss Energy: 10.030466969087074, Test Loss Force: 12.350114594171423, time: 10.906735181808472

Epoch 12, Batch 100/257, Loss: 0.4133495092391968, Variance: 0.07625649869441986
Epoch 12, Batch 200/257, Loss: 0.4254283905029297, Variance: 0.07673457264900208

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.578866822627971, Training Loss Force: 3.2360555601002408, time: 4.12007212638855
Validation Loss Energy: 1.6374907738844733, Validation Loss Force: 3.2688856236285395, time: 0.23463129997253418
Test Loss Energy: 10.12935444871108, Test Loss Force: 12.238898868241101, time: 10.686482191085815

Epoch 13, Batch 100/257, Loss: 0.7311760187149048, Variance: 0.07827481627464294
Epoch 13, Batch 200/257, Loss: 0.5268215537071228, Variance: 0.07684589922428131

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.588070215824884, Training Loss Force: 3.2506156174326306, time: 4.209435701370239
Validation Loss Energy: 1.612920213375584, Validation Loss Force: 3.220419056521435, time: 0.24089884757995605
Test Loss Energy: 9.439915679209236, Test Loss Force: 11.690680510736259, time: 10.854072570800781

Epoch 14, Batch 100/257, Loss: 0.5893625617027283, Variance: 0.07625485956668854
Epoch 14, Batch 200/257, Loss: 0.37049442529678345, Variance: 0.07426676154136658

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.588933575583041, Training Loss Force: 3.238316881377003, time: 4.129782438278198
Validation Loss Energy: 1.3360013372826547, Validation Loss Force: 3.2678926528391896, time: 0.23446416854858398
Test Loss Energy: 9.753392270991116, Test Loss Force: 12.087761576891339, time: 10.752114057540894

Epoch 15, Batch 100/257, Loss: 0.637150764465332, Variance: 0.07656312733888626
Epoch 15, Batch 200/257, Loss: 0.47596806287765503, Variance: 0.07412083446979523

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5989056551623306, Training Loss Force: 3.2521632728332746, time: 4.166846036911011
Validation Loss Energy: 2.0470349114644986, Validation Loss Force: 3.203523990540335, time: 0.24112796783447266
Test Loss Energy: 10.15952055475307, Test Loss Force: 12.334658235744378, time: 10.865181684494019

Epoch 16, Batch 100/257, Loss: 0.1967664361000061, Variance: 0.07607661187648773
Epoch 16, Batch 200/257, Loss: 0.2347630262374878, Variance: 0.07475528120994568

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.598745809473355, Training Loss Force: 3.262228393892095, time: 4.177256107330322
Validation Loss Energy: 1.7741742918906762, Validation Loss Force: 3.2719015436086436, time: 0.23528432846069336
Test Loss Energy: 10.400856164769847, Test Loss Force: 12.472570705220459, time: 10.887269258499146

Epoch 17, Batch 100/257, Loss: 0.4356144666671753, Variance: 0.07604219019412994
Epoch 17, Batch 200/257, Loss: 0.6972888112068176, Variance: 0.07479330152273178

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.5954907898769624, Training Loss Force: 3.2640290898170377, time: 4.216113805770874
Validation Loss Energy: 1.4406335689541223, Validation Loss Force: 3.3120014066251704, time: 0.24784159660339355
Test Loss Energy: 9.990720214102677, Test Loss Force: 12.382775974963344, time: 10.851099729537964

Epoch 18, Batch 100/257, Loss: 0.6352026462554932, Variance: 0.07755135744810104
Epoch 18, Batch 200/257, Loss: 0.3050844073295593, Variance: 0.07562821358442307

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.5826574576925967, Training Loss Force: 3.2526033129857246, time: 4.23978328704834
Validation Loss Energy: 1.2793872393253083, Validation Loss Force: 3.299525432514154, time: 0.23727846145629883
Test Loss Energy: 9.86172710117981, Test Loss Force: 12.202523972314767, time: 10.740403175354004

Epoch 19, Batch 100/257, Loss: 0.3220260739326477, Variance: 0.07554890960454941
Epoch 19, Batch 200/257, Loss: 0.2076624035835266, Variance: 0.07708460837602615

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5748122132908475, Training Loss Force: 3.2368283156096367, time: 4.196629047393799
Validation Loss Energy: 1.8045989133136655, Validation Loss Force: 3.2733376259983906, time: 0.24080443382263184
Test Loss Energy: 10.09516266171339, Test Loss Force: 12.416522894655232, time: 10.96211838722229

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–‡â–‚â–ƒâ–„â–‚â–â–„â–ƒâ–ƒâ–‚â–„â–„â–â–‚â–„â–†â–„â–ƒâ–„
wandb:   test_error_force â–‚â–â–†â–„â–…â–…â–…â–…â–†â–‡â–‡â–‡â–‡â–„â–†â–‡â–ˆâ–‡â–†â–ˆ
wandb:          test_loss â–â–â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb: train_error_energy â–ˆâ–‡â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–ƒâ–…â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–‚â–‚â–â–
wandb:         train_loss â–ˆâ–†â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–‡â–â–‚â–‚â–â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–‚
wandb:  valid_error_force â–ˆâ–„â–ƒâ–â–ƒâ–„â–‚â–‡â–„â–„â–…â–ƒâ–ƒâ–â–ƒâ–â–ƒâ–„â–„â–ƒ
wandb:         valid_loss â–ˆâ–†â–â–‚â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–â–â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 8200
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.09516
wandb:   test_error_force 12.41652
wandb:          test_loss 16.79992
wandb: train_error_energy 1.57481
wandb:  train_error_force 3.23683
wandb:         train_loss 0.42649
wandb: valid_error_energy 1.8046
wandb:  valid_error_force 3.27334
wandb:         valid_loss 0.57881
wandb: 
wandb: ğŸš€ View run al_71_92 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/13cy746w
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_133531-13cy746w/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.8677759170532227, Uncertainty Bias: -0.07112987339496613
1.5258789e-05 0.0022878647
1.9260796 4.9763
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 68 steps.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 2298 steps.
Found uncertainty sample 4 after 3075 steps.
Found uncertainty sample 5 after 288 steps.
Found uncertainty sample 6 after 3389 steps.
Found uncertainty sample 7 after 230 steps.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 2221 steps.
Found uncertainty sample 11 after 3145 steps.
Found uncertainty sample 12 after 322 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1298 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 495 steps.
Found uncertainty sample 17 after 2150 steps.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 883 steps.
Found uncertainty sample 21 after 1051 steps.
Found uncertainty sample 22 after 1428 steps.
Found uncertainty sample 23 after 352 steps.
Found uncertainty sample 24 after 622 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 2277 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 1653 steps.
Found uncertainty sample 29 after 128 steps.
Found uncertainty sample 30 after 1038 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 2105 steps.
Found uncertainty sample 33 after 2137 steps.
Did not find any uncertainty samples for sample 34.
Did not find any uncertainty samples for sample 35.
Did not find any uncertainty samples for sample 36.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1140 steps.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 3670 steps.
Found uncertainty sample 43 after 2416 steps.
Found uncertainty sample 44 after 1836 steps.
Found uncertainty sample 45 after 3249 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 251 steps.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 3372 steps.
Found uncertainty sample 51 after 1228 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 1239 steps.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 1389 steps.
Found uncertainty sample 58 after 534 steps.
Found uncertainty sample 59 after 1676 steps.
Found uncertainty sample 60 after 686 steps.
Found uncertainty sample 61 after 3651 steps.
Found uncertainty sample 62 after 1919 steps.
Found uncertainty sample 63 after 973 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 1367 steps.
Found uncertainty sample 66 after 293 steps.
Found uncertainty sample 67 after 168 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 1818 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 965 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 2258 steps.
Found uncertainty sample 74 after 1308 steps.
Found uncertainty sample 75 after 3801 steps.
Found uncertainty sample 76 after 826 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 3936 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 1803 steps.
Did not find any uncertainty samples for sample 81.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 3400 steps.
Found uncertainty sample 84 after 733 steps.
Found uncertainty sample 85 after 1091 steps.
Found uncertainty sample 86 after 730 steps.
Found uncertainty sample 87 after 1039 steps.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 868 steps.
Found uncertainty sample 92 after 3234 steps.
Found uncertainty sample 93 after 2725 steps.
Found uncertainty sample 94 after 3410 steps.
Found uncertainty sample 95 after 293 steps.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 1500 steps.
Found uncertainty sample 98 after 1324 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_140720-rqzj94yy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_93
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/rqzj94yy
Training model 93. Added 64 samples to the dataset.
Epoch 0, Batch 100/259, Loss: 0.7642592787742615, Variance: 0.10647571086883545
Epoch 0, Batch 200/259, Loss: 1.0111209154129028, Variance: 0.10918618738651276

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.857120784303289, Training Loss Force: 3.351569620318303, time: 4.296833038330078
Validation Loss Energy: 2.170372687408776, Validation Loss Force: 3.6652568326707944, time: 0.2580454349517822
Test Loss Energy: 10.228845704467824, Test Loss Force: 12.261658914551425, time: 10.779815435409546

Epoch 1, Batch 100/259, Loss: 0.7894032001495361, Variance: 0.10401315987110138
Epoch 1, Batch 200/259, Loss: 1.0881597995758057, Variance: 0.11320888996124268

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.619163018775752, Training Loss Force: 3.2583020814033223, time: 4.243093967437744
Validation Loss Energy: 3.86970669920665, Validation Loss Force: 3.260426021970281, time: 0.2418382167816162
Test Loss Energy: 11.05932515582589, Test Loss Force: 11.880054771659697, time: 10.915250539779663

Epoch 2, Batch 100/259, Loss: 0.9525796175003052, Variance: 0.10596982389688492
Epoch 2, Batch 200/259, Loss: 0.6351094245910645, Variance: 0.10754329711198807

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5955234284230237, Training Loss Force: 3.2443829445395536, time: 4.23511266708374
Validation Loss Energy: 2.3598773571405913, Validation Loss Force: 3.4341648700276544, time: 0.24547481536865234
Test Loss Energy: 10.506695295854838, Test Loss Force: 11.842452831174068, time: 10.787643432617188

Epoch 3, Batch 100/259, Loss: 0.6709534525871277, Variance: 0.10739687085151672
Epoch 3, Batch 200/259, Loss: 0.8772112131118774, Variance: 0.11010205745697021

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5899494234801703, Training Loss Force: 3.248169214135816, time: 4.178409814834595
Validation Loss Energy: 2.115163075089369, Validation Loss Force: 3.4178930104521767, time: 0.24333930015563965
Test Loss Energy: 9.642726428265375, Test Loss Force: 11.849167334165607, time: 10.964395761489868

Epoch 4, Batch 100/259, Loss: 1.028932809829712, Variance: 0.11568234860897064
Epoch 4, Batch 200/259, Loss: 1.1215176582336426, Variance: 0.11160680651664734

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.598592387849797, Training Loss Force: 3.238658666724035, time: 4.077949047088623
Validation Loss Energy: 3.195093558540192, Validation Loss Force: 3.2084689823992703, time: 0.2375497817993164
Test Loss Energy: 9.950074813077594, Test Loss Force: 11.828358555068156, time: 10.730713605880737

Epoch 5, Batch 100/259, Loss: 1.2712020874023438, Variance: 0.11576314270496368
Epoch 5, Batch 200/259, Loss: 0.6448270082473755, Variance: 0.11420708894729614

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6154172104845665, Training Loss Force: 3.2305379264109546, time: 4.209373712539673
Validation Loss Energy: 1.6028450802628746, Validation Loss Force: 3.210628338708819, time: 0.24826502799987793
Test Loss Energy: 9.540134100116541, Test Loss Force: 11.715781460290616, time: 10.956285953521729

Epoch 6, Batch 100/259, Loss: 0.9039461016654968, Variance: 0.11430753767490387
Epoch 6, Batch 200/259, Loss: 0.9103321433067322, Variance: 0.1104688048362732

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5870783120375016, Training Loss Force: 3.2477783025452007, time: 4.114905595779419
Validation Loss Energy: 2.6895209469520323, Validation Loss Force: 3.5756963629459113, time: 0.23479485511779785
Test Loss Energy: 10.800617276765315, Test Loss Force: 11.804218006685362, time: 10.764381885528564

Epoch 7, Batch 100/259, Loss: 0.7905926704406738, Variance: 0.10934088379144669
Epoch 7, Batch 200/259, Loss: 1.2367874383926392, Variance: 0.11023341119289398

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5895848564866157, Training Loss Force: 3.3008774812876807, time: 4.163546085357666
Validation Loss Energy: 3.5072461752022, Validation Loss Force: 3.275143873576328, time: 0.23751497268676758
Test Loss Energy: 10.600560977686897, Test Loss Force: 11.721357645965574, time: 10.865728616714478

Epoch 8, Batch 100/259, Loss: 0.9479360580444336, Variance: 0.10549324750900269
Epoch 8, Batch 200/259, Loss: 0.5573998093605042, Variance: 0.11401082575321198

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5962535119779773, Training Loss Force: 3.2307145503286283, time: 4.343690872192383
Validation Loss Energy: 2.1624136905243914, Validation Loss Force: 3.200626174275231, time: 0.23881077766418457
Test Loss Energy: 9.833436732043747, Test Loss Force: 11.61106734699541, time: 10.639079332351685

Epoch 9, Batch 100/259, Loss: 0.47187578678131104, Variance: 0.10948632657527924
Epoch 9, Batch 200/259, Loss: 0.6015938520431519, Variance: 0.10955682396888733

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5940989391003235, Training Loss Force: 3.242440507869445, time: 4.142071723937988
Validation Loss Energy: 1.4920547106916282, Validation Loss Force: 3.6350454392219453, time: 0.23563194274902344
Test Loss Energy: 9.60979452427987, Test Loss Force: 11.888786055556906, time: 10.876535415649414

Epoch 10, Batch 100/259, Loss: 1.060967206954956, Variance: 0.08816300332546234
Epoch 10, Batch 200/259, Loss: 0.17536860704421997, Variance: 0.07488526403903961

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.207630085841107, Training Loss Force: 3.9190151513590967, time: 4.178544998168945
Validation Loss Energy: 1.4543972610029543, Validation Loss Force: 3.371665203472059, time: 0.23802852630615234
Test Loss Energy: 9.708471608374538, Test Loss Force: 12.15269936567221, time: 10.916843175888062

Epoch 11, Batch 100/259, Loss: 0.7044596076011658, Variance: 0.07562869787216187
Epoch 11, Batch 200/259, Loss: 0.5290347933769226, Variance: 0.077507883310318

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.5595663862430802, Training Loss Force: 3.238583006499373, time: 4.216777563095093
Validation Loss Energy: 6.452008533417484, Validation Loss Force: 3.4435156826755753, time: 0.23967909812927246
Test Loss Energy: 12.43931290686988, Test Loss Force: 11.858784277013, time: 11.008871793746948

Epoch 12, Batch 100/259, Loss: 0.978240966796875, Variance: 0.14013516902923584
Epoch 12, Batch 200/259, Loss: 0.8174586296081543, Variance: 0.14643962681293488

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.164397073315053, Training Loss Force: 3.40364631953721, time: 4.099872350692749
Validation Loss Energy: 2.70107275503979, Validation Loss Force: 3.335847624402357, time: 0.2418532371520996
Test Loss Energy: 10.19461305360153, Test Loss Force: 11.516484208853877, time: 10.744642972946167

Epoch 13, Batch 100/259, Loss: 1.69074285030365, Variance: 0.1494114100933075
Epoch 13, Batch 200/259, Loss: 1.5380606651306152, Variance: 0.15422764420509338

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 4.026493171661038, Training Loss Force: 3.331014389242064, time: 4.091645240783691
Validation Loss Energy: 4.4228710998952225, Validation Loss Force: 3.57023883124688, time: 0.25135040283203125
Test Loss Energy: 10.543966711389805, Test Loss Force: 11.523300248975957, time: 10.887630701065063

Epoch 14, Batch 100/259, Loss: 1.6270420551300049, Variance: 0.15250781178474426
Epoch 14, Batch 200/259, Loss: 1.8604943752288818, Variance: 0.14865554869174957

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 3.9912495437221525, Training Loss Force: 3.350196427432904, time: 4.203691244125366
Validation Loss Energy: 6.102617307730291, Validation Loss Force: 3.261856715766696, time: 0.2422044277191162
Test Loss Energy: 12.13384642841448, Test Loss Force: 11.261027984818387, time: 10.766596794128418

Epoch 15, Batch 100/259, Loss: 1.0456126928329468, Variance: 0.151322141289711
Epoch 15, Batch 200/259, Loss: 1.2588794231414795, Variance: 0.1592911183834076

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 3.9008760309683, Training Loss Force: 3.5212428569829153, time: 4.078296184539795
Validation Loss Energy: 5.675992219966146, Validation Loss Force: 6.878221683261662, time: 0.23681306838989258
Test Loss Energy: 11.14677845123096, Test Loss Force: 13.237139576428799, time: 10.954162836074829

Epoch 16, Batch 100/259, Loss: 0.7637931108474731, Variance: 0.1535957157611847
Epoch 16, Batch 200/259, Loss: 1.1133531332015991, Variance: 0.10879000276327133

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 3.5500199678793027, Training Loss Force: 3.6277314692878124, time: 4.281146287918091
Validation Loss Energy: 3.2825547480910964, Validation Loss Force: 3.295318767009286, time: 0.23883938789367676
Test Loss Energy: 9.94193463065891, Test Loss Force: 11.380589588162717, time: 10.802552223205566

Epoch 17, Batch 100/259, Loss: 1.1825393438339233, Variance: 0.11773252487182617
Epoch 17, Batch 200/259, Loss: 0.4903024435043335, Variance: 0.11352694779634476

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5567715070949117, Training Loss Force: 3.2161220628273015, time: 4.157121658325195
Validation Loss Energy: 2.0951386169313997, Validation Loss Force: 3.238001769358989, time: 1.5108287334442139
Test Loss Energy: 9.622649113814875, Test Loss Force: 11.590047855269106, time: 10.909493684768677

Epoch 18, Batch 100/259, Loss: 0.6721663475036621, Variance: 0.11676030606031418
Epoch 18, Batch 200/259, Loss: 0.768673300743103, Variance: 0.11630093306303024

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.584970431762105, Training Loss Force: 3.2274060616608664, time: 4.2068846225738525
Validation Loss Energy: 2.495632233473702, Validation Loss Force: 3.2915895486871163, time: 0.2425374984741211
Test Loss Energy: 10.406821963159825, Test Loss Force: 11.767998247034356, time: 10.796801090240479

Epoch 19, Batch 100/259, Loss: 0.6864025592803955, Variance: 0.11043255031108856
Epoch 19, Batch 200/259, Loss: 1.4333581924438477, Variance: 0.11685493588447571

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.584350556968352, Training Loss Force: 3.2147672285401345, time: 4.083247423171997
Validation Loss Energy: 3.8573478490663895, Validation Loss Force: 3.2585137393450525, time: 0.23428821563720703
Test Loss Energy: 11.05624316303193, Test Loss Force: 11.687698963877446, time: 10.951663494110107

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–…â–ƒâ–â–‚â–â–„â–„â–‚â–â–â–ˆâ–ƒâ–ƒâ–‡â–…â–‚â–â–ƒâ–…
wandb:   test_error_force â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–‚â–â–ˆâ–â–‚â–ƒâ–ƒ
wandb:          test_loss â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–ˆâ–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: train_error_energy â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–ƒâ–â–ˆâ–ˆâ–ˆâ–‡â–†â–„â–„â–„
wandb:  train_error_force â–‚â–â–â–â–â–â–â–‚â–â–â–ˆâ–â–ƒâ–‚â–‚â–„â–…â–â–â–
wandb:         train_loss â–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–…â–â–ˆâ–†â–†â–‡â–‡â–„â–„â–„
wandb: valid_error_energy â–‚â–„â–‚â–‚â–ƒâ–â–ƒâ–„â–‚â–â–â–ˆâ–ƒâ–…â–ˆâ–‡â–„â–‚â–‚â–„
wandb:  valid_error_force â–‚â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–â–ˆâ–â–â–â–
wandb:         valid_loss â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–â–â–â–ˆâ–‚â–‚â–ƒâ–…â–‚â–â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 8257
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.05624
wandb:   test_error_force 11.6877
wandb:          test_loss 13.17331
wandb: train_error_energy 2.58435
wandb:  train_error_force 3.21477
wandb:         train_loss 0.87209
wandb: valid_error_energy 3.85735
wandb:  valid_error_force 3.25851
wandb:         valid_loss 1.4224
wandb: 
wandb: ğŸš€ View run al_71_93 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/rqzj94yy
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_140720-rqzj94yy/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1032962799072266, Uncertainty Bias: -0.13722985982894897
1.1444092e-05 0.040966034
1.8768399 4.847187
(48745, 22, 3)
Found uncertainty sample 0 after 1072 steps.
Found uncertainty sample 1 after 1080 steps.
Found uncertainty sample 2 after 1812 steps.
Found uncertainty sample 3 after 692 steps.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 713 steps.
Found uncertainty sample 8 after 3121 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 1880 steps.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 3407 steps.
Found uncertainty sample 15 after 207 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 897 steps.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 236 steps.
Found uncertainty sample 24 after 3236 steps.
Found uncertainty sample 25 after 2326 steps.
Found uncertainty sample 26 after 2740 steps.
Did not find any uncertainty samples for sample 27.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 1794 steps.
Found uncertainty sample 31 after 6 steps.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 946 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 1697 steps.
Found uncertainty sample 36 after 61 steps.
Found uncertainty sample 37 after 448 steps.
Found uncertainty sample 38 after 3880 steps.
Found uncertainty sample 39 after 741 steps.
Found uncertainty sample 40 after 1920 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 2801 steps.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 528 steps.
Did not find any uncertainty samples for sample 46.
Did not find any uncertainty samples for sample 47.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 2833 steps.
Found uncertainty sample 51 after 437 steps.
Found uncertainty sample 52 after 347 steps.
Found uncertainty sample 53 after 1693 steps.
Found uncertainty sample 54 after 316 steps.
Found uncertainty sample 55 after 3853 steps.
Found uncertainty sample 56 after 1870 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 2208 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 63 steps.
Found uncertainty sample 61 after 4 steps.
Found uncertainty sample 62 after 473 steps.
Found uncertainty sample 63 after 677 steps.
Found uncertainty sample 64 after 259 steps.
Found uncertainty sample 65 after 2515 steps.
Did not find any uncertainty samples for sample 66.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 387 steps.
Did not find any uncertainty samples for sample 70.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 566 steps.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 1445 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 164 steps.
Found uncertainty sample 84 after 1888 steps.
Did not find any uncertainty samples for sample 85.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 1452 steps.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 1205 steps.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 1377 steps.
Found uncertainty sample 94 after 2504 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 1180 steps.
Found uncertainty sample 97 after 1180 steps.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_144048-d0hpyh93
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_94
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/d0hpyh93
Training model 94. Added 51 samples to the dataset.
Epoch 0, Batch 100/260, Loss: 0.42067116498947144, Variance: 0.08715701103210449
Epoch 0, Batch 200/260, Loss: 0.5081652998924255, Variance: 0.08023001253604889

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8508567585756739, Training Loss Force: 3.344638920871749, time: 4.343926668167114
Validation Loss Energy: 1.4466243932576508, Validation Loss Force: 3.216195022823185, time: 0.24348139762878418
Test Loss Energy: 9.436502944624499, Test Loss Force: 11.859675168105206, time: 10.840285539627075

Epoch 1, Batch 100/260, Loss: 0.570437490940094, Variance: 0.08232397586107254
Epoch 1, Batch 200/260, Loss: 0.44918227195739746, Variance: 0.07771480083465576

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5844737934233202, Training Loss Force: 3.2253293693737315, time: 4.12709903717041
Validation Loss Energy: 1.6259122900330325, Validation Loss Force: 3.252073346035896, time: 0.24027585983276367
Test Loss Energy: 9.56100519626387, Test Loss Force: 11.85699524817198, time: 10.937867879867554

Epoch 2, Batch 100/260, Loss: 0.4817966818809509, Variance: 0.07957237958908081
Epoch 2, Batch 200/260, Loss: 0.6533309817314148, Variance: 0.07874658703804016

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.581186116669963, Training Loss Force: 3.237083008201038, time: 4.233654022216797
Validation Loss Energy: 1.3991169957635443, Validation Loss Force: 3.2837846388517145, time: 0.2530229091644287
Test Loss Energy: 9.523856726740057, Test Loss Force: 12.062609252552047, time: 12.113603591918945

Epoch 3, Batch 100/260, Loss: 0.291375994682312, Variance: 0.07804486900568008
Epoch 3, Batch 200/260, Loss: 0.3827398419380188, Variance: 0.07766605913639069

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5678535414200252, Training Loss Force: 3.2360825874159165, time: 4.199053525924683
Validation Loss Energy: 1.5409879032609397, Validation Loss Force: 3.2778953071448997, time: 0.2394266128540039
Test Loss Energy: 9.172989865394797, Test Loss Force: 11.778284736184824, time: 10.927753686904907

Epoch 4, Batch 100/260, Loss: 0.43553078174591064, Variance: 0.07765835523605347
Epoch 4, Batch 200/260, Loss: 0.47742724418640137, Variance: 0.07891148328781128

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5785019651530934, Training Loss Force: 3.242803147326579, time: 4.207955360412598
Validation Loss Energy: 1.5799507285507004, Validation Loss Force: 3.211367733874787, time: 0.24386239051818848
Test Loss Energy: 9.58781714486199, Test Loss Force: 12.08799136678671, time: 10.781240463256836

Epoch 5, Batch 100/260, Loss: 0.615433931350708, Variance: 0.07972531020641327
Epoch 5, Batch 200/260, Loss: 0.31648528575897217, Variance: 0.08053988218307495

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5820222819198069, Training Loss Force: 3.25140448474758, time: 4.261554002761841
Validation Loss Energy: 1.4205902284525782, Validation Loss Force: 3.213511636945609, time: 0.24020862579345703
Test Loss Energy: 9.492490627525486, Test Loss Force: 12.006416098490687, time: 10.877026319503784

Epoch 6, Batch 100/260, Loss: 0.33590131998062134, Variance: 0.07799325883388519
Epoch 6, Batch 200/260, Loss: 0.5685272216796875, Variance: 0.08140061050653458

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.5706740664412135, Training Loss Force: 3.2663932082311296, time: 4.226179361343384
Validation Loss Energy: 1.5947202393986033, Validation Loss Force: 3.2980922893571334, time: 0.2515573501586914
Test Loss Energy: 9.532133896149961, Test Loss Force: 11.966955241363198, time: 10.789400577545166

Epoch 7, Batch 100/260, Loss: 0.6899212002754211, Variance: 0.0762278288602829
Epoch 7, Batch 200/260, Loss: 0.4165721535682678, Variance: 0.07931336015462875

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5829119220214647, Training Loss Force: 3.247481070668308, time: 4.2842183113098145
Validation Loss Energy: 1.5957934824150606, Validation Loss Force: 3.3403258126637696, time: 0.27144885063171387
Test Loss Energy: 9.481540817481557, Test Loss Force: 11.915958625580409, time: 10.894157648086548

Epoch 8, Batch 100/260, Loss: 0.4596572518348694, Variance: 0.07932914793491364
Epoch 8, Batch 200/260, Loss: 0.424446702003479, Variance: 0.0770111158490181

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.5880315051179374, Training Loss Force: 3.2513168738592744, time: 4.2265379428863525
Validation Loss Energy: 1.7531797533315578, Validation Loss Force: 3.2195513148597543, time: 0.246687650680542
Test Loss Energy: 9.531656815263837, Test Loss Force: 11.811852028326946, time: 10.769761562347412

Epoch 9, Batch 100/260, Loss: 0.5843976736068726, Variance: 0.0778922513127327
Epoch 9, Batch 200/260, Loss: 0.3378037214279175, Variance: 0.0761166661977768

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.582476665075191, Training Loss Force: 3.2351311195861077, time: 4.1719043254852295
Validation Loss Energy: 1.3269429114902518, Validation Loss Force: 3.231784624135539, time: 0.24438929557800293
Test Loss Energy: 9.87039098744003, Test Loss Force: 12.15142318467751, time: 10.871182203292847

Epoch 10, Batch 100/260, Loss: 0.4216613173484802, Variance: 0.0786636471748352
Epoch 10, Batch 200/260, Loss: 0.2731286287307739, Variance: 0.07762792706489563

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.6041676178742792, Training Loss Force: 3.2419045171722596, time: 4.311146020889282
Validation Loss Energy: 1.351784460349735, Validation Loss Force: 3.339645068550839, time: 0.24107146263122559
Test Loss Energy: 9.7865574902725, Test Loss Force: 12.410228375983896, time: 10.801630020141602

Epoch 11, Batch 100/260, Loss: 0.4576115012168884, Variance: 0.07944883406162262
Epoch 11, Batch 200/260, Loss: 0.4116184711456299, Variance: 0.07432238757610321

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.5798608012609632, Training Loss Force: 3.2384558542661805, time: 4.239783525466919
Validation Loss Energy: 1.4329090426803996, Validation Loss Force: 3.2174214844789555, time: 0.24553871154785156
Test Loss Energy: 9.605681412458889, Test Loss Force: 12.270270567992723, time: 10.970038175582886

Epoch 12, Batch 100/260, Loss: 0.4895209074020386, Variance: 0.07637803256511688
Epoch 12, Batch 200/260, Loss: 0.5334587693214417, Variance: 0.07969145476818085

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.59171421813256, Training Loss Force: 3.2917503466415887, time: 4.295167922973633
Validation Loss Energy: 1.3931006535758925, Validation Loss Force: 3.212130642902565, time: 0.24332904815673828
Test Loss Energy: 9.807818140844496, Test Loss Force: 12.472645540706406, time: 10.87587308883667

Epoch 13, Batch 100/260, Loss: 0.7474161386489868, Variance: 0.07889536023139954
Epoch 13, Batch 200/260, Loss: 0.5143073797225952, Variance: 0.07596619427204132

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.5816602490798564, Training Loss Force: 3.230199766212361, time: 4.239211797714233
Validation Loss Energy: 1.6442968567319878, Validation Loss Force: 3.2048724748880737, time: 0.2409827709197998
Test Loss Energy: 9.683956714685808, Test Loss Force: 12.149417539560753, time: 10.999766826629639

Epoch 14, Batch 100/260, Loss: 0.6932718753814697, Variance: 0.07371729612350464
Epoch 14, Batch 200/260, Loss: 0.3455110788345337, Variance: 0.07736001908779144

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.581799624121441, Training Loss Force: 3.2311792732261613, time: 4.286636114120483
Validation Loss Energy: 1.4893425114265695, Validation Loss Force: 3.182391708847546, time: 0.2431483268737793
Test Loss Energy: 9.748499770527106, Test Loss Force: 11.845604007621413, time: 10.806224346160889

Epoch 15, Batch 100/260, Loss: 0.7496415376663208, Variance: 0.0807410478591919
Epoch 15, Batch 200/260, Loss: 0.5126886963844299, Variance: 0.07819461077451706

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5948191554563727, Training Loss Force: 3.2332434684505187, time: 4.249748468399048
Validation Loss Energy: 1.580630203449499, Validation Loss Force: 3.2205073225708807, time: 0.2509911060333252
Test Loss Energy: 9.992843582415016, Test Loss Force: 12.148482109976127, time: 11.008514642715454

Epoch 16, Batch 100/260, Loss: 0.5267572999000549, Variance: 0.07914437353610992
Epoch 16, Batch 200/260, Loss: 0.623532235622406, Variance: 0.07826997339725494

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.580233563113963, Training Loss Force: 3.2224926389631334, time: 4.21522331237793
Validation Loss Energy: 1.4560701965632243, Validation Loss Force: 3.34376899020108, time: 0.23556184768676758
Test Loss Energy: 9.336826803528213, Test Loss Force: 11.985892802222839, time: 10.987621307373047

Epoch 17, Batch 100/260, Loss: 0.2739967703819275, Variance: 0.07739962637424469
Epoch 17, Batch 200/260, Loss: 0.47544628381729126, Variance: 0.07669879496097565

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.5791058468743189, Training Loss Force: 3.2491005270946647, time: 4.230650186538696
Validation Loss Energy: 1.4479936971705338, Validation Loss Force: 3.229405181642155, time: 0.24933910369873047
Test Loss Energy: 9.649062524279223, Test Loss Force: 12.159107536540917, time: 10.937026500701904

Epoch 18, Batch 100/260, Loss: 0.63508141040802, Variance: 0.07170370221138
Epoch 18, Batch 200/260, Loss: 0.49203598499298096, Variance: 0.06916922330856323

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.5753019337589382, Training Loss Force: 3.2532030870660336, time: 4.211472272872925
Validation Loss Energy: 1.5322489631591136, Validation Loss Force: 3.2543439478507508, time: 0.2572321891784668
Test Loss Energy: 9.593221569853137, Test Loss Force: 11.979389035176297, time: 10.803711891174316

Epoch 19, Batch 100/260, Loss: 0.4452691674232483, Variance: 0.0794658288359642
Epoch 19, Batch 200/260, Loss: 0.4569265842437744, Variance: 0.07877128571271896

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5812603070319173, Training Loss Force: 3.221183397483105, time: 4.276407718658447
Validation Loss Energy: 1.4639713662704115, Validation Loss Force: 3.298631772163887, time: 0.24784493446350098
Test Loss Energy: 9.523174354227844, Test Loss Force: 12.224859394392844, time: 10.926714420318604

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–„â–„â–â–…â–„â–„â–„â–„â–‡â–†â–…â–†â–…â–†â–ˆâ–‚â–…â–…â–„
wandb:   test_error_force â–‚â–‚â–„â–â–„â–ƒâ–ƒâ–‚â–â–…â–‡â–†â–ˆâ–…â–‚â–…â–ƒâ–…â–ƒâ–†
wandb:          test_loss â–â–„â–†â–ƒâ–…â–„â–…â–‚â–‚â–‡â–†â–…â–ˆâ–†â–†â–…â–…â–„â–…â–„
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–‚â–â–â–â–
wandb:  train_error_force â–ˆâ–â–‚â–‚â–‚â–ƒâ–„â–‚â–ƒâ–‚â–‚â–‚â–…â–‚â–‚â–‚â–â–ƒâ–ƒâ–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–†â–‚â–…â–…â–ƒâ–…â–…â–ˆâ–â–â–ƒâ–‚â–†â–„â–…â–ƒâ–ƒâ–„â–ƒ
wandb:  valid_error_force â–‚â–„â–…â–…â–‚â–‚â–†â–ˆâ–ƒâ–ƒâ–ˆâ–ƒâ–‚â–‚â–â–ƒâ–ˆâ–ƒâ–„â–†
wandb:         valid_loss â–ƒâ–†â–ƒâ–…â–…â–ƒâ–†â–‡â–ˆâ–â–ƒâ–‚â–‚â–†â–„â–…â–„â–ƒâ–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 8302
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.52317
wandb:   test_error_force 12.22486
wandb:          test_loss 16.1271
wandb: train_error_energy 1.58126
wandb:  train_error_force 3.22118
wandb:         train_loss 0.42421
wandb: valid_error_energy 1.46397
wandb:  valid_error_force 3.29863
wandb:         valid_loss 0.3973
wandb: 
wandb: ğŸš€ View run al_71_94 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/d0hpyh93
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_144048-d0hpyh93/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.4245049953460693, Uncertainty Bias: -0.037291720509529114
1.04904175e-05 0.001865387
2.0802076 4.9046593
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 3458 steps.
Found uncertainty sample 2 after 1771 steps.
Found uncertainty sample 3 after 3109 steps.
Found uncertainty sample 4 after 2242 steps.
Found uncertainty sample 5 after 961 steps.
Found uncertainty sample 6 after 681 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 152 steps.
Found uncertainty sample 9 after 1436 steps.
Found uncertainty sample 10 after 3181 steps.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 122 steps.
Found uncertainty sample 15 after 2706 steps.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 676 steps.
Found uncertainty sample 21 after 2182 steps.
Did not find any uncertainty samples for sample 22.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 1889 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 233 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 177 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 443 steps.
Found uncertainty sample 31 after 2442 steps.
Found uncertainty sample 32 after 2223 steps.
Found uncertainty sample 33 after 1388 steps.
Found uncertainty sample 34 after 30 steps.
Found uncertainty sample 35 after 1946 steps.
Found uncertainty sample 36 after 529 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 809 steps.
Found uncertainty sample 39 after 554 steps.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 4 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 590 steps.
Found uncertainty sample 45 after 871 steps.
Found uncertainty sample 46 after 1611 steps.
Found uncertainty sample 47 after 262 steps.
Found uncertainty sample 48 after 3290 steps.
Found uncertainty sample 49 after 3865 steps.
Found uncertainty sample 50 after 464 steps.
Found uncertainty sample 51 after 208 steps.
Found uncertainty sample 52 after 219 steps.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 2610 steps.
Found uncertainty sample 55 after 1693 steps.
Found uncertainty sample 56 after 1447 steps.
Found uncertainty sample 57 after 874 steps.
Did not find any uncertainty samples for sample 58.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 173 steps.
Found uncertainty sample 63 after 29 steps.
Found uncertainty sample 64 after 2021 steps.
Found uncertainty sample 65 after 2040 steps.
Found uncertainty sample 66 after 3115 steps.
Found uncertainty sample 67 after 30 steps.
Found uncertainty sample 68 after 1054 steps.
Did not find any uncertainty samples for sample 69.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 466 steps.
Found uncertainty sample 72 after 1410 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 786 steps.
Found uncertainty sample 75 after 403 steps.
Found uncertainty sample 76 after 1984 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 96 steps.
Found uncertainty sample 79 after 5 steps.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Did not find any uncertainty samples for sample 82.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 2924 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 165 steps.
Found uncertainty sample 87 after 909 steps.
Found uncertainty sample 88 after 3627 steps.
Found uncertainty sample 89 after 876 steps.
Found uncertainty sample 90 after 1165 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 3512 steps.
Found uncertainty sample 93 after 519 steps.
Found uncertainty sample 94 after 125 steps.
Found uncertainty sample 95 after 1606 steps.
Found uncertainty sample 96 after 250 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 209 steps.
Found uncertainty sample 99 after 1484 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_151014-su1ox924
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_95
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/su1ox924
Training model 95. Added 66 samples to the dataset.
Epoch 0, Batch 100/262, Loss: 0.684039831161499, Variance: 0.09212122112512589
Epoch 0, Batch 200/262, Loss: 0.8232985734939575, Variance: 0.10394352674484253

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.52987309553341, Training Loss Force: 3.6431279500016696, time: 4.383636474609375
Validation Loss Energy: 1.8593457968809197, Validation Loss Force: 3.1760660792162, time: 0.2284226417541504
Test Loss Energy: 9.282436971102259, Test Loss Force: 11.234541464262522, time: 10.173237800598145

Epoch 1, Batch 100/262, Loss: 0.6826152801513672, Variance: 0.11091944575309753
Epoch 1, Batch 200/262, Loss: 1.210328459739685, Variance: 0.1074523776769638

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5844157440876523, Training Loss Force: 3.1997833033973384, time: 4.176772117614746
Validation Loss Energy: 3.742128946453273, Validation Loss Force: 3.260838939634489, time: 0.23894906044006348
Test Loss Energy: 10.493447866650909, Test Loss Force: 11.423663304644197, time: 10.304563045501709

Epoch 2, Batch 100/262, Loss: 1.1839933395385742, Variance: 0.1054917573928833
Epoch 2, Batch 200/262, Loss: 0.7243033647537231, Variance: 0.11259578168392181

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.591980961890897, Training Loss Force: 3.2066479500999723, time: 4.3385169506073
Validation Loss Energy: 1.6551994729247934, Validation Loss Force: 3.236598463867523, time: 0.2312910556793213
Test Loss Energy: 9.357274431923882, Test Loss Force: 11.425885449455995, time: 10.257318496704102

Epoch 3, Batch 100/262, Loss: 0.7863732576370239, Variance: 0.11288274824619293
Epoch 3, Batch 200/262, Loss: 0.9090582132339478, Variance: 0.11267808079719543

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6001122399332033, Training Loss Force: 3.207385529747507, time: 4.327544927597046
Validation Loss Energy: 2.191770836061803, Validation Loss Force: 3.19952823725049, time: 0.23287296295166016
Test Loss Energy: 9.468599326357674, Test Loss Force: 11.42328775622539, time: 10.380784034729004

Epoch 4, Batch 100/262, Loss: 0.8662499785423279, Variance: 0.11020208150148392
Epoch 4, Batch 200/262, Loss: 1.0260529518127441, Variance: 0.10996240377426147

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5886049640598543, Training Loss Force: 3.221610368237744, time: 4.345189094543457
Validation Loss Energy: 3.543534031508102, Validation Loss Force: 3.1636238617477055, time: 0.22559428215026855
Test Loss Energy: 10.737855532122383, Test Loss Force: 11.610748321565456, time: 10.23171854019165

Epoch 5, Batch 100/262, Loss: 0.9950342774391174, Variance: 0.10170237720012665
Epoch 5, Batch 200/262, Loss: 0.6463371515274048, Variance: 0.11016848683357239

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.595970676257524, Training Loss Force: 3.21401539818211, time: 4.337153196334839
Validation Loss Energy: 1.6991494636097282, Validation Loss Force: 3.194848889682647, time: 0.22347164154052734
Test Loss Energy: 9.450949170882982, Test Loss Force: 11.61238998306931, time: 10.308357000350952

Epoch 6, Batch 100/262, Loss: 0.626690149307251, Variance: 0.10884391516447067
Epoch 6, Batch 200/262, Loss: 1.0672928094863892, Variance: 0.11286576092243195

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.566602572763497, Training Loss Force: 3.215191122718251, time: 4.334660053253174
Validation Loss Energy: 2.594749262137343, Validation Loss Force: 3.220590200026995, time: 0.23336529731750488
Test Loss Energy: 9.780323747454126, Test Loss Force: 11.734591854917598, time: 11.784008502960205

Epoch 7, Batch 100/262, Loss: 0.7561209201812744, Variance: 0.11231189966201782
Epoch 7, Batch 200/262, Loss: 1.0726943016052246, Variance: 0.1065634936094284

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5873065352261904, Training Loss Force: 3.2217841020143845, time: 4.846311330795288
Validation Loss Energy: 3.5658958922247037, Validation Loss Force: 3.2287371296705425, time: 0.27678632736206055
Test Loss Energy: 10.901576796549005, Test Loss Force: 11.66264371862255, time: 12.170042037963867

Epoch 8, Batch 100/262, Loss: 1.206345796585083, Variance: 0.11153846979141235
Epoch 8, Batch 200/262, Loss: 0.593059778213501, Variance: 0.11117713898420334

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.6138611311979014, Training Loss Force: 3.21713662384057, time: 4.437109470367432
Validation Loss Energy: 1.350402263893303, Validation Loss Force: 3.2608093246380805, time: 0.2427351474761963
Test Loss Energy: 9.596917019641554, Test Loss Force: 11.736668059340053, time: 10.849782466888428

Epoch 9, Batch 100/262, Loss: 0.568945586681366, Variance: 0.10700894892215729
Epoch 9, Batch 200/262, Loss: 0.8908857107162476, Variance: 0.10983386635780334

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5796203787557697, Training Loss Force: 3.213154124124248, time: 4.2760679721832275
Validation Loss Energy: 2.18418574520726, Validation Loss Force: 3.2273889570596084, time: 0.25473809242248535
Test Loss Energy: 9.646388018353923, Test Loss Force: 11.796334054665868, time: 11.063430547714233

Epoch 10, Batch 100/262, Loss: 0.9267210960388184, Variance: 0.11800754070281982
Epoch 10, Batch 200/262, Loss: 1.2000689506530762, Variance: 0.1007033959031105

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5909042953788157, Training Loss Force: 3.2553450260067067, time: 4.31232476234436
Validation Loss Energy: 3.4581841907988844, Validation Loss Force: 3.234627485655429, time: 0.24735140800476074
Test Loss Energy: 10.42797768177116, Test Loss Force: 11.520689638603276, time: 10.82821774482727

Epoch 11, Batch 100/262, Loss: 1.340646505355835, Variance: 0.10583870857954025
Epoch 11, Batch 200/262, Loss: 0.8441238403320312, Variance: 0.11204476654529572

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5993737385883833, Training Loss Force: 3.2138399025995934, time: 4.307433605194092
Validation Loss Energy: 1.6727579593592707, Validation Loss Force: 3.2600111893514168, time: 0.24407505989074707
Test Loss Energy: 9.71765484302117, Test Loss Force: 11.99445460575482, time: 10.991577386856079

Epoch 12, Batch 100/262, Loss: 0.6574177742004395, Variance: 0.10976690798997879
Epoch 12, Batch 200/262, Loss: 0.8513880968093872, Variance: 0.11292938888072968

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.571537299659793, Training Loss Force: 3.224475280462185, time: 4.200435161590576
Validation Loss Energy: 2.1734983832778227, Validation Loss Force: 3.2063286420347827, time: 0.24219274520874023
Test Loss Energy: 9.500043182570645, Test Loss Force: 11.62426156876573, time: 10.903407335281372

Epoch 13, Batch 100/262, Loss: 0.6988904476165771, Variance: 0.11259821057319641
Epoch 13, Batch 200/262, Loss: 1.1870126724243164, Variance: 0.10401232540607452

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.58383287870219, Training Loss Force: 3.221203287394759, time: 4.405196666717529
Validation Loss Energy: 3.7474139986318695, Validation Loss Force: 3.384267475522647, time: 0.2520458698272705
Test Loss Energy: 10.813304297858647, Test Loss Force: 11.429142919526631, time: 11.009597539901733

Epoch 14, Batch 100/262, Loss: 1.219987154006958, Variance: 0.10853438079357147
Epoch 14, Batch 200/262, Loss: 0.7685889005661011, Variance: 0.11052818596363068

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.583062542468561, Training Loss Force: 3.217024784437557, time: 4.24656081199646
Validation Loss Energy: 1.5447086156217344, Validation Loss Force: 3.256736216086941, time: 0.24074196815490723
Test Loss Energy: 9.540477973557891, Test Loss Force: 11.758551190253042, time: 10.932331800460815

Epoch 15, Batch 100/262, Loss: 0.6820251941680908, Variance: 0.11285687983036041
Epoch 15, Batch 200/262, Loss: 0.8799362182617188, Variance: 0.1109100878238678

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.594757291739724, Training Loss Force: 3.203381281544757, time: 4.286127805709839
Validation Loss Energy: 2.1004987531460024, Validation Loss Force: 3.2673486721156877, time: 0.24235773086547852
Test Loss Energy: 9.527952206895261, Test Loss Force: 11.589525988739473, time: 12.35411286354065

Epoch 16, Batch 100/262, Loss: 0.9012177586555481, Variance: 0.11431294679641724
Epoch 16, Batch 200/262, Loss: 1.2264599800109863, Variance: 0.1082911491394043

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.588332044776819, Training Loss Force: 3.2187509895965443, time: 4.248741626739502
Validation Loss Energy: 3.5476755885547, Validation Loss Force: 3.1861207045005693, time: 0.23945403099060059
Test Loss Energy: 11.135450644573421, Test Loss Force: 11.879860431625003, time: 10.887102365493774

Epoch 17, Batch 100/262, Loss: 0.8785853385925293, Variance: 0.10814215242862701
Epoch 17, Batch 200/262, Loss: 0.5868797898292542, Variance: 0.11254509538412094

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.578217353215513, Training Loss Force: 3.2063358240496096, time: 4.138661623001099
Validation Loss Energy: 1.656921930103118, Validation Loss Force: 3.2188044380104834, time: 0.2420368194580078
Test Loss Energy: 9.547717094383692, Test Loss Force: 11.708350228453197, time: 11.04118275642395

Epoch 18, Batch 100/262, Loss: 0.578417956829071, Variance: 0.11187434196472168
Epoch 18, Batch 200/262, Loss: 0.8271343111991882, Variance: 0.11648014187812805

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.6014712517722036, Training Loss Force: 3.2117713351299204, time: 4.2832560539245605
Validation Loss Energy: 2.312547248820473, Validation Loss Force: 3.19530309020651, time: 0.24759602546691895
Test Loss Energy: 9.65177351530186, Test Loss Force: 11.76495315902826, time: 10.923984050750732

Epoch 19, Batch 100/262, Loss: 0.8884298205375671, Variance: 0.10437096655368805
Epoch 19, Batch 200/262, Loss: 0.9068973660469055, Variance: 0.1107681468129158

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5809320415306765, Training Loss Force: 3.225591253177817, time: 4.199293851852417
Validation Loss Energy: 3.6748759616395574, Validation Loss Force: 3.262024563768896, time: 0.2578389644622803
Test Loss Energy: 11.114509644303954, Test Loss Force: 11.590225892658054, time: 11.027996301651001

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–†â–â–‚â–†â–‚â–ƒâ–‡â–‚â–‚â–…â–ƒâ–‚â–‡â–‚â–‚â–ˆâ–‚â–‚â–ˆ
wandb:   test_error_force â–â–ƒâ–ƒâ–ƒâ–„â–„â–†â–…â–†â–†â–„â–ˆâ–…â–ƒâ–†â–„â–‡â–…â–†â–„
wandb:          test_loss â–‚â–…â–‚â–â–…â–‚â–‚â–†â–‚â–ƒâ–„â–ƒâ–ƒâ–…â–ƒâ–â–ˆâ–‚â–ƒâ–†
wandb: train_error_energy â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–ˆâ–‚â–ƒâ–‡â–‚â–…â–‡â–â–ƒâ–‡â–‚â–ƒâ–ˆâ–‚â–ƒâ–‡â–‚â–„â–ˆ
wandb:  valid_error_force â–â–„â–ƒâ–‚â–â–‚â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–‚â–ˆâ–„â–„â–‚â–ƒâ–‚â–„
wandb:         valid_loss â–‚â–ˆâ–â–‚â–‡â–â–„â–‡â–â–ƒâ–‡â–‚â–‚â–ˆâ–â–ƒâ–‡â–â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 8361
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.11451
wandb:   test_error_force 11.59023
wandb:          test_loss 13.18893
wandb: train_error_energy 2.58093
wandb:  train_error_force 3.22559
wandb:         train_loss 0.87359
wandb: valid_error_energy 3.67488
wandb:  valid_error_force 3.26202
wandb:         valid_loss 1.33547
wandb: 
wandb: ğŸš€ View run al_71_95 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/su1ox924
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_151014-su1ox924/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.1401169300079346, Uncertainty Bias: -0.13318191468715668
2.2888184e-05 0.020843506
1.9232969 4.7569485
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 3692 steps.
Found uncertainty sample 2 after 817 steps.
Found uncertainty sample 3 after 1414 steps.
Found uncertainty sample 4 after 1577 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 3478 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 1469 steps.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 2050 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 3081 steps.
Found uncertainty sample 16 after 2478 steps.
Found uncertainty sample 17 after 179 steps.
Found uncertainty sample 18 after 2561 steps.
Found uncertainty sample 19 after 939 steps.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 4 steps.
Found uncertainty sample 24 after 3643 steps.
Found uncertainty sample 25 after 491 steps.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 194 steps.
Found uncertainty sample 29 after 3984 steps.
Found uncertainty sample 30 after 169 steps.
Found uncertainty sample 31 after 673 steps.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 602 steps.
Found uncertainty sample 34 after 1241 steps.
Did not find any uncertainty samples for sample 35.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 811 steps.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 2613 steps.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 715 steps.
Found uncertainty sample 45 after 2514 steps.
Did not find any uncertainty samples for sample 46.
Did not find any uncertainty samples for sample 47.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 973 steps.
Found uncertainty sample 51 after 1697 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 3649 steps.
Found uncertainty sample 55 after 2288 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 1281 steps.
Found uncertainty sample 61 after 445 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 74 steps.
Found uncertainty sample 67 after 972 steps.
Found uncertainty sample 68 after 797 steps.
Found uncertainty sample 69 after 3253 steps.
Found uncertainty sample 70 after 510 steps.
Found uncertainty sample 71 after 123 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 1549 steps.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 3875 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 3724 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 3759 steps.
Found uncertainty sample 82 after 824 steps.
Found uncertainty sample 83 after 283 steps.
Found uncertainty sample 84 after 384 steps.
Found uncertainty sample 85 after 2758 steps.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 1836 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 934 steps.
Found uncertainty sample 91 after 2960 steps.
Found uncertainty sample 92 after 101 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 2244 steps.
Found uncertainty sample 95 after 17 steps.
Found uncertainty sample 96 after 1159 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 1063 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_154443-iz7939bh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_96
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/iz7939bh
Training model 96. Added 53 samples to the dataset.
Epoch 0, Batch 100/263, Loss: 1.154714822769165, Variance: 0.11299017071723938
Epoch 0, Batch 200/263, Loss: 0.6000316143035889, Variance: 0.10847282409667969

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.770939402687592, Training Loss Force: 3.3469603621230553, time: 4.308923244476318
Validation Loss Energy: 2.3748129346375273, Validation Loss Force: 3.1805803844130813, time: 0.2380821704864502
Test Loss Energy: 9.831777709341354, Test Loss Force: 11.795738499804424, time: 10.148854970932007

Epoch 1, Batch 100/263, Loss: 0.9707788825035095, Variance: 0.11071610450744629
Epoch 1, Batch 200/263, Loss: 1.1360814571380615, Variance: 0.10820531100034714

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.584959132746995, Training Loss Force: 3.1943661251946094, time: 4.341867446899414
Validation Loss Energy: 2.2630263433073994, Validation Loss Force: 3.248251809085825, time: 0.25972938537597656
Test Loss Energy: 9.780904263404665, Test Loss Force: 11.729218141947287, time: 10.34480595588684

Epoch 2, Batch 100/263, Loss: 0.4640323519706726, Variance: 0.10806480050086975
Epoch 2, Batch 200/263, Loss: 0.7260673642158508, Variance: 0.11089159548282623

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5993167910173267, Training Loss Force: 3.2054355723080756, time: 4.375046014785767
Validation Loss Energy: 3.640644321294812, Validation Loss Force: 3.2149407275241737, time: 0.2489924430847168
Test Loss Energy: 11.01822972063817, Test Loss Force: 11.823185780600424, time: 10.175994873046875

Epoch 3, Batch 100/263, Loss: 1.3360364437103271, Variance: 0.10518351197242737
Epoch 3, Batch 200/263, Loss: 0.575408935546875, Variance: 0.10886263847351074

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5687700647772465, Training Loss Force: 3.2171042534193117, time: 4.302870988845825
Validation Loss Energy: 2.555621349942791, Validation Loss Force: 3.168149612138688, time: 0.23047494888305664
Test Loss Energy: 10.18723516443134, Test Loss Force: 11.640226545491128, time: 10.308185815811157

Epoch 4, Batch 100/263, Loss: 0.7826512455940247, Variance: 0.10832000523805618
Epoch 4, Batch 200/263, Loss: 1.138708233833313, Variance: 0.1136600524187088

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.59072076801654, Training Loss Force: 3.2126969748670735, time: 4.2960593700408936
Validation Loss Energy: 2.0111862477820255, Validation Loss Force: 3.1741867516245694, time: 0.23573732376098633
Test Loss Energy: 9.55054801430884, Test Loss Force: 11.627339252736848, time: 11.204880714416504

Epoch 5, Batch 100/263, Loss: 0.6569629311561584, Variance: 0.11191952228546143
Epoch 5, Batch 200/263, Loss: 0.863844096660614, Variance: 0.11155933141708374

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6137936124167354, Training Loss Force: 3.2090871761244917, time: 4.769493579864502
Validation Loss Energy: 3.010471399704964, Validation Loss Force: 3.1843764925466873, time: 0.28118419647216797
Test Loss Energy: 9.647249885513896, Test Loss Force: 11.462735509681668, time: 13.754333019256592

Epoch 6, Batch 100/263, Loss: 1.3034160137176514, Variance: 0.11244924366474152
Epoch 6, Batch 200/263, Loss: 0.5292118191719055, Variance: 0.10978434979915619

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.6103565287328268, Training Loss Force: 3.212191012148624, time: 4.303900957107544
Validation Loss Energy: 2.413849819685224, Validation Loss Force: 3.3292733178688394, time: 0.27625298500061035
Test Loss Energy: 9.754587162592061, Test Loss Force: 12.089470771265795, time: 10.813230752944946

Epoch 7, Batch 100/263, Loss: 0.8896159529685974, Variance: 0.11490952968597412
Epoch 7, Batch 200/263, Loss: 1.0409742593765259, Variance: 0.10845344513654709

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.576710782641377, Training Loss Force: 3.227148784933499, time: 4.251974821090698
Validation Loss Energy: 2.099044679089265, Validation Loss Force: 3.2317579483718575, time: 0.24112868309020996
Test Loss Energy: 10.146604751021654, Test Loss Force: 12.004753699611083, time: 10.984436988830566

Epoch 8, Batch 100/263, Loss: 0.45457351207733154, Variance: 0.1082490086555481
Epoch 8, Batch 200/263, Loss: 0.7827964425086975, Variance: 0.10904078185558319

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5722769425386423, Training Loss Force: 3.2173592332976195, time: 4.353082180023193
Validation Loss Energy: 3.538676014217566, Validation Loss Force: 3.3007522059067282, time: 0.25159502029418945
Test Loss Energy: 10.627478492444714, Test Loss Force: 11.629382133730681, time: 10.710530519485474

Epoch 9, Batch 100/263, Loss: 1.2101222276687622, Variance: 0.1090654507279396
Epoch 9, Batch 200/263, Loss: 0.8994817733764648, Variance: 0.11257140338420868

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5838770402829563, Training Loss Force: 3.2102267265413387, time: 4.355376243591309
Validation Loss Energy: 2.7101037257265004, Validation Loss Force: 3.1877959335530424, time: 0.2450721263885498
Test Loss Energy: 10.317289324979733, Test Loss Force: 11.597154554404078, time: 10.973222732543945

Epoch 10, Batch 100/263, Loss: 0.727523922920227, Variance: 0.10732565820217133
Epoch 10, Batch 200/263, Loss: 1.2989667654037476, Variance: 0.11447443068027496

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.583019045264652, Training Loss Force: 3.211612484688125, time: 4.255454063415527
Validation Loss Energy: 1.706518832595696, Validation Loss Force: 3.1991302829528876, time: 0.2699863910675049
Test Loss Energy: 9.569647752918366, Test Loss Force: 11.756427957174319, time: 10.799812078475952

Epoch 11, Batch 100/263, Loss: 0.7089974880218506, Variance: 0.11429350078105927
Epoch 11, Batch 200/263, Loss: 0.7312155961990356, Variance: 0.11055420339107513

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.56653228769644, Training Loss Force: 3.2197910292864704, time: 4.276427268981934
Validation Loss Energy: 3.1925927168284476, Validation Loss Force: 3.198632216607277, time: 0.2471923828125
Test Loss Energy: 9.635774002708509, Test Loss Force: 11.445776598510815, time: 10.936672925949097

Epoch 12, Batch 100/263, Loss: 1.4326146841049194, Variance: 0.11300294101238251
Epoch 12, Batch 200/263, Loss: 0.4070351719856262, Variance: 0.110024593770504

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5671774050846814, Training Loss Force: 3.206542008520985, time: 4.346935510635376
Validation Loss Energy: 2.4647942490482135, Validation Loss Force: 3.2613558437898456, time: 0.2416234016418457
Test Loss Energy: 9.718738190203881, Test Loss Force: 11.796536158975492, time: 10.681363821029663

Epoch 13, Batch 100/263, Loss: 0.9649942517280579, Variance: 0.11515800654888153
Epoch 13, Batch 200/263, Loss: 1.1862224340438843, Variance: 0.11119552701711655

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.613491490178296, Training Loss Force: 3.2163302165221324, time: 4.290644407272339
Validation Loss Energy: 2.1454771190144903, Validation Loss Force: 3.2228872549363863, time: 0.24327564239501953
Test Loss Energy: 10.172276046025875, Test Loss Force: 11.588639294552634, time: 10.97917628288269

Epoch 14, Batch 100/263, Loss: 0.7253178954124451, Variance: 0.11051517724990845
Epoch 14, Batch 200/263, Loss: 0.5120415687561035, Variance: 0.1080857664346695

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5749007043074803, Training Loss Force: 3.2031620923886788, time: 4.2701427936553955
Validation Loss Energy: 3.790163179146645, Validation Loss Force: 3.235343411493416, time: 0.2416989803314209
Test Loss Energy: 10.976385384993305, Test Loss Force: 11.66217505952653, time: 10.740252256393433

Epoch 15, Batch 100/263, Loss: 1.0046552419662476, Variance: 0.10720738768577576
Epoch 15, Batch 200/263, Loss: 0.8030746579170227, Variance: 0.11538763344287872

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.574029266591972, Training Loss Force: 3.2109111683167324, time: 4.284605503082275
Validation Loss Energy: 2.4842231117746545, Validation Loss Force: 3.242285968260469, time: 0.24233603477478027
Test Loss Energy: 10.360130598189004, Test Loss Force: 11.580191334410383, time: 10.924038887023926

Epoch 16, Batch 100/263, Loss: 0.9370639324188232, Variance: 0.10886852443218231
Epoch 16, Batch 200/263, Loss: 1.2048888206481934, Variance: 0.11314745247364044

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5520703889397365, Training Loss Force: 3.2044548324047173, time: 4.271194696426392
Validation Loss Energy: 1.6222066301065023, Validation Loss Force: 3.216865613223061, time: 0.2466721534729004
Test Loss Energy: 9.4221901752935, Test Loss Force: 11.514975094948008, time: 10.816179275512695

Epoch 17, Batch 100/263, Loss: 0.4678996801376343, Variance: 0.10980045795440674
Epoch 17, Batch 200/263, Loss: 0.8079788088798523, Variance: 0.11246900260448456

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5658317210342823, Training Loss Force: 3.2129051916847846, time: 4.835824728012085
Validation Loss Energy: 3.5643916730763037, Validation Loss Force: 3.2182116482101946, time: 0.27996826171875
Test Loss Energy: 10.354627877010902, Test Loss Force: 11.856038652194616, time: 12.360340595245361

Epoch 18, Batch 100/263, Loss: 1.418175458908081, Variance: 0.11355966329574585
Epoch 18, Batch 200/263, Loss: 0.6544409990310669, Variance: 0.11156745254993439

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.604704661309043, Training Loss Force: 3.2101857672715983, time: 4.530474424362183
Validation Loss Energy: 2.1248058515214305, Validation Loss Force: 3.2615683146888896, time: 0.273364782333374
Test Loss Energy: 9.55129398625868, Test Loss Force: 11.51187112902283, time: 12.286936521530151

Epoch 19, Batch 100/263, Loss: 0.8453912734985352, Variance: 0.11286304891109467
Epoch 19, Batch 200/263, Loss: 0.9442089200019836, Variance: 0.10938752442598343

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5867613770215496, Training Loss Force: 3.2033896128433756, time: 4.987864255905151
Validation Loss Energy: 1.968081575975776, Validation Loss Force: 3.2292284714079464, time: 0.2721562385559082
Test Loss Energy: 9.936638098778326, Test Loss Force: 11.604217964791795, time: 12.264172792434692

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–ƒâ–ˆâ–„â–‚â–‚â–‚â–„â–†â–…â–‚â–‚â–‚â–„â–ˆâ–…â–â–…â–‚â–ƒ
wandb:   test_error_force â–…â–„â–…â–ƒâ–ƒâ–â–ˆâ–‡â–ƒâ–ƒâ–„â–â–…â–ƒâ–ƒâ–‚â–‚â–…â–‚â–ƒ
wandb:          test_loss â–‚â–ƒâ–†â–„â–â–â–†â–†â–‡â–†â–„â–ƒâ–ƒâ–„â–ˆâ–…â–‚â–†â–â–…
wandb: train_error_energy â–ˆâ–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–ƒâ–‚â–‚â–â–â–ƒâ–‚
wandb:  train_error_force â–ˆâ–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–‚â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–ƒâ–ˆâ–„â–‚â–…â–„â–ƒâ–‡â–…â–â–†â–„â–ƒâ–ˆâ–„â–â–‡â–ƒâ–‚
wandb:  valid_error_force â–‚â–„â–ƒâ–â–â–‚â–ˆâ–„â–‡â–‚â–‚â–‚â–…â–ƒâ–„â–„â–ƒâ–ƒâ–…â–„
wandb:         valid_loss â–ƒâ–ƒâ–‡â–ƒâ–‚â–…â–ƒâ–‚â–‡â–„â–â–†â–ƒâ–‚â–ˆâ–ƒâ–â–‡â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 8408
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.93664
wandb:   test_error_force 11.60422
wandb:          test_loss 12.71613
wandb: train_error_energy 2.58676
wandb:  train_error_force 3.20339
wandb:         train_loss 0.86626
wandb: valid_error_energy 1.96808
wandb:  valid_error_force 3.22923
wandb:         valid_loss 0.64798
wandb: 
wandb: ğŸš€ View run al_71_96 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/iz7939bh
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_154443-iz7939bh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2711493968963623, Uncertainty Bias: -0.1501038670539856
1.9073486e-05 0.0023345947
1.8627903 4.7203465
(48745, 22, 3)
Found uncertainty sample 0 after 689 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 1801 steps.
Found uncertainty sample 3 after 847 steps.
Found uncertainty sample 4 after 2222 steps.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 974 steps.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 435 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1285 steps.
Found uncertainty sample 16 after 1251 steps.
Found uncertainty sample 17 after 1000 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 2496 steps.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 2510 steps.
Found uncertainty sample 23 after 880 steps.
Found uncertainty sample 24 after 2041 steps.
Found uncertainty sample 25 after 372 steps.
Found uncertainty sample 26 after 1011 steps.
Found uncertainty sample 27 after 871 steps.
Found uncertainty sample 28 after 1783 steps.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 1239 steps.
Found uncertainty sample 32 after 3349 steps.
Found uncertainty sample 33 after 490 steps.
Found uncertainty sample 34 after 505 steps.
Did not find any uncertainty samples for sample 35.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 1295 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 3035 steps.
Found uncertainty sample 40 after 2680 steps.
Found uncertainty sample 41 after 1496 steps.
Found uncertainty sample 42 after 152 steps.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Did not find any uncertainty samples for sample 45.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 44 steps.
Found uncertainty sample 48 after 1395 steps.
Found uncertainty sample 49 after 417 steps.
Found uncertainty sample 50 after 2176 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 1102 steps.
Found uncertainty sample 54 after 124 steps.
Found uncertainty sample 55 after 54 steps.
Found uncertainty sample 56 after 3997 steps.
Found uncertainty sample 57 after 879 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 2683 steps.
Found uncertainty sample 60 after 513 steps.
Found uncertainty sample 61 after 2653 steps.
Found uncertainty sample 62 after 3427 steps.
Found uncertainty sample 63 after 690 steps.
Did not find any uncertainty samples for sample 64.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 3572 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 1474 steps.
Did not find any uncertainty samples for sample 70.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 2353 steps.
Found uncertainty sample 73 after 187 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 3456 steps.
Found uncertainty sample 76 after 1271 steps.
Found uncertainty sample 77 after 1087 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 352 steps.
Found uncertainty sample 80 after 2219 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 2878 steps.
Found uncertainty sample 83 after 613 steps.
Found uncertainty sample 84 after 1396 steps.
Found uncertainty sample 85 after 1825 steps.
Found uncertainty sample 86 after 486 steps.
Found uncertainty sample 87 after 759 steps.
Found uncertainty sample 88 after 694 steps.
Found uncertainty sample 89 after 690 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 2532 steps.
Found uncertainty sample 92 after 1271 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 1330 steps.
Found uncertainty sample 95 after 2155 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_161635-xrt047iu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_97
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/xrt047iu
Training model 97. Added 62 samples to the dataset.
Epoch 0, Batch 100/265, Loss: 1.7138910293579102, Variance: 0.08694680035114288
Epoch 0, Batch 200/265, Loss: 0.7030515074729919, Variance: 0.09815168380737305

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7132472065583504, Training Loss Force: 3.507858670801774, time: 4.642436504364014
Validation Loss Energy: 2.234603918310764, Validation Loss Force: 3.1526501027627747, time: 0.3146827220916748
Test Loss Energy: 9.649332436589914, Test Loss Force: 11.667247434691399, time: 11.80608606338501

Epoch 1, Batch 100/265, Loss: 0.8597922921180725, Variance: 0.1094023808836937
Epoch 1, Batch 200/265, Loss: 1.0630124807357788, Variance: 0.10902372002601624

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.512448609859154, Training Loss Force: 3.1773580543360067, time: 4.792445182800293
Validation Loss Energy: 3.2242190390981804, Validation Loss Force: 3.329722842340638, time: 0.2588672637939453
Test Loss Energy: 9.87239400715932, Test Loss Force: 11.632501567583754, time: 12.19789743423462

Epoch 2, Batch 100/265, Loss: 1.1110944747924805, Variance: 0.1136070117354393
Epoch 2, Batch 200/265, Loss: 0.39133942127227783, Variance: 0.10900961607694626

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5465811132786973, Training Loss Force: 3.1820319208818164, time: 4.511965274810791
Validation Loss Energy: 1.8567652618818482, Validation Loss Force: 3.2610269169274817, time: 0.2671482563018799
Test Loss Energy: 9.136147233691625, Test Loss Force: 11.245847549603164, time: 12.004836320877075

Epoch 3, Batch 100/265, Loss: 0.6609708666801453, Variance: 0.1156303733587265
Epoch 3, Batch 200/265, Loss: 0.8014848232269287, Variance: 0.11013984680175781

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5485117543448115, Training Loss Force: 3.187866828888323, time: 4.606257915496826
Validation Loss Energy: 2.2519969170359877, Validation Loss Force: 3.241854435455661, time: 0.28847718238830566
Test Loss Energy: 10.031297028367584, Test Loss Force: 11.751734986917976, time: 12.030418634414673

Epoch 4, Batch 100/265, Loss: 0.8772997856140137, Variance: 0.110695980489254
Epoch 4, Batch 200/265, Loss: 1.385654091835022, Variance: 0.1130654364824295

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5856475881149743, Training Loss Force: 3.199503084039609, time: 4.637151002883911
Validation Loss Energy: 3.424091317865881, Validation Loss Force: 3.25356531838127, time: 0.27242493629455566
Test Loss Energy: 10.849918823091823, Test Loss Force: 11.897359358609595, time: 12.027023077011108

Epoch 5, Batch 100/265, Loss: 1.2642221450805664, Variance: 0.1096615195274353
Epoch 5, Batch 200/265, Loss: 0.6141664981842041, Variance: 0.1102607473731041

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5810488140077807, Training Loss Force: 3.193688612859394, time: 4.82568097114563
Validation Loss Energy: 2.02034353867522, Validation Loss Force: 3.2478601785919663, time: 0.26941585540771484
Test Loss Energy: 9.741904259129216, Test Loss Force: 11.214579923778219, time: 12.12459659576416

Epoch 6, Batch 100/265, Loss: 0.45056188106536865, Variance: 0.10783051699399948
Epoch 6, Batch 200/265, Loss: 0.6982943415641785, Variance: 0.10935191810131073

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.558087914470368, Training Loss Force: 3.1963865318899813, time: 4.593423366546631
Validation Loss Energy: 2.321651700325339, Validation Loss Force: 3.277662395218259, time: 0.28525757789611816
Test Loss Energy: 9.507327802513867, Test Loss Force: 11.530254940861724, time: 12.063650131225586

Epoch 7, Batch 100/265, Loss: 0.7360669374465942, Variance: 0.11132782697677612
Epoch 7, Batch 200/265, Loss: 1.4155303239822388, Variance: 0.10440495610237122

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5649624777432556, Training Loss Force: 3.2028493257152184, time: 4.711129426956177
Validation Loss Energy: 3.542815032835901, Validation Loss Force: 3.2522216436870837, time: 0.27279019355773926
Test Loss Energy: 10.096541792796772, Test Loss Force: 11.807819096051412, time: 12.06958270072937

Epoch 8, Batch 100/265, Loss: 1.1685153245925903, Variance: 0.11589890718460083
Epoch 8, Batch 200/265, Loss: 0.5937118530273438, Variance: 0.11245085299015045

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.595597013991546, Training Loss Force: 3.211142974999931, time: 4.64638876914978
Validation Loss Energy: 1.5266615485472623, Validation Loss Force: 3.194169125313958, time: 0.25977134704589844
Test Loss Energy: 9.509335961110619, Test Loss Force: 11.942248409202294, time: 12.155422687530518

Epoch 9, Batch 100/265, Loss: 0.8707724809646606, Variance: 0.10838073492050171
Epoch 9, Batch 200/265, Loss: 1.0784651041030884, Variance: 0.11252449452877045

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5631518498851005, Training Loss Force: 3.201333331150173, time: 4.570942401885986
Validation Loss Energy: 2.688278987523705, Validation Loss Force: 3.1908485048745665, time: 0.26927995681762695
Test Loss Energy: 10.354768011681148, Test Loss Force: 11.650782065602165, time: 11.968155145645142

Epoch 10, Batch 100/265, Loss: 0.6307410001754761, Variance: 0.110398069024086
Epoch 10, Batch 200/265, Loss: 1.41102933883667, Variance: 0.11652533710002899

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.602749315431063, Training Loss Force: 3.189147630218309, time: 4.545104503631592
Validation Loss Energy: 3.769795320631826, Validation Loss Force: 3.268296352000757, time: 0.27660417556762695
Test Loss Energy: 10.998319480327153, Test Loss Force: 11.894397159484527, time: 11.536472082138062

Epoch 11, Batch 100/265, Loss: 1.1126145124435425, Variance: 0.10449343174695969
Epoch 11, Batch 200/265, Loss: 0.5452374219894409, Variance: 0.1112837940454483

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.6064759890230116, Training Loss Force: 3.2163894726461058, time: 4.88016939163208
Validation Loss Energy: 2.249263296999091, Validation Loss Force: 3.2751740229786863, time: 0.27596139907836914
Test Loss Energy: 9.844128936036174, Test Loss Force: 11.480710561590602, time: 11.78188443183899

Epoch 12, Batch 100/265, Loss: 0.6009137630462646, Variance: 0.10731358826160431
Epoch 12, Batch 200/265, Loss: 0.8629264235496521, Variance: 0.10876436531543732

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.540162071419852, Training Loss Force: 3.2161014866571986, time: 4.282633543014526
Validation Loss Energy: 2.7943967786691446, Validation Loss Force: 3.1853621152542226, time: 0.2278130054473877
Test Loss Energy: 9.874287524954324, Test Loss Force: 11.473667935210917, time: 10.461490631103516

Epoch 13, Batch 100/265, Loss: 0.9317458271980286, Variance: 0.11027968674898148
Epoch 13, Batch 200/265, Loss: 0.9601571559906006, Variance: 0.10306002199649811

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.571942719792801, Training Loss Force: 3.2299617948484487, time: 4.238184452056885
Validation Loss Energy: 3.226007784881247, Validation Loss Force: 3.244571667043418, time: 0.23267102241516113
Test Loss Energy: 9.825299566733701, Test Loss Force: 11.409302096030409, time: 10.234793424606323

Epoch 14, Batch 100/265, Loss: 1.2894437313079834, Variance: 0.11345703899860382
Epoch 14, Batch 200/265, Loss: 0.4080132246017456, Variance: 0.11140593886375427

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5678066063766245, Training Loss Force: 3.1980327979885264, time: 4.2960898876190186
Validation Loss Energy: 1.979076020924363, Validation Loss Force: 3.1935233847907676, time: 0.23492193222045898
Test Loss Energy: 9.787751407258753, Test Loss Force: 12.058788879543732, time: 10.364336729049683

Epoch 15, Batch 100/265, Loss: 0.727920651435852, Variance: 0.11226192116737366
Epoch 15, Batch 200/265, Loss: 0.9753562211990356, Variance: 0.11552300304174423

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.566020680089101, Training Loss Force: 3.199262492968347, time: 4.438652276992798
Validation Loss Energy: 2.67837395137157, Validation Loss Force: 3.2182804529762863, time: 0.23888826370239258
Test Loss Energy: 10.269048220059076, Test Loss Force: 11.84975318951242, time: 10.195045471191406

Epoch 16, Batch 100/265, Loss: 0.5387006402015686, Variance: 0.11060047149658203
Epoch 16, Batch 200/265, Loss: 1.3021618127822876, Variance: 0.11519838869571686

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5663642678241376, Training Loss Force: 3.18961842505524, time: 4.365283727645874
Validation Loss Energy: 3.623377219704696, Validation Loss Force: 3.216523281678112, time: 0.23384809494018555
Test Loss Energy: 10.556822326835011, Test Loss Force: 11.306739167203679, time: 10.488734006881714

Epoch 17, Batch 100/265, Loss: 0.9878126382827759, Variance: 0.10405565798282623
Epoch 17, Batch 200/265, Loss: 0.6841475963592529, Variance: 0.11343161761760712

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.539788653581609, Training Loss Force: 3.1993678591679395, time: 4.310020685195923
Validation Loss Energy: 2.087581983193707, Validation Loss Force: 3.2266452866460384, time: 0.2333691120147705
Test Loss Energy: 9.71188012040615, Test Loss Force: 11.42140261358821, time: 10.25286865234375

Epoch 18, Batch 100/265, Loss: 0.5536528825759888, Variance: 0.11338210850954056
Epoch 18, Batch 200/265, Loss: 0.7436615824699402, Variance: 0.11059849709272385

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5731914088709, Training Loss Force: 3.2028204014132933, time: 4.3087897300720215
Validation Loss Energy: 2.3405379183431547, Validation Loss Force: 3.161910600805108, time: 0.23496723175048828
Test Loss Energy: 9.622420624960169, Test Loss Force: 11.537785322256852, time: 11.658474683761597

Epoch 19, Batch 100/265, Loss: 0.9746099710464478, Variance: 0.11233643442392349
Epoch 19, Batch 200/265, Loss: 1.273309350013733, Variance: 0.10793322324752808

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.58470702860101, Training Loss Force: 3.2024168148466905, time: 4.344085693359375
Validation Loss Energy: 3.376924960435678, Validation Loss Force: 3.202335738910006, time: 0.2292184829711914
Test Loss Energy: 9.770483533904372, Test Loss Force: 11.382348968198926, time: 10.613900423049927

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–„â–â–„â–‡â–ƒâ–‚â–…â–‚â–†â–ˆâ–„â–„â–„â–ƒâ–…â–†â–ƒâ–ƒâ–ƒ
wandb:   test_error_force â–…â–„â–â–…â–‡â–â–„â–†â–‡â–…â–‡â–ƒâ–ƒâ–ƒâ–ˆâ–†â–‚â–ƒâ–„â–‚
wandb:          test_loss â–‡â–„â–â–…â–ˆâ–‚â–ƒâ–†â–„â–…â–ˆâ–„â–„â–ƒâ–…â–‡â–…â–ƒâ–ƒâ–„
wandb: train_error_energy â–ˆâ–â–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„
wandb:  train_error_force â–ˆâ–â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–â–â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–†â–‚â–ƒâ–‡â–ƒâ–ƒâ–‡â–â–…â–ˆâ–ƒâ–…â–†â–‚â–…â–ˆâ–ƒâ–„â–‡
wandb:  valid_error_force â–â–ˆâ–…â–…â–…â–…â–†â–…â–ƒâ–ƒâ–†â–†â–‚â–…â–ƒâ–„â–„â–„â–â–ƒ
wandb:         valid_loss â–ƒâ–†â–‚â–ƒâ–‡â–‚â–ƒâ–‡â–â–„â–ˆâ–ƒâ–„â–†â–‚â–„â–‡â–‚â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 8463
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.77048
wandb:   test_error_force 11.38235
wandb:          test_loss 12.25651
wandb: train_error_energy 2.58471
wandb:  train_error_force 3.20242
wandb:         train_loss 0.86689
wandb: valid_error_energy 3.37692
wandb:  valid_error_force 3.20234
wandb:         valid_loss 1.19511
wandb: 
wandb: ğŸš€ View run al_71_97 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/xrt047iu
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_161635-xrt047iu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.4495737552642822, Uncertainty Bias: -0.1544608622789383
9.536743e-06 0.03651619
1.8749002 4.6558504
(48745, 22, 3)
Found uncertainty sample 0 after 322 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 1374 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 1679 steps.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 1258 steps.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1596 steps.
Found uncertainty sample 9 after 369 steps.
Found uncertainty sample 10 after 1008 steps.
Found uncertainty sample 11 after 3755 steps.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 531 steps.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 1245 steps.
Found uncertainty sample 21 after 925 steps.
Found uncertainty sample 22 after 1066 steps.
Found uncertainty sample 23 after 765 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 1711 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 122 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1569 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 178 steps.
Found uncertainty sample 32 after 617 steps.
Found uncertainty sample 33 after 506 steps.
Found uncertainty sample 34 after 3205 steps.
Found uncertainty sample 35 after 603 steps.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 829 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 2811 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 3357 steps.
Found uncertainty sample 43 after 936 steps.
Found uncertainty sample 44 after 777 steps.
Found uncertainty sample 45 after 859 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 2500 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 802 steps.
Did not find any uncertainty samples for sample 50.
Found uncertainty sample 51 after 584 steps.
Found uncertainty sample 52 after 2433 steps.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 851 steps.
Found uncertainty sample 58 after 1264 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 2 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 1266 steps.
Found uncertainty sample 63 after 1222 steps.
Found uncertainty sample 64 after 881 steps.
Found uncertainty sample 65 after 1483 steps.
Found uncertainty sample 66 after 1952 steps.
Found uncertainty sample 67 after 2085 steps.
Found uncertainty sample 68 after 3267 steps.
Found uncertainty sample 69 after 1435 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 180 steps.
Found uncertainty sample 72 after 1243 steps.
Found uncertainty sample 73 after 3202 steps.
Found uncertainty sample 74 after 731 steps.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 245 steps.
Found uncertainty sample 79 after 2063 steps.
Found uncertainty sample 80 after 2194 steps.
Found uncertainty sample 81 after 1586 steps.
Found uncertainty sample 82 after 2738 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 1051 steps.
Did not find any uncertainty samples for sample 85.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 650 steps.
Did not find any uncertainty samples for sample 89.
Found uncertainty sample 90 after 3914 steps.
Found uncertainty sample 91 after 1584 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 1827 steps.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 3709 steps.
Found uncertainty sample 97 after 1466 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 670 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_164906-h0emi3ag
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_98
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/h0emi3ag
Training model 98. Added 59 samples to the dataset.
Epoch 0, Batch 100/267, Loss: 0.6712509393692017, Variance: 0.11426246166229248
Epoch 0, Batch 200/267, Loss: 0.7748579978942871, Variance: 0.11042600125074387

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.001014918621575, Training Loss Force: 3.3267774973046373, time: 4.376631259918213
Validation Loss Energy: 1.908883602982169, Validation Loss Force: 3.2459903367059524, time: 0.24822759628295898
Test Loss Energy: 9.26520434475813, Test Loss Force: 11.294516104672349, time: 10.821165084838867

Epoch 1, Batch 100/267, Loss: 0.7141885161399841, Variance: 0.11381325125694275
Epoch 1, Batch 200/267, Loss: 0.7930617332458496, Variance: 0.1107848584651947

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5788368190850592, Training Loss Force: 3.178653322894712, time: 4.349194288253784
Validation Loss Energy: 2.0019548303301513, Validation Loss Force: 3.1815629342864025, time: 0.2474663257598877
Test Loss Energy: 9.81977521750327, Test Loss Force: 11.546720468500709, time: 11.05612850189209

Epoch 2, Batch 100/267, Loss: 0.6503982543945312, Variance: 0.11127111315727234
Epoch 2, Batch 200/267, Loss: 0.6297316551208496, Variance: 0.1084529310464859

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5970207204361415, Training Loss Force: 3.189208349965428, time: 4.339378833770752
Validation Loss Energy: 1.4546691622970467, Validation Loss Force: 3.200087181340598, time: 0.24268126487731934
Test Loss Energy: 9.525420358446656, Test Loss Force: 11.575578403802636, time: 10.91189193725586

Epoch 3, Batch 100/267, Loss: 0.5718266367912292, Variance: 0.11079597473144531
Epoch 3, Batch 200/267, Loss: 0.7761379480361938, Variance: 0.11740376055240631

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.585214628019981, Training Loss Force: 3.2004529366789383, time: 4.3387610912323
Validation Loss Energy: 2.3784743724716764, Validation Loss Force: 3.2116291474521064, time: 0.25480151176452637
Test Loss Energy: 10.067647139236104, Test Loss Force: 11.54532829436809, time: 10.971132040023804

Epoch 4, Batch 100/267, Loss: 0.48178207874298096, Variance: 0.11136007308959961
Epoch 4, Batch 200/267, Loss: 0.6097193360328674, Variance: 0.10546000301837921

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.556517709843715, Training Loss Force: 3.2008795232264267, time: 4.255235195159912
Validation Loss Energy: 1.6684492445848242, Validation Loss Force: 3.2309802095789606, time: 0.24826669692993164
Test Loss Energy: 9.351023052776506, Test Loss Force: 11.363860797976436, time: 10.842225313186646

Epoch 5, Batch 100/267, Loss: 0.5169110894203186, Variance: 0.11144301295280457
Epoch 5, Batch 200/267, Loss: 0.8939003944396973, Variance: 0.11466266214847565

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5723205428830607, Training Loss Force: 3.195167087228045, time: 4.3357179164886475
Validation Loss Energy: 2.250904145935433, Validation Loss Force: 3.241239601409925, time: 0.24297022819519043
Test Loss Energy: 10.43122609585266, Test Loss Force: 11.652835350258087, time: 10.98323392868042

Epoch 6, Batch 100/267, Loss: 0.5940743088722229, Variance: 0.11102871596813202
Epoch 6, Batch 200/267, Loss: 0.848931610584259, Variance: 0.10931067168712616

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5742357010259482, Training Loss Force: 3.219578797736543, time: 4.380636692047119
Validation Loss Energy: 1.6793520389129468, Validation Loss Force: 3.2220927714237155, time: 0.24525952339172363
Test Loss Energy: 9.408274158079873, Test Loss Force: 11.48194796784293, time: 10.832378149032593

Epoch 7, Batch 100/267, Loss: 0.5782020688056946, Variance: 0.11353828758001328
Epoch 7, Batch 200/267, Loss: 0.7094975709915161, Variance: 0.11295711994171143

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.591698673071598, Training Loss Force: 3.200585552417692, time: 4.322786092758179
Validation Loss Energy: 2.1774705781720285, Validation Loss Force: 3.280602385019601, time: 0.2487797737121582
Test Loss Energy: 10.122699672346162, Test Loss Force: 11.665289970048265, time: 12.436821699142456

Epoch 8, Batch 100/267, Loss: 0.5952218174934387, Variance: 0.11090435087680817
Epoch 8, Batch 200/267, Loss: 0.8442549109458923, Variance: 0.11066015809774399

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5751536041250414, Training Loss Force: 3.2030035575376967, time: 4.403063535690308
Validation Loss Energy: 1.8001521436687438, Validation Loss Force: 3.2125264150425368, time: 0.26729345321655273
Test Loss Energy: 9.497913822010087, Test Loss Force: 11.480506615305847, time: 10.85054636001587

Epoch 9, Batch 100/267, Loss: 0.6572439670562744, Variance: 0.11064799129962921
Epoch 9, Batch 200/267, Loss: 0.7690845727920532, Variance: 0.10889335721731186

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5791763324058707, Training Loss Force: 3.220783654647391, time: 4.2857749462127686
Validation Loss Energy: 1.9940139587142451, Validation Loss Force: 3.355561770838561, time: 0.2475571632385254
Test Loss Energy: 10.083541421422188, Test Loss Force: 11.78939701309879, time: 11.140353441238403

Epoch 10, Batch 100/267, Loss: 0.446206271648407, Variance: 0.1091277003288269
Epoch 10, Batch 200/267, Loss: 0.5613071918487549, Variance: 0.10693420469760895

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5620886733755164, Training Loss Force: 3.198979460717286, time: 4.366738319396973
Validation Loss Energy: 1.674314305354733, Validation Loss Force: 3.2315636918089496, time: 0.2444467544555664
Test Loss Energy: 9.27981151301103, Test Loss Force: 11.592453805883117, time: 10.925984382629395

Epoch 11, Batch 100/267, Loss: 0.7018817663192749, Variance: 0.1133241131901741
Epoch 11, Batch 200/267, Loss: 0.798971951007843, Variance: 0.10879020392894745

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5627519455495893, Training Loss Force: 3.2219494243664757, time: 4.40381932258606
Validation Loss Energy: 2.1154273388317564, Validation Loss Force: 3.27046586202336, time: 0.24761319160461426
Test Loss Energy: 9.951751094387507, Test Loss Force: 11.441841188326356, time: 11.071051359176636

Epoch 12, Batch 100/267, Loss: 0.482180118560791, Variance: 0.111626997590065
Epoch 12, Batch 200/267, Loss: 0.5923238396644592, Variance: 0.11107291281223297

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5801823286730845, Training Loss Force: 3.1934239831157116, time: 4.362267732620239
Validation Loss Energy: 1.73057133106967, Validation Loss Force: 3.274361102460733, time: 0.24556231498718262
Test Loss Energy: 9.50806862625979, Test Loss Force: 11.177070492413364, time: 10.881474494934082

Epoch 13, Batch 100/267, Loss: 0.6350765228271484, Variance: 0.11021840572357178
Epoch 13, Batch 200/267, Loss: 0.8108001351356506, Variance: 0.11341933906078339

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5680180203957876, Training Loss Force: 3.1981319160645043, time: 4.383478879928589
Validation Loss Energy: 2.2903447301446964, Validation Loss Force: 3.174227589145933, time: 0.2472989559173584
Test Loss Energy: 9.783322156546065, Test Loss Force: 11.305765317282026, time: 11.13007926940918

Epoch 14, Batch 100/267, Loss: 0.5337975025177002, Variance: 0.11462501436471939
Epoch 14, Batch 200/267, Loss: 0.8270570039749146, Variance: 0.11228737980127335

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5807920439137613, Training Loss Force: 3.19927451318101, time: 4.369252920150757
Validation Loss Energy: 1.97008236958651, Validation Loss Force: 3.3848682814121887, time: 0.2480146884918213
Test Loss Energy: 9.358523463604076, Test Loss Force: 11.51938577202283, time: 10.92096734046936

Epoch 15, Batch 100/267, Loss: 0.7102214097976685, Variance: 0.11408954858779907
Epoch 15, Batch 200/267, Loss: 0.8014394044876099, Variance: 0.11059935390949249

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5708487202606656, Training Loss Force: 3.2164766979584365, time: 4.349343776702881
Validation Loss Energy: 2.1585416076781945, Validation Loss Force: 3.178426550298351, time: 0.2503664493560791
Test Loss Energy: 10.046928290494238, Test Loss Force: 11.488153190357709, time: 11.211786270141602

Epoch 16, Batch 100/267, Loss: 0.5256165266036987, Variance: 0.11133516579866409
Epoch 16, Batch 200/267, Loss: 0.8608608841896057, Variance: 0.11046211421489716

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5652344513328313, Training Loss Force: 3.213967019867656, time: 4.287410497665405
Validation Loss Energy: 1.428112214871165, Validation Loss Force: 3.1870338261341895, time: 0.2451794147491455
Test Loss Energy: 9.396548109403556, Test Loss Force: 11.451808143755004, time: 10.882023811340332

Epoch 17, Batch 100/267, Loss: 0.7579478621482849, Variance: 0.11051227152347565
Epoch 17, Batch 200/267, Loss: 0.6934976577758789, Variance: 0.11134839057922363

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5744298193171535, Training Loss Force: 3.216817796233225, time: 4.369894981384277
Validation Loss Energy: 2.095877611292905, Validation Loss Force: 3.1850050875456413, time: 0.24709868431091309
Test Loss Energy: 10.124619709919866, Test Loss Force: 11.591794831118332, time: 11.165925025939941

Epoch 18, Batch 100/267, Loss: 0.5118029117584229, Variance: 0.11196061968803406
Epoch 18, Batch 200/267, Loss: 0.8495889902114868, Variance: 0.10992640256881714

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.568858293691703, Training Loss Force: 3.1874507806982777, time: 4.341159820556641
Validation Loss Energy: 1.7915343578492173, Validation Loss Force: 3.2193934424352917, time: 0.24764680862426758
Test Loss Energy: 9.634534968989547, Test Loss Force: 11.524500671165534, time: 11.116343021392822

Epoch 19, Batch 100/267, Loss: 0.4520854353904724, Variance: 0.10808368027210236
Epoch 19, Batch 200/267, Loss: 0.9156622886657715, Variance: 0.11436498165130615

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5804223243140525, Training Loss Force: 3.200463330407649, time: 4.523216485977173
Validation Loss Energy: 2.3331252624752183, Validation Loss Force: 3.276744330329598, time: 0.2441720962524414
Test Loss Energy: 9.951084047574732, Test Loss Force: 11.507711403572893, time: 10.998985052108765

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–„â–ƒâ–†â–‚â–ˆâ–‚â–†â–‚â–†â–â–…â–‚â–„â–‚â–†â–‚â–†â–ƒâ–…
wandb:   test_error_force â–‚â–…â–†â–…â–ƒâ–†â–„â–‡â–„â–ˆâ–†â–„â–â–‚â–…â–…â–„â–†â–…â–…
wandb:          test_loss â–‚â–…â–‚â–†â–ƒâ–ˆâ–ƒâ–…â–„â–…â–ƒâ–„â–‚â–‚â–â–…â–ƒâ–…â–ƒâ–„
wandb: train_error_energy â–ˆâ–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–…â–…â–â–ˆâ–ƒâ–‡â–ƒâ–‡â–„â–…â–ƒâ–†â–ƒâ–‡â–…â–†â–â–†â–„â–ˆ
wandb:  valid_error_force â–ƒâ–â–‚â–‚â–ƒâ–ƒâ–ƒâ–…â–‚â–‡â–ƒâ–„â–„â–â–ˆâ–â–â–â–ƒâ–„
wandb:         valid_loss â–…â–…â–â–ˆâ–ƒâ–‡â–ƒâ–‡â–ƒâ–†â–ƒâ–†â–„â–‡â–†â–†â–â–…â–„â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 8516
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.95108
wandb:   test_error_force 11.50771
wandb:          test_loss 12.2822
wandb: train_error_energy 2.58042
wandb:  train_error_force 3.20046
wandb:         train_loss 0.86514
wandb: valid_error_energy 2.33313
wandb:  valid_error_force 3.27674
wandb:         valid_loss 0.77928
wandb: 
wandb: ğŸš€ View run al_71_98 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/h0emi3ag
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_164906-h0emi3ag/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.448716878890991, Uncertainty Bias: -0.16738227009773254
5.340576e-05 0.0070014
1.9071056 4.789146
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 2600 steps.
Found uncertainty sample 3 after 179 steps.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 1082 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 2237 steps.
Found uncertainty sample 10 after 531 steps.
Found uncertainty sample 11 after 639 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 1935 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1492 steps.
Found uncertainty sample 16 after 2083 steps.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 1708 steps.
Found uncertainty sample 20 after 3366 steps.
Found uncertainty sample 21 after 490 steps.
Found uncertainty sample 22 after 439 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 3987 steps.
Found uncertainty sample 28 after 41 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 1199 steps.
Found uncertainty sample 31 after 1269 steps.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 3192 steps.
Found uncertainty sample 35 after 739 steps.
Found uncertainty sample 36 after 1979 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 280 steps.
Found uncertainty sample 39 after 3922 steps.
Found uncertainty sample 40 after 763 steps.
Did not find any uncertainty samples for sample 41.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 2130 steps.
Found uncertainty sample 44 after 2664 steps.
Found uncertainty sample 45 after 2534 steps.
Found uncertainty sample 46 after 1621 steps.
Found uncertainty sample 47 after 189 steps.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 178 steps.
Did not find any uncertainty samples for sample 51.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 3433 steps.
Found uncertainty sample 54 after 728 steps.
Found uncertainty sample 55 after 944 steps.
Found uncertainty sample 56 after 1215 steps.
Found uncertainty sample 57 after 1096 steps.
Found uncertainty sample 58 after 102 steps.
Found uncertainty sample 59 after 828 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 3135 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 994 steps.
Found uncertainty sample 65 after 1624 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 703 steps.
Found uncertainty sample 69 after 3999 steps.
Did not find any uncertainty samples for sample 70.
Did not find any uncertainty samples for sample 71.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 2266 steps.
Found uncertainty sample 74 after 803 steps.
Found uncertainty sample 75 after 1022 steps.
Found uncertainty sample 76 after 358 steps.
Found uncertainty sample 77 after 697 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 627 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 595 steps.
Found uncertainty sample 82 after 1205 steps.
Found uncertainty sample 83 after 1252 steps.
Found uncertainty sample 84 after 1216 steps.
Found uncertainty sample 85 after 670 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 2102 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 2342 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 1624 steps.
Found uncertainty sample 92 after 1133 steps.
Found uncertainty sample 93 after 1526 steps.
Found uncertainty sample 94 after 3737 steps.
Found uncertainty sample 95 after 939 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_172120-v5ehl270
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_99
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/v5ehl270
Training model 99. Added 60 samples to the dataset.
Epoch 0, Batch 100/268, Loss: 0.6232456564903259, Variance: 0.08520642668008804
Epoch 0, Batch 200/268, Loss: 0.5305655598640442, Variance: 0.07864357531070709

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8358467583969378, Training Loss Force: 3.3021824395756325, time: 4.5090532302856445
Validation Loss Energy: 1.4235754646243737, Validation Loss Force: 3.1906240543754714, time: 0.2519686222076416
Test Loss Energy: 9.388570748260246, Test Loss Force: 11.550023568259158, time: 10.813050746917725

Epoch 1, Batch 100/268, Loss: 0.08270198106765747, Variance: 0.07796250283718109
Epoch 1, Batch 200/268, Loss: 0.6413795948028564, Variance: 0.0776807963848114

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6708446305213989, Training Loss Force: 3.355657058111457, time: 4.314335584640503
Validation Loss Energy: 1.5524089154923801, Validation Loss Force: 3.952637150577703, time: 0.24866366386413574
Test Loss Energy: 8.972045426528805, Test Loss Force: 11.518878767201429, time: 10.96500539779663

Epoch 2, Batch 100/268, Loss: 0.6228477954864502, Variance: 0.10193022340536118
Epoch 2, Batch 200/268, Loss: 1.4099534749984741, Variance: 0.10665574669837952

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.551132356951872, Training Loss Force: 3.403559519192372, time: 4.422788381576538
Validation Loss Energy: 3.8840183577566956, Validation Loss Force: 3.1592264133186063, time: 0.2524688243865967
Test Loss Energy: 10.85636321791709, Test Loss Force: 11.455390062589977, time: 10.768317222595215

Epoch 3, Batch 100/268, Loss: 1.2049146890640259, Variance: 0.10777866840362549
Epoch 3, Batch 200/268, Loss: 0.739789605140686, Variance: 0.10885737091302872

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.567989968944058, Training Loss Force: 3.1778651204561883, time: 4.4447267055511475
Validation Loss Energy: 1.6049225833943808, Validation Loss Force: 3.1884040875244, time: 0.24387717247009277
Test Loss Energy: 9.236478919748366, Test Loss Force: 11.28093753963486, time: 11.067750692367554

Epoch 4, Batch 100/268, Loss: 0.6256877183914185, Variance: 0.1118384301662445
Epoch 4, Batch 200/268, Loss: 0.8143599033355713, Variance: 0.11647488176822662

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5659443469660332, Training Loss Force: 3.1901821132836297, time: 4.406649589538574
Validation Loss Energy: 2.017232739372036, Validation Loss Force: 3.185694782958238, time: 0.24866104125976562
Test Loss Energy: 9.15350480890341, Test Loss Force: 11.033766407833964, time: 10.821617364883423

Epoch 5, Batch 100/268, Loss: 0.9712538719177246, Variance: 0.11354246735572815
Epoch 5, Batch 200/268, Loss: 1.0568122863769531, Variance: 0.10753896087408066

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.541113396327041, Training Loss Force: 3.2016419636669085, time: 4.463476657867432
Validation Loss Energy: 3.354265079334165, Validation Loss Force: 3.2813937511642295, time: 0.25191712379455566
Test Loss Energy: 10.268836579447811, Test Loss Force: 11.160666220939842, time: 11.014420509338379

Epoch 6, Batch 100/268, Loss: 1.1068722009658813, Variance: 0.10803703218698502
Epoch 6, Batch 200/268, Loss: 0.5821273326873779, Variance: 0.11342528462409973

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5444788518636496, Training Loss Force: 3.192566880726851, time: 4.327019453048706
Validation Loss Energy: 1.475334083719448, Validation Loss Force: 3.161117408773484, time: 0.257784366607666
Test Loss Energy: 9.482090173741351, Test Loss Force: 11.563011633374563, time: 10.825786828994751

Epoch 7, Batch 100/268, Loss: 0.6330996751785278, Variance: 0.1152009665966034
Epoch 7, Batch 200/268, Loss: 0.9408494830131531, Variance: 0.11202580481767654

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5576773137394477, Training Loss Force: 3.1953526179728913, time: 4.395242929458618
Validation Loss Energy: 2.180633548856103, Validation Loss Force: 3.165882617563997, time: 0.242919921875
Test Loss Energy: 9.498196236435547, Test Loss Force: 11.511618783092722, time: 10.9927237033844

Epoch 8, Batch 100/268, Loss: 0.8594244718551636, Variance: 0.1141316145658493
Epoch 8, Batch 200/268, Loss: 0.9382850527763367, Variance: 0.11138418316841125

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.567574324164411, Training Loss Force: 3.197327600241508, time: 4.3747875690460205
Validation Loss Energy: 3.6304077753182193, Validation Loss Force: 3.2201082103937195, time: 0.2578999996185303
Test Loss Energy: 10.631936412684297, Test Loss Force: 11.500825270467933, time: 10.859634160995483

Epoch 9, Batch 100/268, Loss: 1.2722755670547485, Variance: 0.11052048206329346
Epoch 9, Batch 200/268, Loss: 0.5399588942527771, Variance: 0.1126178503036499

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5575287211347266, Training Loss Force: 3.1963397451927005, time: 4.296855926513672
Validation Loss Energy: 1.4833296428775937, Validation Loss Force: 3.2226381772297508, time: 0.24857783317565918
Test Loss Energy: 9.361712703651497, Test Loss Force: 11.640453206360384, time: 10.9960618019104

Epoch 10, Batch 100/268, Loss: 0.7825679779052734, Variance: 0.11391738802194595
Epoch 10, Batch 200/268, Loss: 0.817802906036377, Variance: 0.10994100570678711

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.558295927677021, Training Loss Force: 3.1899491814164693, time: 4.471716403961182
Validation Loss Energy: 2.0474538411956473, Validation Loss Force: 3.219953633472616, time: 0.2567777633666992
Test Loss Energy: 9.673614024309222, Test Loss Force: 11.685195999464229, time: 10.862466096878052

Epoch 11, Batch 100/268, Loss: 0.8610068559646606, Variance: 0.11081372201442719
Epoch 11, Batch 200/268, Loss: 1.0778582096099854, Variance: 0.10963161289691925

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5448163057080206, Training Loss Force: 3.19741460654857, time: 4.3906965255737305
Validation Loss Energy: 3.8734440226857303, Validation Loss Force: 3.1909650499055453, time: 0.24894046783447266
Test Loss Energy: 11.11982879284125, Test Loss Force: 11.48145508327648, time: 11.015588521957397

Epoch 12, Batch 100/268, Loss: 1.1026490926742554, Variance: 0.10632568597793579
Epoch 12, Batch 200/268, Loss: 0.6331070065498352, Variance: 0.11077196896076202

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.565001490609796, Training Loss Force: 3.197452539234605, time: 4.496638298034668
Validation Loss Energy: 1.7940455091424177, Validation Loss Force: 3.154736601727777, time: 0.25002408027648926
Test Loss Energy: 9.401755296163692, Test Loss Force: 11.414437877108934, time: 10.88481092453003

Epoch 13, Batch 100/268, Loss: 0.607047975063324, Variance: 0.11842392385005951
Epoch 13, Batch 200/268, Loss: 0.9522586464881897, Variance: 0.11475461721420288

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.581175395950842, Training Loss Force: 3.188449249646154, time: 4.38837742805481
Validation Loss Energy: 2.0443456471214705, Validation Loss Force: 3.168639306124412, time: 0.24714016914367676
Test Loss Energy: 9.344919402366475, Test Loss Force: 11.173671166941277, time: 11.053282022476196

Epoch 14, Batch 100/268, Loss: 0.9185978174209595, Variance: 0.11606191098690033
Epoch 14, Batch 200/268, Loss: 1.047555923461914, Variance: 0.11028102040290833

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.570288418443604, Training Loss Force: 3.1948794186195792, time: 4.345237970352173
Validation Loss Energy: 3.5809565207139924, Validation Loss Force: 3.1768262716803974, time: 0.25089025497436523
Test Loss Energy: 10.745117255615206, Test Loss Force: 11.459347828290364, time: 10.768763065338135

Epoch 15, Batch 100/268, Loss: 0.8196933269500732, Variance: 0.10834575444459915
Epoch 15, Batch 200/268, Loss: 0.5994002223014832, Variance: 0.11269912868738174

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5746055012574915, Training Loss Force: 3.201759107578187, time: 4.28529691696167
Validation Loss Energy: 1.6402074265324509, Validation Loss Force: 3.238838604543215, time: 0.25093650817871094
Test Loss Energy: 9.301639599178502, Test Loss Force: 11.336355739783201, time: 10.9811532497406

Epoch 16, Batch 100/268, Loss: 0.4971008896827698, Variance: 0.10908006131649017
Epoch 16, Batch 200/268, Loss: 0.888271689414978, Variance: 0.11398907005786896

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5677321592003977, Training Loss Force: 3.191028280963806, time: 4.415756940841675
Validation Loss Energy: 2.334729059265678, Validation Loss Force: 3.214196522704653, time: 0.24785470962524414
Test Loss Energy: 9.604439758005684, Test Loss Force: 11.459176813723065, time: 10.872753381729126

Epoch 17, Batch 100/268, Loss: 0.7872564196586609, Variance: 0.11410047858953476
Epoch 17, Batch 200/268, Loss: 1.3915257453918457, Variance: 0.11084900796413422

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5605681217424405, Training Loss Force: 3.2044161304259107, time: 4.355197191238403
Validation Loss Energy: 3.795238566057435, Validation Loss Force: 3.181696001368009, time: 0.2604033946990967
Test Loss Energy: 10.770109962316882, Test Loss Force: 11.28583095875482, time: 11.044371843338013

Epoch 18, Batch 100/268, Loss: 1.1569740772247314, Variance: 0.11418554186820984
Epoch 18, Batch 200/268, Loss: 0.4792535901069641, Variance: 0.11197971552610397

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.574993321948731, Training Loss Force: 3.196034741637501, time: 4.26059889793396
Validation Loss Energy: 1.6389641310892482, Validation Loss Force: 3.1721318271444603, time: 0.2533996105194092
Test Loss Energy: 9.373441258812967, Test Loss Force: 11.370484232784712, time: 10.87391185760498

Epoch 19, Batch 100/268, Loss: 0.7213788628578186, Variance: 0.112052783370018
Epoch 19, Batch 200/268, Loss: 0.8064694404602051, Variance: 0.1159515529870987

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5568682267034815, Training Loss Force: 3.192418349605402, time: 4.4632086753845215
Validation Loss Energy: 2.039884960654928, Validation Loss Force: 3.1763782168249612, time: 0.24634861946105957
Test Loss Energy: 9.299810881602387, Test Loss Force: 11.206831615740228, time: 10.946287155151367

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‡â–‚â–‚â–…â–ƒâ–ƒâ–†â–‚â–ƒâ–ˆâ–‚â–‚â–‡â–‚â–ƒâ–‡â–‚â–‚
wandb:   test_error_force â–‡â–†â–†â–„â–â–‚â–‡â–†â–†â–ˆâ–ˆâ–†â–…â–ƒâ–†â–„â–†â–„â–…â–ƒ
wandb:          test_loss â–ˆâ–ˆâ–„â–â–â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–…â–‚â–â–„â–‚â–‚â–„â–â–‚
wandb: train_error_energy â–‚â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  train_error_force â–…â–‡â–ˆâ–â–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–
wandb:         train_loss â–…â–â–ˆâ–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†
wandb: valid_error_energy â–â–â–ˆâ–‚â–ƒâ–†â–â–ƒâ–‡â–â–ƒâ–ˆâ–‚â–ƒâ–‡â–‚â–„â–ˆâ–‚â–ƒ
wandb:  valid_error_force â–â–ˆâ–â–â–â–‚â–â–â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–â–â–
wandb:         valid_loss â–â–ƒâ–ˆâ–‚â–ƒâ–†â–‚â–ƒâ–‡â–‚â–ƒâ–ˆâ–‚â–ƒâ–‡â–‚â–„â–‡â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 8570
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.29981
wandb:   test_error_force 11.20683
wandb:          test_loss 11.51216
wandb: train_error_energy 2.55687
wandb:  train_error_force 3.19242
wandb:         train_loss 0.85204
wandb: valid_error_energy 2.03988
wandb:  valid_error_force 3.17638
wandb:         valid_loss 0.63374
wandb: 
wandb: ğŸš€ View run al_71_99 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/v5ehl270
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_172120-v5ehl270/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.4398956298828125, Uncertainty Bias: -0.15703897178173065
9.1552734e-05 0.0008649826
1.8112231 4.695189
(48745, 22, 3)
Found uncertainty sample 0 after 3365 steps.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 1427 steps.
Found uncertainty sample 3 after 2247 steps.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 961 steps.
Found uncertainty sample 11 after 512 steps.
Found uncertainty sample 12 after 1808 steps.
Did not find any uncertainty samples for sample 13.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 2770 steps.
Found uncertainty sample 17 after 3697 steps.
Found uncertainty sample 18 after 1327 steps.
Did not find any uncertainty samples for sample 19.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 1425 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 3464 steps.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 1587 steps.
Did not find any uncertainty samples for sample 27.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1839 steps.
Found uncertainty sample 30 after 3917 steps.
Did not find any uncertainty samples for sample 31.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 1110 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 1396 steps.
Found uncertainty sample 38 after 631 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 2735 steps.
Found uncertainty sample 41 after 448 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 3443 steps.
Did not find any uncertainty samples for sample 44.
Did not find any uncertainty samples for sample 45.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 2284 steps.
Found uncertainty sample 48 after 377 steps.
Did not find any uncertainty samples for sample 49.
Did not find any uncertainty samples for sample 50.
Found uncertainty sample 51 after 696 steps.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 659 steps.
Did not find any uncertainty samples for sample 54.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 1509 steps.
Found uncertainty sample 58 after 2674 steps.
Found uncertainty sample 59 after 275 steps.
Did not find any uncertainty samples for sample 60.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 38 steps.
Found uncertainty sample 66 after 323 steps.
Found uncertainty sample 67 after 535 steps.
Found uncertainty sample 68 after 566 steps.
Found uncertainty sample 69 after 3719 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 897 steps.
Found uncertainty sample 72 after 1671 steps.
Found uncertainty sample 73 after 1806 steps.
Found uncertainty sample 74 after 1457 steps.
Found uncertainty sample 75 after 3168 steps.
Found uncertainty sample 76 after 2288 steps.
Found uncertainty sample 77 after 3624 steps.
Found uncertainty sample 78 after 3082 steps.
Found uncertainty sample 79 after 1352 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 867 steps.
Did not find any uncertainty samples for sample 82.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 3618 steps.
Found uncertainty sample 85 after 1929 steps.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 1197 steps.
Found uncertainty sample 90 after 2207 steps.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 1540 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 180 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 2233 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_175743-htp4xyu5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_100
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/htp4xyu5
Training model 100. Added 49 samples to the dataset.
Epoch 0, Batch 100/270, Loss: 0.9723998308181763, Variance: 0.08923102915287018
Epoch 0, Batch 200/270, Loss: 0.8120349645614624, Variance: 0.10197579860687256

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7390338788479296, Training Loss Force: 3.4637120150237832, time: 4.842360496520996
Validation Loss Energy: 1.664811654616204, Validation Loss Force: 3.1554959480492197, time: 0.29718828201293945
Test Loss Energy: 9.638135242290664, Test Loss Force: 11.297584975142192, time: 12.325022220611572

Epoch 1, Batch 100/270, Loss: 0.4656159281730652, Variance: 0.10668662190437317
Epoch 1, Batch 200/270, Loss: 0.6700159311294556, Variance: 0.10423389077186584

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5084547471641767, Training Loss Force: 3.1553267978448662, time: 4.8494298458099365
Validation Loss Energy: 2.0347149393571184, Validation Loss Force: 3.297452204630249, time: 0.2585718631744385
Test Loss Energy: 9.838843003673258, Test Loss Force: 11.229191561735334, time: 12.496382474899292

Epoch 2, Batch 100/270, Loss: 0.3792233467102051, Variance: 0.10979471355676651
Epoch 2, Batch 200/270, Loss: 0.8401174545288086, Variance: 0.10994270443916321

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.523515976035459, Training Loss Force: 3.1725951678244138, time: 4.961674213409424
Validation Loss Energy: 2.1599902931416013, Validation Loss Force: 3.2144078357241392, time: 0.29489707946777344
Test Loss Energy: 9.650591489547484, Test Loss Force: 11.475354932619203, time: 12.299640655517578

Epoch 3, Batch 100/270, Loss: 0.628657341003418, Variance: 0.11090756207704544
Epoch 3, Batch 200/270, Loss: 0.8120182752609253, Variance: 0.11073091626167297

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.570642420433605, Training Loss Force: 3.1739757635370967, time: 4.783078193664551
Validation Loss Energy: 2.013760659379584, Validation Loss Force: 3.212663222965795, time: 0.2804248332977295
Test Loss Energy: 9.827651997477812, Test Loss Force: 11.295235957881118, time: 12.38801622390747

Epoch 4, Batch 100/270, Loss: 0.7185302972793579, Variance: 0.11212372779846191
Epoch 4, Batch 200/270, Loss: 0.6874060034751892, Variance: 0.11092932522296906

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.528871192086638, Training Loss Force: 3.193731795689051, time: 4.95562744140625
Validation Loss Energy: 2.1696999148512695, Validation Loss Force: 3.166415687499348, time: 0.28685760498046875
Test Loss Energy: 9.903029779554616, Test Loss Force: 11.442775843815093, time: 12.226057767868042

Epoch 5, Batch 100/270, Loss: 0.46503448486328125, Variance: 0.10942709445953369
Epoch 5, Batch 200/270, Loss: 0.7455079555511475, Variance: 0.11297445744276047

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.547724009732564, Training Loss Force: 3.171514785375378, time: 4.962991714477539
Validation Loss Energy: 2.078473657245701, Validation Loss Force: 3.191041737159402, time: 0.2928924560546875
Test Loss Energy: 9.670259391216081, Test Loss Force: 11.373177713385504, time: 12.221229553222656

Epoch 6, Batch 100/270, Loss: 0.4990231990814209, Variance: 0.11100374162197113
Epoch 6, Batch 200/270, Loss: 0.8990490436553955, Variance: 0.1069602370262146

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5504751464089406, Training Loss Force: 3.192096438433609, time: 4.926519155502319
Validation Loss Energy: 1.8199001580820435, Validation Loss Force: 3.246612162562871, time: 0.2824983596801758
Test Loss Energy: 9.645646378543068, Test Loss Force: 11.401311388728029, time: 12.481730222702026

Epoch 7, Batch 100/270, Loss: 0.557546854019165, Variance: 0.10705415159463882
Epoch 7, Batch 200/270, Loss: 0.7017903327941895, Variance: 0.11133591830730438

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.562406315920616, Training Loss Force: 3.185667787210747, time: 4.866244077682495
Validation Loss Energy: 2.271299036484567, Validation Loss Force: 3.1802860158021646, time: 0.2927823066711426
Test Loss Energy: 9.85566199934559, Test Loss Force: 11.194954839870837, time: 12.264250040054321

Epoch 8, Batch 100/270, Loss: 0.5017825961112976, Variance: 0.11166146397590637
Epoch 8, Batch 200/270, Loss: 0.6348015666007996, Variance: 0.111838698387146

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5788932161620224, Training Loss Force: 3.1882902854474193, time: 4.955419540405273
Validation Loss Energy: 2.087976285717316, Validation Loss Force: 3.206300541188567, time: 0.28243446350097656
Test Loss Energy: 9.83035354546337, Test Loss Force: 11.380302976831892, time: 12.425865173339844

Epoch 9, Batch 100/270, Loss: 0.4643203616142273, Variance: 0.10842057317495346
Epoch 9, Batch 200/270, Loss: 0.6086087226867676, Variance: 0.10600113868713379

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.533286495282437, Training Loss Force: 3.186290303342017, time: 4.798060655593872
Validation Loss Energy: 1.9568811398002914, Validation Loss Force: 3.2127816384175993, time: 0.263108491897583
Test Loss Energy: 9.769173931553256, Test Loss Force: 11.341646223456827, time: 11.064382553100586

Epoch 10, Batch 100/270, Loss: 0.42082762718200684, Variance: 0.10779044032096863
Epoch 10, Batch 200/270, Loss: 0.6518086194992065, Variance: 0.10963627696037292

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5483717299494018, Training Loss Force: 3.176667563834287, time: 4.844645023345947
Validation Loss Energy: 2.1561015101094596, Validation Loss Force: 3.143698001487553, time: 0.3046092987060547
Test Loss Energy: 9.843540856329513, Test Loss Force: 11.20461405488174, time: 11.91758418083191

Epoch 11, Batch 100/270, Loss: 0.6374030113220215, Variance: 0.11399728059768677
Epoch 11, Batch 200/270, Loss: 0.9121932983398438, Variance: 0.11151857674121857

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5638597326910233, Training Loss Force: 3.1895954383793605, time: 4.466552257537842
Validation Loss Energy: 1.8522028824163606, Validation Loss Force: 3.1840213797927617, time: 0.23794913291931152
Test Loss Energy: 9.715557490858885, Test Loss Force: 11.303358849049683, time: 10.213356018066406

Epoch 12, Batch 100/270, Loss: 0.5854809284210205, Variance: 0.11097261309623718
Epoch 12, Batch 200/270, Loss: 0.5199524164199829, Variance: 0.1073431596159935

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5543959657582773, Training Loss Force: 3.1848349641946503, time: 4.413602590560913
Validation Loss Energy: 1.8974099517646308, Validation Loss Force: 3.2045789524199115, time: 0.23283696174621582
Test Loss Energy: 9.974630262861867, Test Loss Force: 11.599077674371612, time: 11.705696821212769

Epoch 13, Batch 100/270, Loss: 0.58106529712677, Variance: 0.11282923817634583
Epoch 13, Batch 200/270, Loss: 0.5446109771728516, Variance: 0.11099358648061752

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5444368354359415, Training Loss Force: 3.1971261138428413, time: 4.353238105773926
Validation Loss Energy: 2.1555547237547197, Validation Loss Force: 3.156281524796053, time: 0.23360538482666016
Test Loss Energy: 9.874055580647534, Test Loss Force: 11.223528038459666, time: 10.206627130508423

Epoch 14, Batch 100/270, Loss: 0.6645056009292603, Variance: 0.10994129627943039
Epoch 14, Batch 200/270, Loss: 0.5659852623939514, Variance: 0.11253183335065842

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5372014798754496, Training Loss Force: 3.1808941903910366, time: 4.381791353225708
Validation Loss Energy: 2.443031323800327, Validation Loss Force: 3.1998339174540193, time: 0.3493664264678955
Test Loss Energy: 9.783325779874367, Test Loss Force: 11.01870683883012, time: 10.240065336227417

Epoch 15, Batch 100/270, Loss: 0.33995556831359863, Variance: 0.10832428932189941
Epoch 15, Batch 200/270, Loss: 0.5838509798049927, Variance: 0.11024140566587448

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5675338049374523, Training Loss Force: 3.186180541472984, time: 4.354943752288818
Validation Loss Energy: 2.087705564569697, Validation Loss Force: 3.168569209877274, time: 0.24463343620300293
Test Loss Energy: 9.808170074675768, Test Loss Force: 11.336102619757213, time: 10.20305585861206

Epoch 16, Batch 100/270, Loss: 0.6277375221252441, Variance: 0.11010293662548065
Epoch 16, Batch 200/270, Loss: 0.6699591875076294, Variance: 0.11204829812049866

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5340792918701522, Training Loss Force: 3.188132354469218, time: 4.405608415603638
Validation Loss Energy: 2.136425529164868, Validation Loss Force: 3.220142545976022, time: 0.2352886199951172
Test Loss Energy: 9.637660772812326, Test Loss Force: 11.390475473369337, time: 10.349587202072144

Epoch 17, Batch 100/270, Loss: 0.8092524409294128, Variance: 0.11203449964523315
Epoch 17, Batch 200/270, Loss: 0.8310050964355469, Variance: 0.11128858476877213

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5554694120634176, Training Loss Force: 3.1864797304585593, time: 4.417996644973755
Validation Loss Energy: 2.3988172029744925, Validation Loss Force: 3.2612048295750773, time: 0.24603652954101562
Test Loss Energy: 10.182070681183328, Test Loss Force: 11.301213801389501, time: 10.16467547416687

Epoch 18, Batch 100/270, Loss: 0.32758235931396484, Variance: 0.10955240577459335
Epoch 18, Batch 200/270, Loss: 0.757014274597168, Variance: 0.10926206409931183

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5417625039455993, Training Loss Force: 3.173148011604624, time: 4.581006050109863
Validation Loss Energy: 1.9761743302161192, Validation Loss Force: 3.1534469465683928, time: 0.23443174362182617
Test Loss Energy: 9.942696280381917, Test Loss Force: 11.387904556394979, time: 10.708631038665771

Epoch 19, Batch 100/270, Loss: 0.5752843022346497, Variance: 0.11176978051662445
Epoch 19, Batch 200/270, Loss: 0.6870094537734985, Variance: 0.10822302103042603

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.553405732562371, Training Loss Force: 3.194195735101597, time: 4.684463262557983
Validation Loss Energy: 2.291175152457519, Validation Loss Force: 3.272734682485048, time: 0.28757405281066895
Test Loss Energy: 9.75164642364311, Test Loss Force: 10.9042030100832, time: 12.418614864349365

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–„â–â–ƒâ–„â–â–â–„â–ƒâ–ƒâ–„â–‚â–…â–„â–ƒâ–ƒâ–â–ˆâ–…â–‚
wandb:   test_error_force â–…â–„â–‡â–…â–†â–†â–†â–„â–†â–…â–„â–…â–ˆâ–„â–‚â–…â–†â–…â–†â–
wandb:          test_loss â–„â–„â–„â–‚â–„â–ƒâ–„â–â–ƒâ–…â–â–ƒâ–ˆâ–…â–„â–…â–‚â–„â–‡â–
wandb: train_error_energy â–ˆâ–â–â–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–
wandb: valid_error_energy â–â–„â–…â–„â–†â–…â–‚â–†â–…â–„â–…â–ƒâ–ƒâ–…â–ˆâ–…â–…â–ˆâ–„â–‡
wandb:  valid_error_force â–‚â–ˆâ–„â–„â–‚â–ƒâ–†â–ƒâ–„â–„â–â–ƒâ–„â–‚â–„â–‚â–„â–†â–â–‡
wandb:         valid_loss â–â–…â–…â–„â–…â–…â–ƒâ–†â–…â–„â–…â–ƒâ–ƒâ–…â–ˆâ–…â–…â–ˆâ–„â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 8614
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.75165
wandb:   test_error_force 10.9042
wandb:          test_loss 11.69011
wandb: train_error_energy 2.55341
wandb:  train_error_force 3.1942
wandb:         train_loss 0.85148
wandb: valid_error_energy 2.29118
wandb:  valid_error_force 3.27273
wandb:         valid_loss 0.75447
wandb: 
wandb: ğŸš€ View run al_71_100 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/htp4xyu5
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_175743-htp4xyu5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.397526741027832, Uncertainty Bias: -0.16154742240905762
0.00017547607 0.003982544
2.0036058 4.6757565
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 3354 steps.
Found uncertainty sample 3 after 3441 steps.
Found uncertainty sample 4 after 1243 steps.
Found uncertainty sample 5 after 855 steps.
Found uncertainty sample 6 after 2021 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 3542 steps.
Found uncertainty sample 12 after 1084 steps.
Found uncertainty sample 13 after 412 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1629 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 680 steps.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 931 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 642 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 11 steps.
Found uncertainty sample 27 after 3389 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 139 steps.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 2164 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 1520 steps.
Found uncertainty sample 35 after 3637 steps.
Found uncertainty sample 36 after 1151 steps.
Found uncertainty sample 37 after 2387 steps.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 28 steps.
Found uncertainty sample 40 after 59 steps.
Found uncertainty sample 41 after 1323 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 987 steps.
Found uncertainty sample 45 after 2482 steps.
Found uncertainty sample 46 after 1797 steps.
Found uncertainty sample 47 after 2302 steps.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 582 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 3950 steps.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 2630 steps.
Found uncertainty sample 56 after 2694 steps.
Found uncertainty sample 57 after 3429 steps.
Did not find any uncertainty samples for sample 58.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 3491 steps.
Did not find any uncertainty samples for sample 64.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 1293 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 1961 steps.
Found uncertainty sample 69 after 1017 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 1600 steps.
Found uncertainty sample 72 after 2294 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 1940 steps.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 1198 steps.
Found uncertainty sample 78 after 427 steps.
Found uncertainty sample 79 after 2194 steps.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 1317 steps.
Found uncertainty sample 83 after 1087 steps.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 742 steps.
Did not find any uncertainty samples for sample 92.
Did not find any uncertainty samples for sample 93.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 771 steps.
Found uncertainty sample 97 after 524 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 2893 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_183400-7s62m8ch
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_101
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/7s62m8ch
Training model 101. Added 48 samples to the dataset.
Epoch 0, Batch 100/271, Loss: 0.2123824954032898, Variance: 0.08805485814809799
Epoch 0, Batch 200/271, Loss: 0.4179984927177429, Variance: 0.0812121033668518

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.792005915720586, Training Loss Force: 3.2782092256138338, time: 4.41242527961731
Validation Loss Energy: 1.4591958033646129, Validation Loss Force: 3.1988259387642337, time: 0.26513171195983887
Test Loss Energy: 9.146778319083818, Test Loss Force: 11.460189373101896, time: 10.814851760864258

Epoch 1, Batch 100/271, Loss: 0.3038710951805115, Variance: 0.07756724953651428
Epoch 1, Batch 200/271, Loss: 0.4856102466583252, Variance: 0.07831721752882004

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5570233474038546, Training Loss Force: 3.192939418768922, time: 4.434125661849976
Validation Loss Energy: 1.7476995761803855, Validation Loss Force: 3.1732885042898964, time: 0.2508223056793213
Test Loss Energy: 9.497872351921819, Test Loss Force: 11.627994436297117, time: 11.057885885238647

Epoch 2, Batch 100/271, Loss: 0.419502317905426, Variance: 0.07782295346260071
Epoch 2, Batch 200/271, Loss: 0.46033352613449097, Variance: 0.07907895743846893

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5604576525206064, Training Loss Force: 3.1984139796455073, time: 4.409947872161865
Validation Loss Energy: 1.8827168832345251, Validation Loss Force: 3.1637556373482654, time: 0.27150607109069824
Test Loss Energy: 9.688786587995265, Test Loss Force: 11.537897458905062, time: 12.265011310577393

Epoch 3, Batch 100/271, Loss: 0.24355536699295044, Variance: 0.07555889338254929
Epoch 3, Batch 200/271, Loss: 0.2760005593299866, Variance: 0.07570718228816986

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5591252478020872, Training Loss Force: 3.2072732717633814, time: 4.524259567260742
Validation Loss Energy: 1.24287637986976, Validation Loss Force: 3.194510833385299, time: 0.2568345069885254
Test Loss Energy: 9.344920796671703, Test Loss Force: 11.733372355461508, time: 11.0861234664917

Epoch 4, Batch 100/271, Loss: 0.485592782497406, Variance: 0.07289890199899673
Epoch 4, Batch 200/271, Loss: 0.31132811307907104, Variance: 0.07542671263217926

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5579469085640443, Training Loss Force: 3.1930086318987243, time: 4.311339616775513
Validation Loss Energy: 1.5452601670961976, Validation Loss Force: 3.2039207869583777, time: 0.24666333198547363
Test Loss Energy: 9.767611223165837, Test Loss Force: 11.864928299321056, time: 10.871367454528809

Epoch 5, Batch 100/271, Loss: 0.47589027881622314, Variance: 0.07440263032913208
Epoch 5, Batch 200/271, Loss: 1.1301772594451904, Variance: 0.07425836473703384

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8737869189698664, Training Loss Force: 3.780972389857837, time: 4.499264240264893
Validation Loss Energy: 2.493705922692281, Validation Loss Force: 3.879374815387517, time: 0.25577425956726074
Test Loss Energy: 9.615126310165548, Test Loss Force: 11.741029516041356, time: 11.02388882637024

Epoch 6, Batch 100/271, Loss: 1.2399890422821045, Variance: 0.10945992916822433
Epoch 6, Batch 200/271, Loss: 0.7496463656425476, Variance: 0.11059708893299103

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5678032961842012, Training Loss Force: 3.2350015801734697, time: 4.346916437149048
Validation Loss Energy: 1.5828447149064948, Validation Loss Force: 3.2296450408461483, time: 0.2521805763244629
Test Loss Energy: 9.342231102612198, Test Loss Force: 11.495667474289139, time: 10.77666187286377

Epoch 7, Batch 100/271, Loss: 0.5262979865074158, Variance: 0.11347860097885132
Epoch 7, Batch 200/271, Loss: 0.8594956398010254, Variance: 0.11123628914356232

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5548592269124484, Training Loss Force: 3.1610462269526347, time: 4.375938415527344
Validation Loss Energy: 2.56819565474865, Validation Loss Force: 3.2042632075709863, time: 0.25319743156433105
Test Loss Energy: 10.234154384057835, Test Loss Force: 11.250306276581224, time: 11.124863624572754

Epoch 8, Batch 100/271, Loss: 0.8317383527755737, Variance: 0.11451806128025055
Epoch 8, Batch 200/271, Loss: 1.4337323904037476, Variance: 0.11895312368869781

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5437238789824623, Training Loss Force: 3.1577951119887357, time: 4.453002691268921
Validation Loss Energy: 3.328004103737051, Validation Loss Force: 3.198273363011699, time: 0.2705881595611572
Test Loss Energy: 10.277523893766443, Test Loss Force: 11.180111383514086, time: 10.957041501998901

Epoch 9, Batch 100/271, Loss: 1.1588507890701294, Variance: 0.10917209088802338
Epoch 9, Batch 200/271, Loss: 0.7266941070556641, Variance: 0.11355487257242203

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5634977992664933, Training Loss Force: 3.1679572996171057, time: 4.403745651245117
Validation Loss Energy: 1.997315095603368, Validation Loss Force: 3.1889455298196383, time: 0.24995946884155273
Test Loss Energy: 9.649755885688661, Test Loss Force: 10.93421041714353, time: 11.088830471038818

Epoch 10, Batch 100/271, Loss: 0.5893812775611877, Variance: 0.11353912949562073
Epoch 10, Batch 200/271, Loss: 0.810309886932373, Variance: 0.11342090368270874

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5556988432425243, Training Loss Force: 3.173997109818087, time: 4.483630657196045
Validation Loss Energy: 2.1450624199971147, Validation Loss Force: 3.229272693500328, time: 0.2509572505950928
Test Loss Energy: 9.49870819826851, Test Loss Force: 11.269685790229053, time: 10.811529159545898

Epoch 11, Batch 100/271, Loss: 0.7436760663986206, Variance: 0.11457479000091553
Epoch 11, Batch 200/271, Loss: 1.0612024068832397, Variance: 0.11168370395898819

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.534734421137304, Training Loss Force: 3.175548886166113, time: 4.492581605911255
Validation Loss Energy: 3.1290540760379537, Validation Loss Force: 3.212987473776095, time: 0.25142884254455566
Test Loss Energy: 9.672430053988691, Test Loss Force: 11.125926733128361, time: 11.18805480003357

Epoch 12, Batch 100/271, Loss: 1.190659999847412, Variance: 0.11424942314624786
Epoch 12, Batch 200/271, Loss: 0.6616451740264893, Variance: 0.1151483952999115

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5683479810680643, Training Loss Force: 3.1756508341253387, time: 4.401666879653931
Validation Loss Energy: 1.5621972284782197, Validation Loss Force: 3.218753669445212, time: 0.2499067783355713
Test Loss Energy: 9.291962046022752, Test Loss Force: 11.271742810130748, time: 10.84112024307251

Epoch 13, Batch 100/271, Loss: 0.5348729491233826, Variance: 0.11363794654607773
Epoch 13, Batch 200/271, Loss: 0.6941276788711548, Variance: 0.11377087235450745

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.552928488630316, Training Loss Force: 3.166666007551919, time: 4.385395050048828
Validation Loss Energy: 2.469274441331939, Validation Loss Force: 3.245950478928832, time: 0.25287652015686035
Test Loss Energy: 10.237444156310127, Test Loss Force: 11.622669845772531, time: 11.091592073440552

Epoch 14, Batch 100/271, Loss: 0.7301154136657715, Variance: 0.11046862602233887
Epoch 14, Batch 200/271, Loss: 1.254246473312378, Variance: 0.1127568781375885

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.547752991314831, Training Loss Force: 3.1795076264209663, time: 4.373179912567139
Validation Loss Energy: 3.6984733243792554, Validation Loss Force: 3.2104098112090056, time: 0.250821590423584
Test Loss Energy: 10.465560876079477, Test Loss Force: 11.21896292459375, time: 10.796559810638428

Epoch 15, Batch 100/271, Loss: 0.9799492359161377, Variance: 0.11071158200502396
Epoch 15, Batch 200/271, Loss: 0.5823505520820618, Variance: 0.11559593677520752

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5522105605776617, Training Loss Force: 3.1719249088415573, time: 4.459032773971558
Validation Loss Energy: 1.9541474084288217, Validation Loss Force: 3.1848377739493916, time: 0.36017918586730957
Test Loss Energy: 9.834001462966874, Test Loss Force: 11.310152454380955, time: 11.035820722579956

Epoch 16, Batch 100/271, Loss: 0.6770500540733337, Variance: 0.11006619036197662
Epoch 16, Batch 200/271, Loss: 0.7062828540802002, Variance: 0.11310245841741562

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5600089003586075, Training Loss Force: 3.17546951406465, time: 4.385730266571045
Validation Loss Energy: 2.2653741667820624, Validation Loss Force: 3.176788017121127, time: 0.250110387802124
Test Loss Energy: 9.39193182360313, Test Loss Force: 11.365321656797025, time: 10.877722978591919

Epoch 17, Batch 100/271, Loss: 0.6745844483375549, Variance: 0.11228592693805695
Epoch 17, Batch 200/271, Loss: 1.0344866514205933, Variance: 0.11007889360189438

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5311432189033822, Training Loss Force: 3.1711088839377477, time: 4.67107892036438
Validation Loss Energy: 3.1555963718340334, Validation Loss Force: 3.1930838804305353, time: 0.2532789707183838
Test Loss Energy: 9.619523822947196, Test Loss Force: 11.302211956056468, time: 10.79294490814209

Epoch 18, Batch 100/271, Loss: 1.4611902236938477, Variance: 0.11296398937702179
Epoch 18, Batch 200/271, Loss: 0.4487053155899048, Variance: 0.11296190321445465

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5578717490106855, Training Loss Force: 3.181733962924263, time: 4.397764682769775
Validation Loss Energy: 1.7815500292142175, Validation Loss Force: 3.203126926919316, time: 0.24861955642700195
Test Loss Energy: 9.232978989791373, Test Loss Force: 11.435183665320032, time: 10.87901759147644

Epoch 19, Batch 100/271, Loss: 0.5926094055175781, Variance: 0.11397098004817963
Epoch 19, Batch 200/271, Loss: 0.9197105765342712, Variance: 0.11299460381269455

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.541324458711904, Training Loss Force: 3.1892421841170124, time: 4.619332313537598
Validation Loss Energy: 2.49936430196094, Validation Loss Force: 3.1885313206743833, time: 0.24867773056030273
Test Loss Energy: 9.728227756973343, Test Loss Force: 11.109265830218025, time: 11.045526266098022

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–ƒâ–„â–‚â–„â–ƒâ–‚â–‡â–‡â–„â–ƒâ–„â–‚â–‡â–ˆâ–…â–‚â–„â–â–„
wandb:   test_error_force â–…â–†â–†â–‡â–ˆâ–‡â–…â–ƒâ–ƒâ–â–„â–‚â–„â–†â–ƒâ–„â–„â–„â–…â–‚
wandb:          test_loss â–†â–‡â–‡â–‡â–ˆâ–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚
wandb: train_error_energy â–ƒâ–â–â–â–â–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  train_error_force â–‚â–â–â–‚â–â–ˆâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–†â–â–â–â–â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡
wandb: valid_error_energy â–‚â–‚â–ƒâ–â–‚â–…â–‚â–…â–‡â–ƒâ–„â–†â–‚â–„â–ˆâ–ƒâ–„â–†â–ƒâ–…
wandb:  valid_error_force â–â–â–â–â–â–ˆâ–‚â–â–â–â–‚â–â–‚â–‚â–â–â–â–â–â–
wandb:         valid_loss â–‚â–ƒâ–ƒâ–â–‚â–†â–ƒâ–…â–‡â–ƒâ–„â–†â–ƒâ–…â–ˆâ–ƒâ–„â–†â–ƒâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 8657
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.72823
wandb:   test_error_force 11.10927
wandb:          test_loss 11.64961
wandb: train_error_energy 2.54132
wandb:  train_error_force 3.18924
wandb:         train_loss 0.84503
wandb: valid_error_energy 2.49936
wandb:  valid_error_force 3.18853
wandb:         valid_loss 0.80544
wandb: 
wandb: ğŸš€ View run al_71_101 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/7s62m8ch
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_183400-7s62m8ch/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.452336311340332, Uncertainty Bias: -0.17570212483406067
7.6293945e-06 0.0018005371
1.863132 4.578059
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 1557 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 799 steps.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 1241 steps.
Found uncertainty sample 9 after 3354 steps.
Found uncertainty sample 10 after 2069 steps.
Found uncertainty sample 11 after 223 steps.
Found uncertainty sample 12 after 2622 steps.
Found uncertainty sample 13 after 5 steps.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 1185 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 1429 steps.
Found uncertainty sample 19 after 1111 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 1841 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 30 steps.
Found uncertainty sample 24 after 1366 steps.
Found uncertainty sample 25 after 890 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 2309 steps.
Found uncertainty sample 28 after 1681 steps.
Found uncertainty sample 29 after 678 steps.
Found uncertainty sample 30 after 2075 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 619 steps.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 749 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 1770 steps.
Found uncertainty sample 37 after 2374 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 137 steps.
Found uncertainty sample 41 after 2813 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1142 steps.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 3746 steps.
Found uncertainty sample 47 after 1531 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 820 steps.
Did not find any uncertainty samples for sample 50.
Found uncertainty sample 51 after 387 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 3944 steps.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 3578 steps.
Found uncertainty sample 59 after 676 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 565 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 209 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 1679 steps.
Found uncertainty sample 69 after 729 steps.
Did not find any uncertainty samples for sample 70.
Did not find any uncertainty samples for sample 71.
Found uncertainty sample 72 after 971 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 895 steps.
Found uncertainty sample 75 after 63 steps.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 2349 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 1856 steps.
Did not find any uncertainty samples for sample 81.
Did not find any uncertainty samples for sample 82.
Found uncertainty sample 83 after 296 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 2916 steps.
Found uncertainty sample 86 after 2151 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 2264 steps.
Found uncertainty sample 89 after 1369 steps.
Found uncertainty sample 90 after 2538 steps.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 3371 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 3651 steps.
Found uncertainty sample 95 after 3102 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 2216 steps.
Found uncertainty sample 99 after 2874 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_190900-o9gsymts
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_102
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/o9gsymts
Training model 102. Added 53 samples to the dataset.
Epoch 0, Batch 100/272, Loss: 0.278606116771698, Variance: 0.08892136812210083
Epoch 0, Batch 200/272, Loss: 0.6589474678039551, Variance: 0.0809321403503418

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8220448709890884, Training Loss Force: 3.23159342796357, time: 4.59969687461853
Validation Loss Energy: 1.2195326372872746, Validation Loss Force: 3.2360761265850058, time: 0.27882885932922363
Test Loss Energy: 9.047572338643027, Test Loss Force: 11.501815982622714, time: 10.884723424911499

Epoch 1, Batch 100/272, Loss: 0.47480106353759766, Variance: 0.07907954603433609
Epoch 1, Batch 200/272, Loss: 0.2657635807991028, Variance: 0.0768512636423111

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5637692111279318, Training Loss Force: 3.1790016804706402, time: 4.481604337692261
Validation Loss Energy: 1.1944516522951751, Validation Loss Force: 3.1646407472033284, time: 0.2516357898712158
Test Loss Energy: 9.313837048929013, Test Loss Force: 11.538021673314283, time: 10.940021276473999

Epoch 2, Batch 100/272, Loss: 0.5466324687004089, Variance: 0.07756276428699493
Epoch 2, Batch 200/272, Loss: 0.5276936888694763, Variance: 0.07675443589687347

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5426163108862074, Training Loss Force: 3.180671025327205, time: 4.3353962898254395
Validation Loss Energy: 1.5133086365388377, Validation Loss Force: 3.1912636937640633, time: 0.2470254898071289
Test Loss Energy: 9.507757910702464, Test Loss Force: 11.958820193158749, time: 10.765179872512817

Epoch 3, Batch 100/272, Loss: 0.3528425693511963, Variance: 0.07474256306886673
Epoch 3, Batch 200/272, Loss: 0.30880481004714966, Variance: 0.07539461553096771

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.562417963677124, Training Loss Force: 3.2019367040121365, time: 4.449950933456421
Validation Loss Energy: 1.22892337451306, Validation Loss Force: 3.2080401661580424, time: 0.2476341724395752
Test Loss Energy: 9.291866323937757, Test Loss Force: 11.679556202693572, time: 10.99458909034729

Epoch 4, Batch 100/272, Loss: 0.44657111167907715, Variance: 0.07401420176029205
Epoch 4, Batch 200/272, Loss: 0.3293348550796509, Variance: 0.07304464280605316

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5570035233287172, Training Loss Force: 3.1885181226156676, time: 4.439098358154297
Validation Loss Energy: 1.4464651402844833, Validation Loss Force: 3.215338325551142, time: 0.2473890781402588
Test Loss Energy: 9.395547536852776, Test Loss Force: 11.679160555761278, time: 10.793503284454346

Epoch 5, Batch 100/272, Loss: 0.34463465213775635, Variance: 0.0780390053987503
Epoch 5, Batch 200/272, Loss: 0.4236322045326233, Variance: 0.0789218619465828

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5637011963461263, Training Loss Force: 3.1914598014279734, time: 4.432598114013672
Validation Loss Energy: 1.8580476580270755, Validation Loss Force: 4.755278548213576, time: 0.2477128505706787
Test Loss Energy: 9.351033540376696, Test Loss Force: 12.283847957896711, time: 10.936462879180908

Epoch 6, Batch 100/272, Loss: 0.2793034315109253, Variance: 0.07704096287488937
Epoch 6, Batch 200/272, Loss: 0.4919624328613281, Variance: 0.07449164986610413

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.5734005757253324, Training Loss Force: 3.231884063591746, time: 4.40781307220459
Validation Loss Energy: 1.3053814572856015, Validation Loss Force: 3.1785432734646464, time: 0.24762964248657227
Test Loss Energy: 9.224771073983273, Test Loss Force: 11.488991130493854, time: 10.872394323348999

Epoch 7, Batch 100/272, Loss: 0.286798894405365, Variance: 0.0747620016336441
Epoch 7, Batch 200/272, Loss: 0.5141893029212952, Variance: 0.07687126845121384

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5508572734644892, Training Loss Force: 3.180648915590833, time: 4.417012929916382
Validation Loss Energy: 1.2235926306257197, Validation Loss Force: 3.2393989291820278, time: 0.2475297451019287
Test Loss Energy: 9.08350471286733, Test Loss Force: 11.40618698032663, time: 10.929405450820923

Epoch 8, Batch 100/272, Loss: 0.43841177225112915, Variance: 0.07709021866321564
Epoch 8, Batch 200/272, Loss: 0.253418505191803, Variance: 0.0756736546754837

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.5604816516127773, Training Loss Force: 3.1826079513689765, time: 4.526345252990723
Validation Loss Energy: 1.4560754515550685, Validation Loss Force: 3.250018822360249, time: 0.2540926933288574
Test Loss Energy: 9.168678953423807, Test Loss Force: 11.281646322339913, time: 10.798322677612305

Epoch 9, Batch 100/272, Loss: 0.11619222164154053, Variance: 0.07406584918498993
Epoch 9, Batch 200/272, Loss: 0.6290467977523804, Variance: 0.0762682855129242

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.5523281587142899, Training Loss Force: 3.18677887438274, time: 4.462657690048218
Validation Loss Energy: 1.353423661534397, Validation Loss Force: 3.1929072147301563, time: 0.2545590400695801
Test Loss Energy: 9.243278540643999, Test Loss Force: 11.46766146414469, time: 11.014072179794312

Epoch 10, Batch 100/272, Loss: 0.32566332817077637, Variance: 0.074417345225811
Epoch 10, Batch 200/272, Loss: 0.506964921951294, Variance: 0.07589314877986908

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.5494248739569252, Training Loss Force: 3.177804756859441, time: 4.429767370223999
Validation Loss Energy: 1.6620550996572763, Validation Loss Force: 3.278286714969611, time: 0.24697160720825195
Test Loss Energy: 9.340634459913263, Test Loss Force: 11.43965033843892, time: 10.785802841186523

Epoch 11, Batch 100/272, Loss: 0.22844892740249634, Variance: 0.07467690110206604
Epoch 11, Batch 200/272, Loss: 0.37298840284347534, Variance: 0.07693862915039062

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.5470536951903553, Training Loss Force: 3.2169306131301214, time: 4.534498691558838
Validation Loss Energy: 1.3247378412050435, Validation Loss Force: 3.188331274713682, time: 0.2437901496887207
Test Loss Energy: 9.406446429108131, Test Loss Force: 11.872832415139229, time: 11.003092765808105

Epoch 12, Batch 100/272, Loss: 0.40543460845947266, Variance: 0.07412904500961304
Epoch 12, Batch 200/272, Loss: 0.46939408779144287, Variance: 0.07538200169801712

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.5590457331959346, Training Loss Force: 3.177769818269445, time: 4.418459892272949
Validation Loss Energy: 1.5366356566684642, Validation Loss Force: 3.1770243136187855, time: 0.25076889991760254
Test Loss Energy: 9.760894822240427, Test Loss Force: 12.09600214928643, time: 10.695482730865479

Epoch 13, Batch 100/272, Loss: 0.28452104330062866, Variance: 0.07158063352108002
Epoch 13, Batch 200/272, Loss: 0.49756819009780884, Variance: 0.07435048371553421

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.5412699457838788, Training Loss Force: 3.187894096427388, time: 4.386043071746826
Validation Loss Energy: 1.3599250439226236, Validation Loss Force: 3.264227206960414, time: 0.24786090850830078
Test Loss Energy: 9.379103282860775, Test Loss Force: 11.87750448014361, time: 11.037147998809814

Epoch 14, Batch 100/272, Loss: 0.49110686779022217, Variance: 0.07681456208229065
Epoch 14, Batch 200/272, Loss: 0.614776074886322, Variance: 0.07692701369524002

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.5567965824424712, Training Loss Force: 3.17268982963879, time: 4.458817005157471
Validation Loss Energy: 1.220471633075652, Validation Loss Force: 3.2037938655451996, time: 0.25476598739624023
Test Loss Energy: 9.470413240515912, Test Loss Force: 11.721249258552824, time: 10.812392711639404

Epoch 15, Batch 100/272, Loss: 0.1223069429397583, Variance: 0.07134290039539337
Epoch 15, Batch 200/272, Loss: 0.21422165632247925, Variance: 0.07396546006202698

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5425333408422974, Training Loss Force: 3.1749074432371884, time: 4.533111333847046
Validation Loss Energy: 1.4190985455770935, Validation Loss Force: 3.1629014339706574, time: 0.24947166442871094
Test Loss Energy: 9.4496830018392, Test Loss Force: 11.693677338993844, time: 11.06110429763794

Epoch 16, Batch 100/272, Loss: 0.2912493944168091, Variance: 0.07492047548294067
Epoch 16, Batch 200/272, Loss: 0.16529804468154907, Variance: 0.07408156245946884

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.5549395594227153, Training Loss Force: 3.1696492087738553, time: 4.311168432235718
Validation Loss Energy: 1.2137457111206718, Validation Loss Force: 3.174988494151397, time: 0.253709077835083
Test Loss Energy: 9.06399519541249, Test Loss Force: 11.393322411483307, time: 10.95424747467041

Epoch 17, Batch 100/272, Loss: 0.8041157722473145, Variance: 0.07562112808227539
Epoch 17, Batch 200/272, Loss: 0.46551764011383057, Variance: 0.07016031444072723

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.5628198575404044, Training Loss Force: 3.193409028302784, time: 4.492162466049194
Validation Loss Energy: 1.4835907892519378, Validation Loss Force: 3.1957536389563805, time: 0.24902892112731934
Test Loss Energy: 9.451605804707215, Test Loss Force: 11.692580505755217, time: 12.347322940826416

Epoch 18, Batch 100/272, Loss: 0.2530779242515564, Variance: 0.07269500195980072
Epoch 18, Batch 200/272, Loss: 0.15446025133132935, Variance: 0.07387733459472656

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.5614225183950259, Training Loss Force: 3.1825277176177877, time: 4.524477005004883
Validation Loss Energy: 1.2869520672875712, Validation Loss Force: 3.163686164574736, time: 0.25104594230651855
Test Loss Energy: 9.06065440511222, Test Loss Force: 11.363772961410305, time: 10.832409858703613

Epoch 19, Batch 100/272, Loss: 0.272391676902771, Variance: 0.07452283799648285
Epoch 19, Batch 200/272, Loss: 0.3586576581001282, Variance: 0.07496120035648346

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.570742625295178, Training Loss Force: 3.1798618521999957, time: 4.6615660190582275
Validation Loss Energy: 1.2681697689601603, Validation Loss Force: 3.2003360363780873, time: 0.24364495277404785
Test Loss Energy: 9.08774987798411, Test Loss Force: 11.35789581565139, time: 10.736046314239502

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–„â–†â–ƒâ–„â–„â–ƒâ–â–‚â–ƒâ–„â–…â–ˆâ–„â–…â–…â–â–…â–â–
wandb:   test_error_force â–ƒâ–ƒâ–†â–„â–„â–ˆâ–‚â–‚â–â–‚â–‚â–…â–‡â–…â–„â–„â–‚â–„â–‚â–‚
wandb:          test_loss â–â–…â–‡â–ˆâ–ˆâ–†â–†â–…â–‚â–„â–‡â–ˆâ–ˆâ–‡â–†â–†â–„â–‡â–ƒâ–ƒ
wandb: train_error_energy â–ˆâ–‚â–â–‚â–â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚
wandb:  train_error_force â–ˆâ–‚â–‚â–…â–ƒâ–ƒâ–ˆâ–‚â–‚â–ƒâ–‚â–†â–‚â–ƒâ–â–‚â–â–„â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–â–â–„â–â–„â–ˆâ–‚â–â–„â–ƒâ–†â–‚â–…â–ƒâ–â–ƒâ–â–„â–‚â–‚
wandb:  valid_error_force â–â–â–â–â–â–ˆâ–â–â–â–â–‚â–â–â–â–â–â–â–â–â–
wandb:         valid_loss â–â–â–‚â–â–‚â–ˆâ–â–â–‚â–‚â–ƒâ–â–‚â–‚â–â–‚â–â–‚â–â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 8704
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.08775
wandb:   test_error_force 11.3579
wandb:          test_loss 15.29654
wandb: train_error_energy 1.57074
wandb:  train_error_force 3.17986
wandb:         train_loss 0.39805
wandb: valid_error_energy 1.26817
wandb:  valid_error_force 3.20034
wandb:         valid_loss 0.25131
wandb: 
wandb: ğŸš€ View run al_71_102 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/o9gsymts
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_190900-o9gsymts/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.918957233428955, Uncertainty Bias: -0.07889877259731293
0.0 0.00023841858
1.8774838 4.753295
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 2971 steps.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 2335 steps.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 127 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 170 steps.
Found uncertainty sample 14 after 526 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 201 steps.
Found uncertainty sample 17 after 2068 steps.
Found uncertainty sample 18 after 1119 steps.
Found uncertainty sample 19 after 2134 steps.
Found uncertainty sample 20 after 2389 steps.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 1227 steps.
Found uncertainty sample 24 after 3564 steps.
Found uncertainty sample 25 after 1014 steps.
Found uncertainty sample 26 after 3144 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 2461 steps.
Found uncertainty sample 29 after 398 steps.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 490 steps.
Found uncertainty sample 33 after 2871 steps.
Found uncertainty sample 34 after 3441 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 3281 steps.
Found uncertainty sample 37 after 3226 steps.
Found uncertainty sample 38 after 3271 steps.
Found uncertainty sample 39 after 1575 steps.
Found uncertainty sample 40 after 663 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 739 steps.
Found uncertainty sample 43 after 95 steps.
Found uncertainty sample 44 after 419 steps.
Found uncertainty sample 45 after 1168 steps.
Found uncertainty sample 46 after 583 steps.
Did not find any uncertainty samples for sample 47.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 2373 steps.
Did not find any uncertainty samples for sample 50.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 1838 steps.
Found uncertainty sample 53 after 1864 steps.
Found uncertainty sample 54 after 3423 steps.
Found uncertainty sample 55 after 3512 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 968 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 1088 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 2756 steps.
Found uncertainty sample 63 after 2543 steps.
Found uncertainty sample 64 after 1612 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 1296 steps.
Found uncertainty sample 68 after 361 steps.
Did not find any uncertainty samples for sample 69.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 3576 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 3185 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 731 steps.
Found uncertainty sample 76 after 324 steps.
Found uncertainty sample 77 after 3205 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 2910 steps.
Found uncertainty sample 80 after 2212 steps.
Found uncertainty sample 81 after 3920 steps.
Found uncertainty sample 82 after 1185 steps.
Did not find any uncertainty samples for sample 83.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 3332 steps.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 1852 steps.
Found uncertainty sample 90 after 296 steps.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 3736 steps.
Found uncertainty sample 95 after 1455 steps.
Found uncertainty sample 96 after 1839 steps.
Found uncertainty sample 97 after 875 steps.
Found uncertainty sample 98 after 3946 steps.
Found uncertainty sample 99 after 443 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_194310-avp3shti
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_103
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/avp3shti
Training model 103. Added 60 samples to the dataset.
Epoch 0, Batch 100/274, Loss: 0.6947447657585144, Variance: 0.08235052227973938
Epoch 0, Batch 200/274, Loss: 0.5864834189414978, Variance: 0.0752279981970787

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.594909015303428, Training Loss Force: 3.5896064775116714, time: 4.434312105178833
Validation Loss Energy: 1.4621664324404353, Validation Loss Force: 3.1976728688589615, time: 0.2593069076538086
Test Loss Energy: 8.855641029185861, Test Loss Force: 10.852048598679694, time: 10.833378076553345

Epoch 1, Batch 100/274, Loss: 0.3954904079437256, Variance: 0.07177165895700455
Epoch 1, Batch 200/274, Loss: 0.38489460945129395, Variance: 0.07333646714687347

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5195087539520138, Training Loss Force: 3.146051566413004, time: 4.507228374481201
Validation Loss Energy: 1.4864119850639284, Validation Loss Force: 3.218449592459533, time: 0.2494189739227295
Test Loss Energy: 9.268941336914848, Test Loss Force: 11.129644421852003, time: 10.905553579330444

Epoch 2, Batch 100/274, Loss: 0.7034770846366882, Variance: 0.0749916210770607
Epoch 2, Batch 200/274, Loss: 0.46203339099884033, Variance: 0.07499122619628906

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5565803787118648, Training Loss Force: 3.1460192940147444, time: 4.384417533874512
Validation Loss Energy: 1.224222851109439, Validation Loss Force: 3.1666611134319895, time: 0.2524697780609131
Test Loss Energy: 8.971779086250905, Test Loss Force: 11.238773866123754, time: 10.776617050170898

Epoch 3, Batch 100/274, Loss: 0.25510334968566895, Variance: 0.07035735249519348
Epoch 3, Batch 200/274, Loss: 0.3146839737892151, Variance: 0.07551617920398712

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5557672031808534, Training Loss Force: 3.1494024412505706, time: 4.427626609802246
Validation Loss Energy: 1.6458616641549948, Validation Loss Force: 3.139174210585055, time: 0.24938035011291504
Test Loss Energy: 9.46901313889537, Test Loss Force: 11.249517981172358, time: 10.928912162780762

Epoch 4, Batch 100/274, Loss: 0.2905123233795166, Variance: 0.07741342484951019
Epoch 4, Batch 200/274, Loss: 0.4511164426803589, Variance: 0.07699352502822876

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5651998915987964, Training Loss Force: 3.175537786915553, time: 4.493465423583984
Validation Loss Energy: 1.8841999933478921, Validation Loss Force: 3.342988891115237, time: 0.2548353672027588
Test Loss Energy: 9.267363377439644, Test Loss Force: 11.313602865517828, time: 10.732868671417236

Epoch 5, Batch 100/274, Loss: 0.522899329662323, Variance: 0.10178006440401077
Epoch 5, Batch 200/274, Loss: 1.021754503250122, Variance: 0.10984215885400772

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.500035369067827, Training Loss Force: 3.342769828559344, time: 4.448441028594971
Validation Loss Energy: 2.3188569798747993, Validation Loss Force: 3.1635924208643003, time: 0.24990582466125488
Test Loss Energy: 9.42210221981505, Test Loss Force: 11.456487772515505, time: 10.86038851737976

Epoch 6, Batch 100/274, Loss: 0.768510639667511, Variance: 0.11117782443761826
Epoch 6, Batch 200/274, Loss: 1.1042300462722778, Variance: 0.10642333328723907

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5750981259496717, Training Loss Force: 3.1751895144927773, time: 4.55236029624939
Validation Loss Energy: 3.64508171656869, Validation Loss Force: 3.19236461455536, time: 0.24393439292907715
Test Loss Energy: 10.548851512864077, Test Loss Force: 11.080687693270548, time: 10.690734386444092

Epoch 7, Batch 100/274, Loss: 1.2690690755844116, Variance: 0.10726848244667053
Epoch 7, Batch 200/274, Loss: 0.6584501266479492, Variance: 0.11070463061332703

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5715731868212255, Training Loss Force: 3.1681874312741316, time: 4.450042724609375
Validation Loss Energy: 1.5224911335793518, Validation Loss Force: 3.1886380237291116, time: 0.2450087070465088
Test Loss Energy: 9.388135312333446, Test Loss Force: 11.394336761541762, time: 10.903401613235474

Epoch 8, Batch 100/274, Loss: 0.5666564702987671, Variance: 0.11079344153404236
Epoch 8, Batch 200/274, Loss: 0.8711462020874023, Variance: 0.11188400536775589

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.581951847316901, Training Loss Force: 3.1546633622699725, time: 4.531789302825928
Validation Loss Energy: 2.057091318102035, Validation Loss Force: 3.185736911960648, time: 0.2539341449737549
Test Loss Energy: 9.303683977068584, Test Loss Force: 11.24836836485388, time: 10.895782470703125

Epoch 9, Batch 100/274, Loss: 0.5545041561126709, Variance: 0.10902701318264008
Epoch 9, Batch 200/274, Loss: 0.9909154772758484, Variance: 0.10629326105117798

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5517252161470525, Training Loss Force: 3.168387527944887, time: 4.549320697784424
Validation Loss Energy: 3.77175374030409, Validation Loss Force: 3.1662820860117913, time: 0.24988341331481934
Test Loss Energy: 10.59595003827759, Test Loss Force: 11.11987390491073, time: 12.268004655838013

Epoch 10, Batch 100/274, Loss: 0.9864842891693115, Variance: 0.11076831072568893
Epoch 10, Batch 200/274, Loss: 0.5938175916671753, Variance: 0.11113925278186798

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.6004230359705964, Training Loss Force: 3.1664892117136074, time: 4.54193639755249
Validation Loss Energy: 1.657294878655632, Validation Loss Force: 3.1507522045643745, time: 0.24927878379821777
Test Loss Energy: 9.069705747344427, Test Loss Force: 10.904183359239623, time: 10.801568984985352

Epoch 11, Batch 100/274, Loss: 0.4685029983520508, Variance: 0.11082558333873749
Epoch 11, Batch 200/274, Loss: 0.7514268159866333, Variance: 0.11136055737733841

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.552688116768503, Training Loss Force: 3.159244979862071, time: 4.479705333709717
Validation Loss Energy: 2.134167040551551, Validation Loss Force: 3.1295231063269355, time: 0.24734926223754883
Test Loss Energy: 9.185707542550235, Test Loss Force: 11.146073492373022, time: 10.910313367843628

Epoch 12, Batch 100/274, Loss: 0.6612555980682373, Variance: 0.10908664017915726
Epoch 12, Batch 200/274, Loss: 1.2028634548187256, Variance: 0.10733223706483841

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5690399384596057, Training Loss Force: 3.1570355363223306, time: 4.501527547836304
Validation Loss Energy: 3.7637666871073185, Validation Loss Force: 3.1905155699675753, time: 0.24706792831420898
Test Loss Energy: 10.343458177105044, Test Loss Force: 10.840413900311942, time: 10.7082040309906

Epoch 13, Batch 100/274, Loss: 0.9549418091773987, Variance: 0.1057104542851448
Epoch 13, Batch 200/274, Loss: 0.684302806854248, Variance: 0.11138284206390381

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.578609686238156, Training Loss Force: 3.170028853100643, time: 4.5406224727630615
Validation Loss Energy: 1.5453219132503635, Validation Loss Force: 3.2377252142797466, time: 0.25620365142822266
Test Loss Energy: 8.999931400516491, Test Loss Force: 11.008162207356898, time: 10.946641206741333

Epoch 14, Batch 100/274, Loss: 0.7310366630554199, Variance: 0.11699409037828445
Epoch 14, Batch 200/274, Loss: 0.8468379378318787, Variance: 0.1098744198679924

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5528597938673525, Training Loss Force: 3.1619641602111854, time: 4.55012845993042
Validation Loss Energy: 2.154295128930035, Validation Loss Force: 3.1969296437836836, time: 0.24644160270690918
Test Loss Energy: 9.348276192202654, Test Loss Force: 10.954055344280558, time: 10.946175575256348

Epoch 15, Batch 100/274, Loss: 0.7969573736190796, Variance: 0.11365514993667603
Epoch 15, Batch 200/274, Loss: 0.9674957990646362, Variance: 0.1102704256772995

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5627856850979227, Training Loss Force: 3.1625910941308177, time: 4.52166485786438
Validation Loss Energy: 3.58452020679308, Validation Loss Force: 3.211704142420455, time: 0.26526761054992676
Test Loss Energy: 10.474316175601848, Test Loss Force: 11.254456400444324, time: 11.017401695251465

Epoch 16, Batch 100/274, Loss: 1.3746427297592163, Variance: 0.11001824587583542
Epoch 16, Batch 200/274, Loss: 0.5040844678878784, Variance: 0.11415305733680725

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.562659541807763, Training Loss Force: 3.1775370096523488, time: 4.492077112197876
Validation Loss Energy: 1.7316589807798686, Validation Loss Force: 3.12423805414834, time: 0.2502021789550781
Test Loss Energy: 9.263022935576162, Test Loss Force: 11.070804433461399, time: 10.73776912689209

Epoch 17, Batch 100/274, Loss: 0.6478096842765808, Variance: 0.11061977595090866
Epoch 17, Batch 200/274, Loss: 0.8566726446151733, Variance: 0.11220717430114746

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5490284802789853, Training Loss Force: 3.1539601349120603, time: 4.530273914337158
Validation Loss Energy: 2.0800470819646564, Validation Loss Force: 3.1419669270132813, time: 0.3585817813873291
Test Loss Energy: 9.113171830177173, Test Loss Force: 11.126615846336922, time: 10.74205756187439

Epoch 18, Batch 100/274, Loss: 0.7480769157409668, Variance: 0.11119119822978973
Epoch 18, Batch 200/274, Loss: 1.3250482082366943, Variance: 0.1090739443898201

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5635234704634846, Training Loss Force: 3.169553619444294, time: 4.54925012588501
Validation Loss Energy: 3.7743646251077516, Validation Loss Force: 3.1615860382580037, time: 0.24422621726989746
Test Loss Energy: 10.395163146575893, Test Loss Force: 10.925149983947971, time: 10.778331518173218

Epoch 19, Batch 100/274, Loss: 1.3454500436782837, Variance: 0.10946240276098251
Epoch 19, Batch 200/274, Loss: 0.7527087926864624, Variance: 0.11085419356822968

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5404934808975277, Training Loss Force: 3.1682108838612937, time: 4.670300245285034
Validation Loss Energy: 1.7884941669322068, Validation Loss Force: 3.2134927100682047, time: 0.27071690559387207
Test Loss Energy: 9.336576838176091, Test Loss Force: 11.032985261153911, time: 10.852023601531982

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–ƒâ–â–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–ˆâ–‚â–‚â–‡â–‚â–ƒâ–ˆâ–ƒâ–‚â–‡â–ƒ
wandb:   test_error_force â–â–„â–†â–†â–†â–ˆâ–„â–‡â–†â–„â–‚â–„â–â–ƒâ–‚â–†â–„â–„â–‚â–ƒ
wandb:          test_loss â–†â–‡â–‡â–ˆâ–ˆâ–‚â–ƒâ–‚â–‚â–ƒâ–â–‚â–ƒâ–â–‚â–„â–‚â–‚â–ƒâ–‚
wandb: train_error_energy â–ˆâ–â–â–â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:  train_error_force â–ˆâ–â–â–â–â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–â–â–â–â–„â–„â–„â–„â–ƒâ–„â–ƒâ–„â–„â–ƒâ–„â–„â–ƒâ–„â–ƒ
wandb: valid_error_energy â–‚â–‚â–â–‚â–ƒâ–„â–ˆâ–‚â–ƒâ–ˆâ–‚â–ƒâ–ˆâ–‚â–„â–‡â–‚â–ƒâ–ˆâ–ƒ
wandb:  valid_error_force â–ƒâ–„â–‚â–â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–â–ƒâ–…â–ƒâ–„â–â–‚â–‚â–„
wandb:         valid_loss â–‚â–‚â–â–‚â–„â–„â–ˆâ–ƒâ–„â–ˆâ–ƒâ–„â–ˆâ–ƒâ–„â–‡â–ƒâ–„â–ˆâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 8758
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.33658
wandb:   test_error_force 11.03299
wandb:          test_loss 11.73444
wandb: train_error_energy 2.54049
wandb:  train_error_force 3.16821
wandb:         train_loss 0.83575
wandb: valid_error_energy 1.78849
wandb:  valid_error_force 3.21349
wandb:         valid_loss 0.57519
wandb: 
wandb: ğŸš€ View run al_71_103 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/avp3shti
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_194310-avp3shti/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.297062873840332, Uncertainty Bias: -0.14454542100429535
2.3841858e-05 0.0020484924
1.9099902 4.617336
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 735 steps.
Found uncertainty sample 3 after 3610 steps.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Found uncertainty sample 6 after 860 steps.
Found uncertainty sample 7 after 838 steps.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 1206 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 1070 steps.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1501 steps.
Found uncertainty sample 16 after 1564 steps.
Found uncertainty sample 17 after 1349 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 3034 steps.
Found uncertainty sample 20 after 2350 steps.
Found uncertainty sample 21 after 3444 steps.
Found uncertainty sample 22 after 65 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 872 steps.
Found uncertainty sample 25 after 1146 steps.
Found uncertainty sample 26 after 1228 steps.
Did not find any uncertainty samples for sample 27.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 2535 steps.
Found uncertainty sample 31 after 1620 steps.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 1740 steps.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 17 steps.
Found uncertainty sample 37 after 3395 steps.
Found uncertainty sample 38 after 3077 steps.
Found uncertainty sample 39 after 1124 steps.
Found uncertainty sample 40 after 2361 steps.
Did not find any uncertainty samples for sample 41.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1832 steps.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 1203 steps.
Found uncertainty sample 47 after 814 steps.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 1714 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 3714 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 3672 steps.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 2774 steps.
Found uncertainty sample 60 after 2690 steps.
Found uncertainty sample 61 after 1726 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 645 steps.
Found uncertainty sample 64 after 768 steps.
Found uncertainty sample 65 after 2068 steps.
Found uncertainty sample 66 after 3676 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 2017 steps.
Found uncertainty sample 69 after 2593 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 2986 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 3142 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 3220 steps.
Found uncertainty sample 76 after 734 steps.
Found uncertainty sample 77 after 2986 steps.
Did not find any uncertainty samples for sample 78.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 608 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 658 steps.
Found uncertainty sample 83 after 168 steps.
Found uncertainty sample 84 after 548 steps.
Found uncertainty sample 85 after 1969 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 3550 steps.
Found uncertainty sample 88 after 3698 steps.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 26 steps.
Found uncertainty sample 94 after 352 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 2180 steps.
Found uncertainty sample 97 after 1590 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 1295 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_201756-zwileun7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_104
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zwileun7
Training model 104. Added 57 samples to the dataset.
Epoch 0, Batch 100/276, Loss: 1.869345784187317, Variance: 0.09706717729568481
Epoch 0, Batch 200/276, Loss: 0.4347460865974426, Variance: 0.10613024234771729

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.8122324046893725, Training Loss Force: 3.4816832673775693, time: 4.484763860702515
Validation Loss Energy: 3.0381469478523293, Validation Loss Force: 3.158267733577729, time: 0.2463524341583252
Test Loss Energy: 9.266509997354973, Test Loss Force: 10.785524384332843, time: 11.964904308319092

Epoch 1, Batch 100/276, Loss: 1.6264508962631226, Variance: 0.11423347890377045
Epoch 1, Batch 200/276, Loss: 0.6627375483512878, Variance: 0.10691840946674347

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5335217751663612, Training Loss Force: 3.139890468006444, time: 4.547245025634766
Validation Loss Energy: 3.110690284370314, Validation Loss Force: 3.2020965031796207, time: 0.24553704261779785
Test Loss Energy: 9.163201931296246, Test Loss Force: 10.781814991492151, time: 10.718704223632812

Epoch 2, Batch 100/276, Loss: 1.355310082435608, Variance: 0.11271888762712479
Epoch 2, Batch 200/276, Loss: 0.5659001469612122, Variance: 0.11193065345287323

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5534182528611957, Training Loss Force: 3.144711576111143, time: 4.483969688415527
Validation Loss Energy: 3.319676767551473, Validation Loss Force: 3.161869629682257, time: 0.2513430118560791
Test Loss Energy: 9.607214765824871, Test Loss Force: 10.900858743346319, time: 10.85812520980835

Epoch 3, Batch 100/276, Loss: 1.172263741493225, Variance: 0.11378289759159088
Epoch 3, Batch 200/276, Loss: 0.48345947265625, Variance: 0.11000962555408478

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5236794245740213, Training Loss Force: 3.1522320191743813, time: 4.570310831069946
Validation Loss Energy: 3.018587511938715, Validation Loss Force: 3.141553697612268, time: 0.24892139434814453
Test Loss Energy: 9.541518894690558, Test Loss Force: 11.042710083122177, time: 10.7375648021698

Epoch 4, Batch 100/276, Loss: 1.5163226127624512, Variance: 0.11316773295402527
Epoch 4, Batch 200/276, Loss: 0.5022456049919128, Variance: 0.10959116369485855

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.546325663317838, Training Loss Force: 3.155904582075395, time: 4.5208659172058105
Validation Loss Energy: 3.343036052440344, Validation Loss Force: 3.248848295193936, time: 0.24475550651550293
Test Loss Energy: 9.552614505226982, Test Loss Force: 11.188021813343228, time: 10.771939516067505

Epoch 5, Batch 100/276, Loss: 1.3590362071990967, Variance: 0.1169622540473938
Epoch 5, Batch 200/276, Loss: 0.5157597064971924, Variance: 0.10832075774669647

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5471514491911154, Training Loss Force: 3.162068494044295, time: 4.481705188751221
Validation Loss Energy: 3.0556230021689843, Validation Loss Force: 3.1848716196595728, time: 0.24993181228637695
Test Loss Energy: 9.555759838649408, Test Loss Force: 11.037528280644676, time: 10.812111616134644

Epoch 6, Batch 100/276, Loss: 1.2364717721939087, Variance: 0.11357112228870392
Epoch 6, Batch 200/276, Loss: 0.6532210111618042, Variance: 0.11134542524814606

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5501516339655845, Training Loss Force: 3.166993924257406, time: 4.524963855743408
Validation Loss Energy: 3.2495549134616235, Validation Loss Force: 3.1646844348897596, time: 0.24560260772705078
Test Loss Energy: 9.671602419586842, Test Loss Force: 11.026228532631137, time: 10.6710786819458

Epoch 7, Batch 100/276, Loss: 1.0657740831375122, Variance: 0.11236467957496643
Epoch 7, Batch 200/276, Loss: 0.6950467824935913, Variance: 0.11035594344139099

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5169205011153606, Training Loss Force: 3.163715560774505, time: 4.6068761348724365
Validation Loss Energy: 3.345230635008779, Validation Loss Force: 3.324524542061295, time: 0.2479403018951416
Test Loss Energy: 9.459084603803756, Test Loss Force: 10.964761500651722, time: 10.787885904312134

Epoch 8, Batch 100/276, Loss: 1.3175148963928223, Variance: 0.1136016696691513
Epoch 8, Batch 200/276, Loss: 0.6401834487915039, Variance: 0.10960257053375244

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5530475982849477, Training Loss Force: 3.1762131223365118, time: 4.506155014038086
Validation Loss Energy: 3.1572138850438822, Validation Loss Force: 3.1813788759703785, time: 0.25400471687316895
Test Loss Energy: 9.432937547606988, Test Loss Force: 11.003575330205036, time: 10.615691423416138

Epoch 9, Batch 100/276, Loss: 1.8254854679107666, Variance: 0.11222583055496216
Epoch 9, Batch 200/276, Loss: 0.47169119119644165, Variance: 0.10951229929924011

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.538206544405199, Training Loss Force: 3.1703032710224814, time: 4.560400724411011
Validation Loss Energy: 2.99187710093746, Validation Loss Force: 3.197328533291698, time: 0.24534964561462402
Test Loss Energy: 9.358462580496868, Test Loss Force: 10.867295466243668, time: 10.818161249160767

Epoch 10, Batch 100/276, Loss: 1.4082133769989014, Variance: 0.11519826203584671
Epoch 10, Batch 200/276, Loss: 0.5863351225852966, Variance: 0.10975174605846405

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5362361261737356, Training Loss Force: 3.1699030030633697, time: 4.476268291473389
Validation Loss Energy: 3.4127416933626242, Validation Loss Force: 3.2052906920779547, time: 0.24837183952331543
Test Loss Energy: 9.630322189992526, Test Loss Force: 10.94274660089985, time: 10.557706117630005

Epoch 11, Batch 100/276, Loss: 1.5201940536499023, Variance: 0.1118527427315712
Epoch 11, Batch 200/276, Loss: 0.6720044612884521, Variance: 0.11307759582996368

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5495691217985694, Training Loss Force: 3.1599159522173834, time: 4.510993480682373
Validation Loss Energy: 3.156947350076207, Validation Loss Force: 3.1603341032519388, time: 0.24243998527526855
Test Loss Energy: 9.590856828186615, Test Loss Force: 11.11630674519297, time: 10.876166820526123

Epoch 12, Batch 100/276, Loss: 1.2242732048034668, Variance: 0.11374582350254059
Epoch 12, Batch 200/276, Loss: 0.5645209550857544, Variance: 0.11236587166786194

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.547220215635524, Training Loss Force: 3.1673225169546972, time: 4.5302894115448
Validation Loss Energy: 3.2875917023073002, Validation Loss Force: 3.276028493559269, time: 0.2481391429901123
Test Loss Energy: 9.305506415607947, Test Loss Force: 10.951444015770734, time: 10.707273483276367

Epoch 13, Batch 100/276, Loss: 1.3161554336547852, Variance: 0.11609424650669098
Epoch 13, Batch 200/276, Loss: 0.4681636095046997, Variance: 0.11297697573900223

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.550054048002223, Training Loss Force: 3.1545518912170523, time: 4.388485431671143
Validation Loss Energy: 3.3222834661751905, Validation Loss Force: 3.1336579832991567, time: 0.25472307205200195
Test Loss Energy: 9.556021501709436, Test Loss Force: 10.960319104298776, time: 10.837316989898682

Epoch 14, Batch 100/276, Loss: 1.0559148788452148, Variance: 0.11257743090391159
Epoch 14, Batch 200/276, Loss: 0.560621976852417, Variance: 0.10834438353776932

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5550521704135982, Training Loss Force: 3.153941821322912, time: 4.404415130615234
Validation Loss Energy: 3.5332196403980936, Validation Loss Force: 3.234092901569885, time: 0.2578279972076416
Test Loss Energy: 9.467142261073214, Test Loss Force: 11.053344612308384, time: 10.608484983444214

Epoch 15, Batch 100/276, Loss: 1.100606918334961, Variance: 0.11098001897335052
Epoch 15, Batch 200/276, Loss: 0.49541401863098145, Variance: 0.11122500896453857

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5514692120778997, Training Loss Force: 3.1614686395006104, time: 4.504453897476196
Validation Loss Energy: 2.8666979847578475, Validation Loss Force: 3.2015745667015665, time: 0.25098228454589844
Test Loss Energy: 9.282446178646316, Test Loss Force: 10.896273969310366, time: 10.841091871261597

Epoch 16, Batch 100/276, Loss: 1.1802833080291748, Variance: 0.10998782515525818
Epoch 16, Batch 200/276, Loss: 0.6081173419952393, Variance: 0.11067970842123032

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.548622512500301, Training Loss Force: 3.174436953487682, time: 4.581054925918579
Validation Loss Energy: 3.1519363590824185, Validation Loss Force: 3.279790800516524, time: 0.24341273307800293
Test Loss Energy: 9.533225049576657, Test Loss Force: 11.017604004016814, time: 10.64015245437622

Epoch 17, Batch 100/276, Loss: 1.208338737487793, Variance: 0.11594755947589874
Epoch 17, Batch 200/276, Loss: 0.4399070739746094, Variance: 0.10886506736278534

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.528625544865327, Training Loss Force: 3.173117844524988, time: 4.401602029800415
Validation Loss Energy: 2.6641324336024916, Validation Loss Force: 3.332022435678293, time: 0.2739090919494629
Test Loss Energy: 9.53286338825139, Test Loss Force: 11.216180800688347, time: 10.755380392074585

Epoch 18, Batch 100/276, Loss: 1.4468802213668823, Variance: 0.113050177693367
Epoch 18, Batch 200/276, Loss: 0.6482955813407898, Variance: 0.11147850751876831

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5509095029256503, Training Loss Force: 3.177405657604874, time: 4.38998556137085
Validation Loss Energy: 3.0974791012732723, Validation Loss Force: 3.1706448871217834, time: 0.24235820770263672
Test Loss Energy: 9.572937632424901, Test Loss Force: 11.027742864689346, time: 10.628599643707275

Epoch 19, Batch 100/276, Loss: 1.2655242681503296, Variance: 0.11568066477775574
Epoch 19, Batch 200/276, Loss: 0.4409977197647095, Variance: 0.10931220650672913

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5432340018630057, Training Loss Force: 3.1640015857466275, time: 4.375212669372559
Validation Loss Energy: 3.316279727061198, Validation Loss Force: 3.1604123491898184, time: 0.2562079429626465
Test Loss Energy: 9.727296916005258, Test Loss Force: 11.07839636014214, time: 10.769355773925781

wandb: - 0.039 MB of 0.061 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‡â–†â–†â–†â–‡â–…â–„â–ƒâ–‡â–†â–ƒâ–†â–…â–‚â–†â–†â–†â–ˆ
wandb:   test_error_force â–â–â–ƒâ–…â–ˆâ–…â–…â–„â–…â–‚â–„â–†â–„â–„â–…â–ƒâ–…â–ˆâ–…â–†
wandb:          test_loss â–„â–‚â–…â–…â–ˆâ–‡â–‡â–…â–†â–ƒâ–…â–†â–â–…â–„â–â–„â–‡â–…â–‡
wandb: train_error_energy â–ˆâ–â–‚â–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–â–â–â–‚â–â–‚â–‚â–‚â–â–‚â–â–â–â–‚â–‚â–‚â–
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–„â–…â–†â–„â–†â–„â–†â–†â–…â–„â–‡â–…â–†â–†â–ˆâ–ƒâ–…â–â–„â–†
wandb:  valid_error_force â–‚â–ƒâ–‚â–â–…â–ƒâ–‚â–ˆâ–ƒâ–ƒâ–„â–‚â–†â–â–…â–ƒâ–†â–ˆâ–‚â–‚
wandb:         valid_loss â–„â–„â–†â–ƒâ–‡â–ƒâ–…â–‡â–„â–ƒâ–‡â–„â–†â–…â–ˆâ–‚â–…â–â–„â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 8809
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.7273
wandb:   test_error_force 11.0784
wandb:          test_loss 11.96459
wandb: train_error_energy 2.54323
wandb:  train_error_force 3.164
wandb:         train_loss 0.8368
wandb: valid_error_energy 3.31628
wandb:  valid_error_force 3.16041
wandb:         valid_loss 1.1703
wandb: 
wandb: ğŸš€ View run al_71_104 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/zwileun7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_201756-zwileun7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.614051103591919, Uncertainty Bias: -0.1719151735305786
1.6212463e-05 0.13164759
1.7649233 4.5133314
(48745, 22, 3)
Found uncertainty sample 0 after 509 steps.
Found uncertainty sample 1 after 3225 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 1283 steps.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 535 steps.
Found uncertainty sample 9 after 624 steps.
Found uncertainty sample 10 after 1066 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 1082 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 3494 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 287 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 473 steps.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 389 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 438 steps.
Found uncertainty sample 23 after 333 steps.
Found uncertainty sample 24 after 2172 steps.
Found uncertainty sample 25 after 3973 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 2514 steps.
Found uncertainty sample 28 after 2974 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 1712 steps.
Found uncertainty sample 31 after 1816 steps.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 2819 steps.
Found uncertainty sample 36 after 1858 steps.
Found uncertainty sample 37 after 1746 steps.
Found uncertainty sample 38 after 2676 steps.
Found uncertainty sample 39 after 316 steps.
Found uncertainty sample 40 after 2009 steps.
Found uncertainty sample 41 after 2310 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 292 steps.
Did not find any uncertainty samples for sample 46.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 3071 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 577 steps.
Found uncertainty sample 51 after 1088 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 2 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 3030 steps.
Found uncertainty sample 57 after 1020 steps.
Found uncertainty sample 58 after 2086 steps.
Found uncertainty sample 59 after 1997 steps.
Found uncertainty sample 60 after 3546 steps.
Did not find any uncertainty samples for sample 61.
Found uncertainty sample 62 after 2275 steps.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 137 steps.
Found uncertainty sample 65 after 1476 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 2069 steps.
Found uncertainty sample 69 after 705 steps.
Found uncertainty sample 70 after 671 steps.
Did not find any uncertainty samples for sample 71.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 657 steps.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Did not find any uncertainty samples for sample 78.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 2294 steps.
Did not find any uncertainty samples for sample 83.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 3561 steps.
Found uncertainty sample 87 after 304 steps.
Found uncertainty sample 88 after 3738 steps.
Found uncertainty sample 89 after 1773 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 3938 steps.
Found uncertainty sample 92 after 994 steps.
Found uncertainty sample 93 after 1272 steps.
Found uncertainty sample 94 after 3668 steps.
Found uncertainty sample 95 after 379 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 2643 steps.
Found uncertainty sample 99 after 691 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_205238-u34i182d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_105
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/u34i182d
Training model 105. Added 55 samples to the dataset.
Epoch 0, Batch 100/277, Loss: 0.4035578966140747, Variance: 0.10469943284988403
Epoch 0, Batch 200/277, Loss: 0.9317499995231628, Variance: 0.10988441109657288

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.64593709805694, Training Loss Force: 3.2536983425367674, time: 4.684556722640991
Validation Loss Energy: 2.2709345270926837, Validation Loss Force: 3.13935764542747, time: 0.2538919448852539
Test Loss Energy: 9.388959705791708, Test Loss Force: 11.1832647050464, time: 9.904424905776978

Epoch 1, Batch 100/277, Loss: 0.7525099515914917, Variance: 0.11116812378168106
Epoch 1, Batch 200/277, Loss: 1.3134428262710571, Variance: 0.10857070982456207

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.531057715087428, Training Loss Force: 3.1357706069016076, time: 4.51983380317688
Validation Loss Energy: 3.0185576297393317, Validation Loss Force: 3.1345976470949717, time: 0.22853827476501465
Test Loss Energy: 9.432727091455684, Test Loss Force: 11.012812420473999, time: 10.07356882095337

Epoch 2, Batch 100/277, Loss: 1.2440531253814697, Variance: 0.11578812450170517
Epoch 2, Batch 200/277, Loss: 0.45597517490386963, Variance: 0.11228767037391663

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5570952185631284, Training Loss Force: 3.1482075090887784, time: 4.6065075397491455
Validation Loss Energy: 1.8098502311166533, Validation Loss Force: 3.1847973817663204, time: 0.2321007251739502
Test Loss Energy: 8.976458108791743, Test Loss Force: 10.88706332887168, time: 9.997944593429565

Epoch 3, Batch 100/277, Loss: 0.7859504222869873, Variance: 0.11188681423664093
Epoch 3, Batch 200/277, Loss: 0.9162750244140625, Variance: 0.11268508434295654

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5222909675481886, Training Loss Force: 3.151292185764162, time: 4.514708757400513
Validation Loss Energy: 2.6307785104971644, Validation Loss Force: 3.158951862261678, time: 0.23750615119934082
Test Loss Energy: 9.806158372025594, Test Loss Force: 10.88706916723337, time: 10.194815874099731

Epoch 4, Batch 100/277, Loss: 0.8334813117980957, Variance: 0.10796979069709778
Epoch 4, Batch 200/277, Loss: 1.1842124462127686, Variance: 0.10943495482206345

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.533897691233193, Training Loss Force: 3.153187506145173, time: 4.6533238887786865
Validation Loss Energy: 3.928111569236291, Validation Loss Force: 3.202460611767042, time: 0.23423147201538086
Test Loss Energy: 10.664219526395378, Test Loss Force: 11.190702577320756, time: 10.045098304748535

Epoch 5, Batch 100/277, Loss: 1.1566152572631836, Variance: 0.10733632743358612
Epoch 5, Batch 200/277, Loss: 0.6520079374313354, Variance: 0.11175340414047241

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5553935061909923, Training Loss Force: 3.1663398909220746, time: 4.520106315612793
Validation Loss Energy: 1.865830100545654, Validation Loss Force: 3.1620140083564765, time: 0.2637917995452881
Test Loss Energy: 9.59919338380748, Test Loss Force: 11.11551492904682, time: 10.320654153823853

Epoch 6, Batch 100/277, Loss: 0.5196613073348999, Variance: 0.11054122447967529
Epoch 6, Batch 200/277, Loss: 0.7040991187095642, Variance: 0.11076134443283081

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.557660128980557, Training Loss Force: 3.1765416916694447, time: 4.556821584701538
Validation Loss Energy: 1.93573750198106, Validation Loss Force: 3.2452774837021794, time: 0.23033642768859863
Test Loss Energy: 9.047475203599891, Test Loss Force: 10.677453609440235, time: 9.916839361190796

Epoch 7, Batch 100/277, Loss: 0.665194034576416, Variance: 0.11168275773525238
Epoch 7, Batch 200/277, Loss: 1.4833133220672607, Variance: 0.11531376838684082

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5589165042666266, Training Loss Force: 3.159140109169224, time: 4.594801425933838
Validation Loss Energy: 3.1483891950539595, Validation Loss Force: 3.153871576629677, time: 0.23395895957946777
Test Loss Energy: 9.519924935220669, Test Loss Force: 11.053379040922088, time: 10.137219190597534

Epoch 8, Batch 100/277, Loss: 1.2082158327102661, Variance: 0.11452438682317734
Epoch 8, Batch 200/277, Loss: 0.5666832327842712, Variance: 0.10740457475185394

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.53369920821697, Training Loss Force: 3.1633961300539535, time: 4.574455976486206
Validation Loss Energy: 1.62849033113767, Validation Loss Force: 3.155352326450097, time: 0.24064421653747559
Test Loss Energy: 8.777063102102524, Test Loss Force: 10.70395276268126, time: 12.35001540184021

Epoch 9, Batch 100/277, Loss: 0.6772653460502625, Variance: 0.11248652637004852
Epoch 9, Batch 200/277, Loss: 0.8831618428230286, Variance: 0.11083879321813583

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5451640836174207, Training Loss Force: 3.1533094910557335, time: 5.1286070346832275
Validation Loss Energy: 2.621721353918893, Validation Loss Force: 3.1668276254193146, time: 0.2855238914489746
Test Loss Energy: 9.750931098428913, Test Loss Force: 11.034509689101142, time: 11.846924304962158

Epoch 10, Batch 100/277, Loss: 0.7150230407714844, Variance: 0.11149515956640244
Epoch 10, Batch 200/277, Loss: 1.2526869773864746, Variance: 0.11258290708065033

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5201672304241622, Training Loss Force: 3.1632312158182336, time: 4.4958930015563965
Validation Loss Energy: 3.64857797843039, Validation Loss Force: 3.201470891947222, time: 0.24892592430114746
Test Loss Energy: 9.99104847738767, Test Loss Force: 10.900604793265712, time: 10.709033489227295

Epoch 11, Batch 100/277, Loss: 1.1706538200378418, Variance: 0.10783651471138
Epoch 11, Batch 200/277, Loss: 0.6007460951805115, Variance: 0.11414936929941177

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.54815784134547, Training Loss Force: 3.1617711541318916, time: 4.53078556060791
Validation Loss Energy: 2.2216430598675743, Validation Loss Force: 3.1896669778524442, time: 0.26502037048339844
Test Loss Energy: 9.572354463097524, Test Loss Force: 10.962273248723438, time: 10.763854742050171

Epoch 12, Batch 100/277, Loss: 0.5741995573043823, Variance: 0.10671600699424744
Epoch 12, Batch 200/277, Loss: 0.9213687777519226, Variance: 0.109553761780262

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.516965485646272, Training Loss Force: 3.1644685906737333, time: 4.49029278755188
Validation Loss Energy: 2.097622669389541, Validation Loss Force: 3.1807543780837095, time: 0.24369478225708008
Test Loss Energy: 9.113089673422975, Test Loss Force: 11.095612177880271, time: 10.587169170379639

Epoch 13, Batch 100/277, Loss: 0.8428506851196289, Variance: 0.11085617542266846
Epoch 13, Batch 200/277, Loss: 1.0020225048065186, Variance: 0.1099950522184372

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.551737024795183, Training Loss Force: 3.1481191255150396, time: 4.563629627227783
Validation Loss Energy: 3.0632591287917728, Validation Loss Force: 3.1965606364750516, time: 0.24304819107055664
Test Loss Energy: 9.538708087268581, Test Loss Force: 10.949010929831903, time: 10.771004438400269

Epoch 14, Batch 100/277, Loss: 1.321386456489563, Variance: 0.11473329365253448
Epoch 14, Batch 200/277, Loss: 0.5106804370880127, Variance: 0.11025659739971161

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5525363833152013, Training Loss Force: 3.156802956128652, time: 4.519996643066406
Validation Loss Energy: 1.6427431604993619, Validation Loss Force: 3.1848453447003537, time: 0.24341750144958496
Test Loss Energy: 9.09126308415175, Test Loss Force: 10.984010537174882, time: 10.576428413391113

Epoch 15, Batch 100/277, Loss: 0.6222952604293823, Variance: 0.11298660933971405
Epoch 15, Batch 200/277, Loss: 0.8451886773109436, Variance: 0.10933583974838257

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.531694650131108, Training Loss Force: 3.1611275261520784, time: 4.579732656478882
Validation Loss Energy: 2.8326642972838316, Validation Loss Force: 3.160090948544586, time: 0.24681901931762695
Test Loss Energy: 10.096697567934358, Test Loss Force: 10.866251569971416, time: 11.003356695175171

Epoch 16, Batch 100/277, Loss: 0.6129674911499023, Variance: 0.10980953276157379
Epoch 16, Batch 200/277, Loss: 1.161986231803894, Variance: 0.11532393097877502

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5325513222423406, Training Loss Force: 3.1694256857663463, time: 4.4504334926605225
Validation Loss Energy: 3.673963908817838, Validation Loss Force: 3.2147810737465434, time: 0.24612164497375488
Test Loss Energy: 10.249400533484671, Test Loss Force: 10.971388973815605, time: 10.66802978515625

Epoch 17, Batch 100/277, Loss: 1.0670055150985718, Variance: 0.10900766402482986
Epoch 17, Batch 200/277, Loss: 0.6690142154693604, Variance: 0.112025186419487

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.528450596205842, Training Loss Force: 3.154394199282399, time: 4.447016954421997
Validation Loss Energy: 2.1674292239687567, Validation Loss Force: 3.1812436267311415, time: 0.24496054649353027
Test Loss Energy: 9.668643498711189, Test Loss Force: 11.034644498364802, time: 10.973225355148315

Epoch 18, Batch 100/277, Loss: 0.46263253688812256, Variance: 0.10938739776611328
Epoch 18, Batch 200/277, Loss: 0.6594508290290833, Variance: 0.10939022898674011

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5235992906617573, Training Loss Force: 3.1463925119799683, time: 4.54239559173584
Validation Loss Energy: 2.2344638594179753, Validation Loss Force: 3.1911403795910482, time: 0.257127046585083
Test Loss Energy: 9.01976262708911, Test Loss Force: 10.68439678315238, time: 10.670698404312134

Epoch 19, Batch 100/277, Loss: 0.6826298236846924, Variance: 0.1128796637058258
Epoch 19, Batch 200/277, Loss: 1.0172017812728882, Variance: 0.11029462516307831

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5381893745510995, Training Loss Force: 3.159565801129215, time: 4.542730331420898
Validation Loss Energy: 3.1053993091924363, Validation Loss Force: 3.117604918426468, time: 0.24868249893188477
Test Loss Energy: 9.522596988287942, Test Loss Force: 10.885088106655612, time: 10.915202617645264

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–ƒâ–‚â–…â–ˆâ–„â–‚â–„â–â–…â–†â–„â–‚â–„â–‚â–†â–†â–„â–‚â–„
wandb:   test_error_force â–ˆâ–†â–„â–„â–ˆâ–‡â–â–†â–â–†â–„â–…â–‡â–…â–…â–„â–…â–†â–â–„
wandb:          test_loss â–…â–„â–‚â–„â–ˆâ–„â–‚â–ƒâ–â–ƒâ–…â–„â–ƒâ–ƒâ–‚â–…â–…â–„â–â–ƒ
wandb: train_error_energy â–ˆâ–‚â–ƒâ–â–‚â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–â–ƒâ–â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚
wandb:  train_error_force â–ˆâ–â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–‚â–‚â–‚â–â–â–â–‚â–â–â–‚â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–…â–‚â–„â–ˆâ–‚â–‚â–†â–â–„â–‡â–ƒâ–‚â–…â–â–…â–‡â–ƒâ–ƒâ–…
wandb:  valid_error_force â–‚â–‚â–…â–ƒâ–†â–ƒâ–ˆâ–ƒâ–ƒâ–„â–†â–…â–„â–…â–…â–ƒâ–†â–„â–…â–
wandb:         valid_loss â–‚â–…â–â–ƒâ–ˆâ–‚â–‚â–…â–â–ƒâ–‡â–‚â–‚â–…â–â–„â–‡â–‚â–‚â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 8858
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.5226
wandb:   test_error_force 10.88509
wandb:          test_loss 11.45525
wandb: train_error_energy 2.53819
wandb:  train_error_force 3.15957
wandb:         train_loss 0.83241
wandb: valid_error_energy 3.1054
wandb:  valid_error_force 3.1176
wandb:         valid_loss 1.02465
wandb: 
wandb: ğŸš€ View run al_71_105 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/u34i182d
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_205238-u34i182d/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.5671093463897705, Uncertainty Bias: -0.17933674156665802
1.6212463e-05 0.1314261
1.7723005 4.4646463
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 392 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 1356 steps.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 1906 steps.
Found uncertainty sample 11 after 499 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 161 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1097 steps.
Found uncertainty sample 16 after 952 steps.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 2877 steps.
Found uncertainty sample 19 after 417 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 2045 steps.
Found uncertainty sample 22 after 3794 steps.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 962 steps.
Found uncertainty sample 25 after 2190 steps.
Found uncertainty sample 26 after 1716 steps.
Found uncertainty sample 27 after 1990 steps.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 490 steps.
Found uncertainty sample 31 after 1358 steps.
Found uncertainty sample 32 after 1943 steps.
Did not find any uncertainty samples for sample 33.
Did not find any uncertainty samples for sample 34.
Did not find any uncertainty samples for sample 35.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 2347 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 2800 steps.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 2725 steps.
Found uncertainty sample 47 after 1510 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 516 steps.
Did not find any uncertainty samples for sample 50.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 871 steps.
Found uncertainty sample 54 after 414 steps.
Did not find any uncertainty samples for sample 55.
Found uncertainty sample 56 after 281 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 3364 steps.
Found uncertainty sample 59 after 2799 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 204 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 222 steps.
Found uncertainty sample 65 after 1036 steps.
Found uncertainty sample 66 after 1651 steps.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 342 steps.
Found uncertainty sample 70 after 930 steps.
Found uncertainty sample 71 after 17 steps.
Did not find any uncertainty samples for sample 72.
Found uncertainty sample 73 after 1934 steps.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 1181 steps.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 822 steps.
Found uncertainty sample 80 after 478 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 1800 steps.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 1219 steps.
Found uncertainty sample 85 after 186 steps.
Found uncertainty sample 86 after 901 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 1053 steps.
Found uncertainty sample 89 after 1387 steps.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 3917 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 852 steps.
Did not find any uncertainty samples for sample 95.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 2583 steps.
Found uncertainty sample 98 after 3261 steps.
Found uncertainty sample 99 after 405 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_212651-1lmk7zzy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_106
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/1lmk7zzy
Training model 106. Added 50 samples to the dataset.
Epoch 0, Batch 100/279, Loss: 4.1261186599731445, Variance: 0.09346523880958557
Epoch 0, Batch 200/279, Loss: 2.3137307167053223, Variance: 0.1325046718120575

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.839254569205448, Training Loss Force: 3.6139060721809706, time: 4.562131881713867
Validation Loss Energy: 6.094430475775793, Validation Loss Force: 3.206932387157051, time: 0.25542616844177246
Test Loss Energy: 11.406194809886758, Test Loss Force: 10.676948828978684, time: 10.794237613677979

Epoch 1, Batch 100/279, Loss: 1.13600754737854, Variance: 0.14198750257492065
Epoch 1, Batch 200/279, Loss: 1.0088553428649902, Variance: 0.1471947729587555

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.928678950332166, Training Loss Force: 3.190156649012586, time: 4.560869216918945
Validation Loss Energy: 3.3333306614444473, Validation Loss Force: 3.2257565250169824, time: 0.24875617027282715
Test Loss Energy: 10.0905722731507, Test Loss Force: 10.59735204249523, time: 10.999468564987183

Epoch 2, Batch 100/279, Loss: 0.9427765607833862, Variance: 0.15406233072280884
Epoch 2, Batch 200/279, Loss: 0.7419211268424988, Variance: 0.14980506896972656

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.9552309179501526, Training Loss Force: 3.2532702335578394, time: 4.5969462394714355
Validation Loss Energy: 1.40756751252065, Validation Loss Force: 3.296479808469983, time: 0.26222896575927734
Test Loss Energy: 9.000904942665086, Test Loss Force: 10.714846194296413, time: 10.779365062713623

Epoch 3, Batch 100/279, Loss: 1.6586014032363892, Variance: 0.15388324856758118
Epoch 3, Batch 200/279, Loss: 0.6921813488006592, Variance: 0.14032259583473206

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.3715473110533183, Training Loss Force: 3.4431351359973617, time: 4.7464659214019775
Validation Loss Energy: 1.256986923647204, Validation Loss Force: 4.180633522220325, time: 0.2512238025665283
Test Loss Energy: 8.823916441972807, Test Loss Force: 10.809206625986194, time: 11.11721420288086

Epoch 4, Batch 100/279, Loss: 0.7162554264068604, Variance: 0.108722984790802
Epoch 4, Batch 200/279, Loss: 0.7647954821586609, Variance: 0.10926701128482819

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.488530417867467, Training Loss Force: 3.2022649829556418, time: 4.51023268699646
Validation Loss Energy: 1.8994391802253, Validation Loss Force: 3.23895808880692, time: 0.25304245948791504
Test Loss Energy: 9.103306724315356, Test Loss Force: 10.627959257281105, time: 10.82061505317688

Epoch 5, Batch 100/279, Loss: 0.4125581383705139, Variance: 0.10940542817115784
Epoch 5, Batch 200/279, Loss: 0.6539279818534851, Variance: 0.11189043521881104

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5021926310867633, Training Loss Force: 3.1248357771476423, time: 4.5440497398376465
Validation Loss Energy: 1.6650615784655938, Validation Loss Force: 3.177119972283876, time: 0.2538764476776123
Test Loss Energy: 8.727101270644106, Test Loss Force: 10.552779886237202, time: 10.990463733673096

Epoch 6, Batch 100/279, Loss: 0.555263340473175, Variance: 0.11449006199836731
Epoch 6, Batch 200/279, Loss: 0.8308238387107849, Variance: 0.11117896437644958

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5018347327338413, Training Loss Force: 3.129415682430728, time: 4.468560218811035
Validation Loss Energy: 1.9670258792880175, Validation Loss Force: 3.1455156051919326, time: 0.281048059463501
Test Loss Energy: 9.235244073608314, Test Loss Force: 10.785891473655722, time: 10.798199892044067

Epoch 7, Batch 100/279, Loss: 0.5065422058105469, Variance: 0.1122530996799469
Epoch 7, Batch 200/279, Loss: 0.6723688840866089, Variance: 0.10917298495769501

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.530195938639766, Training Loss Force: 3.1312118411011114, time: 4.460789680480957
Validation Loss Energy: 1.6400148399881374, Validation Loss Force: 3.167182248301119, time: 0.259296178817749
Test Loss Energy: 9.007737516262706, Test Loss Force: 10.86213100631179, time: 10.95328140258789

Epoch 8, Batch 100/279, Loss: 0.5182874798774719, Variance: 0.11170519888401031
Epoch 8, Batch 200/279, Loss: 0.7923223972320557, Variance: 0.11566878855228424

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5197757907789944, Training Loss Force: 3.1283037453428193, time: 4.544322490692139
Validation Loss Energy: 2.133324371435702, Validation Loss Force: 3.1580712057895335, time: 0.263627290725708
Test Loss Energy: 9.511536916213533, Test Loss Force: 10.866365096261626, time: 10.872391700744629

Epoch 9, Batch 100/279, Loss: 0.4073449969291687, Variance: 0.10981451719999313
Epoch 9, Batch 200/279, Loss: 0.6567732095718384, Variance: 0.10875428467988968

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.498451836921966, Training Loss Force: 3.1373485157702756, time: 4.524588346481323
Validation Loss Energy: 1.7874214137878706, Validation Loss Force: 3.173692604255926, time: 0.2518000602722168
Test Loss Energy: 8.885147391030415, Test Loss Force: 10.696052969675616, time: 10.948115825653076

Epoch 10, Batch 100/279, Loss: 0.6930415630340576, Variance: 0.10970678925514221
Epoch 10, Batch 200/279, Loss: 0.6405473351478577, Variance: 0.11431917548179626

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.523230555651534, Training Loss Force: 3.1504445126767973, time: 4.518027067184448
Validation Loss Energy: 2.0387278381712, Validation Loss Force: 3.2418638766274483, time: 0.25543808937072754
Test Loss Energy: 9.360664398296997, Test Loss Force: 10.862215895717707, time: 10.717161655426025

Epoch 11, Batch 100/279, Loss: 0.4817262887954712, Variance: 0.1106950044631958
Epoch 11, Batch 200/279, Loss: 0.47849053144454956, Variance: 0.10837478935718536

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5277637163759215, Training Loss Force: 3.146059786078236, time: 4.5977044105529785
Validation Loss Energy: 1.7627904654066058, Validation Loss Force: 3.140873755108263, time: 0.2523679733276367
Test Loss Energy: 9.118193646622034, Test Loss Force: 10.774618455266152, time: 10.985419750213623

Epoch 12, Batch 100/279, Loss: 0.749139666557312, Variance: 0.1113634705543518
Epoch 12, Batch 200/279, Loss: 0.719040036201477, Variance: 0.10978011786937714

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.515812590441328, Training Loss Force: 3.1519908668931467, time: 4.532550811767578
Validation Loss Energy: 1.9411386761192575, Validation Loss Force: 3.1247945934116714, time: 0.2583198547363281
Test Loss Energy: 9.600806109980432, Test Loss Force: 10.844860703415524, time: 10.852481842041016

Epoch 13, Batch 100/279, Loss: 0.6172929406166077, Variance: 0.11261393129825592
Epoch 13, Batch 200/279, Loss: 0.6052719354629517, Variance: 0.1092771589756012

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5257158805161968, Training Loss Force: 3.153565950217905, time: 4.649773120880127
Validation Loss Energy: 1.534300503068205, Validation Loss Force: 3.148201484963942, time: 0.2527651786804199
Test Loss Energy: 9.079093224390805, Test Loss Force: 10.870158170732134, time: 10.924409627914429

Epoch 14, Batch 100/279, Loss: 0.5186715126037598, Variance: 0.11555279791355133
Epoch 14, Batch 200/279, Loss: 0.9016000032424927, Variance: 0.11309211701154709

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5386835349973014, Training Loss Force: 3.150641542148954, time: 4.504219055175781
Validation Loss Energy: 1.9049579236725473, Validation Loss Force: 3.1334932526649992, time: 0.27344822883605957
Test Loss Energy: 9.242945628120504, Test Loss Force: 10.777574158514359, time: 10.758214950561523

Epoch 15, Batch 100/279, Loss: 0.6014314889907837, Variance: 0.11020712554454803
Epoch 15, Batch 200/279, Loss: 0.6037909984588623, Variance: 0.11114456504583359

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.514810370981259, Training Loss Force: 3.149545266922406, time: 4.473062515258789
Validation Loss Energy: 1.6924230295157128, Validation Loss Force: 3.2680269668680126, time: 0.254108190536499
Test Loss Energy: 8.841216736421156, Test Loss Force: 10.618197441923632, time: 10.918133974075317

Epoch 16, Batch 100/279, Loss: 0.5740401744842529, Variance: 0.11159828305244446
Epoch 16, Batch 200/279, Loss: 1.0905952453613281, Variance: 0.1121339425444603

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5402042271931555, Training Loss Force: 3.1572301011040063, time: 4.513730049133301
Validation Loss Energy: 1.9998959200723299, Validation Loss Force: 3.139717924105581, time: 0.25652647018432617
Test Loss Energy: 9.225780149930653, Test Loss Force: 10.796961604313331, time: 10.899503946304321

Epoch 17, Batch 100/279, Loss: 0.5071108341217041, Variance: 0.10754933953285217
Epoch 17, Batch 200/279, Loss: 0.7249556183815002, Variance: 0.10905726999044418

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.524003107051428, Training Loss Force: 3.148834763197837, time: 4.648940086364746
Validation Loss Energy: 1.4726872155818045, Validation Loss Force: 3.191148217544985, time: 0.2651631832122803
Test Loss Energy: 8.777583348160361, Test Loss Force: 10.807876957764188, time: 11.29285478591919

Epoch 18, Batch 100/279, Loss: 0.44646888971328735, Variance: 0.11043436080217361
Epoch 18, Batch 200/279, Loss: 0.9386091828346252, Variance: 0.1119384691119194

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.527612484919069, Training Loss Force: 3.1627674055010466, time: 4.953531265258789
Validation Loss Energy: 1.7381556425822016, Validation Loss Force: 3.3219680775968987, time: 0.281200647354126
Test Loss Energy: 9.263707565090815, Test Loss Force: 10.934026308231482, time: 12.206741094589233

Epoch 19, Batch 100/279, Loss: 0.5901467800140381, Variance: 0.11157737672328949
Epoch 19, Batch 200/279, Loss: 0.6040481328964233, Variance: 0.11094336211681366

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5288213525199215, Training Loss Force: 3.1507610859507174, time: 5.094266176223755
Validation Loss Energy: 1.6391283718512846, Validation Loss Force: 3.196877806406511, time: 0.2887301445007324
Test Loss Energy: 8.922607214973818, Test Loss Force: 10.692342830718411, time: 12.02760124206543

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.039 MB of 0.057 MB uploadedwandb: - 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–…â–‚â–â–‚â–â–‚â–‚â–ƒâ–â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚
wandb:   test_error_force â–ƒâ–‚â–„â–†â–‚â–â–…â–‡â–‡â–„â–‡â–…â–†â–‡â–…â–‚â–…â–†â–ˆâ–„
wandb:          test_loss â–…â–‚â–â–ˆâ–‡â–…â–‡â–†â–‡â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–‡â–†
wandb: train_error_energy â–‡â–ˆâ–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–ˆâ–‚â–ƒâ–†â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–‚â–
wandb:         train_loss â–ˆâ–…â–…â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–„â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–‚
wandb:  valid_error_force â–‚â–‚â–‚â–ˆâ–‚â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–‚â–
wandb:         valid_loss â–ˆâ–ƒâ–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–â–â–â–â–‚â–â–‚â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 8903
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.92261
wandb:   test_error_force 10.69234
wandb:          test_loss 11.00606
wandb: train_error_energy 2.52882
wandb:  train_error_force 3.15076
wandb:         train_loss 0.8269
wandb: valid_error_energy 1.63913
wandb:  valid_error_force 3.19688
wandb:         valid_loss 0.52951
wandb: 
wandb: ğŸš€ View run al_71_106 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/1lmk7zzy
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_212651-1lmk7zzy/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.469592332839966, Uncertainty Bias: -0.16470465064048767
7.6293945e-06 0.0013370514
1.8805847 4.5300417
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 2935 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 618 steps.
Found uncertainty sample 5 after 2761 steps.
Found uncertainty sample 6 after 3099 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 2335 steps.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 43 steps.
Did not find any uncertainty samples for sample 12.
Found uncertainty sample 13 after 828 steps.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 720 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 63 steps.
Found uncertainty sample 20 after 1529 steps.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 88 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Found uncertainty sample 26 after 474 steps.
Did not find any uncertainty samples for sample 27.
Found uncertainty sample 28 after 1428 steps.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 428 steps.
Found uncertainty sample 32 after 1416 steps.
Did not find any uncertainty samples for sample 33.
Did not find any uncertainty samples for sample 34.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 850 steps.
Did not find any uncertainty samples for sample 37.
Did not find any uncertainty samples for sample 38.
Found uncertainty sample 39 after 514 steps.
Found uncertainty sample 40 after 686 steps.
Did not find any uncertainty samples for sample 41.
Found uncertainty sample 42 after 3613 steps.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Did not find any uncertainty samples for sample 45.
Found uncertainty sample 46 after 1909 steps.
Found uncertainty sample 47 after 48 steps.
Found uncertainty sample 48 after 926 steps.
Did not find any uncertainty samples for sample 49.
Did not find any uncertainty samples for sample 50.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Found uncertainty sample 54 after 3633 steps.
Found uncertainty sample 55 after 1385 steps.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 614 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 3083 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 1184 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 3493 steps.
Found uncertainty sample 64 after 524 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 1763 steps.
Found uncertainty sample 67 after 3522 steps.
Did not find any uncertainty samples for sample 68.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 646 steps.
Found uncertainty sample 71 after 2292 steps.
Found uncertainty sample 72 after 528 steps.
Found uncertainty sample 73 after 1722 steps.
Found uncertainty sample 74 after 921 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 1982 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 1678 steps.
Found uncertainty sample 79 after 2019 steps.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 659 steps.
Found uncertainty sample 83 after 1675 steps.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 2699 steps.
Found uncertainty sample 86 after 324 steps.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 2504 steps.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 577 steps.
Found uncertainty sample 94 after 8 steps.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 3134 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 128 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_220203-42dyoct0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_107
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/42dyoct0
Training model 107. Added 48 samples to the dataset.
Epoch 0, Batch 100/280, Loss: 0.37012672424316406, Variance: 0.10999016463756561
Epoch 0, Batch 200/280, Loss: 0.45843011140823364, Variance: 0.1059621274471283

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.669500349718004, Training Loss Force: 3.255636059527963, time: 4.598365783691406
Validation Loss Energy: 2.672893974622629, Validation Loss Force: 3.1225342343775297, time: 0.2541193962097168
Test Loss Energy: 9.648083320971612, Test Loss Force: 10.747212486008664, time: 10.750320672988892

Epoch 1, Batch 100/280, Loss: 0.7449078559875488, Variance: 0.10905785858631134
Epoch 1, Batch 200/280, Loss: 1.2462204694747925, Variance: 0.11428290605545044

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.4992084721282675, Training Loss Force: 3.133134763292525, time: 4.6239402294158936
Validation Loss Energy: 3.078324166130151, Validation Loss Force: 3.1798435722092067, time: 0.24716854095458984
Test Loss Energy: 9.435021264490054, Test Loss Force: 10.856143701113417, time: 10.96935486793518

Epoch 2, Batch 100/280, Loss: 1.2120829820632935, Variance: 0.11186248064041138
Epoch 2, Batch 200/280, Loss: 0.4804794192314148, Variance: 0.10922597348690033

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.510655071430316, Training Loss Force: 3.1355403861107414, time: 4.436015605926514
Validation Loss Energy: 2.014147435686061, Validation Loss Force: 3.1671158928956324, time: 0.2623884677886963
Test Loss Energy: 9.643099346623485, Test Loss Force: 10.673056222565751, time: 10.79682207107544

Epoch 3, Batch 100/280, Loss: 0.33142411708831787, Variance: 0.10812371969223022
Epoch 3, Batch 200/280, Loss: 0.626962423324585, Variance: 0.10984392464160919

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5303502993392093, Training Loss Force: 3.1471564135706798, time: 4.545623779296875
Validation Loss Energy: 2.4333504709443123, Validation Loss Force: 3.1805856394049252, time: 0.2511606216430664
Test Loss Energy: 9.630208052819421, Test Loss Force: 10.715254940236685, time: 11.083940029144287

Epoch 4, Batch 100/280, Loss: 0.5747214555740356, Variance: 0.10745547711849213
Epoch 4, Batch 200/280, Loss: 1.0463112592697144, Variance: 0.11303797364234924

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5144240214829647, Training Loss Force: 3.1456458523278545, time: 4.500857353210449
Validation Loss Energy: 3.1602419182255246, Validation Loss Force: 3.1594122975216274, time: 0.27164721488952637
Test Loss Energy: 9.124416297952164, Test Loss Force: 10.467119751293325, time: 10.841155529022217

Epoch 5, Batch 100/280, Loss: 1.20194673538208, Variance: 0.1149209588766098
Epoch 5, Batch 200/280, Loss: 0.3947490453720093, Variance: 0.10832367837429047

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5206805362019087, Training Loss Force: 3.1521964096064656, time: 4.539663553237915
Validation Loss Energy: 2.3728417782901157, Validation Loss Force: 3.204254300805149, time: 0.2555992603302002
Test Loss Energy: 9.363271287413585, Test Loss Force: 10.679013650461135, time: 10.981412887573242

Epoch 6, Batch 100/280, Loss: 0.5788973569869995, Variance: 0.11101698130369186
Epoch 6, Batch 200/280, Loss: 0.6070796847343445, Variance: 0.10934360325336456

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5018520614627375, Training Loss Force: 3.16058147435862, time: 4.461143255233765
Validation Loss Energy: 2.476561378200084, Validation Loss Force: 3.160290638234654, time: 0.2609703540802002
Test Loss Energy: 9.623103214312493, Test Loss Force: 10.774597102344284, time: 10.916264533996582

Epoch 7, Batch 100/280, Loss: 0.7325937747955322, Variance: 0.11164738237857819
Epoch 7, Batch 200/280, Loss: 1.1674320697784424, Variance: 0.11299145966768265

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.505210075208953, Training Loss Force: 3.1552091287175976, time: 4.547821521759033
Validation Loss Energy: 3.1925405231806425, Validation Loss Force: 3.166864989302001, time: 0.26149654388427734
Test Loss Energy: 9.286218795534767, Test Loss Force: 10.735506855276961, time: 11.023067951202393

Epoch 8, Batch 100/280, Loss: 1.0982131958007812, Variance: 0.11432160437107086
Epoch 8, Batch 200/280, Loss: 0.5790544748306274, Variance: 0.11153186112642288

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.5516373464266286, Training Loss Force: 3.1542388924480464, time: 4.562750339508057
Validation Loss Energy: 2.1633975318787586, Validation Loss Force: 3.2045064958798264, time: 0.2563910484313965
Test Loss Energy: 9.559774141328337, Test Loss Force: 10.672520524901316, time: 10.773200988769531

Epoch 9, Batch 100/280, Loss: 0.508122444152832, Variance: 0.10565678030252457
Epoch 9, Batch 200/280, Loss: 0.7063419818878174, Variance: 0.11228163540363312

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5154385534052404, Training Loss Force: 3.1432570267157107, time: 4.554821491241455
Validation Loss Energy: 2.6552633879187004, Validation Loss Force: 3.1285065771419465, time: 0.25194764137268066
Test Loss Energy: 9.669900136116556, Test Loss Force: 10.688647362277084, time: 10.962507247924805

Epoch 10, Batch 100/280, Loss: 0.4153692126274109, Variance: 0.10721472650766373
Epoch 10, Batch 200/280, Loss: 1.5075390338897705, Variance: 0.1136322095990181

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5202289985624566, Training Loss Force: 3.147928485433812, time: 4.58998966217041
Validation Loss Energy: 3.1491005339075406, Validation Loss Force: 3.215976406255713, time: 0.26447296142578125
Test Loss Energy: 9.37358445022929, Test Loss Force: 10.859766301951094, time: 12.242483854293823

Epoch 11, Batch 100/280, Loss: 1.1353164911270142, Variance: 0.11343379318714142
Epoch 11, Batch 200/280, Loss: 0.5621016025543213, Variance: 0.10891378670930862

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5409741202421157, Training Loss Force: 3.148322748990321, time: 4.5576465129852295
Validation Loss Energy: 2.0158642148011636, Validation Loss Force: 3.1771222435091646, time: 0.2578999996185303
Test Loss Energy: 9.207847696611205, Test Loss Force: 10.833806161929743, time: 10.949634552001953

Epoch 12, Batch 100/280, Loss: 0.572329044342041, Variance: 0.11385776102542877
Epoch 12, Batch 200/280, Loss: 0.6787745952606201, Variance: 0.1093815267086029

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5198080270347645, Training Loss Force: 3.148850785807727, time: 4.587528467178345
Validation Loss Energy: 2.535367230827712, Validation Loss Force: 3.1842271706174285, time: 0.25463008880615234
Test Loss Energy: 9.502086473928559, Test Loss Force: 10.775661232363635, time: 10.818178653717041

Epoch 13, Batch 100/280, Loss: 0.6517027616500854, Variance: 0.11276527494192123
Epoch 13, Batch 200/280, Loss: 1.2100259065628052, Variance: 0.11609435081481934

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.5457892828674007, Training Loss Force: 3.1517252949752286, time: 4.570017337799072
Validation Loss Energy: 3.097277363027062, Validation Loss Force: 3.224594236609069, time: 0.36080050468444824
Test Loss Energy: 9.378116408542796, Test Loss Force: 10.791185543351826, time: 10.841278553009033

Epoch 14, Batch 100/280, Loss: 1.4360544681549072, Variance: 0.11393606662750244
Epoch 14, Batch 200/280, Loss: 0.6793498992919922, Variance: 0.11394188553094864

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.550774198906577, Training Loss Force: 3.1456416836433445, time: 4.52880597114563
Validation Loss Energy: 2.090676327247007, Validation Loss Force: 3.148364968650882, time: 0.26432323455810547
Test Loss Energy: 9.649893357655612, Test Loss Force: 10.704811594079684, time: 10.88118314743042

Epoch 15, Batch 100/280, Loss: 0.3697527050971985, Variance: 0.10975965857505798
Epoch 15, Batch 200/280, Loss: 0.710835337638855, Variance: 0.11036896705627441

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5128798431634576, Training Loss Force: 3.148775053645456, time: 4.736960172653198
Validation Loss Energy: 2.464384715955023, Validation Loss Force: 3.1938119194700603, time: 0.26056694984436035
Test Loss Energy: 9.669118098759917, Test Loss Force: 10.955323677062223, time: 10.826772212982178

Epoch 16, Batch 100/280, Loss: 0.7037048935890198, Variance: 0.10738880932331085
Epoch 16, Batch 200/280, Loss: 1.3741934299468994, Variance: 0.11277700960636139

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5430926549352413, Training Loss Force: 3.1562118707062194, time: 4.49742317199707
Validation Loss Energy: 3.21985062672571, Validation Loss Force: 3.1321672579009987, time: 0.2626185417175293
Test Loss Energy: 9.275301199071153, Test Loss Force: 10.604069245495886, time: 10.841397523880005

Epoch 17, Batch 100/280, Loss: 1.4616576433181763, Variance: 0.11454857885837555
Epoch 17, Batch 200/280, Loss: 0.48559463024139404, Variance: 0.10817930847406387

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.51099164777309, Training Loss Force: 3.15493021811689, time: 4.762695789337158
Validation Loss Energy: 2.0941980179251716, Validation Loss Force: 3.097520339599098, time: 0.25901198387145996
Test Loss Energy: 9.44807863450277, Test Loss Force: 10.687145070674056, time: 10.825305461883545

Epoch 18, Batch 100/280, Loss: 0.6160887479782104, Variance: 0.1113346517086029
Epoch 18, Batch 200/280, Loss: 0.5597695112228394, Variance: 0.10797912627458572

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5125053351316744, Training Loss Force: 3.1491698770558174, time: 4.544719457626343
Validation Loss Energy: 2.1390454988696828, Validation Loss Force: 3.2711813870568847, time: 0.2614169120788574
Test Loss Energy: 9.48930962040743, Test Loss Force: 10.568513440937366, time: 10.84729290008545

Epoch 19, Batch 100/280, Loss: 0.4048615097999573, Variance: 0.10879476368427277
Epoch 19, Batch 200/280, Loss: 1.1293277740478516, Variance: 0.11122559010982513

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.529216608806855, Training Loss Force: 3.1457204178628504, time: 4.68506646156311
Validation Loss Energy: 2.8634234568316628, Validation Loss Force: 3.1933536663677415, time: 0.2681422233581543
Test Loss Energy: 9.149683715426342, Test Loss Force: 10.858434465327983, time: 11.029857873916626

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.039 MB of 0.049 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ˆâ–…â–ˆâ–‡â–â–„â–‡â–ƒâ–‡â–ˆâ–„â–‚â–†â–„â–ˆâ–ˆâ–ƒâ–…â–†â–
wandb:   test_error_force â–…â–‡â–„â–…â–â–„â–…â–…â–„â–„â–‡â–†â–…â–†â–„â–ˆâ–ƒâ–„â–‚â–‡
wandb:          test_loss â–ˆâ–…â–†â–…â–â–„â–‡â–…â–„â–…â–…â–ƒâ–†â–ƒâ–†â–‡â–ƒâ–…â–„â–ƒ
wandb: train_error_energy â–ˆâ–â–â–‚â–‚â–‚â–â–â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–â–‚â–‚
wandb:  train_error_force â–ˆâ–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–‚â–â–‚â–â–â–
wandb: valid_error_energy â–…â–‡â–â–ƒâ–ˆâ–ƒâ–„â–ˆâ–‚â–…â–ˆâ–â–„â–‡â–â–„â–ˆâ–â–‚â–†
wandb:  valid_error_force â–‚â–„â–„â–„â–ƒâ–…â–„â–„â–…â–‚â–†â–„â–„â–†â–ƒâ–…â–‚â–â–ˆâ–…
wandb:         valid_loss â–„â–‡â–â–ƒâ–ˆâ–ƒâ–ƒâ–ˆâ–‚â–„â–ˆâ–â–„â–‡â–â–ƒâ–ˆâ–â–‚â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 8946
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.14968
wandb:   test_error_force 10.85843
wandb:          test_loss 11.09549
wandb: train_error_energy 2.52922
wandb:  train_error_force 3.14572
wandb:         train_loss 0.82397
wandb: valid_error_energy 2.86342
wandb:  valid_error_force 3.19335
wandb:         valid_loss 0.95535
wandb: 
wandb: ğŸš€ View run al_71_107 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/42dyoct0
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_220203-42dyoct0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.604954957962036, Uncertainty Bias: -0.18267208337783813
2.2888184e-05 0.040183067
1.8210207 4.501756
(48745, 22, 3)
Found uncertainty sample 0 after 67 steps.
Found uncertainty sample 1 after 367 steps.
Found uncertainty sample 2 after 1281 steps.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 680 steps.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 1632 steps.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 175 steps.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 241 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 914 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 1572 steps.
Found uncertainty sample 17 after 870 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 677 steps.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 627 steps.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 16 steps.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 702 steps.
Found uncertainty sample 26 after 1474 steps.
Found uncertainty sample 27 after 1745 steps.
Found uncertainty sample 28 after 463 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 202 steps.
Did not find any uncertainty samples for sample 31.
Found uncertainty sample 32 after 850 steps.
Did not find any uncertainty samples for sample 33.
Did not find any uncertainty samples for sample 34.
Did not find any uncertainty samples for sample 35.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 3027 steps.
Found uncertainty sample 38 after 3983 steps.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 772 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 1014 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 3495 steps.
Found uncertainty sample 46 after 156 steps.
Did not find any uncertainty samples for sample 47.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 1081 steps.
Found uncertainty sample 50 after 3932 steps.
Found uncertainty sample 51 after 1096 steps.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 800 steps.
Found uncertainty sample 54 after 1021 steps.
Found uncertainty sample 55 after 2090 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 3735 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 1522 steps.
Found uncertainty sample 64 after 2129 steps.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 2157 steps.
Found uncertainty sample 68 after 517 steps.
Found uncertainty sample 69 after 2341 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 3930 steps.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Found uncertainty sample 75 after 2034 steps.
Found uncertainty sample 76 after 2685 steps.
Did not find any uncertainty samples for sample 77.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 2751 steps.
Found uncertainty sample 80 after 1486 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 220 steps.
Found uncertainty sample 83 after 1404 steps.
Found uncertainty sample 84 after 3000 steps.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 1550 steps.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 798 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 485 steps.
Found uncertainty sample 92 after 3867 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 2598 steps.
Found uncertainty sample 95 after 1557 steps.
Found uncertainty sample 96 after 3328 steps.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_223631-lyydo0x9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_108
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/lyydo0x9
Training model 108. Added 52 samples to the dataset.
Epoch 0, Batch 100/281, Loss: 0.5782619118690491, Variance: 0.09162883460521698
Epoch 0, Batch 200/281, Loss: 0.32817453145980835, Variance: 0.08332320302724838

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7843036590058186, Training Loss Force: 3.2042117080803756, time: 4.585541009902954
Validation Loss Energy: 1.4510318395314017, Validation Loss Force: 3.154445216883416, time: 0.25015854835510254
Test Loss Energy: 8.863562818638867, Test Loss Force: 10.896726474983947, time: 12.45127010345459

Epoch 1, Batch 100/281, Loss: 0.3360031843185425, Variance: 0.07905726134777069
Epoch 1, Batch 200/281, Loss: 0.4030916690826416, Variance: 0.07885400950908661

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5470965723176289, Training Loss Force: 3.1586767108697136, time: 5.160641670227051
Validation Loss Energy: 1.448597019221425, Validation Loss Force: 3.14456921678791, time: 0.25292253494262695
Test Loss Energy: 8.919565325942262, Test Loss Force: 10.91495469355115, time: 10.499409675598145

Epoch 2, Batch 100/281, Loss: 0.3946111798286438, Variance: 0.07562762498855591
Epoch 2, Batch 200/281, Loss: 0.13148987293243408, Variance: 0.07895845174789429

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5333420450059523, Training Loss Force: 3.1618353598705564, time: 4.642887830734253
Validation Loss Energy: 1.6291811621629109, Validation Loss Force: 3.1677871067691172, time: 0.2479696273803711
Test Loss Energy: 8.988154336123806, Test Loss Force: 10.71680616309308, time: 11.632984638214111

Epoch 3, Batch 100/281, Loss: 0.2240673303604126, Variance: 0.07383580505847931
Epoch 3, Batch 200/281, Loss: 0.38109052181243896, Variance: 0.07216604053974152

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.529658626240466, Training Loss Force: 3.161848975177551, time: 4.607917547225952
Validation Loss Energy: 1.3463624221125643, Validation Loss Force: 3.187265001241492, time: 0.2419147491455078
Test Loss Energy: 9.48138869831487, Test Loss Force: 11.210509438526097, time: 10.414519309997559

Epoch 4, Batch 100/281, Loss: 0.505557119846344, Variance: 0.07638387382030487
Epoch 4, Batch 200/281, Loss: 0.4421239495277405, Variance: 0.07525025308132172

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5378467531626443, Training Loss Force: 3.1523435328335623, time: 4.521534204483032
Validation Loss Energy: 1.548550081460366, Validation Loss Force: 3.290824858306171, time: 0.2517571449279785
Test Loss Energy: 8.905416856129973, Test Loss Force: 10.986728112675717, time: 10.277468919754028

Epoch 5, Batch 100/281, Loss: 0.3469216823577881, Variance: 0.07589089870452881
Epoch 5, Batch 200/281, Loss: 0.5885761976242065, Variance: 0.07433534413576126

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5437034796482978, Training Loss Force: 3.1574991294497674, time: 4.484062671661377
Validation Loss Energy: 1.4955413087123948, Validation Loss Force: 3.3915242629884332, time: 0.24526190757751465
Test Loss Energy: 8.858315873195206, Test Loss Force: 10.87126450762981, time: 10.33823275566101

Epoch 6, Batch 100/281, Loss: 0.26047277450561523, Variance: 0.07469415664672852
Epoch 6, Batch 200/281, Loss: 0.5521001219749451, Variance: 0.07526397705078125

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.5426580917891999, Training Loss Force: 3.157401418107966, time: 4.578650951385498
Validation Loss Energy: 1.616890582382793, Validation Loss Force: 3.170669781532298, time: 0.24248909950256348
Test Loss Energy: 9.257266616391183, Test Loss Force: 11.365890836439842, time: 10.326177597045898

Epoch 7, Batch 100/281, Loss: 0.3462391495704651, Variance: 0.07518582046031952
Epoch 7, Batch 200/281, Loss: 0.3597206473350525, Variance: 0.07197407633066177

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5475257136082647, Training Loss Force: 3.200489279820048, time: 4.61395001411438
Validation Loss Energy: 1.4964215643800765, Validation Loss Force: 3.1997758898745907, time: 0.25496840476989746
Test Loss Energy: 9.338809414629342, Test Loss Force: 11.21335016522012, time: 10.468087673187256

Epoch 8, Batch 100/281, Loss: 0.6231317520141602, Variance: 0.07705730199813843
Epoch 8, Batch 200/281, Loss: 0.4688378572463989, Variance: 0.07271669805049896

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.5265874563002344, Training Loss Force: 3.1492893616223383, time: 4.557885408401489
Validation Loss Energy: 1.2296885324992117, Validation Loss Force: 3.235173158664439, time: 0.25330448150634766
Test Loss Energy: 8.91340481679095, Test Loss Force: 11.059232091780963, time: 10.276844501495361

Epoch 9, Batch 100/281, Loss: 0.32286494970321655, Variance: 0.07451438903808594
Epoch 9, Batch 200/281, Loss: 0.22951483726501465, Variance: 0.07248008251190186

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.5477008682488773, Training Loss Force: 3.166945757027389, time: 4.82331657409668
Validation Loss Energy: 1.2244301338173833, Validation Loss Force: 3.118930156115371, time: 0.29891085624694824
Test Loss Energy: 9.021955111438022, Test Loss Force: 11.162033554055911, time: 12.916948318481445

Epoch 10, Batch 100/281, Loss: 0.3992060422897339, Variance: 0.07189598679542542
Epoch 10, Batch 200/281, Loss: 0.16562169790267944, Variance: 0.07390405237674713

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.5284849371722806, Training Loss Force: 3.156945899636332, time: 5.157663583755493
Validation Loss Energy: 1.9186046030851944, Validation Loss Force: 3.172212077104653, time: 0.28886890411376953
Test Loss Energy: 9.620415926478731, Test Loss Force: 11.048911052772851, time: 11.349273920059204

Epoch 11, Batch 100/281, Loss: 0.23540884256362915, Variance: 0.07118526846170425
Epoch 11, Batch 200/281, Loss: 0.3002414107322693, Variance: 0.07547999918460846

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.5364833161063065, Training Loss Force: 3.140880911108225, time: 4.719252824783325
Validation Loss Energy: 1.6527961825658803, Validation Loss Force: 3.2037956023645373, time: 0.2703080177307129
Test Loss Energy: 9.65656572838048, Test Loss Force: 11.028137527680492, time: 10.768872022628784

Epoch 12, Batch 100/281, Loss: 0.29777348041534424, Variance: 0.07435008883476257
Epoch 12, Batch 200/281, Loss: 0.5609886050224304, Variance: 0.07461607456207275

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.523831248499146, Training Loss Force: 3.162383644210185, time: 4.729953050613403
Validation Loss Energy: 1.4104386975559011, Validation Loss Force: 3.2387562614930507, time: 0.25453805923461914
Test Loss Energy: 9.06705552984314, Test Loss Force: 11.557619803762744, time: 10.823529720306396

Epoch 13, Batch 100/281, Loss: 0.4293731451034546, Variance: 0.07544562965631485
Epoch 13, Batch 200/281, Loss: 0.1555613875389099, Variance: 0.07598643004894257

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.528422965218922, Training Loss Force: 3.147324094834216, time: 4.768651962280273
Validation Loss Energy: 1.5461012107272816, Validation Loss Force: 3.167233863009145, time: 0.2508723735809326
Test Loss Energy: 8.57830063666911, Test Loss Force: 10.819436219089562, time: 10.834115743637085

Epoch 14, Batch 100/281, Loss: 0.40496158599853516, Variance: 0.0747520700097084
Epoch 14, Batch 200/281, Loss: 0.2632785439491272, Variance: 0.07466640323400497

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.5538173325482332, Training Loss Force: 3.1582513858588173, time: 4.6589884757995605
Validation Loss Energy: 1.7682684604001484, Validation Loss Force: 3.2587694080584066, time: 0.2619435787200928
Test Loss Energy: 9.829621982888625, Test Loss Force: 11.225961931136974, time: 10.852304935455322

Epoch 15, Batch 100/281, Loss: 0.033029019832611084, Variance: 0.07537400722503662
Epoch 15, Batch 200/281, Loss: 0.43752825260162354, Variance: 0.07302047312259674

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.549884478379128, Training Loss Force: 3.1491278987569493, time: 4.835381984710693
Validation Loss Energy: 1.6545208219018348, Validation Loss Force: 3.280521155315167, time: 0.25939416885375977
Test Loss Energy: 9.334119125729202, Test Loss Force: 11.36899585625459, time: 10.886549234390259

Epoch 16, Batch 100/281, Loss: 0.3729628920555115, Variance: 0.07561290264129639
Epoch 16, Batch 200/281, Loss: 0.35730814933776855, Variance: 0.07379520684480667

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.5355269061643673, Training Loss Force: 3.1396111834902922, time: 4.61339545249939
Validation Loss Energy: 1.4127292505262312, Validation Loss Force: 3.154968890180809, time: 0.25518250465393066
Test Loss Energy: 8.869612681043371, Test Loss Force: 10.96049975309551, time: 10.773215770721436

Epoch 17, Batch 100/281, Loss: 0.3389713764190674, Variance: 0.07671371102333069
Epoch 17, Batch 200/281, Loss: 0.4661203622817993, Variance: 0.07880658656358719

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.5387698498985525, Training Loss Force: 3.16018651307055, time: 4.781622886657715
Validation Loss Energy: 1.4651665206442641, Validation Loss Force: 3.109530356255347, time: 0.25622987747192383
Test Loss Energy: 8.998360930185756, Test Loss Force: 11.172135439992145, time: 10.797044515609741

Epoch 18, Batch 100/281, Loss: 0.24607199430465698, Variance: 0.07397682219743729
Epoch 18, Batch 200/281, Loss: 0.625121533870697, Variance: 0.0753372460603714

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.5376128942978209, Training Loss Force: 3.1402988769408595, time: 4.611354827880859
Validation Loss Energy: 1.945292925307174, Validation Loss Force: 3.19249033355515, time: 0.2564692497253418
Test Loss Energy: 9.546812647287146, Test Loss Force: 11.092353406768968, time: 11.011240005493164

Epoch 19, Batch 100/281, Loss: 0.31997817754745483, Variance: 0.07575201988220215
Epoch 19, Batch 200/281, Loss: 0.13287311792373657, Variance: 0.07599872350692749

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5451990234958348, Training Loss Force: 3.1552940297658925, time: 4.515265941619873
Validation Loss Energy: 1.9441141816493759, Validation Loss Force: 3.1698131287940847, time: 0.2542285919189453
Test Loss Energy: 9.608805919904233, Test Loss Force: 11.255718036830308, time: 10.839733362197876

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–ƒâ–ƒâ–†â–ƒâ–ƒâ–…â–…â–ƒâ–ƒâ–‡â–‡â–„â–â–ˆâ–…â–ƒâ–ƒâ–†â–‡
wandb:   test_error_force â–‚â–ƒâ–â–…â–ƒâ–‚â–†â–…â–„â–…â–„â–„â–ˆâ–‚â–…â–†â–ƒâ–…â–„â–…
wandb:          test_loss â–â–„â–ƒâ–‡â–„â–…â–…â–†â–„â–…â–ˆâ–…â–†â–ƒâ–ˆâ–…â–…â–„â–†â–…
wandb: train_error_energy â–ˆâ–‚â–â–â–â–‚â–‚â–‚â–â–‚â–â–â–â–â–‚â–‚â–â–â–â–‚
wandb:  train_error_force â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ˆâ–‚â–„â–ƒâ–â–ƒâ–‚â–ƒâ–‚â–â–ƒâ–â–ƒ
wandb:         train_loss â–ˆâ–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–ƒâ–…â–‚â–„â–„â–…â–„â–â–â–ˆâ–…â–ƒâ–„â–†â–…â–ƒâ–ƒâ–ˆâ–ˆ
wandb:  valid_error_force â–‚â–‚â–‚â–ƒâ–†â–ˆâ–ƒâ–ƒâ–„â–â–ƒâ–ƒâ–„â–‚â–…â–…â–‚â–â–ƒâ–‚
wandb:         valid_loss â–ƒâ–ƒâ–…â–‚â–…â–…â–…â–„â–‚â–â–ˆâ–…â–ƒâ–„â–†â–…â–ƒâ–ƒâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 8992
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.60881
wandb:   test_error_force 11.25572
wandb:          test_loss 15.7194
wandb: train_error_energy 1.5452
wandb:  train_error_force 3.15529
wandb:         train_loss 0.37444
wandb: valid_error_energy 1.94411
wandb:  valid_error_force 3.16981
wandb:         valid_loss 0.64338
wandb: 
wandb: ğŸš€ View run al_71_108 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/lyydo0x9
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_223631-lyydo0x9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.9253504276275635, Uncertainty Bias: -0.08151066303253174
0.0 0.006881714
1.8194984 4.6368127
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 1011 steps.
Found uncertainty sample 2 after 2085 steps.
Found uncertainty sample 3 after 1044 steps.
Found uncertainty sample 4 after 3631 steps.
Found uncertainty sample 5 after 2028 steps.
Found uncertainty sample 6 after 2764 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Found uncertainty sample 9 after 1134 steps.
Found uncertainty sample 10 after 3407 steps.
Found uncertainty sample 11 after 3672 steps.
Found uncertainty sample 12 after 98 steps.
Found uncertainty sample 13 after 837 steps.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Found uncertainty sample 18 after 7 steps.
Did not find any uncertainty samples for sample 19.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Found uncertainty sample 23 after 995 steps.
Found uncertainty sample 24 after 1422 steps.
Found uncertainty sample 25 after 470 steps.
Found uncertainty sample 26 after 788 steps.
Found uncertainty sample 27 after 1315 steps.
Found uncertainty sample 28 after 699 steps.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 1124 steps.
Found uncertainty sample 31 after 126 steps.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 1319 steps.
Found uncertainty sample 35 after 1759 steps.
Did not find any uncertainty samples for sample 36.
Found uncertainty sample 37 after 838 steps.
Found uncertainty sample 38 after 1896 steps.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 1296 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 494 steps.
Found uncertainty sample 45 after 1205 steps.
Found uncertainty sample 46 after 2637 steps.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 57 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 2191 steps.
Found uncertainty sample 51 after 265 steps.
Found uncertainty sample 52 after 778 steps.
Found uncertainty sample 53 after 877 steps.
Did not find any uncertainty samples for sample 54.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 3241 steps.
Found uncertainty sample 58 after 2506 steps.
Found uncertainty sample 59 after 1892 steps.
Found uncertainty sample 60 after 1365 steps.
Found uncertainty sample 61 after 3966 steps.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 1163 steps.
Did not find any uncertainty samples for sample 69.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 676 steps.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 2460 steps.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 2517 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 3867 steps.
Did not find any uncertainty samples for sample 79.
Found uncertainty sample 80 after 749 steps.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 1592 steps.
Did not find any uncertainty samples for sample 83.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 436 steps.
Found uncertainty sample 86 after 3659 steps.
Found uncertainty sample 87 after 2622 steps.
Found uncertainty sample 88 after 1157 steps.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 1395 steps.
Did not find any uncertainty samples for sample 93.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Did not find any uncertainty samples for sample 96.
Found uncertainty sample 97 after 860 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 2966 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_231132-ccf6z4fz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_109
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ccf6z4fz
Training model 109. Added 52 samples to the dataset.
Epoch 0, Batch 100/283, Loss: 0.4705440402030945, Variance: 0.07756279408931732
Epoch 0, Batch 200/283, Loss: 0.3810464143753052, Variance: 0.07358865439891815

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.772167248436974, Training Loss Force: 3.178868055690993, time: 5.072827577590942
Validation Loss Energy: 1.492340439739683, Validation Loss Force: 3.132181107921876, time: 0.30580663681030273
Test Loss Energy: 8.861614064905988, Test Loss Force: 11.056742007204004, time: 12.516361713409424

Epoch 1, Batch 100/283, Loss: 0.17219722270965576, Variance: 0.07254815101623535
Epoch 1, Batch 200/283, Loss: 0.24597245454788208, Variance: 0.07265692204236984

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5388695093405749, Training Loss Force: 3.160383846909686, time: 5.152336835861206
Validation Loss Energy: 1.5346927570356712, Validation Loss Force: 3.1558260773249724, time: 0.2788853645324707
Test Loss Energy: 9.236828552276524, Test Loss Force: 10.93197563032012, time: 12.578525304794312

Epoch 2, Batch 100/283, Loss: 0.43028968572616577, Variance: 0.07292737066745758
Epoch 2, Batch 200/283, Loss: 0.3752118945121765, Variance: 0.0732073038816452

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5467138833086682, Training Loss Force: 3.1516600852114895, time: 5.106254577636719
Validation Loss Energy: 1.8806285138486978, Validation Loss Force: 3.122539934707665, time: 0.30481576919555664
Test Loss Energy: 9.589971229858397, Test Loss Force: 11.000448257984191, time: 12.421107292175293

Epoch 3, Batch 100/283, Loss: 0.3016695976257324, Variance: 0.0766863077878952
Epoch 3, Batch 200/283, Loss: 0.46091383695602417, Variance: 0.07374270260334015

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.54866386842318, Training Loss Force: 3.1550372766751056, time: 5.294072389602661
Validation Loss Energy: 1.229657626021757, Validation Loss Force: 3.19125732542649, time: 0.3061072826385498
Test Loss Energy: 9.043383684857032, Test Loss Force: 11.327865097113355, time: 12.62476372718811

Epoch 4, Batch 100/283, Loss: 0.1639169454574585, Variance: 0.0712939128279686
Epoch 4, Batch 200/283, Loss: 0.4141027331352234, Variance: 0.07317604124546051

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5435971565213469, Training Loss Force: 3.1470107298400145, time: 5.182587146759033
Validation Loss Energy: 1.3157901042450812, Validation Loss Force: 3.331134564725821, time: 0.2897615432739258
Test Loss Energy: 9.101471447908795, Test Loss Force: 11.470964270119763, time: 12.69840121269226

Epoch 5, Batch 100/283, Loss: 0.6349749565124512, Variance: 0.07384747266769409
Epoch 5, Batch 200/283, Loss: 0.4152972102165222, Variance: 0.07605870068073273

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5453274804904424, Training Loss Force: 3.1592398192600983, time: 5.0936291217803955
Validation Loss Energy: 1.7767565859760008, Validation Loss Force: 3.1559125620212507, time: 0.3039360046386719
Test Loss Energy: 9.584992860710553, Test Loss Force: 11.16907208612125, time: 12.436265230178833

Epoch 6, Batch 100/283, Loss: 0.3240707516670227, Variance: 0.07374322414398193
Epoch 6, Batch 200/283, Loss: 0.45000070333480835, Variance: 0.07377271354198456

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.526603695051044, Training Loss Force: 3.135523345837299, time: 5.062572956085205
Validation Loss Energy: 1.7073566029903346, Validation Loss Force: 3.142704273623106, time: 0.2977285385131836
Test Loss Energy: 9.693270393814512, Test Loss Force: 11.102482390724198, time: 12.53352665901184

Epoch 7, Batch 100/283, Loss: 0.3179718255996704, Variance: 0.07152363657951355
Epoch 7, Batch 200/283, Loss: 0.708417534828186, Variance: 0.07319080829620361

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5350716195390746, Training Loss Force: 3.1327116309313423, time: 4.90418004989624
Validation Loss Energy: 1.2346039761624823, Validation Loss Force: 3.1372609036817556, time: 0.3029000759124756
Test Loss Energy: 9.029439163661113, Test Loss Force: 11.396469460973705, time: 12.409797191619873

Epoch 8, Batch 100/283, Loss: 0.14576047658920288, Variance: 0.07294375449419022
Epoch 8, Batch 200/283, Loss: 0.10707730054855347, Variance: 0.07393443584442139

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.5380015064557264, Training Loss Force: 3.126523592420668, time: 5.088656663894653
Validation Loss Energy: 1.3387744810929538, Validation Loss Force: 3.166137618269913, time: 0.29836273193359375
Test Loss Energy: 8.812122781151002, Test Loss Force: 10.852584035203348, time: 12.556206464767456

Epoch 9, Batch 100/283, Loss: 0.4439889192581177, Variance: 0.07253359258174896
Epoch 9, Batch 200/283, Loss: 0.623155951499939, Variance: 0.07289303839206696

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 1.517808707748259, Training Loss Force: 3.129147904095147, time: 5.206855773925781
Validation Loss Energy: 1.708615552074496, Validation Loss Force: 3.1961336906546496, time: 0.30391836166381836
Test Loss Energy: 9.156065968072785, Test Loss Force: 10.977267224759604, time: 12.42896056175232

Epoch 10, Batch 100/283, Loss: 0.25123846530914307, Variance: 0.07646860927343369
Epoch 10, Batch 200/283, Loss: 0.5460624098777771, Variance: 0.07504236698150635

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 1.5353410421242963, Training Loss Force: 3.153996590326915, time: 5.304895401000977
Validation Loss Energy: 1.972495791391563, Validation Loss Force: 3.255188932725717, time: 0.30464816093444824
Test Loss Energy: 9.869408482566861, Test Loss Force: 10.97356307338739, time: 12.520360469818115

Epoch 11, Batch 100/283, Loss: 0.43874335289001465, Variance: 0.07591976970434189
Epoch 11, Batch 200/283, Loss: 0.3098767399787903, Variance: 0.07380207628011703

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 1.5417716658836869, Training Loss Force: 3.156774350155159, time: 5.068017959594727
Validation Loss Energy: 1.274865853249586, Validation Loss Force: 3.138905760662724, time: 0.30206918716430664
Test Loss Energy: 8.696369958029715, Test Loss Force: 10.826906948469722, time: 12.510148048400879

Epoch 12, Batch 100/283, Loss: 0.1717783808708191, Variance: 0.07214823365211487
Epoch 12, Batch 200/283, Loss: 0.45445042848587036, Variance: 0.07408000528812408

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 1.551111136095777, Training Loss Force: 3.1553997521456387, time: 5.120210647583008
Validation Loss Energy: 1.4752449937941623, Validation Loss Force: 3.1483067184023077, time: 0.3065934181213379
Test Loss Energy: 9.068670645920538, Test Loss Force: 11.202406771778175, time: 12.478720426559448

Epoch 13, Batch 100/283, Loss: 0.295898973941803, Variance: 0.07182031869888306
Epoch 13, Batch 200/283, Loss: 0.20633631944656372, Variance: 0.07138773053884506

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 1.5207574489584677, Training Loss Force: 3.1326520397896283, time: 5.019750595092773
Validation Loss Energy: 1.8120179152522606, Validation Loss Force: 3.141667570613496, time: 0.2919034957885742
Test Loss Energy: 9.283855410867645, Test Loss Force: 11.150663302695415, time: 12.037463665008545

Epoch 14, Batch 100/283, Loss: 0.44669002294540405, Variance: 0.07782801985740662
Epoch 14, Batch 200/283, Loss: 0.4031352996826172, Variance: 0.07445918023586273

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 1.5262621231383453, Training Loss Force: 3.1745634259905353, time: 4.633025646209717
Validation Loss Energy: 1.7770457663958172, Validation Loss Force: 3.159914906317815, time: 0.29799652099609375
Test Loss Energy: 9.324829723366438, Test Loss Force: 11.177345888680401, time: 12.617122650146484

Epoch 15, Batch 100/283, Loss: 0.24695515632629395, Variance: 0.07246164977550507
Epoch 15, Batch 200/283, Loss: 0.20017802715301514, Variance: 0.0711643248796463

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 1.5356526148772986, Training Loss Force: 3.129837051725908, time: 5.052506923675537
Validation Loss Energy: 1.2267557571781968, Validation Loss Force: 3.1415782357521493, time: 0.2487349510192871
Test Loss Energy: 8.74237227274556, Test Loss Force: 11.046946327483433, time: 10.191219329833984

Epoch 16, Batch 100/283, Loss: 0.26678138971328735, Variance: 0.07255827635526657
Epoch 16, Batch 200/283, Loss: 0.5996769666671753, Variance: 0.07284355163574219

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 1.5228805677922184, Training Loss Force: 3.1249599475194376, time: 4.524376630783081
Validation Loss Energy: 1.4124386004900518, Validation Loss Force: 3.2379706847462173, time: 0.29036521911621094
Test Loss Energy: 9.08558610746276, Test Loss Force: 11.288784181165632, time: 10.2167809009552

Epoch 17, Batch 100/283, Loss: 0.45104271173477173, Variance: 0.07108955830335617
Epoch 17, Batch 200/283, Loss: 0.3920058608055115, Variance: 0.07511866837739944

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 1.544097130779603, Training Loss Force: 3.1625917782967155, time: 4.878479957580566
Validation Loss Energy: 1.2490362545888771, Validation Loss Force: 3.6709463856198825, time: 0.26979804039001465
Test Loss Energy: 8.893719852370417, Test Loss Force: 11.704692485527902, time: 10.321338415145874

Epoch 18, Batch 100/283, Loss: 0.38235563039779663, Variance: 0.07363734394311905
Epoch 18, Batch 200/283, Loss: 0.7118954658508301, Variance: 0.07443808019161224

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 1.5372680089141997, Training Loss Force: 3.154903284926431, time: 4.729208707809448
Validation Loss Energy: 1.7174084671772945, Validation Loss Force: 3.211366264258424, time: 0.2423102855682373
Test Loss Energy: 9.80600920001912, Test Loss Force: 11.298792900465921, time: 10.220405101776123

Epoch 19, Batch 100/283, Loss: 0.08193111419677734, Variance: 0.06888705492019653
Epoch 19, Batch 200/283, Loss: 0.031958937644958496, Variance: 0.07171282172203064

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 1.5105136032892912, Training Loss Force: 3.131728427842739, time: 4.82066011428833
Validation Loss Energy: 1.2428222712672998, Validation Loss Force: 3.144944369764969, time: 0.25681495666503906
Test Loss Energy: 8.94611660008473, Test Loss Force: 11.173945425380639, time: 11.648051977157593

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–„â–†â–ƒâ–ƒâ–†â–‡â–ƒâ–‚â–„â–ˆâ–â–ƒâ–…â–…â–â–ƒâ–‚â–ˆâ–‚
wandb:   test_error_force â–ƒâ–‚â–‚â–…â–†â–„â–ƒâ–†â–â–‚â–‚â–â–„â–„â–„â–ƒâ–…â–ˆâ–…â–„
wandb:          test_loss â–‚â–„â–„â–ƒâ–ƒâ–…â–…â–ƒâ–‚â–…â–†â–â–„â–„â–…â–ƒâ–ƒâ–„â–ˆâ–…
wandb: train_error_energy â–ˆâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–‚â–‚â–
wandb:  train_error_force â–ˆâ–†â–„â–…â–„â–…â–‚â–‚â–â–‚â–…â–…â–…â–‚â–‡â–‚â–â–†â–…â–‚
wandb:         train_loss â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ƒâ–„â–‡â–â–‚â–†â–†â–â–‚â–†â–ˆâ–â–ƒâ–†â–†â–â–ƒâ–â–†â–
wandb:  valid_error_force â–â–â–â–‚â–„â–â–â–â–‚â–‚â–ƒâ–â–â–â–â–â–‚â–ˆâ–‚â–
wandb:         valid_loss â–ƒâ–ƒâ–†â–â–ƒâ–†â–…â–â–‚â–…â–ˆâ–â–ƒâ–†â–†â–â–ƒâ–„â–…â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 9038
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.94612
wandb:   test_error_force 11.17395
wandb:          test_loss 15.97592
wandb: train_error_energy 1.51051
wandb:  train_error_force 3.13173
wandb:         train_loss 0.34537
wandb: valid_error_energy 1.24282
wandb:  valid_error_force 3.14494
wandb:         valid_loss 0.20985
wandb: 
wandb: ğŸš€ View run al_71_109 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/ccf6z4fz
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_231132-ccf6z4fz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 4.233267307281494, Uncertainty Bias: -0.08850528299808502
9.536743e-06 0.0028009117
1.7843913 4.527074
(48745, 22, 3)
Found uncertainty sample 0 after 1313 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Did not find any uncertainty samples for sample 4.
Found uncertainty sample 5 after 2664 steps.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Found uncertainty sample 11 after 263 steps.
Found uncertainty sample 12 after 1922 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 885 steps.
Did not find any uncertainty samples for sample 15.
Found uncertainty sample 16 after 1182 steps.
Found uncertainty sample 17 after 2824 steps.
Found uncertainty sample 18 after 2117 steps.
Did not find any uncertainty samples for sample 19.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Found uncertainty sample 22 after 3753 steps.
Found uncertainty sample 23 after 1389 steps.
Found uncertainty sample 24 after 1562 steps.
Found uncertainty sample 25 after 1409 steps.
Found uncertainty sample 26 after 2197 steps.
Did not find any uncertainty samples for sample 27.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1257 steps.
Found uncertainty sample 30 after 506 steps.
Found uncertainty sample 31 after 3541 steps.
Found uncertainty sample 32 after 803 steps.
Found uncertainty sample 33 after 929 steps.
Did not find any uncertainty samples for sample 34.
Found uncertainty sample 35 after 454 steps.
Found uncertainty sample 36 after 3187 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 955 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 877 steps.
Found uncertainty sample 41 after 3785 steps.
Found uncertainty sample 42 after 1880 steps.
Found uncertainty sample 43 after 167 steps.
Found uncertainty sample 44 after 3833 steps.
Found uncertainty sample 45 after 510 steps.
Found uncertainty sample 46 after 2743 steps.
Found uncertainty sample 47 after 193 steps.
Found uncertainty sample 48 after 842 steps.
Found uncertainty sample 49 after 1353 steps.
Found uncertainty sample 50 after 641 steps.
Did not find any uncertainty samples for sample 51.
Did not find any uncertainty samples for sample 52.
Found uncertainty sample 53 after 831 steps.
Did not find any uncertainty samples for sample 54.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Found uncertainty sample 57 after 1431 steps.
Found uncertainty sample 58 after 140 steps.
Did not find any uncertainty samples for sample 59.
Found uncertainty sample 60 after 1751 steps.
Found uncertainty sample 61 after 1192 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 486 steps.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 1273 steps.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 1651 steps.
Found uncertainty sample 68 after 3309 steps.
Did not find any uncertainty samples for sample 69.
Found uncertainty sample 70 after 2564 steps.
Found uncertainty sample 71 after 2911 steps.
Found uncertainty sample 72 after 755 steps.
Found uncertainty sample 73 after 1843 steps.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 2791 steps.
Found uncertainty sample 78 after 1641 steps.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 2532 steps.
Found uncertainty sample 82 after 422 steps.
Did not find any uncertainty samples for sample 83.
Did not find any uncertainty samples for sample 84.
Found uncertainty sample 85 after 732 steps.
Did not find any uncertainty samples for sample 86.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Found uncertainty sample 89 after 2776 steps.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 2351 steps.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 3500 steps.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 2232 steps.
Found uncertainty sample 97 after 1253 steps.
Found uncertainty sample 98 after 3192 steps.
Found uncertainty sample 99 after 5 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241204_234535-nje36yf1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_110
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/nje36yf1
Training model 110. Added 58 samples to the dataset.
Epoch 0, Batch 100/285, Loss: 0.9320061206817627, Variance: 0.12795192003250122
Epoch 0, Batch 200/285, Loss: 0.5411900281906128, Variance: 0.12881451845169067

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.323101942057141, Training Loss Force: 3.4620307463854147, time: 5.3299360275268555
Validation Loss Energy: 5.552798199222478, Validation Loss Force: 3.271052506155222, time: 0.2991039752960205
Test Loss Energy: 10.072477735345931, Test Loss Force: 10.564838672565958, time: 12.481745481491089

Epoch 1, Batch 100/285, Loss: 1.1759225130081177, Variance: 0.1401703655719757
Epoch 1, Batch 200/285, Loss: 1.0502814054489136, Variance: 0.1376248598098755

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.015275998631522, Training Loss Force: 3.2743901838324865, time: 5.149652481079102
Validation Loss Energy: 5.12529071816685, Validation Loss Force: 3.3246377470550854, time: 0.27179956436157227
Test Loss Energy: 10.951264887186392, Test Loss Force: 10.578146318035468, time: 11.211086750030518

Epoch 2, Batch 100/285, Loss: 1.8838918209075928, Variance: 0.14182612299919128
Epoch 2, Batch 200/285, Loss: 1.8107752799987793, Variance: 0.15106749534606934

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 4.022917697061379, Training Loss Force: 3.2508883547506406, time: 4.735195875167847
Validation Loss Energy: 1.7469261571689199, Validation Loss Force: 3.1940300016315826, time: 0.27689456939697266
Test Loss Energy: 8.78499099338961, Test Loss Force: 10.404548625428992, time: 10.812348365783691

Epoch 3, Batch 100/285, Loss: 1.4208904504776, Variance: 0.1533624827861786
Epoch 3, Batch 200/285, Loss: 1.4751684665679932, Variance: 0.13829228281974792

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.964849189480985, Training Loss Force: 3.268099749208217, time: 4.610507488250732
Validation Loss Energy: 3.645785840941943, Validation Loss Force: 3.234118731190813, time: 0.2544896602630615
Test Loss Energy: 9.183787369742912, Test Loss Force: 10.237138803465216, time: 10.933220624923706

Epoch 4, Batch 100/285, Loss: 0.6820385456085205, Variance: 0.14283111691474915
Epoch 4, Batch 200/285, Loss: 0.8143234252929688, Variance: 0.14897187054157257

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.9585278284562313, Training Loss Force: 3.2050179794087468, time: 4.645042896270752
Validation Loss Energy: 6.012919050337908, Validation Loss Force: 3.3742058138932656, time: 0.27173542976379395
Test Loss Energy: 12.0210504183897, Test Loss Force: 10.573880667357265, time: 10.857089519500732

Epoch 5, Batch 100/285, Loss: 0.6562786102294922, Variance: 0.08574680984020233
Epoch 5, Batch 200/285, Loss: 0.3456689715385437, Variance: 0.08122118562459946

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8310310062429491, Training Loss Force: 3.2683369004428777, time: 4.65556788444519
Validation Loss Energy: 1.3099236183926386, Validation Loss Force: 3.2110648147686667, time: 0.2851884365081787
Test Loss Energy: 8.385460748495225, Test Loss Force: 10.420592723147042, time: 11.029324293136597

Epoch 6, Batch 100/285, Loss: 0.39364808797836304, Variance: 0.07339063286781311
Epoch 6, Batch 200/285, Loss: 0.3652695417404175, Variance: 0.07291553169488907

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 1.5317659006121378, Training Loss Force: 3.1223582937191416, time: 4.622957706451416
Validation Loss Energy: 1.865676169365074, Validation Loss Force: 3.123716785677723, time: 0.2609860897064209
Test Loss Energy: 9.418154494044948, Test Loss Force: 11.011200169950321, time: 10.837512493133545

Epoch 7, Batch 100/285, Loss: 0.5037616491317749, Variance: 0.07265548408031464
Epoch 7, Batch 200/285, Loss: 0.19428104162216187, Variance: 0.07135973870754242

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 1.5078061395403195, Training Loss Force: 3.1100059533308957, time: 4.66219687461853
Validation Loss Energy: 1.627759352865418, Validation Loss Force: 3.2002943972477977, time: 0.37090587615966797
Test Loss Energy: 9.307421873891569, Test Loss Force: 11.036176732556019, time: 10.934690237045288

Epoch 8, Batch 100/285, Loss: 0.2091018557548523, Variance: 0.07335841655731201
Epoch 8, Batch 200/285, Loss: 0.5362106561660767, Variance: 0.0734916627407074

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 1.5204297809925826, Training Loss Force: 3.1201970466750897, time: 4.622155666351318
Validation Loss Energy: 4.056728919698174, Validation Loss Force: 3.114866622337506, time: 0.2687413692474365
Test Loss Energy: 10.933313776290971, Test Loss Force: 11.306619644002017, time: 10.83028531074524

Epoch 9, Batch 100/285, Loss: 0.8773173689842224, Variance: 0.13573665916919708
Epoch 9, Batch 200/285, Loss: 1.0431782007217407, Variance: 0.14631980657577515

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 3.963548946989683, Training Loss Force: 3.271176607873847, time: 4.828300714492798
Validation Loss Energy: 5.508012397631389, Validation Loss Force: 3.2005697499136523, time: 0.2529301643371582
Test Loss Energy: 9.989709445403465, Test Loss Force: 10.36545685814928, time: 10.774831295013428

Epoch 10, Batch 100/285, Loss: 1.5534043312072754, Variance: 0.15345612168312073
Epoch 10, Batch 200/285, Loss: 1.7048029899597168, Variance: 0.14606964588165283

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 3.9636103392171007, Training Loss Force: 3.293818343393204, time: 4.687526226043701
Validation Loss Energy: 2.087691491879674, Validation Loss Force: 3.4116586306368326, time: 0.2595250606536865
Test Loss Energy: 9.455476071925517, Test Loss Force: 10.505931319180968, time: 10.877245903015137

Epoch 11, Batch 100/285, Loss: 1.1642440557479858, Variance: 0.10735564678907394
Epoch 11, Batch 200/285, Loss: 0.608352541923523, Variance: 0.11493149399757385

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.521560107747938, Training Loss Force: 3.2388270374670554, time: 4.95045018196106
Validation Loss Energy: 3.3789063150299725, Validation Loss Force: 3.1187974007705694, time: 0.26098179817199707
Test Loss Energy: 9.226644194818187, Test Loss Force: 10.438794976859702, time: 10.85237431526184

Epoch 12, Batch 100/285, Loss: 1.2019243240356445, Variance: 0.11077044904232025
Epoch 12, Batch 200/285, Loss: 0.47972720861434937, Variance: 0.10902190953493118

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.5065466132384233, Training Loss Force: 3.0986070517549984, time: 4.689096212387085
Validation Loss Energy: 3.6384544593122703, Validation Loss Force: 3.170270758422795, time: 0.28994035720825195
Test Loss Energy: 10.01707026062242, Test Loss Force: 10.754432907289434, time: 10.94080138206482

Epoch 13, Batch 100/285, Loss: 1.166900396347046, Variance: 0.10761117190122604
Epoch 13, Batch 200/285, Loss: 0.5614066123962402, Variance: 0.11163757741451263

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.50103838368134, Training Loss Force: 3.1025894394270206, time: 4.843391180038452
Validation Loss Energy: 3.23447923253807, Validation Loss Force: 3.121140459125509, time: 0.25873374938964844
Test Loss Energy: 9.251009911837215, Test Loss Force: 10.722912040194567, time: 10.922888278961182

Epoch 14, Batch 100/285, Loss: 1.0871615409851074, Variance: 0.11121624708175659
Epoch 14, Batch 200/285, Loss: 0.3003844618797302, Variance: 0.10465668886899948

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5322614615033405, Training Loss Force: 3.124514426404494, time: 4.820918321609497
Validation Loss Energy: 3.50931419262809, Validation Loss Force: 3.1660494412881257, time: 0.2550778388977051
Test Loss Energy: 10.128035118865967, Test Loss Force: 10.828171737197959, time: 10.99150037765503

Epoch 15, Batch 100/285, Loss: 1.1782530546188354, Variance: 0.10888125002384186
Epoch 15, Batch 200/285, Loss: 0.527368426322937, Variance: 0.10992307215929031

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.509912669121718, Training Loss Force: 3.111216667980925, time: 4.72823691368103
Validation Loss Energy: 3.1370104008925868, Validation Loss Force: 3.151183381098549, time: 0.2672610282897949
Test Loss Energy: 9.250480738029015, Test Loss Force: 10.879774148291295, time: 10.989342451095581

Epoch 16, Batch 100/285, Loss: 1.0130897760391235, Variance: 0.11234492063522339
Epoch 16, Batch 200/285, Loss: 0.7244383096694946, Variance: 0.11215971410274506

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5208991018875273, Training Loss Force: 3.1147549674334902, time: 4.717832326889038
Validation Loss Energy: 3.706227198446411, Validation Loss Force: 3.1871163918535, time: 0.2608916759490967
Test Loss Energy: 10.182572285986486, Test Loss Force: 10.638468345629088, time: 12.367702007293701

Epoch 17, Batch 100/285, Loss: 0.920026957988739, Variance: 0.10504038631916046
Epoch 17, Batch 200/285, Loss: 0.5700874924659729, Variance: 0.11155575513839722

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5292773851643418, Training Loss Force: 3.1072165128736478, time: 4.662448406219482
Validation Loss Energy: 2.9494814299587597, Validation Loss Force: 3.1977777905605222, time: 0.26729893684387207
Test Loss Energy: 9.162698027521593, Test Loss Force: 10.870092186053292, time: 11.034023761749268

Epoch 18, Batch 100/285, Loss: 1.2064764499664307, Variance: 0.1164400652050972
Epoch 18, Batch 200/285, Loss: 0.37335526943206787, Variance: 0.10761062800884247

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.502491216650575, Training Loss Force: 3.1151308384208827, time: 4.7891223430633545
Validation Loss Energy: 3.33266510336727, Validation Loss Force: 3.0890512522352225, time: 0.2602391242980957
Test Loss Energy: 10.153799216774337, Test Loss Force: 10.849894107982431, time: 11.228260517120361

Epoch 19, Batch 100/285, Loss: 1.299950361251831, Variance: 0.11023829877376556
Epoch 19, Batch 200/285, Loss: 0.67017662525177, Variance: 0.11130593717098236

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.523769684544142, Training Loss Force: 3.119821726500847, time: 4.673102140426636
Validation Loss Energy: 2.977106298608511, Validation Loss Force: 3.245112263195901, time: 0.26277875900268555
Test Loss Energy: 9.166283124346576, Test Loss Force: 10.652557944517126, time: 11.53008222579956

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.039 MB of 0.061 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–„â–†â–‚â–ƒâ–ˆâ–â–ƒâ–ƒâ–†â–„â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–„â–‚â–„â–ƒ
wandb:   test_error_force â–ƒâ–ƒâ–‚â–â–ƒâ–‚â–†â–†â–ˆâ–‚â–ƒâ–‚â–„â–„â–…â–…â–„â–…â–…â–„
wandb:          test_loss â–‚â–‚â–â–â–ƒâ–…â–‡â–†â–ˆâ–‚â–â–ƒâ–„â–ƒâ–„â–ƒâ–„â–ƒâ–„â–ƒ
wandb: train_error_energy â–ˆâ–‡â–‡â–‡â–‡â–‚â–â–â–â–‡â–‡â–„â–ƒâ–ƒâ–„â–ƒâ–„â–„â–ƒâ–„
wandb:  train_error_force â–ˆâ–„â–„â–„â–ƒâ–„â–â–â–â–„â–…â–„â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–…â–…â–…â–…â–‚â–â–â–â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: valid_error_energy â–‡â–‡â–‚â–„â–ˆâ–â–‚â–â–…â–‡â–‚â–„â–„â–„â–„â–„â–…â–ƒâ–„â–ƒ
wandb:  valid_error_force â–…â–†â–ƒâ–„â–‡â–„â–‚â–ƒâ–‚â–ƒâ–ˆâ–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–ƒâ–â–„
wandb:         valid_loss â–†â–…â–ƒâ–„â–†â–â–‚â–â–ˆâ–†â–ƒâ–„â–„â–„â–„â–„â–„â–ƒâ–„â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 9090
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.16628
wandb:   test_error_force 10.65256
wandb:          test_loss 11.15663
wandb: train_error_energy 2.52377
wandb:  train_error_force 3.11982
wandb:         train_loss 0.81101
wandb: valid_error_energy 2.97711
wandb:  valid_error_force 3.24511
wandb:         valid_loss 1.01058
wandb: 
wandb: ğŸš€ View run al_71_110 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/nje36yf1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241204_234535-nje36yf1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.2777695655822754, Uncertainty Bias: -0.13861499726772308
6.1035156e-05 0.09056473
1.9017434 4.4200287
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 3571 steps.
Found uncertainty sample 4 after 3264 steps.
Found uncertainty sample 5 after 1001 steps.
Found uncertainty sample 6 after 1459 steps.
Found uncertainty sample 7 after 84 steps.
Found uncertainty sample 8 after 2232 steps.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 3186 steps.
Found uncertainty sample 11 after 1617 steps.
Found uncertainty sample 12 after 463 steps.
Found uncertainty sample 13 after 3915 steps.
Did not find any uncertainty samples for sample 14.
Found uncertainty sample 15 after 1099 steps.
Did not find any uncertainty samples for sample 16.
Found uncertainty sample 17 after 474 steps.
Found uncertainty sample 18 after 1181 steps.
Found uncertainty sample 19 after 1847 steps.
Found uncertainty sample 20 after 1745 steps.
Found uncertainty sample 21 after 581 steps.
Found uncertainty sample 22 after 194 steps.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Found uncertainty sample 25 after 1812 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 808 steps.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 2458 steps.
Found uncertainty sample 32 after 3037 steps.
Did not find any uncertainty samples for sample 33.
Did not find any uncertainty samples for sample 34.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 837 steps.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 544 steps.
Did not find any uncertainty samples for sample 39.
Found uncertainty sample 40 after 1986 steps.
Found uncertainty sample 41 after 2139 steps.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Found uncertainty sample 44 after 1015 steps.
Found uncertainty sample 45 after 375 steps.
Found uncertainty sample 46 after 2233 steps.
Found uncertainty sample 47 after 1364 steps.
Found uncertainty sample 48 after 947 steps.
Did not find any uncertainty samples for sample 49.
Found uncertainty sample 50 after 1787 steps.
Found uncertainty sample 51 after 2756 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 3554 steps.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Found uncertainty sample 64 after 2021 steps.
Did not find any uncertainty samples for sample 65.
Found uncertainty sample 66 after 1778 steps.
Did not find any uncertainty samples for sample 67.
Found uncertainty sample 68 after 2718 steps.
Did not find any uncertainty samples for sample 69.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 3326 steps.
Found uncertainty sample 72 after 1958 steps.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Found uncertainty sample 76 after 296 steps.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 2076 steps.
Found uncertainty sample 79 after 2297 steps.
Did not find any uncertainty samples for sample 80.
Did not find any uncertainty samples for sample 81.
Found uncertainty sample 82 after 2361 steps.
Found uncertainty sample 83 after 573 steps.
Found uncertainty sample 84 after 1251 steps.
Found uncertainty sample 85 after 4 steps.
Found uncertainty sample 86 after 1616 steps.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 798 steps.
Did not find any uncertainty samples for sample 93.
Found uncertainty sample 94 after 1779 steps.
Found uncertainty sample 95 after 785 steps.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 1185 steps.
Found uncertainty sample 99 after 3541 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241205_002116-dfu0e64f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_111
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/dfu0e64f
Training model 111. Added 51 samples to the dataset.
Epoch 0, Batch 100/286, Loss: 0.6751863360404968, Variance: 0.09562596678733826
Epoch 0, Batch 200/286, Loss: 1.1384834051132202, Variance: 0.1108749657869339

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.569809572085361, Training Loss Force: 3.2426488369077435, time: 4.7241294384002686
Validation Loss Energy: 3.2779748445638246, Validation Loss Force: 3.0920377353542348, time: 0.26728272438049316
Test Loss Energy: 9.268568355603156, Test Loss Force: 10.737261245659004, time: 10.70271348953247

Epoch 1, Batch 100/286, Loss: 1.0234795808792114, Variance: 0.10978534817695618
Epoch 1, Batch 200/286, Loss: 0.3720235824584961, Variance: 0.1074814572930336

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.4881011344371036, Training Loss Force: 3.1026475095433312, time: 4.689728498458862
Validation Loss Energy: 1.9817482733446419, Validation Loss Force: 3.1336004455918487, time: 0.2568833827972412
Test Loss Energy: 9.598013745625934, Test Loss Force: 10.702110559049228, time: 10.910658836364746

Epoch 2, Batch 100/286, Loss: 0.37307971715927124, Variance: 0.1067245677113533
Epoch 2, Batch 200/286, Loss: 0.6350456476211548, Variance: 0.1099746823310852

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.515619779471497, Training Loss Force: 3.0992775412612468, time: 4.644538402557373
Validation Loss Energy: 2.7955285614040664, Validation Loss Force: 3.0705262481059306, time: 0.2637159824371338
Test Loss Energy: 9.57049762625544, Test Loss Force: 10.825498714179224, time: 10.755402565002441

Epoch 3, Batch 100/286, Loss: 0.733002781867981, Variance: 0.11171138286590576
Epoch 3, Batch 200/286, Loss: 1.3209316730499268, Variance: 0.11621993780136108

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.541664772459139, Training Loss Force: 3.1026343681709045, time: 4.742525577545166
Validation Loss Energy: 3.092133260417838, Validation Loss Force: 3.2238454893389714, time: 0.2580394744873047
Test Loss Energy: 9.238874102968495, Test Loss Force: 10.564859232739035, time: 11.655146598815918

Epoch 4, Batch 100/286, Loss: 1.3851301670074463, Variance: 0.10890457034111023
Epoch 4, Batch 200/286, Loss: 0.41184717416763306, Variance: 0.10781295597553253

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.508299366632952, Training Loss Force: 3.1182314823091004, time: 5.102560043334961
Validation Loss Energy: 1.7791565585646396, Validation Loss Force: 3.096741799197276, time: 0.29236316680908203
Test Loss Energy: 9.389358864134827, Test Loss Force: 10.813023371002979, time: 12.089261293411255

Epoch 5, Batch 100/286, Loss: 0.498260498046875, Variance: 0.11064302921295166
Epoch 5, Batch 200/286, Loss: 0.5767914056777954, Variance: 0.10869473218917847

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5145303321361547, Training Loss Force: 3.110981602258218, time: 5.259373903274536
Validation Loss Energy: 2.5926833823352546, Validation Loss Force: 3.18318049203008, time: 0.29537296295166016
Test Loss Energy: 9.605863786650055, Test Loss Force: 10.498172943233321, time: 12.264916181564331

Epoch 6, Batch 100/286, Loss: 0.8089704513549805, Variance: 0.11127431690692902
Epoch 6, Batch 200/286, Loss: 1.388298511505127, Variance: 0.11158992350101471

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.518929531710159, Training Loss Force: 3.11255825630264, time: 4.981675624847412
Validation Loss Energy: 3.4310910115357793, Validation Loss Force: 3.127444890654162, time: 0.291961669921875
Test Loss Energy: 9.2920477514936, Test Loss Force: 10.722479633034474, time: 11.93280029296875

Epoch 7, Batch 100/286, Loss: 1.203810453414917, Variance: 0.11335436999797821
Epoch 7, Batch 200/286, Loss: 0.5767937898635864, Variance: 0.11263450980186462

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5055230494758978, Training Loss Force: 3.1183717833351214, time: 5.202252149581909
Validation Loss Energy: 2.038550994335504, Validation Loss Force: 3.1058454490932172, time: 0.28673362731933594
Test Loss Energy: 9.267119308739565, Test Loss Force: 10.645315004507443, time: 11.99283504486084

Epoch 8, Batch 100/286, Loss: 0.3869587779045105, Variance: 0.10797904431819916
Epoch 8, Batch 200/286, Loss: 0.6469465494155884, Variance: 0.1125694066286087

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.4957650903431783, Training Loss Force: 3.1116289559648718, time: 5.026050090789795
Validation Loss Energy: 2.5904597637101903, Validation Loss Force: 3.1402311319531155, time: 0.3019075393676758
Test Loss Energy: 9.901536786428592, Test Loss Force: 10.92115017924548, time: 12.143974304199219

Epoch 9, Batch 100/286, Loss: 0.5594990253448486, Variance: 0.10984861850738525
Epoch 9, Batch 200/286, Loss: 1.2068663835525513, Variance: 0.11379493772983551

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.5200998437593918, Training Loss Force: 3.1382937629236274, time: 5.185464859008789
Validation Loss Energy: 3.1696474629495146, Validation Loss Force: 3.127466489561318, time: 0.29396796226501465
Test Loss Energy: 9.233389568380323, Test Loss Force: 10.776504880291082, time: 11.969608545303345

Epoch 10, Batch 100/286, Loss: 1.3319852352142334, Variance: 0.11454496532678604
Epoch 10, Batch 200/286, Loss: 0.7059164047241211, Variance: 0.11122770607471466

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5203803383696775, Training Loss Force: 3.1136060259688034, time: 5.054828643798828
Validation Loss Energy: 2.0634872666496653, Validation Loss Force: 3.1565519787406973, time: 0.28473901748657227
Test Loss Energy: 9.181126352205142, Test Loss Force: 10.790863976462505, time: 12.224134922027588

Epoch 11, Batch 100/286, Loss: 0.5565751791000366, Variance: 0.10889185965061188
Epoch 11, Batch 200/286, Loss: 0.7246379852294922, Variance: 0.11050679534673691

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5303265144372187, Training Loss Force: 3.1353692123468693, time: 5.3210835456848145
Validation Loss Energy: 2.473318781029425, Validation Loss Force: 3.1700836718063874, time: 0.2821967601776123
Test Loss Energy: 9.84265648909354, Test Loss Force: 10.67420660365365, time: 12.050491571426392

Epoch 12, Batch 100/286, Loss: 0.5692709684371948, Variance: 0.10924223065376282
Epoch 12, Batch 200/286, Loss: 1.4979448318481445, Variance: 0.11127299070358276

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.513272062660027, Training Loss Force: 3.1127387607879524, time: 5.176023244857788
Validation Loss Energy: 3.2496563169906785, Validation Loss Force: 3.2100717103778287, time: 0.28916120529174805
Test Loss Energy: 9.33056343755278, Test Loss Force: 10.717195920368177, time: 12.006702899932861

Epoch 13, Batch 100/286, Loss: 1.2707401514053345, Variance: 0.11423102766275406
Epoch 13, Batch 200/286, Loss: 0.395871639251709, Variance: 0.1083582192659378

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.519663683562735, Training Loss Force: 3.1151211649366903, time: 5.06956148147583
Validation Loss Energy: 1.9665914072504833, Validation Loss Force: 3.096366200881925, time: 0.28514766693115234
Test Loss Energy: 9.131588331245656, Test Loss Force: 10.554208405570316, time: 13.316575527191162

Epoch 14, Batch 100/286, Loss: 0.45485013723373413, Variance: 0.11009535193443298
Epoch 14, Batch 200/286, Loss: 0.5844389200210571, Variance: 0.10746128112077713

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.5199895703684074, Training Loss Force: 3.114815015103689, time: 5.088212966918945
Validation Loss Energy: 2.826275696818018, Validation Loss Force: 3.136157622597513, time: 0.29050421714782715
Test Loss Energy: 9.835113129934365, Test Loss Force: 10.729196831371903, time: 12.026970386505127

Epoch 15, Batch 100/286, Loss: 0.7449749708175659, Variance: 0.10975154489278793
Epoch 15, Batch 200/286, Loss: 1.2171024084091187, Variance: 0.11350187659263611

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.519986430032742, Training Loss Force: 3.123970410506315, time: 5.127161026000977
Validation Loss Energy: 3.086552548147279, Validation Loss Force: 3.128149282230391, time: 0.2985405921936035
Test Loss Energy: 9.365180249820344, Test Loss Force: 10.910123192775055, time: 12.051156997680664

Epoch 16, Batch 100/286, Loss: 1.2697302103042603, Variance: 0.11217041313648224
Epoch 16, Batch 200/286, Loss: 0.6549367308616638, Variance: 0.11368252336978912

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.523727615420091, Training Loss Force: 3.1135329811461774, time: 5.009343385696411
Validation Loss Energy: 2.077113727703862, Validation Loss Force: 3.1430095975560026, time: 0.2778160572052002
Test Loss Energy: 9.247167127354233, Test Loss Force: 10.789523662090554, time: 10.862646579742432

Epoch 17, Batch 100/286, Loss: 0.482014000415802, Variance: 0.10739400237798691
Epoch 17, Batch 200/286, Loss: 0.6443140506744385, Variance: 0.11240613460540771

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.511856257098845, Training Loss Force: 3.122096308632543, time: 5.124282360076904
Validation Loss Energy: 2.5711935830617634, Validation Loss Force: 3.082377991533254, time: 0.3003838062286377
Test Loss Energy: 9.521733143919219, Test Loss Force: 10.777267919656088, time: 11.766295671463013

Epoch 18, Batch 100/286, Loss: 0.7862574458122253, Variance: 0.10572848469018936
Epoch 18, Batch 200/286, Loss: 1.054482340812683, Variance: 0.11025002598762512

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5202753377996396, Training Loss Force: 3.128537237904777, time: 4.672881841659546
Validation Loss Energy: 2.976792067909778, Validation Loss Force: 3.112274530809761, time: 0.24028253555297852
Test Loss Energy: 9.184423070335814, Test Loss Force: 10.630337643553306, time: 10.126781702041626

Epoch 19, Batch 100/286, Loss: 1.1515605449676514, Variance: 0.11089114844799042
Epoch 19, Batch 200/286, Loss: 0.6029907464981079, Variance: 0.1075257882475853

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.504269593468283, Training Loss Force: 3.1252668525589824, time: 4.696859836578369
Validation Loss Energy: 2.1800316296208315, Validation Loss Force: 3.1437853768604147, time: 0.2544889450073242
Test Loss Energy: 9.351845063325163, Test Loss Force: 10.81686818865345, time: 10.361770153045654

wandb: - 0.039 MB of 0.061 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–…â–…â–‚â–ƒâ–…â–‚â–‚â–ˆâ–‚â–â–‡â–ƒâ–â–‡â–ƒâ–‚â–…â–â–ƒ
wandb:   test_error_force â–…â–„â–†â–‚â–†â–â–…â–ƒâ–ˆâ–†â–†â–„â–…â–‚â–…â–ˆâ–†â–†â–ƒâ–†
wandb:          test_loss â–„â–…â–…â–‚â–„â–„â–‚â–„â–ˆâ–‚â–‚â–…â–ƒâ–‚â–…â–„â–„â–„â–â–ƒ
wandb: train_error_energy â–ˆâ–â–ƒâ–†â–ƒâ–ƒâ–„â–‚â–‚â–„â–„â–…â–ƒâ–„â–„â–„â–„â–ƒâ–„â–‚
wandb:  train_error_force â–ˆâ–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–â–‚â–â–â–â–â–‚â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb: valid_error_energy â–‡â–‚â–…â–‡â–â–„â–ˆâ–‚â–„â–‡â–‚â–„â–‡â–‚â–…â–‡â–‚â–„â–†â–ƒ
wandb:  valid_error_force â–‚â–„â–â–ˆâ–‚â–†â–„â–ƒâ–„â–„â–…â–†â–‡â–‚â–„â–„â–„â–‚â–ƒâ–„
wandb:         valid_loss â–‡â–‚â–…â–‡â–â–„â–ˆâ–‚â–„â–‡â–‚â–„â–‡â–‚â–…â–†â–‚â–„â–†â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 9135
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.35185
wandb:   test_error_force 10.81687
wandb:          test_loss 11.52204
wandb: train_error_energy 2.50427
wandb:  train_error_force 3.12527
wandb:         train_loss 0.81051
wandb: valid_error_energy 2.18003
wandb:  valid_error_force 3.14379
wandb:         valid_loss 0.674
wandb: 
wandb: ğŸš€ View run al_71_111 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/dfu0e64f
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241205_002116-dfu0e64f/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.5347301959991455, Uncertainty Bias: -0.17954665422439575
1.5258789e-05 0.010410309
1.7719592 4.443405
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Found uncertainty sample 1 after 3447 steps.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 2502 steps.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Found uncertainty sample 8 after 188 steps.
Found uncertainty sample 9 after 2630 steps.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 1691 steps.
Found uncertainty sample 13 after 556 steps.
Found uncertainty sample 14 after 2329 steps.
Found uncertainty sample 15 after 2382 steps.
Found uncertainty sample 16 after 377 steps.
Found uncertainty sample 17 after 1392 steps.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 2968 steps.
Did not find any uncertainty samples for sample 20.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 2318 steps.
Found uncertainty sample 25 after 1725 steps.
Did not find any uncertainty samples for sample 26.
Found uncertainty sample 27 after 1284 steps.
Found uncertainty sample 28 after 2087 steps.
Found uncertainty sample 29 after 1851 steps.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Found uncertainty sample 34 after 1759 steps.
Found uncertainty sample 35 after 3017 steps.
Did not find any uncertainty samples for sample 36.
Did not find any uncertainty samples for sample 37.
Found uncertainty sample 38 after 1739 steps.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 1930 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 466 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 2083 steps.
Did not find any uncertainty samples for sample 46.
Found uncertainty sample 47 after 1032 steps.
Did not find any uncertainty samples for sample 48.
Found uncertainty sample 49 after 1462 steps.
Found uncertainty sample 50 after 873 steps.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 2138 steps.
Found uncertainty sample 53 after 3721 steps.
Found uncertainty sample 54 after 1786 steps.
Found uncertainty sample 55 after 1725 steps.
Found uncertainty sample 56 after 308 steps.
Did not find any uncertainty samples for sample 57.
Found uncertainty sample 58 after 3566 steps.
Found uncertainty sample 59 after 1663 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 3627 steps.
Did not find any uncertainty samples for sample 62.
Found uncertainty sample 63 after 386 steps.
Found uncertainty sample 64 after 1735 steps.
Did not find any uncertainty samples for sample 65.
Did not find any uncertainty samples for sample 66.
Found uncertainty sample 67 after 792 steps.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 3308 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 1580 steps.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 2760 steps.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 1489 steps.
Did not find any uncertainty samples for sample 78.
Found uncertainty sample 79 after 614 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 2968 steps.
Did not find any uncertainty samples for sample 82.
Did not find any uncertainty samples for sample 83.
Found uncertainty sample 84 after 1364 steps.
Found uncertainty sample 85 after 277 steps.
Did not find any uncertainty samples for sample 86.
Found uncertainty sample 87 after 1095 steps.
Found uncertainty sample 88 after 1629 steps.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Found uncertainty sample 92 after 3499 steps.
Found uncertainty sample 93 after 2735 steps.
Did not find any uncertainty samples for sample 94.
Found uncertainty sample 95 after 1736 steps.
Found uncertainty sample 96 after 3807 steps.
Did not find any uncertainty samples for sample 97.
Found uncertainty sample 98 after 894 steps.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241205_005755-turhwdeb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_112
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/turhwdeb
Training model 112. Added 51 samples to the dataset.
Epoch 0, Batch 100/287, Loss: 0.7415650486946106, Variance: 0.09188076853752136
Epoch 0, Batch 200/287, Loss: 0.5193659067153931, Variance: 0.1271134912967682

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.4680424251551183, Training Loss Force: 3.3778830928914836, time: 4.734202146530151
Validation Loss Energy: 1.7286458441640886, Validation Loss Force: 3.2845808591837224, time: 0.2686588764190674
Test Loss Energy: 8.952990897916552, Test Loss Force: 10.959933138228196, time: 11.100666284561157

Epoch 1, Batch 100/287, Loss: 1.7860536575317383, Variance: 0.14954549074172974
Epoch 1, Batch 200/287, Loss: 1.403071641921997, Variance: 0.1461087316274643

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.949099462246386, Training Loss Force: 3.202338188736121, time: 4.696821689605713
Validation Loss Energy: 4.807847438063767, Validation Loss Force: 3.136906013596976, time: 0.2628602981567383
Test Loss Energy: 9.929372374235719, Test Loss Force: 10.630212785618602, time: 11.203386545181274

Epoch 2, Batch 100/287, Loss: 1.7801005840301514, Variance: 0.15219992399215698
Epoch 2, Batch 200/287, Loss: 1.7059892416000366, Variance: 0.1441255807876587

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.9120089602458776, Training Loss Force: 3.2240095960340103, time: 4.6650660037994385
Validation Loss Energy: 5.3357817322332135, Validation Loss Force: 3.216684761342738, time: 0.28668713569641113
Test Loss Energy: 10.099220094589818, Test Loss Force: 10.528574612247175, time: 11.154982805252075

Epoch 3, Batch 100/287, Loss: 1.1693475246429443, Variance: 0.15243276953697205
Epoch 3, Batch 200/287, Loss: 0.9499800205230713, Variance: 0.14696738123893738

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.8962935266595324, Training Loss Force: 3.221319765164813, time: 4.74478006362915
Validation Loss Energy: 3.279597746367004, Validation Loss Force: 3.2408840433177075, time: 0.2709486484527588
Test Loss Energy: 9.252190160901653, Test Loss Force: 10.50325205209969, time: 12.654165744781494

Epoch 4, Batch 100/287, Loss: 0.8237248063087463, Variance: 0.1528964340686798
Epoch 4, Batch 200/287, Loss: 0.6797412633895874, Variance: 0.15323160588741302

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.9401439776415077, Training Loss Force: 3.195442363472897, time: 4.747217655181885
Validation Loss Energy: 2.6945177761881394, Validation Loss Force: 3.184716508332519, time: 0.2609236240386963
Test Loss Energy: 9.457707888272575, Test Loss Force: 10.522311761558456, time: 11.034287214279175

Epoch 5, Batch 100/287, Loss: 1.5105563402175903, Variance: 0.14954107999801636
Epoch 5, Batch 200/287, Loss: 1.5487115383148193, Variance: 0.15168282389640808

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 3.88675661761343, Training Loss Force: 3.221034036427099, time: 4.577332496643066
Validation Loss Energy: 5.351465567128125, Validation Loss Force: 3.1653495030948164, time: 0.2774646282196045
Test Loss Energy: 10.786414827755147, Test Loss Force: 10.43323510315694, time: 11.234134674072266

Epoch 6, Batch 100/287, Loss: 1.8406836986541748, Variance: 0.14581452310085297
Epoch 6, Batch 200/287, Loss: 2.4449801445007324, Variance: 0.12696947157382965

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 3.591421919948342, Training Loss Force: 3.5205398689331537, time: 4.671452045440674
Validation Loss Energy: 5.570531480936509, Validation Loss Force: 3.1961789815589317, time: 0.2634758949279785
Test Loss Energy: 11.246736533435046, Test Loss Force: 11.020056666187578, time: 11.139864206314087

Epoch 7, Batch 100/287, Loss: 0.9778191447257996, Variance: 0.1473887860774994
Epoch 7, Batch 200/287, Loss: 1.0706048011779785, Variance: 0.15068374574184418

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 3.9045843764495465, Training Loss Force: 3.1685256841884653, time: 4.813662528991699
Validation Loss Energy: 3.324074527717376, Validation Loss Force: 3.1840363431593683, time: 0.2659335136413574
Test Loss Energy: 9.756871174452211, Test Loss Force: 10.538073561431748, time: 11.268843650817871

Epoch 8, Batch 100/287, Loss: 0.7893733978271484, Variance: 0.1519896239042282
Epoch 8, Batch 200/287, Loss: 0.836857795715332, Variance: 0.15631619095802307

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 3.6239238755613514, Training Loss Force: 3.2694860124207548, time: 4.6494081020355225
Validation Loss Energy: 1.371912014322269, Validation Loss Force: 3.494696764806856, time: 0.27010512351989746
Test Loss Energy: 8.593651869611522, Test Loss Force: 10.580806713032276, time: 11.086251020431519

Epoch 9, Batch 100/287, Loss: 2.6335787773132324, Variance: 0.108616903424263
Epoch 9, Batch 200/287, Loss: 0.70268714427948, Variance: 0.10996764153242111

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.7883936743173883, Training Loss Force: 3.6478651705811265, time: 4.9783408641815186
Validation Loss Energy: 3.426258467595538, Validation Loss Force: 3.1238432172187807, time: 0.27300024032592773
Test Loss Energy: 10.120896709779322, Test Loss Force: 10.728789801499603, time: 11.183876752853394

Epoch 10, Batch 100/287, Loss: 0.950736403465271, Variance: 0.10861024260520935
Epoch 10, Batch 200/287, Loss: 0.6638529896736145, Variance: 0.11523343622684479

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.4548896806564544, Training Loss Force: 3.0815538103184927, time: 4.750642776489258
Validation Loss Energy: 2.384783613653881, Validation Loss Force: 3.0968979348024, time: 0.25997042655944824
Test Loss Energy: 9.541891668488596, Test Loss Force: 10.65586398677256, time: 11.140527248382568

Epoch 11, Batch 100/287, Loss: 0.871384859085083, Variance: 0.11091144382953644
Epoch 11, Batch 200/287, Loss: 1.369868278503418, Variance: 0.11294881999492645

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.4607686154986115, Training Loss Force: 3.087949258442432, time: 4.899935245513916
Validation Loss Energy: 1.6754087910077118, Validation Loss Force: 3.0904571852226104, time: 0.2681429386138916
Test Loss Energy: 8.715667265959452, Test Loss Force: 10.605823071627322, time: 11.162133693695068

Epoch 12, Batch 100/287, Loss: 0.5527448654174805, Variance: 0.11271977424621582
Epoch 12, Batch 200/287, Loss: 0.7889872789382935, Variance: 0.10666709393262863

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.4615730202018864, Training Loss Force: 3.0802240402650702, time: 4.739420413970947
Validation Loss Energy: 3.3519836557988185, Validation Loss Force: 3.0975845128469546, time: 0.2690010070800781
Test Loss Energy: 9.45868823873432, Test Loss Force: 10.677672156292449, time: 11.234193801879883

Epoch 13, Batch 100/287, Loss: 1.2130532264709473, Variance: 0.11304144561290741
Epoch 13, Batch 200/287, Loss: 0.49957728385925293, Variance: 0.1101929247379303

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.474409815388227, Training Loss Force: 3.0937387726141568, time: 4.738675355911255
Validation Loss Energy: 2.0307151334888176, Validation Loss Force: 3.0571377758349016, time: 0.26301026344299316
Test Loss Energy: 8.894501528326872, Test Loss Force: 10.52116361415686, time: 11.040946960449219

Epoch 14, Batch 100/287, Loss: 0.7511564493179321, Variance: 0.11298810690641403
Epoch 14, Batch 200/287, Loss: 1.0193575620651245, Variance: 0.1094542071223259

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.4943503502433617, Training Loss Force: 3.083051645922646, time: 4.675999402999878
Validation Loss Energy: 1.9552281553954967, Validation Loss Force: 3.0942439412520875, time: 0.2683432102203369
Test Loss Energy: 9.378552903022396, Test Loss Force: 10.702864346569509, time: 11.199331283569336

Epoch 15, Batch 100/287, Loss: 0.4138808250427246, Variance: 0.11067462712526321
Epoch 15, Batch 200/287, Loss: 0.4899546504020691, Variance: 0.10798615217208862

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.4742581520967777, Training Loss Force: 3.087523200040041, time: 4.748132944107056
Validation Loss Energy: 3.2344958881901853, Validation Loss Force: 3.149163282072863, time: 0.26994776725769043
Test Loss Energy: 9.78067392324837, Test Loss Force: 10.617019132762122, time: 11.080003023147583

Epoch 16, Batch 100/287, Loss: 1.0661659240722656, Variance: 0.10714636743068695
Epoch 16, Batch 200/287, Loss: 0.5654655694961548, Variance: 0.11266036331653595

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.5010685138466853, Training Loss Force: 3.10184455641328, time: 4.767297267913818
Validation Loss Energy: 2.464995141151669, Validation Loss Force: 3.0833822739152175, time: 0.2643311023712158
Test Loss Energy: 9.389128598590183, Test Loss Force: 10.817839669655188, time: 11.363164901733398

Epoch 17, Batch 100/287, Loss: 0.8179184198379517, Variance: 0.10945191234350204
Epoch 17, Batch 200/287, Loss: 1.211892008781433, Variance: 0.11429442465305328

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.4869925446911387, Training Loss Force: 3.098763273185542, time: 4.735972166061401
Validation Loss Energy: 1.6084786764555914, Validation Loss Force: 3.117748361890274, time: 0.2635345458984375
Test Loss Energy: 8.793129725544327, Test Loss Force: 10.629825770321679, time: 11.084006309509277

Epoch 18, Batch 100/287, Loss: 0.5632539987564087, Variance: 0.11401291191577911
Epoch 18, Batch 200/287, Loss: 0.8646982908248901, Variance: 0.11134999245405197

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.4940594306766553, Training Loss Force: 3.0924036127260424, time: 4.720118522644043
Validation Loss Energy: 3.216770622565408, Validation Loss Force: 3.085042806804059, time: 0.27797651290893555
Test Loss Energy: 9.249505036805772, Test Loss Force: 10.53235807114116, time: 11.267825603485107

Epoch 19, Batch 100/287, Loss: 1.3195269107818604, Variance: 0.11421281099319458
Epoch 19, Batch 200/287, Loss: 0.6495575904846191, Variance: 0.10933101177215576

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.4940598791183493, Training Loss Force: 3.1072051911895313, time: 4.601070404052734
Validation Loss Energy: 2.2630481203498714, Validation Loss Force: 3.147308893425576, time: 0.2574465274810791
Test Loss Energy: 9.013961937287931, Test Loss Force: 10.786009462655308, time: 11.023850440979004

wandb: - 0.039 MB of 0.061 MB uploadedwandb: \ 0.039 MB of 0.061 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–…â–…â–ƒâ–ƒâ–‡â–ˆâ–„â–â–…â–„â–â–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–‚
wandb:   test_error_force â–‡â–ƒâ–‚â–‚â–‚â–â–ˆâ–‚â–ƒâ–…â–„â–ƒâ–„â–‚â–„â–ƒâ–†â–ƒâ–‚â–…
wandb:          test_loss â–‚â–‚â–‚â–â–â–ƒâ–…â–‚â–†â–ˆâ–‡â–†â–‡â–†â–‡â–‡â–‡â–†â–†â–†
wandb: train_error_energy â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–†â–ƒâ–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–…â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–†â–‚â–ƒâ–ˆâ–â–â–â–â–â–â–â–â–â–
wandb:         train_loss â–ˆâ–…â–…â–…â–…â–…â–†â–…â–…â–…â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–‚â–‡â–ˆâ–„â–ƒâ–ˆâ–ˆâ–„â–â–„â–ƒâ–‚â–„â–‚â–‚â–„â–ƒâ–â–„â–‚
wandb:  valid_error_force â–…â–‚â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–‚
wandb:         valid_loss â–‚â–†â–‡â–„â–ƒâ–‡â–ˆâ–„â–â–…â–‚â–â–…â–‚â–â–„â–‚â–â–„â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 9180
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.01396
wandb:   test_error_force 10.78601
wandb:          test_loss 11.12313
wandb: train_error_energy 2.49406
wandb:  train_error_force 3.10721
wandb:         train_loss 0.79757
wandb: valid_error_energy 2.26305
wandb:  valid_error_force 3.14731
wandb:         valid_loss 0.70188
wandb: 
wandb: ğŸš€ View run al_71_112 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/turhwdeb
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241205_005755-turhwdeb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.48116397857666, Uncertainty Bias: -0.1726434826850891
3.0517578e-05 0.009158611
1.7377443 4.3864927
(48745, 22, 3)
Found uncertainty sample 0 after 5 steps.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Found uncertainty sample 3 after 387 steps.
Found uncertainty sample 4 after 1406 steps.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Found uncertainty sample 7 after 1862 steps.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Found uncertainty sample 10 after 3632 steps.
Found uncertainty sample 11 after 1929 steps.
Found uncertainty sample 12 after 3245 steps.
Did not find any uncertainty samples for sample 13.
Found uncertainty sample 14 after 2898 steps.
Found uncertainty sample 15 after 1232 steps.
Found uncertainty sample 16 after 431 steps.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Found uncertainty sample 19 after 3421 steps.
Found uncertainty sample 20 after 1108 steps.
Found uncertainty sample 21 after 3905 steps.
Did not find any uncertainty samples for sample 22.
Did not find any uncertainty samples for sample 23.
Found uncertainty sample 24 after 3735 steps.
Found uncertainty sample 25 after 2340 steps.
Found uncertainty sample 26 after 1142 steps.
Found uncertainty sample 27 after 1379 steps.
Did not find any uncertainty samples for sample 28.
Found uncertainty sample 29 after 1617 steps.
Did not find any uncertainty samples for sample 30.
Found uncertainty sample 31 after 2705 steps.
Did not find any uncertainty samples for sample 32.
Found uncertainty sample 33 after 3316 steps.
Found uncertainty sample 34 after 3681 steps.
Found uncertainty sample 35 after 1553 steps.
Found uncertainty sample 36 after 2742 steps.
Did not find any uncertainty samples for sample 37.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Found uncertainty sample 41 after 526 steps.
Did not find any uncertainty samples for sample 42.
Found uncertainty sample 43 after 3005 steps.
Did not find any uncertainty samples for sample 44.
Found uncertainty sample 45 after 288 steps.
Did not find any uncertainty samples for sample 46.
Did not find any uncertainty samples for sample 47.
Found uncertainty sample 48 after 3123 steps.
Did not find any uncertainty samples for sample 49.
Did not find any uncertainty samples for sample 50.
Did not find any uncertainty samples for sample 51.
Found uncertainty sample 52 after 2846 steps.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Found uncertainty sample 55 after 1503 steps.
Found uncertainty sample 56 after 1895 steps.
Found uncertainty sample 57 after 1412 steps.
Did not find any uncertainty samples for sample 58.
Found uncertainty sample 59 after 3257 steps.
Did not find any uncertainty samples for sample 60.
Found uncertainty sample 61 after 3862 steps.
Found uncertainty sample 62 after 1510 steps.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 2893 steps.
Found uncertainty sample 66 after 2745 steps.
Found uncertainty sample 67 after 2294 steps.
Found uncertainty sample 68 after 2476 steps.
Found uncertainty sample 69 after 165 steps.
Did not find any uncertainty samples for sample 70.
Found uncertainty sample 71 after 2174 steps.
Found uncertainty sample 72 after 680 steps.
Did not find any uncertainty samples for sample 73.
Found uncertainty sample 74 after 300 steps.
Found uncertainty sample 75 after 184 steps.
Did not find any uncertainty samples for sample 76.
Did not find any uncertainty samples for sample 77.
Found uncertainty sample 78 after 2670 steps.
Found uncertainty sample 79 after 337 steps.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 2547 steps.
Found uncertainty sample 82 after 3995 steps.
Did not find any uncertainty samples for sample 83.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 888 steps.
Did not find any uncertainty samples for sample 87.
Found uncertainty sample 88 after 523 steps.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Did not find any uncertainty samples for sample 91.
Did not find any uncertainty samples for sample 92.
Found uncertainty sample 93 after 1437 steps.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Found uncertainty sample 96 after 3418 steps.
Found uncertainty sample 97 after 1179 steps.
Did not find any uncertainty samples for sample 98.
Found uncertainty sample 99 after 3792 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241205_013447-btqv2zyw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_71_113
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/btqv2zyw
Training model 113. Added 53 samples to the dataset.
Epoch 0, Batch 100/289, Loss: 0.38580405712127686, Variance: 0.09341297298669815
Epoch 0, Batch 200/289, Loss: 0.5646117925643921, Variance: 0.10460638999938965

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.6017197957416194, Training Loss Force: 3.2827696879209753, time: 4.747697114944458
Validation Loss Energy: 2.219348053514326, Validation Loss Force: 3.0856589768646705, time: 0.2682528495788574
Test Loss Energy: 8.783522309672124, Test Loss Force: 10.487371060113936, time: 10.90105128288269

Epoch 1, Batch 100/289, Loss: 0.7968219518661499, Variance: 0.11289452016353607
Epoch 1, Batch 200/289, Loss: 0.9484161734580994, Variance: 0.10896173119544983

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.4627553370222848, Training Loss Force: 3.0831101213671164, time: 4.748154163360596
Validation Loss Energy: 3.0953297205414603, Validation Loss Force: 3.069334968175217, time: 0.26488780975341797
Test Loss Energy: 9.211732184343864, Test Loss Force: 10.475186916885058, time: 11.085466861724854

Epoch 2, Batch 100/289, Loss: 1.3493602275848389, Variance: 0.11218806356191635
Epoch 2, Batch 200/289, Loss: 0.4753641188144684, Variance: 0.10763950645923615

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.4745485398836142, Training Loss Force: 3.0874585346601635, time: 4.699378490447998
Validation Loss Energy: 1.6256263160489177, Validation Loss Force: 3.142718880719079, time: 0.2766587734222412
Test Loss Energy: 8.704980583229199, Test Loss Force: 10.441822740973079, time: 10.918693542480469

Epoch 3, Batch 100/289, Loss: 0.7051366567611694, Variance: 0.11493664979934692
Epoch 3, Batch 200/289, Loss: 0.7591136693954468, Variance: 0.11559729278087616

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5038069424415474, Training Loss Force: 3.101625621761525, time: 4.6797709465026855
Validation Loss Energy: 2.344711539746767, Validation Loss Force: 3.184690589643933, time: 0.2674441337585449
Test Loss Energy: 9.191685159892053, Test Loss Force: 10.766786549536453, time: 11.18460726737976

Epoch 4, Batch 100/289, Loss: 0.5971622467041016, Variance: 0.1106574684381485
Epoch 4, Batch 200/289, Loss: 1.1093653440475464, Variance: 0.11700476706027985

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.520581375448113, Training Loss Force: 3.101345219668284, time: 4.685896158218384
Validation Loss Energy: 3.6707030527772124, Validation Loss Force: 3.0859495155662775, time: 0.27042722702026367
Test Loss Energy: 10.07626867405793, Test Loss Force: 10.685496083238718, time: 10.894999742507935

Epoch 5, Batch 100/289, Loss: 1.0112165212631226, Variance: 0.1073724776506424
Epoch 5, Batch 200/289, Loss: 0.5758178234100342, Variance: 0.11268630623817444

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5113075749660876, Training Loss Force: 3.1035095244721473, time: 4.654290199279785
Validation Loss Energy: 1.9783816939935306, Validation Loss Force: 3.1766844759682704, time: 0.2739415168762207
Test Loss Energy: 9.055047519074494, Test Loss Force: 10.469793958772025, time: 11.012449741363525

Epoch 6, Batch 100/289, Loss: 0.33912789821624756, Variance: 0.10566580295562744
Epoch 6, Batch 200/289, Loss: 0.5550026893615723, Variance: 0.10750387609004974

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 2.5059096522597097, Training Loss Force: 3.126646240994067, time: 4.722221612930298
Validation Loss Energy: 2.198405307798551, Validation Loss Force: 3.139703985017046, time: 0.2600545883178711
Test Loss Energy: 9.173261918127224, Test Loss Force: 10.829047719949656, time: 10.89136028289795

Epoch 7, Batch 100/289, Loss: 0.5546423196792603, Variance: 0.1080494299530983
Epoch 7, Batch 200/289, Loss: 0.8859798908233643, Variance: 0.11006844788789749

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 2.5096738391147118, Training Loss Force: 3.109071900670496, time: 4.628741502761841
Validation Loss Energy: 3.2324910197340806, Validation Loss Force: 3.173668110649874, time: 0.2647533416748047
Test Loss Energy: 9.489598889778323, Test Loss Force: 10.739807158403822, time: 11.158856391906738

Epoch 8, Batch 100/289, Loss: 1.2685657739639282, Variance: 0.11081577837467194
Epoch 8, Batch 200/289, Loss: 0.45522528886795044, Variance: 0.10813470929861069

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 2.504338671353893, Training Loss Force: 3.0983632452527594, time: 4.653741359710693
Validation Loss Energy: 1.613262010514583, Validation Loss Force: 3.121454511688926, time: 0.27240562438964844
Test Loss Energy: 8.905485624756045, Test Loss Force: 10.605933967856833, time: 10.883357763290405

Epoch 9, Batch 100/289, Loss: 0.5784732699394226, Variance: 0.11343633383512497
Epoch 9, Batch 200/289, Loss: 0.7873055934906006, Variance: 0.11517509818077087

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 2.4888332887518025, Training Loss Force: 3.1105888176617773, time: 4.6749043464660645
Validation Loss Energy: 2.3061829192915644, Validation Loss Force: 3.1780004060868743, time: 0.2678494453430176
Test Loss Energy: 9.306770183089212, Test Loss Force: 10.818589885142677, time: 11.256485223770142

Epoch 10, Batch 100/289, Loss: 0.6703923344612122, Variance: 0.1101265549659729
Epoch 10, Batch 200/289, Loss: 1.2327607870101929, Variance: 0.11316321045160294

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 2.5020564632372615, Training Loss Force: 3.104633854185676, time: 4.660559415817261
Validation Loss Energy: 3.529316562006783, Validation Loss Force: 3.137248256074267, time: 0.26627063751220703
Test Loss Energy: 10.239744358898545, Test Loss Force: 10.711915019628469, time: 10.918451309204102

Epoch 11, Batch 100/289, Loss: 1.0280983448028564, Variance: 0.10921334475278854
Epoch 11, Batch 200/289, Loss: 0.719409167766571, Variance: 0.11493854224681854

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 2.5098097928822085, Training Loss Force: 3.1290005573161097, time: 4.930032253265381
Validation Loss Energy: 2.1282672433948777, Validation Loss Force: 3.0794315443266793, time: 0.26526689529418945
Test Loss Energy: 9.296460321839065, Test Loss Force: 10.709159387522405, time: 11.124320030212402

Epoch 12, Batch 100/289, Loss: 0.3673275113105774, Variance: 0.11037585139274597
Epoch 12, Batch 200/289, Loss: 0.5956204533576965, Variance: 0.11114360392093658

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 2.4972191251301457, Training Loss Force: 3.1154141435774765, time: 4.6745922565460205
Validation Loss Energy: 2.0030893741624825, Validation Loss Force: 3.1220572770669524, time: 0.27420926094055176
Test Loss Energy: 8.921818635108032, Test Loss Force: 10.623989306565466, time: 10.91636061668396

Epoch 13, Batch 100/289, Loss: 0.6835196018218994, Variance: 0.11215974390506744
Epoch 13, Batch 200/289, Loss: 1.3157601356506348, Variance: 0.10674430429935455

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 2.504750673223822, Training Loss Force: 3.0977195465054748, time: 4.963031768798828
Validation Loss Energy: 3.2740172122317626, Validation Loss Force: 3.1674397429014696, time: 0.26441287994384766
Test Loss Energy: 9.27360428763974, Test Loss Force: 10.615953976832566, time: 10.928077697753906

Epoch 14, Batch 100/289, Loss: 1.2949343919754028, Variance: 0.11218778789043427
Epoch 14, Batch 200/289, Loss: 0.6049935817718506, Variance: 0.11659646034240723

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 2.523035912918341, Training Loss Force: 3.096579596050545, time: 4.666522979736328
Validation Loss Energy: 1.5548153009524217, Validation Loss Force: 3.1192061322548343, time: 0.2639331817626953
Test Loss Energy: 9.000440408194399, Test Loss Force: 10.51305737071292, time: 10.922154903411865

Epoch 15, Batch 100/289, Loss: 0.36073315143585205, Variance: 0.11254696547985077
Epoch 15, Batch 200/289, Loss: 0.7243191003799438, Variance: 0.1126459464430809

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 2.5144656386848006, Training Loss Force: 3.1166909723776706, time: 4.90625786781311
Validation Loss Energy: 2.447830556436338, Validation Loss Force: 3.237577896372801, time: 0.27166748046875
Test Loss Energy: 9.570093436289106, Test Loss Force: 10.724372726475714, time: 10.997895240783691

Epoch 16, Batch 100/289, Loss: 0.7766906023025513, Variance: 0.10835226625204086
Epoch 16, Batch 200/289, Loss: 1.1254791021347046, Variance: 0.11496996879577637

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 2.489088388618685, Training Loss Force: 3.1137300351883064, time: 4.648719549179077
Validation Loss Energy: 3.6563548764872342, Validation Loss Force: 3.163828494692813, time: 0.26732420921325684
Test Loss Energy: 10.441458188446333, Test Loss Force: 10.600095568860924, time: 12.676284790039062

Epoch 17, Batch 100/289, Loss: 1.040055751800537, Variance: 0.11049363762140274
Epoch 17, Batch 200/289, Loss: 0.7636399269104004, Variance: 0.11500285565853119

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 2.5189936531355936, Training Loss Force: 3.1020022700890286, time: 4.742144346237183
Validation Loss Energy: 1.9022078261188518, Validation Loss Force: 3.0788812843332622, time: 0.27829575538635254
Test Loss Energy: 9.30812580917092, Test Loss Force: 10.610664882800938, time: 11.072135925292969

Epoch 18, Batch 100/289, Loss: 0.5510299205780029, Variance: 0.11038434505462646
Epoch 18, Batch 200/289, Loss: 0.6748262643814087, Variance: 0.1098480373620987

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 2.5041119972452646, Training Loss Force: 3.114587957834085, time: 4.622531175613403
Validation Loss Energy: 2.476239354081243, Validation Loss Force: 3.1018686672146054, time: 0.26651525497436523
Test Loss Energy: 9.215690588906513, Test Loss Force: 10.854260577654042, time: 11.271751642227173

Epoch 19, Batch 100/289, Loss: 0.8092431426048279, Variance: 0.11011166870594025
Epoch 19, Batch 200/289, Loss: 0.8994672298431396, Variance: 0.10738888382911682

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 2.5129376175145186, Training Loss Force: 3.1029387656650234, time: 4.616056680679321
Validation Loss Energy: 3.315938286192832, Validation Loss Force: 3.113862740760005, time: 0.2650463581085205
Test Loss Energy: 9.162279458491144, Test Loss Force: 10.490521774901692, time: 10.999070882797241

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.039 MB of 0.039 MB uploadedwandb: - 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–ƒâ–â–ƒâ–‡â–‚â–ƒâ–„â–‚â–ƒâ–‡â–ƒâ–‚â–ƒâ–‚â–„â–ˆâ–ƒâ–ƒâ–ƒ
wandb:   test_error_force â–‚â–‚â–â–‡â–…â–â–ˆâ–†â–„â–‡â–†â–†â–„â–„â–‚â–†â–„â–„â–ˆâ–‚
wandb:          test_loss â–ƒâ–„â–â–ƒâ–‡â–ƒâ–…â–„â–â–…â–‡â–„â–‚â–‚â–‚â–†â–ˆâ–ƒâ–„â–
wandb: train_error_energy â–ˆâ–â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–‚â–„â–ƒâ–„
wandb:  train_error_force â–ˆâ–â–â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–‚
wandb:         train_loss â–ˆâ–â–â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚
wandb: valid_error_energy â–ƒâ–†â–â–„â–ˆâ–‚â–ƒâ–‡â–â–ƒâ–ˆâ–ƒâ–‚â–‡â–â–„â–ˆâ–‚â–„â–‡
wandb:  valid_error_force â–‚â–â–„â–†â–‚â–…â–„â–…â–ƒâ–†â–„â–â–ƒâ–…â–ƒâ–ˆâ–…â–â–‚â–ƒ
wandb:         valid_loss â–ƒâ–†â–â–ƒâ–ˆâ–‚â–ƒâ–†â–â–ƒâ–‡â–‚â–‚â–‡â–â–„â–ˆâ–‚â–ƒâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 9227
wandb:                 lr 0.0001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.16228
wandb:   test_error_force 10.49052
wandb:          test_loss 10.72916
wandb: train_error_energy 2.51294
wandb:  train_error_force 3.10294
wandb:         train_loss 0.80068
wandb: valid_error_energy 3.31594
wandb:  valid_error_force 3.11386
wandb:         valid_loss 1.12482
wandb: 
wandb: ğŸš€ View run al_71_113 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential/runs/btqv2zyw
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Evidential
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241205_013447-btqv2zyw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 3.640526533126831, Uncertainty Bias: -0.19096721708774567
/var/lib/slurmd/job5124170/slurm_script: line 15: 244387 Killed                  python3 active_learning.py
slurmstepd: error: Detected 1 oom-kill event(s) in step 5124170.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
