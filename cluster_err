/pfs/data5/home/kit/iti/fq0795/gnn_uncertainty/active_learning.py:170: DeprecationWarning: Please use atoms.calc = calc
  self.atoms.set_calculator(self.calc)
wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /pfs/data5/home/kit/iti/fq0795/gnn_uncertainty/wandb/run-20241104_200405-b0xavzog
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run treasured-galaxy-27
wandb: ‚≠êÔ∏è View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: üöÄ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/b0xavzog
Traceback (most recent call last):
  File "/pfs/data5/home/kit/iti/fq0795/gnn_uncertainty/active_learning.py", line 328, in <module>
    al.improve_model(100, 100,run_idx=28, use_wandb=True, model_path=model_path)
  File "/pfs/data5/home/kit/iti/fq0795/gnn_uncertainty/active_learning.py", line 232, in improve_model
    self.model.valid_epoch(validloader, criterion, self.device, self.dtype, force_weight=force_weight, energy_weight=energy_weight)
  File "/pfs/data5/home/kit/iti/fq0795/gnn_uncertainty/uncertainty/ensemble.py", line 99, in valid_epoch
    energies, mean_energy, forces, mean_force = self.forward(x=atom_positions, h0=nodes, edges=edges, edge_attr=None, node_mask=atom_mask, edge_mask=edge_mask, n_nodes=n_nodes)
                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/pfs/data5/home/kit/iti/fq0795/gnn_uncertainty/uncertainty/ensemble.py", line 137, in forward
    forces = torch.stack([-torch.autograd.grad(outputs=e, inputs=x, grad_outputs=grad_outputs, create_graph=True)[0] for e in energies])
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/kit/iti/fq0795/.conda/envs/torch/lib/python3.12/site-packages/torch/autograd/__init__.py", line 412, in grad
    result = _engine_run_backward(
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/kit/iti/fq0795/.conda/envs/torch/lib/python3.12/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 
wandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.021 MB uploadedwandb: | 0.009 MB of 0.021 MB uploadedwandb: üöÄ View run treasured-galaxy-27 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/b0xavzog
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241104_200405-b0xavzog/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
slurmstepd: error: *** JOB 24613576 ON uc2n904 CANCELLED AT 2024-11-04T23:15:41 DUE TO TIME LIMIT ***
