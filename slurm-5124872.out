wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_091223-s87si16f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 20241207_091221_0
wandb: ‚≠êÔ∏è View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: üöÄ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/s87si16f
Number of Cores: 384
Sequential time: 0.06480836868286133
Traceback (most recent call last):
  File "/home/ws/fq0795/git/gnn_uncertainty/evaluate_models.py", line 290, in <module>
    ens.fit(
  File "/home/ws/fq0795/git/gnn_uncertainty/uncertainty/ensemble.py", line 86, in fit
    self.train_epoch(
  File "/home/ws/fq0795/git/gnn_uncertainty/uncertainty/ensemble.py", line 170, in train_epoch
    energies, mean_energy, forces, mean_force = self.forward(
                                                ^^^^^^^^^^^^^
  File "/home/ws/fq0795/git/gnn_uncertainty/uncertainty/ensemble.py", line 314, in forward
    pool.starmap(
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/multiprocessing/pool.py", line 375, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/multiprocessing/pool.py", line 774, in get
    raise self._value
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/multiprocessing/pool.py", line 540, in _handle_tasks
    put(task)
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/site-packages/torch/multiprocessing/reductions.py", line 192, in reduce_tensor
    raise RuntimeError(
RuntimeError: Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).
wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.041 MB of 0.052 MB uploadedwandb: üöÄ View run 20241207_091221_0 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble/runs/s87si16f
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-Ensemble
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_091223-s87si16f/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]
