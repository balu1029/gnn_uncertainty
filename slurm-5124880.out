wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_100124-q7h01mra
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/q7h01mra
/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/site-packages/torch/autograd/graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
['H1', 'CH3', 'H2', 'H3', 'C', 'O', 'N', 'H', 'CA', 'HA', 'CB', 'HB1', 'HB2', 'HB3', 'C', 'O', 'N', 'H', 'C', 'H1', 'H2', 'H3']
76
Uncertainty Slope: 0.283833384513855, Uncertainty Bias: 0.03459283709526062
0.0007266998 0.011629283
1.5700754 6.190563
(48745, 22, 3)

Training and Validation Results of Epoch Initital validation:
================================
Training Loss Energy: 0.0, Training Loss Force: 0.0, time: 0
Validation Loss Energy: 0.0, Validation Loss Force: 0.0, time: 0
Test Loss Energy: 9.371962192417923, Test Loss Force: 10.67303076133985, time: 6.70497727394104

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.051 MB of 0.051 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–
wandb:    max_uncertainty â–
wandb:  test_error_energy â–
wandb:   test_error_force â–
wandb:          test_loss â–
wandb: train_error_energy â–
wandb:  train_error_force â–
wandb:         train_loss â–
wandb: valid_error_energy â–
wandb:  valid_error_force â–
wandb:         valid_loss â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 800
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.37196
wandb:   test_error_force 10.67303
wandb:          test_loss 4.19841
wandb: train_error_energy 0.0
wandb:  train_error_force 0.0
wandb:         train_loss 0.0
wandb: valid_error_energy 0.0
wandb:  valid_error_force 0.0
wandb:         valid_loss 0.0
wandb: 
wandb: ğŸš€ View run al_77 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/q7h01mra
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_100124-q7h01mra/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 5 steps.
Found uncertainty sample 55 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 3 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 3 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 3 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 3 steps.
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 5 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 7 steps.
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_100551-6qomum5e
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_0
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/6qomum5e
Training model 0. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.856656331743032, Training Loss Force: 3.188440503472723, time: 0.6352822780609131
Validation Loss Energy: 2.295618470730679, Validation Loss Force: 3.328882578051565, time: 0.04811525344848633
Test Loss Energy: 9.714161464899782, Test Loss Force: 10.62793564269357, time: 9.634173154830933


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6989321959823926, Training Loss Force: 2.8696737797578558, time: 0.4714481830596924
Validation Loss Energy: 1.4731142727367794, Validation Loss Force: 3.3291967642164693, time: 0.04150652885437012
Test Loss Energy: 9.339198164647149, Test Loss Force: 10.676543752998995, time: 9.55405855178833


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7211409929315278, Training Loss Force: 2.822619367650273, time: 0.44490480422973633
Validation Loss Energy: 1.4795155653439114, Validation Loss Force: 3.2243984213621397, time: 0.03327798843383789
Test Loss Energy: 9.70239075881873, Test Loss Force: 10.753057199015538, time: 7.573829412460327


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6300038514943949, Training Loss Force: 2.799306043263616, time: 0.38398289680480957
Validation Loss Energy: 1.8895251705065843, Validation Loss Force: 3.1924834753454556, time: 0.03822946548461914
Test Loss Energy: 9.442540836522246, Test Loss Force: 10.67323865737888, time: 7.7176525592803955


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.643326039136916, Training Loss Force: 2.7537724614227197, time: 0.41766786575317383
Validation Loss Energy: 1.4635396107964576, Validation Loss Force: 3.157228714809001, time: 0.03999805450439453
Test Loss Energy: 9.655508467296247, Test Loss Force: 10.733779040632355, time: 7.755289554595947


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.417649096278992, Training Loss Force: 2.726081008336361, time: 0.4008321762084961
Validation Loss Energy: 1.1587352763522973, Validation Loss Force: 3.1917246188961337, time: 0.03794693946838379
Test Loss Energy: 9.564122746172758, Test Loss Force: 10.741637867178374, time: 7.836531639099121


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 14.545203556878109, Training Loss Force: 5.807156875238059, time: 0.4390573501586914
Validation Loss Energy: 11.963938393867009, Validation Loss Force: 6.463350943440411, time: 0.03621339797973633
Test Loss Energy: 13.827069810581857, Test Loss Force: 12.70805843589842, time: 7.8901755809783936


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.177938347018017, Training Loss Force: 6.033701442458737, time: 0.3961515426635742
Validation Loss Energy: 14.706913206922808, Validation Loss Force: 7.023532183323119, time: 0.03611493110656738
Test Loss Energy: 13.02514900387745, Test Loss Force: 12.025919556080494, time: 7.808192014694214


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.806164580025978, Training Loss Force: 6.1149996335605055, time: 0.3848128318786621
Validation Loss Energy: 6.603134171826331, Validation Loss Force: 8.722945331735454, time: 0.036309003829956055
Test Loss Energy: 10.194923354131548, Test Loss Force: 13.511204449850222, time: 7.782397031784058


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.920804189367416, Training Loss Force: 7.147941713581565, time: 0.41357994079589844
Validation Loss Energy: 19.140554279000607, Validation Loss Force: 5.891536576697852, time: 0.03536868095397949
Test Loss Energy: 16.023939468607708, Test Loss Force: 11.788100584367303, time: 7.873141527175903


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 13.664971142217189, Training Loss Force: 6.780877177343315, time: 0.40280747413635254
Validation Loss Energy: 14.866365691645113, Validation Loss Force: 7.972408692382, time: 0.03796577453613281
Test Loss Energy: 14.236357613306383, Test Loss Force: 13.259345365426508, time: 8.382956981658936


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 12.672360559972699, Training Loss Force: 5.536049340983582, time: 0.3983950614929199
Validation Loss Energy: 2.4771993698369963, Validation Loss Force: 7.3201697928366585, time: 0.03809046745300293
Test Loss Energy: 9.200749428592792, Test Loss Force: 12.99270716320534, time: 7.829665660858154


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.635526565227024, Training Loss Force: 6.253266119249062, time: 0.4063262939453125
Validation Loss Energy: 3.7646721489223194, Validation Loss Force: 6.769853686795136, time: 0.03644251823425293
Test Loss Energy: 9.540528979947801, Test Loss Force: 12.651030200259791, time: 7.803033828735352


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.458350602605071, Training Loss Force: 5.229436217352233, time: 0.4118223190307617
Validation Loss Energy: 8.90635434452984, Validation Loss Force: 5.470059510519898, time: 0.03665018081665039
Test Loss Energy: 10.815413004981565, Test Loss Force: 11.671217254836646, time: 7.975567817687988


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.032962583500511, Training Loss Force: 4.889101545446124, time: 0.4078061580657959
Validation Loss Energy: 19.37612042186053, Validation Loss Force: 5.603275335115824, time: 0.033956050872802734
Test Loss Energy: 22.365806535112167, Test Loss Force: 13.668409053404368, time: 7.796029090881348


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.689530767677903, Training Loss Force: 4.794046830651529, time: 0.41142821311950684
Validation Loss Energy: 3.2313465893916704, Validation Loss Force: 5.505281316022756, time: 0.035964250564575195
Test Loss Energy: 9.363267915899609, Test Loss Force: 11.783109190823131, time: 7.8140952587127686


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.297170046260977, Training Loss Force: 5.120416321970137, time: 0.38164567947387695
Validation Loss Energy: 12.3957971863047, Validation Loss Force: 4.951329468166524, time: 0.039257049560546875
Test Loss Energy: 13.091800934717883, Test Loss Force: 11.330204363564294, time: 7.782984495162964


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.769201524662815, Training Loss Force: 4.79177568488776, time: 0.40804457664489746
Validation Loss Energy: 1.7580998285130638, Validation Loss Force: 4.370745945119036, time: 0.03839612007141113
Test Loss Energy: 9.840662637280865, Test Loss Force: 10.95630592545784, time: 8.003911972045898


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 19.536399708931544, Training Loss Force: 4.896651452985116, time: 0.40020179748535156
Validation Loss Energy: 23.223144973051077, Validation Loss Force: 6.202540799506653, time: 0.03774285316467285
Test Loss Energy: 24.64754592955711, Test Loss Force: 11.947375404758912, time: 7.7139739990234375


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 16.332569284293573, Training Loss Force: 6.553950629155266, time: 0.3990976810455322
Validation Loss Energy: 4.099110963325144, Validation Loss Force: 6.769367377380429, time: 0.03633594512939453
Test Loss Energy: 10.39963810625127, Test Loss Force: 12.857468211050183, time: 7.83522891998291

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–ƒâ–ƒâ–â–„â–ƒâ–â–â–‚â–‡â–â–ƒâ–â–ˆâ–‚
wandb:   test_error_force â–â–â–â–â–â–â–†â–„â–ˆâ–„â–‡â–†â–†â–ƒâ–ˆâ–„â–ƒâ–‚â–„â–†
wandb:          test_loss â–â–â–â–â–â–â–…â–„â–…â–„â–…â–„â–„â–ƒâ–ˆâ–‚â–ƒâ–â–†â–„
wandb: train_error_energy â–‚â–â–â–â–â–â–†â–„â–…â–„â–†â–…â–ƒâ–…â–…â–…â–…â–…â–ˆâ–‡
wandb:  train_error_force â–‚â–â–â–â–â–â–†â–†â–†â–ˆâ–‡â–…â–‡â–…â–„â–„â–…â–„â–„â–‡
wandb:         train_loss â–‚â–â–â–â–â–â–‡â–†â–‡â–‡â–ˆâ–†â–†â–†â–…â–…â–…â–…â–‡â–ˆ
wandb: valid_error_energy â–â–â–â–â–â–â–„â–…â–ƒâ–‡â–…â–â–‚â–ƒâ–‡â–‚â–…â–â–ˆâ–‚
wandb:  valid_error_force â–â–â–â–â–â–â–…â–†â–ˆâ–„â–‡â–†â–†â–„â–„â–„â–ƒâ–ƒâ–…â–†
wandb:         valid_loss â–â–â–â–â–â–â–†â–‡â–‡â–‡â–ˆâ–…â–…â–…â–‡â–„â–…â–‚â–ˆâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 890
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.39964
wandb:   test_error_force 12.85747
wandb:          test_loss 4.9981
wandb: train_error_energy 16.33257
wandb:  train_error_force 6.55395
wandb:         train_loss 3.28596
wandb: valid_error_energy 4.09911
wandb:  valid_error_force 6.76937
wandb:         valid_loss 2.53937
wandb: 
wandb: ğŸš€ View run al_77_0 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/6qomum5e
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_100551-6qomum5e/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.45589110255241394, Uncertainty Bias: -0.1129431277513504
7.247925e-05 0.2117095
7.017541 14.386499
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 5 steps.
Found uncertainty sample 8 after 4 steps.
Found uncertainty sample 9 after 7 steps.
Found uncertainty sample 10 after 4 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 10 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 11 steps.
Found uncertainty sample 25 after 3 steps.
Found uncertainty sample 26 after 5 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 5 steps.
Found uncertainty sample 31 after 4 steps.
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 7 steps.
Found uncertainty sample 38 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 5 steps.
Found uncertainty sample 41 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 3 steps.
Found uncertainty sample 46 after 6 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 2 steps.
Found uncertainty sample 51 after 14 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 3 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 3 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 5 steps.
Found uncertainty sample 62 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 4 steps.
Found uncertainty sample 70 after 4 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 2 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 6 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 4 steps.
Found uncertainty sample 86 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 6 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 10 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_101258-vtswf9rz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_1
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/vtswf9rz
Training model 1. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.699162606058213, Training Loss Force: 3.1830729744372506, time: 0.4473705291748047
Validation Loss Energy: 1.4124171129174696, Validation Loss Force: 3.1927488969674016, time: 0.04353785514831543
Test Loss Energy: 9.779538571744917, Test Loss Force: 10.713869495142333, time: 8.40198826789856


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.3496997226258487, Training Loss Force: 2.7788854347872705, time: 0.4481468200683594
Validation Loss Energy: 1.1142056778805123, Validation Loss Force: 3.1483239529942026, time: 0.040235280990600586
Test Loss Energy: 9.482423729790336, Test Loss Force: 10.720524560627402, time: 8.491559982299805


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.606374358212615, Training Loss Force: 2.7169480150080183, time: 0.47421979904174805
Validation Loss Energy: 1.5042601195234286, Validation Loss Force: 3.11443771702862, time: 0.03918743133544922
Test Loss Energy: 9.574052964689749, Test Loss Force: 10.745010015356588, time: 8.483687400817871


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.2272128121768535, Training Loss Force: 2.7120753604534316, time: 0.6270411014556885
Validation Loss Energy: 1.7424150138739105, Validation Loss Force: 3.0938018539297625, time: 0.06068754196166992
Test Loss Energy: 9.579955469083547, Test Loss Force: 10.83157871969073, time: 8.507408142089844


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6703152400409143, Training Loss Force: 2.6979152404680775, time: 0.4229254722595215
Validation Loss Energy: 2.6026300130838487, Validation Loss Force: 3.0910129228770167, time: 0.0435025691986084
Test Loss Energy: 9.41227017624108, Test Loss Force: 10.764603864961575, time: 8.465455293655396


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8655453088628748, Training Loss Force: 2.6742438725603006, time: 0.44171714782714844
Validation Loss Energy: 1.9437310793838072, Validation Loss Force: 3.092748628869524, time: 0.04241657257080078
Test Loss Energy: 9.278133214854927, Test Loss Force: 10.749081316673642, time: 8.784538745880127


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 17.999875179045713, Training Loss Force: 5.770933055756681, time: 0.48156118392944336
Validation Loss Energy: 2.1044973120670862, Validation Loss Force: 6.706088814152379, time: 0.04138064384460449
Test Loss Energy: 9.522124841097241, Test Loss Force: 13.404446490272951, time: 8.649524688720703


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.567948165834978, Training Loss Force: 5.206779664077024, time: 0.4510674476623535
Validation Loss Energy: 1.4698857927899724, Validation Loss Force: 4.237481578649298, time: 0.04014301300048828
Test Loss Energy: 9.205451039810969, Test Loss Force: 11.221262418371483, time: 8.483678340911865


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.883221254201414, Training Loss Force: 5.212408797549082, time: 0.47326016426086426
Validation Loss Energy: 10.616244966934662, Validation Loss Force: 4.469339834891234, time: 0.04009819030761719
Test Loss Energy: 13.647641609161562, Test Loss Force: 11.469177377037255, time: 8.530998706817627


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 14.214752359516629, Training Loss Force: 4.198018191682916, time: 0.4359424114227295
Validation Loss Energy: 16.102802034476348, Validation Loss Force: 5.011488436660126, time: 0.04077577590942383
Test Loss Energy: 19.24686934381759, Test Loss Force: 11.339739368829209, time: 8.689382553100586


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.955065831235988, Training Loss Force: 5.030552571635264, time: 0.4184856414794922
Validation Loss Energy: 6.039449010263303, Validation Loss Force: 4.296657240194484, time: 0.043701887130737305
Test Loss Energy: 12.02544038606814, Test Loss Force: 11.314961548345615, time: 8.4135582447052


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.925045585931029, Training Loss Force: 4.406446324079499, time: 0.4679844379425049
Validation Loss Energy: 9.067553446032802, Validation Loss Force: 3.813288394229317, time: 0.0475773811340332
Test Loss Energy: 12.492471026845124, Test Loss Force: 11.01552231563801, time: 8.463563680648804


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.587349960553586, Training Loss Force: 3.7777329944314824, time: 0.44729089736938477
Validation Loss Energy: 9.858423483803033, Validation Loss Force: 4.130723747310804, time: 0.04765796661376953
Test Loss Energy: 12.73920949034515, Test Loss Force: 11.70680103876429, time: 8.482581377029419


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.854517251140578, Training Loss Force: 3.5916691836047088, time: 0.42558813095092773
Validation Loss Energy: 5.355064256796951, Validation Loss Force: 4.473888074865955, time: 0.04153299331665039
Test Loss Energy: 9.831616184443414, Test Loss Force: 11.464750998873127, time: 8.656052112579346


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.6863136139912225, Training Loss Force: 3.531686662452065, time: 0.4532959461212158
Validation Loss Energy: 10.325156717875934, Validation Loss Force: 3.7963152159118256, time: 0.04005002975463867
Test Loss Energy: 11.333243025641945, Test Loss Force: 10.95778151075304, time: 8.482268810272217


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 6.9006613517607285, Training Loss Force: 3.4216576058736208, time: 0.44346141815185547
Validation Loss Energy: 10.594293351852516, Validation Loss Force: 3.5392329988191897, time: 0.04306340217590332
Test Loss Energy: 16.442173744557355, Test Loss Force: 11.434076204584365, time: 8.43398904800415


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.883736485465026, Training Loss Force: 3.386091661614295, time: 0.4348280429840088
Validation Loss Energy: 7.093927252455022, Validation Loss Force: 3.67101817415253, time: 0.040712833404541016
Test Loss Energy: 11.682696705011598, Test Loss Force: 11.451283441982632, time: 8.981940984725952


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.9775357769926005, Training Loss Force: 3.4162727979176504, time: 0.43835973739624023
Validation Loss Energy: 6.251236760367893, Validation Loss Force: 3.5814942667014087, time: 0.04172682762145996
Test Loss Energy: 9.926241879205381, Test Loss Force: 11.231203735371556, time: 8.470191240310669


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.9672143581570305, Training Loss Force: 3.290451650451845, time: 0.4169771671295166
Validation Loss Energy: 8.71863623842346, Validation Loss Force: 3.9479390988016014, time: 0.0425412654876709
Test Loss Energy: 10.27834776027992, Test Loss Force: 11.150879424668968, time: 8.455435514450073


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.148910599880939, Training Loss Force: 3.339074834416745, time: 0.4233736991882324
Validation Loss Energy: 8.589848857801746, Validation Loss Force: 3.838333329086691, time: 0.041379690170288086
Test Loss Energy: 15.238072547737307, Test Loss Force: 12.645333232311565, time: 8.709508180618286

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–â–„â–ˆâ–ƒâ–ƒâ–ƒâ–â–‚â–†â–ƒâ–‚â–‚â–…
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–‚â–ƒâ–ƒâ–ƒâ–‚â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–†
wandb:          test_loss â–â–â–â–â–â–â–‡â–‚â–…â–‡â–„â–ƒâ–…â–ƒâ–‚â–†â–„â–‚â–‚â–ˆ
wandb: train_error_energy â–‚â–â–â–â–â–â–ˆâ–‚â–ƒâ–†â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:  train_error_force â–‚â–â–â–â–â–â–ˆâ–‡â–‡â–„â–†â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒ
wandb:         train_loss â–‚â–â–â–â–â–â–ˆâ–„â–…â–…â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: valid_error_energy â–â–â–â–â–‚â–â–â–â–…â–ˆâ–ƒâ–…â–…â–ƒâ–…â–…â–„â–ƒâ–…â–„
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–ƒâ–„â–…â–ƒâ–‚â–ƒâ–„â–‚â–‚â–‚â–‚â–ƒâ–‚
wandb:         valid_loss â–â–â–â–â–â–â–†â–ƒâ–†â–ˆâ–„â–„â–…â–„â–…â–„â–ƒâ–ƒâ–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 980
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 15.23807
wandb:   test_error_force 12.64533
wandb:          test_loss 5.25091
wandb: train_error_energy 7.14891
wandb:  train_error_force 3.33907
wandb:         train_loss 1.59567
wandb: valid_error_energy 8.58985
wandb:  valid_error_force 3.83833
wandb:         valid_loss 1.85916
wandb: 
wandb: ğŸš€ View run al_77_1 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/vtswf9rz
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_101258-vtswf9rz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4625822901725769, Uncertainty Bias: -0.12249688804149628
0.00011062622 0.0060682297
7.950641 14.200311
(48745, 22, 3)
Found uncertainty sample 0 after 16 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 9 steps.
Found uncertainty sample 6 after 9 steps.
Found uncertainty sample 7 after 4 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 2 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 5 steps.
Found uncertainty sample 20 after 3 steps.
Found uncertainty sample 21 after 11 steps.
Found uncertainty sample 22 after 19 steps.
Found uncertainty sample 23 after 4 steps.
Found uncertainty sample 24 after 7 steps.
Found uncertainty sample 25 after 5 steps.
Found uncertainty sample 26 after 8 steps.
Found uncertainty sample 27 after 14 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 3 steps.
Found uncertainty sample 33 after 5 steps.
Found uncertainty sample 34 after 4 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 9 steps.
Found uncertainty sample 37 after 10 steps.
Found uncertainty sample 38 after 5 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 4 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 3 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 6 steps.
Found uncertainty sample 54 after 6 steps.
Found uncertainty sample 55 after 15 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 7 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 9 steps.
Found uncertainty sample 65 after 13 steps.
Found uncertainty sample 66 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 9 steps.
Found uncertainty sample 69 after 3 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 3 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 8 steps.
Found uncertainty sample 77 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 16 steps.
Found uncertainty sample 83 after 6 steps.
Found uncertainty sample 84 after 22 steps.
Found uncertainty sample 85 after 5 steps.
Found uncertainty sample 86 after 5 steps.
Found uncertainty sample 87 after 16 steps.
Found uncertainty sample 88 after 17 steps.
Found uncertainty sample 89 after 5 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 11 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 7 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_102020-0ek53njt
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_2
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/0ek53njt
Training model 2. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.000120103695904, Training Loss Force: 3.0027837055276154, time: 0.5524032115936279
Validation Loss Energy: 1.260686905510513, Validation Loss Force: 3.0888056036334346, time: 0.048929452896118164
Test Loss Energy: 9.33249875652525, Test Loss Force: 10.70682116097135, time: 8.897443771362305


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6632384253876613, Training Loss Force: 2.691574758086412, time: 0.49056172370910645
Validation Loss Energy: 5.21710958743739, Validation Loss Force: 3.094142537723032, time: 0.04610109329223633
Test Loss Energy: 11.337251886333576, Test Loss Force: 10.94943227581474, time: 9.282698154449463


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.685072074828705, Training Loss Force: 2.798237077459488, time: 0.5183439254760742
Validation Loss Energy: 4.972986714637752, Validation Loss Force: 3.3389926479532703, time: 0.04403996467590332
Test Loss Energy: 9.717103789346316, Test Loss Force: 10.759308708733304, time: 9.065124750137329


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.5745062867384045, Training Loss Force: 2.7551317690814447, time: 0.5212881565093994
Validation Loss Energy: 3.081465092568779, Validation Loss Force: 3.0818556542507367, time: 0.044921875
Test Loss Energy: 10.198245857582483, Test Loss Force: 10.937618206374774, time: 8.871370077133179


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.703972647802431, Training Loss Force: 2.7491627755229464, time: 0.4841272830963135
Validation Loss Energy: 1.868632347213326, Validation Loss Force: 3.028374088105217, time: 0.04551887512207031
Test Loss Energy: 9.812833041721959, Test Loss Force: 10.834489264237169, time: 8.950983047485352


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 3.534666948587341, Training Loss Force: 2.7375240360907998, time: 0.46062159538269043
Validation Loss Energy: 4.129837078771719, Validation Loss Force: 3.024906906833975, time: 0.046536922454833984
Test Loss Energy: 9.630401026528256, Test Loss Force: 10.66253805957572, time: 8.992663621902466


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.024448095736362, Training Loss Force: 5.940685950882034, time: 0.6342408657073975
Validation Loss Energy: 31.94965924585746, Validation Loss Force: 6.225737580452826, time: 0.04543614387512207
Test Loss Energy: 29.916122680480726, Test Loss Force: 13.982366263130949, time: 9.02140736579895


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 12.92692411723755, Training Loss Force: 6.081844180891335, time: 0.5037157535552979
Validation Loss Energy: 26.02957247660704, Validation Loss Force: 5.398129360295969, time: 0.04560542106628418
Test Loss Energy: 25.689612938633704, Test Loss Force: 12.978491554565231, time: 8.979954957962036


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 15.982647376189488, Training Loss Force: 5.395938701943775, time: 0.5416696071624756
Validation Loss Energy: 47.88739397042779, Validation Loss Force: 6.592399737639444, time: 0.045722007751464844
Test Loss Energy: 44.61667619079183, Test Loss Force: 12.071870097305391, time: 8.89238452911377


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 21.648026403353974, Training Loss Force: 7.191388083729073, time: 0.5157954692840576
Validation Loss Energy: 16.798304658398877, Validation Loss Force: 7.843346983372503, time: 0.05977177619934082
Test Loss Energy: 13.705477749083585, Test Loss Force: 12.947565808651586, time: 9.12106966972351


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.315241527795243, Training Loss Force: 6.2405397345975215, time: 0.4981508255004883
Validation Loss Energy: 9.721896123668868, Validation Loss Force: 4.861924243370182, time: 0.04540133476257324
Test Loss Energy: 13.580367319224068, Test Loss Force: 11.555221454210676, time: 8.93060302734375


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 12.661005049145079, Training Loss Force: 5.645251979436807, time: 0.47460246086120605
Validation Loss Energy: 1.9069303269671758, Validation Loss Force: 5.66534569555807, time: 0.04622650146484375
Test Loss Energy: 8.978003633183295, Test Loss Force: 11.62200375958476, time: 8.941018104553223


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.97307176005479, Training Loss Force: 6.733808136434196, time: 0.4896237850189209
Validation Loss Energy: 3.598871589518759, Validation Loss Force: 5.866517471461406, time: 0.05059504508972168
Test Loss Energy: 9.747227044984491, Test Loss Force: 11.936343610500227, time: 9.46001935005188


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 17.10182317676169, Training Loss Force: 5.479786838356124, time: 0.48146605491638184
Validation Loss Energy: 10.931340512630738, Validation Loss Force: 5.978573937796398, time: 0.045395612716674805
Test Loss Energy: 12.321981823495166, Test Loss Force: 12.163605789041089, time: 8.949424505233765


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 5.824192091237891, Training Loss Force: 5.699440480826092, time: 0.47472572326660156
Validation Loss Energy: 1.6915982407322814, Validation Loss Force: 5.980820224140517, time: 0.04577493667602539
Test Loss Energy: 10.187245829234207, Test Loss Force: 12.037812933042234, time: 8.925128936767578


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.093586001941341, Training Loss Force: 6.046383150136641, time: 0.48912906646728516
Validation Loss Energy: 23.386138787870216, Validation Loss Force: 6.42295876036911, time: 0.04411458969116211
Test Loss Energy: 19.757489911109893, Test Loss Force: 11.761911941501566, time: 9.151535987854004


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.74753667229523, Training Loss Force: 5.687911563126534, time: 0.5252254009246826
Validation Loss Energy: 5.627365464675459, Validation Loss Force: 5.86876019509919, time: 0.047766685485839844
Test Loss Energy: 9.68221155749557, Test Loss Force: 12.155510788640306, time: 8.961998462677002


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.891038197956322, Training Loss Force: 5.343510157836626, time: 0.5149104595184326
Validation Loss Energy: 7.679495447003249, Validation Loss Force: 4.439603261129162, time: 0.04599142074584961
Test Loss Energy: 12.351482151122449, Test Loss Force: 11.356949320226283, time: 8.973642587661743


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 13.471544156602558, Training Loss Force: 6.551372384280292, time: 0.4932425022125244
Validation Loss Energy: 4.204950061767541, Validation Loss Force: 6.846588591849894, time: 0.04853248596191406
Test Loss Energy: 9.398457568444115, Test Loss Force: 12.370077286473089, time: 9.099976778030396


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.015555250870033, Training Loss Force: 5.767905676470239, time: 0.4653966426849365
Validation Loss Energy: 10.75572313859066, Validation Loss Force: 6.126402647762981, time: 0.04530143737792969
Test Loss Energy: 15.265583770718205, Test Loss Force: 14.107921448378798, time: 8.99357557296753

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–…â–„â–ˆâ–‚â–‚â–â–â–‚â–â–ƒâ–â–‚â–â–‚
wandb:   test_error_force â–â–‚â–â–‚â–â–â–ˆâ–†â–„â–†â–ƒâ–ƒâ–„â–„â–„â–ƒâ–„â–‚â–„â–ˆ
wandb:          test_loss â–â–‚â–â–â–â–â–‡â–†â–ˆâ–„â–‚â–‚â–‚â–ƒâ–‚â–„â–‚â–‚â–‚â–…
wandb: train_error_energy â–â–â–‚â–‚â–‚â–‚â–…â–…â–†â–ˆâ–ƒâ–…â–„â–†â–‚â–„â–„â–„â–…â–„
wandb:  train_error_force â–â–â–â–â–â–â–†â–†â–…â–ˆâ–‡â–†â–‡â–…â–†â–†â–†â–…â–‡â–†
wandb:         train_loss â–â–â–â–â–â–â–…â–†â–†â–ˆâ–…â–…â–†â–†â–„â–…â–…â–„â–†â–…
wandb: valid_error_energy â–â–‚â–‚â–â–â–â–†â–…â–ˆâ–ƒâ–‚â–â–â–‚â–â–„â–‚â–‚â–â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–†â–„â–†â–ˆâ–„â–…â–…â–…â–…â–†â–…â–ƒâ–‡â–†
wandb:         valid_loss â–â–â–‚â–â–â–â–†â–…â–ˆâ–…â–ƒâ–‚â–ƒâ–„â–ƒâ–…â–ƒâ–‚â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1070
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 15.26558
wandb:   test_error_force 14.10792
wandb:          test_loss 5.74214
wandb: train_error_energy 11.01556
wandb:  train_error_force 5.76791
wandb:         train_loss 2.66713
wandb: valid_error_energy 10.75572
wandb:  valid_error_force 6.1264
wandb:         valid_loss 2.76969
wandb: 
wandb: ğŸš€ View run al_77_2 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/0ek53njt
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_102020-0ek53njt/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.028716664761304855, Uncertainty Bias: 0.19291746616363525
4.386902e-05 0.7670202
2.9269357 3.4706821
(48745, 22, 3)
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Did not find any uncertainty samples for sample 2.
Did not find any uncertainty samples for sample 3.
Found uncertainty sample 4 after 1440 steps.
Did not find any uncertainty samples for sample 5.
Did not find any uncertainty samples for sample 6.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Did not find any uncertainty samples for sample 12.
Did not find any uncertainty samples for sample 13.
Did not find any uncertainty samples for sample 14.
Did not find any uncertainty samples for sample 15.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Found uncertainty sample 20 after 2759 steps.
Did not find any uncertainty samples for sample 21.
Did not find any uncertainty samples for sample 22.
Did not find any uncertainty samples for sample 23.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Did not find any uncertainty samples for sample 30.
Did not find any uncertainty samples for sample 31.
Did not find any uncertainty samples for sample 32.
Did not find any uncertainty samples for sample 33.
Did not find any uncertainty samples for sample 34.
Did not find any uncertainty samples for sample 35.
Found uncertainty sample 36 after 1819 steps.
Found uncertainty sample 37 after 3896 steps.
Did not find any uncertainty samples for sample 38.
Did not find any uncertainty samples for sample 39.
Did not find any uncertainty samples for sample 40.
Did not find any uncertainty samples for sample 41.
Did not find any uncertainty samples for sample 42.
Did not find any uncertainty samples for sample 43.
Did not find any uncertainty samples for sample 44.
Did not find any uncertainty samples for sample 45.
Did not find any uncertainty samples for sample 46.
Did not find any uncertainty samples for sample 47.
Did not find any uncertainty samples for sample 48.
Did not find any uncertainty samples for sample 49.
Did not find any uncertainty samples for sample 50.
Found uncertainty sample 51 after 1228 steps.
Did not find any uncertainty samples for sample 52.
Did not find any uncertainty samples for sample 53.
Did not find any uncertainty samples for sample 54.
Did not find any uncertainty samples for sample 55.
Did not find any uncertainty samples for sample 56.
Did not find any uncertainty samples for sample 57.
Did not find any uncertainty samples for sample 58.
Did not find any uncertainty samples for sample 59.
Did not find any uncertainty samples for sample 60.
Did not find any uncertainty samples for sample 61.
Did not find any uncertainty samples for sample 62.
Did not find any uncertainty samples for sample 63.
Did not find any uncertainty samples for sample 64.
Found uncertainty sample 65 after 39 steps.
Did not find any uncertainty samples for sample 66.
Did not find any uncertainty samples for sample 67.
Did not find any uncertainty samples for sample 68.
Found uncertainty sample 69 after 1213 steps.
Found uncertainty sample 70 after 2212 steps.
Did not find any uncertainty samples for sample 71.
Did not find any uncertainty samples for sample 72.
Did not find any uncertainty samples for sample 73.
Did not find any uncertainty samples for sample 74.
Did not find any uncertainty samples for sample 75.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 2815 steps.
Did not find any uncertainty samples for sample 78.
Did not find any uncertainty samples for sample 79.
Did not find any uncertainty samples for sample 80.
Found uncertainty sample 81 after 1412 steps.
Did not find any uncertainty samples for sample 82.
Did not find any uncertainty samples for sample 83.
Did not find any uncertainty samples for sample 84.
Did not find any uncertainty samples for sample 85.
Found uncertainty sample 86 after 2074 steps.
Did not find any uncertainty samples for sample 87.
Did not find any uncertainty samples for sample 88.
Did not find any uncertainty samples for sample 89.
Did not find any uncertainty samples for sample 90.
Found uncertainty sample 91 after 2988 steps.
Found uncertainty sample 92 after 3176 steps.
Did not find any uncertainty samples for sample 93.
Did not find any uncertainty samples for sample 94.
Did not find any uncertainty samples for sample 95.
Did not find any uncertainty samples for sample 96.
Did not find any uncertainty samples for sample 97.
Did not find any uncertainty samples for sample 98.
Did not find any uncertainty samples for sample 99.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_122953-cctftj4q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_3
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/cctftj4q
Training model 3. Added 13 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 5.8481529728192285, Training Loss Force: 3.5972313103534677, time: 0.4993457794189453
Validation Loss Energy: 5.262750972954793, Validation Loss Force: 3.825493335455911, time: 0.050148963928222656
Test Loss Energy: 9.78790463627375, Test Loss Force: 10.783122602644829, time: 8.999913930892944


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 4.36587591220844, Training Loss Force: 3.2828478605860663, time: 0.48088884353637695
Validation Loss Energy: 5.814853776223241, Validation Loss Force: 3.8344882782748098, time: 0.046051740646362305
Test Loss Energy: 10.213237651227857, Test Loss Force: 10.899712568490246, time: 9.038602113723755


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.954412388857432, Training Loss Force: 3.1877298492799864, time: 0.5118412971496582
Validation Loss Energy: 5.113204593196523, Validation Loss Force: 3.5894404378429874, time: 0.051909685134887695
Test Loss Energy: 9.908023667925866, Test Loss Force: 10.762367250152714, time: 9.24255633354187


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.530413349010876, Training Loss Force: 3.1170697776680596, time: 0.5095651149749756
Validation Loss Energy: 4.517619849787881, Validation Loss Force: 3.5552694080397633, time: 0.04750204086303711
Test Loss Energy: 9.343701807839102, Test Loss Force: 10.62161055189824, time: 9.098083972930908


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.4909169409918195, Training Loss Force: 3.0876278213824198, time: 0.49523496627807617
Validation Loss Energy: 4.363950528123612, Validation Loss Force: 3.567876267074757, time: 0.04758095741271973
Test Loss Energy: 9.284688095545622, Test Loss Force: 10.605286399348197, time: 9.192256689071655


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.8887920118217263, Training Loss Force: 3.101357201425683, time: 0.46140122413635254
Validation Loss Energy: 4.319960902330702, Validation Loss Force: 3.5797175895860507, time: 0.0473475456237793
Test Loss Energy: 9.747922271685203, Test Loss Force: 10.6698145002167, time: 9.270169019699097


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 15.776309898769773, Training Loss Force: 5.368853757509615, time: 0.4979288578033447
Validation Loss Energy: 19.360225407547624, Validation Loss Force: 7.38217646990317, time: 0.04698920249938965
Test Loss Energy: 13.88432307495988, Test Loss Force: 13.037672065867733, time: 9.116260766983032


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.053060621026289, Training Loss Force: 6.995465182817213, time: 0.47145652770996094
Validation Loss Energy: 5.004990950981953, Validation Loss Force: 5.662678119189855, time: 0.04747176170349121
Test Loss Energy: 12.324891687210295, Test Loss Force: 11.690886975164238, time: 9.143759965896606


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.870118694310817, Training Loss Force: 6.35032015551195, time: 0.4823763370513916
Validation Loss Energy: 13.952654430126023, Validation Loss Force: 6.8282718279058745, time: 0.04457521438598633
Test Loss Energy: 16.994454659826403, Test Loss Force: 12.412634834889777, time: 9.148311853408813


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 12.07075508844914, Training Loss Force: 5.633403767280174, time: 0.5502257347106934
Validation Loss Energy: 11.943139314284187, Validation Loss Force: 5.009400245409615, time: 0.04711294174194336
Test Loss Energy: 10.913012053310426, Test Loss Force: 10.587590875326471, time: 9.66612434387207


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.392655809288504, Training Loss Force: 5.411802110336149, time: 0.49833011627197266
Validation Loss Energy: 7.217873805844266, Validation Loss Force: 6.627179322218489, time: 0.048780202865600586
Test Loss Energy: 10.555804092307474, Test Loss Force: 11.897367906306862, time: 9.217015504837036


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.7145702349786, Training Loss Force: 5.6317256671055365, time: 0.5046243667602539
Validation Loss Energy: 18.04454467838702, Validation Loss Force: 5.7750810587159105, time: 0.049819231033325195
Test Loss Energy: 22.868178569878857, Test Loss Force: 12.745312093860452, time: 9.144561290740967


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 13.496934238459758, Training Loss Force: 5.347285781718626, time: 0.48420286178588867
Validation Loss Energy: 8.976790830122884, Validation Loss Force: 6.448869432865852, time: 0.04662466049194336
Test Loss Energy: 10.579787974876096, Test Loss Force: 11.59437239785608, time: 9.341119289398193


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.760435996555147, Training Loss Force: 5.746242208361109, time: 0.48813796043395996
Validation Loss Energy: 35.87310384793042, Validation Loss Force: 6.093305551250566, time: 0.047830820083618164
Test Loss Energy: 35.90631382953917, Test Loss Force: 12.194807999101025, time: 9.302936315536499


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 15.38495585848029, Training Loss Force: 7.368105417153664, time: 0.48991918563842773
Validation Loss Energy: 5.368569140497471, Validation Loss Force: 5.563279503124274, time: 0.05787515640258789
Test Loss Energy: 9.799186785264107, Test Loss Force: 11.427073290255919, time: 9.089769124984741


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 17.446378767336967, Training Loss Force: 6.868461696410822, time: 0.471149206161499
Validation Loss Energy: 21.927611348214022, Validation Loss Force: 8.721083817675499, time: 0.04641556739807129
Test Loss Energy: 16.632854096421084, Test Loss Force: 13.830663956401139, time: 9.278489589691162


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 19.08273270127727, Training Loss Force: 9.853441038993752, time: 0.4782135486602783
Validation Loss Energy: 22.47062301559253, Validation Loss Force: 11.207349607127236, time: 0.04844188690185547
Test Loss Energy: 23.885626625329724, Test Loss Force: 16.543747369524215, time: 9.015429735183716


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.408890266565557, Training Loss Force: 5.309945980045166, time: 0.5502710342407227
Validation Loss Energy: 6.214745740733246, Validation Loss Force: 4.7075161054797245, time: 0.050220489501953125
Test Loss Energy: 11.3639116971121, Test Loss Force: 10.79907666741627, time: 8.840912580490112


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.222247905448685, Training Loss Force: 3.97981306059725, time: 0.4692702293395996
Validation Loss Energy: 7.532988946425185, Validation Loss Force: 4.513883461519212, time: 0.05174136161804199
Test Loss Energy: 10.303516321979759, Test Loss Force: 10.719532668675892, time: 9.193612098693848


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.324541299000439, Training Loss Force: 3.8178220821204425, time: 0.4896364212036133
Validation Loss Energy: 7.655921909862165, Validation Loss Force: 4.089109555966652, time: 0.04917502403259277
Test Loss Energy: 13.070233364456307, Test Loss Force: 10.84607699923385, time: 9.09137225151062

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–‚â–ƒâ–â–â–…â–â–ˆâ–â–ƒâ–…â–‚â–â–‚
wandb:   test_error_force â–â–â–â–â–â–â–„â–‚â–ƒâ–â–ƒâ–„â–‚â–ƒâ–‚â–…â–ˆâ–â–â–
wandb:          test_loss â–â–â–â–â–â–â–„â–‚â–„â–â–‚â–…â–‚â–†â–‚â–…â–ˆâ–â–â–‚
wandb: train_error_energy â–‚â–‚â–â–â–â–â–‡â–„â–…â–…â–…â–…â–†â–„â–†â–‡â–ˆâ–ƒâ–ƒâ–ƒ
wandb:  train_error_force â–‚â–â–â–â–â–â–ƒâ–…â–„â–„â–ƒâ–„â–ƒâ–„â–…â–…â–ˆâ–ƒâ–‚â–‚
wandb:         train_loss â–‚â–â–â–â–â–â–„â–…â–…â–„â–„â–„â–„â–„â–†â–†â–ˆâ–ƒâ–‚â–‚
wandb: valid_error_energy â–â–â–â–â–â–â–„â–â–ƒâ–ƒâ–‚â–„â–‚â–ˆâ–â–…â–…â–â–‚â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–…â–ƒâ–„â–‚â–„â–ƒâ–„â–ƒâ–ƒâ–†â–ˆâ–‚â–‚â–
wandb:         valid_loss â–â–â–â–â–â–â–…â–‚â–„â–ƒâ–ƒâ–„â–ƒâ–†â–‚â–†â–ˆâ–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 1081
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 13.07023
wandb:   test_error_force 10.84608
wandb:          test_loss 4.5038
wandb: train_error_energy 7.32454
wandb:  train_error_force 3.81782
wandb:         train_loss 1.76762
wandb: valid_error_energy 7.65592
wandb:  valid_error_force 4.08911
wandb:         valid_loss 1.88057
wandb: 
wandb: ğŸš€ View run al_77_3 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/cctftj4q
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_122953-cctftj4q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.9602270126342773, Uncertainty Bias: -1.4217959642410278
0.00012397766 0.009353638
7.099991 60.053604
(48745, 22, 3)
Found uncertainty sample 0 after 8 steps.
Found uncertainty sample 1 after 11 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 6 steps.
Found uncertainty sample 5 after 27 steps.
Found uncertainty sample 6 after 21 steps.
Found uncertainty sample 7 after 10 steps.
Found uncertainty sample 8 after 10 steps.
Found uncertainty sample 9 after 7 steps.
Found uncertainty sample 10 after 16 steps.
Found uncertainty sample 11 after 11 steps.
Found uncertainty sample 12 after 4 steps.
Found uncertainty sample 13 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 14 steps.
Found uncertainty sample 17 after 10 steps.
Found uncertainty sample 18 after 6 steps.
Found uncertainty sample 19 after 14 steps.
Found uncertainty sample 20 after 3 steps.
Found uncertainty sample 21 after 14 steps.
Found uncertainty sample 22 after 26 steps.
Found uncertainty sample 23 after 6 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 22 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 9 steps.
Found uncertainty sample 30 after 7 steps.
Found uncertainty sample 31 after 5 steps.
Found uncertainty sample 32 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 17 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 13 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 10 steps.
Found uncertainty sample 45 after 40 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 4 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 8 steps.
Found uncertainty sample 50 after 4 steps.
Found uncertainty sample 51 after 2 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 27 steps.
Found uncertainty sample 54 after 69 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 9 steps.
Found uncertainty sample 59 after 7 steps.
Found uncertainty sample 60 after 11 steps.
Found uncertainty sample 61 after 13 steps.
Found uncertainty sample 62 after 6 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 32 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 37 steps.
Found uncertainty sample 70 after 7 steps.
Found uncertainty sample 71 after 12 steps.
Found uncertainty sample 72 after 4 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 6 steps.
Found uncertainty sample 75 after 5 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 6 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 4 steps.
Found uncertainty sample 80 after 5 steps.
Found uncertainty sample 81 after 5 steps.
Found uncertainty sample 82 after 26 steps.
Found uncertainty sample 83 after 5 steps.
Found uncertainty sample 84 after 7 steps.
Found uncertainty sample 85 after 22 steps.
Found uncertainty sample 86 after 11 steps.
Found uncertainty sample 87 after 17 steps.
Found uncertainty sample 88 after 12 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 12 steps.
Found uncertainty sample 91 after 17 steps.
Found uncertainty sample 92 after 17 steps.
Found uncertainty sample 93 after 6 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 14 steps.
Found uncertainty sample 97 after 3 steps.
Found uncertainty sample 98 after 7 steps.
Found uncertainty sample 99 after 6 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_123739-6rikmvux
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_4
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/6rikmvux
Training model 4. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.5900106362757995, Training Loss Force: 3.3727508130514345, time: 0.5685653686523438
Validation Loss Energy: 5.998876465183802, Validation Loss Force: 3.630764936628288, time: 0.05219888687133789
Test Loss Energy: 11.492720845682987, Test Loss Force: 10.564074705218545, time: 9.22432827949524


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.8163495143187207, Training Loss Force: 3.1094517335853458, time: 0.5397424697875977
Validation Loss Energy: 3.6435534936863587, Validation Loss Force: 3.5681866678641803, time: 0.05205392837524414
Test Loss Energy: 9.736124281056032, Test Loss Force: 10.611401071298044, time: 9.20504355430603


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5750826772289, Training Loss Force: 3.067184489744697, time: 0.5472726821899414
Validation Loss Energy: 7.293353302254284, Validation Loss Force: 3.5767790248672418, time: 0.05011940002441406
Test Loss Energy: 11.71988948919287, Test Loss Force: 10.657907800412032, time: 9.570209741592407


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.7926008387891366, Training Loss Force: 3.124993582596643, time: 0.5472254753112793
Validation Loss Energy: 4.205647016194294, Validation Loss Force: 3.5178385019401563, time: 0.05248117446899414
Test Loss Energy: 9.372295911679894, Test Loss Force: 10.476414087351515, time: 9.309600830078125


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.442783704877503, Training Loss Force: 3.027146164054785, time: 0.5147356986999512
Validation Loss Energy: 4.815402424049781, Validation Loss Force: 3.6278838205491284, time: 0.05069279670715332
Test Loss Energy: 10.293686857170977, Test Loss Force: 10.669285251796849, time: 9.305657625198364


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.199335397723804, Training Loss Force: 3.002316213248762, time: 0.5260639190673828
Validation Loss Energy: 3.5593809938193197, Validation Loss Force: 3.4867977549903846, time: 0.05144214630126953
Test Loss Energy: 9.282977119817998, Test Loss Force: 10.494468805850795, time: 9.876328468322754


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 16.393378722235603, Training Loss Force: 6.531019140012347, time: 0.5110251903533936
Validation Loss Energy: 11.1188190267361, Validation Loss Force: 8.03124945953171, time: 0.050887346267700195
Test Loss Energy: 11.134743731822876, Test Loss Force: 13.416613673340914, time: 9.32930588722229


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.40078709385182, Training Loss Force: 7.309640009649134, time: 0.5200445652008057
Validation Loss Energy: 18.24638446172814, Validation Loss Force: 9.394864621074477, time: 0.050011396408081055
Test Loss Energy: 19.64775358499543, Test Loss Force: 14.050677883515407, time: 9.312628984451294


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 16.37055260718229, Training Loss Force: 6.738012769242345, time: 0.5361454486846924
Validation Loss Energy: 17.01678762438322, Validation Loss Force: 5.508607993062917, time: 0.04933476448059082
Test Loss Energy: 19.047743027673274, Test Loss Force: 11.438830869349625, time: 9.582667589187622


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.27086814473901, Training Loss Force: 6.094839657412819, time: 0.5149803161621094
Validation Loss Energy: 18.51798559781185, Validation Loss Force: 4.881597507751194, time: 0.05125617980957031
Test Loss Energy: 13.435792425688838, Test Loss Force: 11.056858723142291, time: 9.351130485534668


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 13.572642342892111, Training Loss Force: 6.071627036866859, time: 0.5111300945281982
Validation Loss Energy: 11.46692482797768, Validation Loss Force: 7.058454276155557, time: 0.05007457733154297
Test Loss Energy: 10.545699734860296, Test Loss Force: 11.757196457179534, time: 9.448327541351318


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 16.306235918269333, Training Loss Force: 5.9297939825224635, time: 0.5393240451812744
Validation Loss Energy: 15.355143171164865, Validation Loss Force: 5.417230364971947, time: 0.050177812576293945
Test Loss Energy: 16.30267092307716, Test Loss Force: 11.431355285548921, time: 9.571841955184937


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.598401937772891, Training Loss Force: 6.073944180144063, time: 0.5286481380462646
Validation Loss Energy: 10.249141924839106, Validation Loss Force: 5.798616296763894, time: 0.05062150955200195
Test Loss Energy: 14.229052381894576, Test Loss Force: 11.343573451728396, time: 9.312714576721191


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.452459662834456, Training Loss Force: 4.357317719785565, time: 0.5116794109344482
Validation Loss Energy: 9.099896584817104, Validation Loss Force: 4.6543618629787185, time: 0.05229377746582031
Test Loss Energy: 9.827511116691054, Test Loss Force: 10.83386816421172, time: 9.410069465637207


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.268846661030062, Training Loss Force: 3.878146722830705, time: 0.5948951244354248
Validation Loss Energy: 7.99722472335736, Validation Loss Force: 4.168197628555575, time: 0.05076432228088379
Test Loss Energy: 9.540858935984623, Test Loss Force: 10.461965340618194, time: 9.580956935882568


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.37372345564078, Training Loss Force: 3.8065170736463325, time: 0.5492942333221436
Validation Loss Energy: 5.593584773885047, Validation Loss Force: 4.27273322181771, time: 0.05039572715759277
Test Loss Energy: 11.62360273856102, Test Loss Force: 10.93637138972797, time: 9.399818181991577


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.261971769203797, Training Loss Force: 3.7585014633152634, time: 0.5497360229492188
Validation Loss Energy: 8.796795780673865, Validation Loss Force: 4.212014017752954, time: 0.051744937896728516
Test Loss Energy: 13.98937681116394, Test Loss Force: 10.777209745884457, time: 9.418185472488403


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.318222788869878, Training Loss Force: 3.7600326085796825, time: 0.5947198867797852
Validation Loss Energy: 12.086352092855686, Validation Loss Force: 4.3825861542846, time: 0.05056166648864746
Test Loss Energy: 10.537087339965716, Test Loss Force: 10.806055398243336, time: 9.97352147102356


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 15.982817186459114, Training Loss Force: 4.996415191857389, time: 0.5270226001739502
Validation Loss Energy: 8.83301069056756, Validation Loss Force: 6.352467498166167, time: 0.04973196983337402
Test Loss Energy: 9.92072288802414, Test Loss Force: 11.59311730864247, time: 9.395442962646484


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.377849307056582, Training Loss Force: 7.3845367748861745, time: 0.5331335067749023
Validation Loss Energy: 11.244191553335865, Validation Loss Force: 7.513599807874376, time: 0.05105161666870117
Test Loss Energy: 10.84680422034315, Test Loss Force: 12.586461394072446, time: 9.640762567520142

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–ƒâ–â–‚â–â–‚â–ˆâ–ˆâ–„â–‚â–†â–„â–â–â–ƒâ–„â–‚â–â–‚
wandb:   test_error_force â–â–â–â–â–â–â–‡â–ˆâ–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–…
wandb:          test_loss â–‚â–â–‚â–â–â–â–…â–ˆâ–…â–ƒâ–ƒâ–„â–ƒâ–‚â–â–‚â–ƒâ–‚â–ƒâ–„
wandb: train_error_energy â–‚â–â–â–‚â–â–â–ˆâ–„â–ˆâ–„â–‡â–ˆâ–„â–„â–„â–„â–ƒâ–„â–ˆâ–…
wandb:  train_error_force â–‚â–â–â–â–â–â–‡â–ˆâ–‡â–†â–†â–†â–†â–ƒâ–‚â–‚â–‚â–‚â–„â–ˆ
wandb:         train_loss â–‚â–â–â–â–â–â–ˆâ–‡â–ˆâ–†â–‡â–‡â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–†â–‡
wandb: valid_error_energy â–‚â–â–ƒâ–â–‚â–â–…â–ˆâ–‡â–ˆâ–…â–‡â–„â–„â–ƒâ–‚â–ƒâ–…â–ƒâ–…
wandb:  valid_error_force â–â–â–â–â–â–â–†â–ˆâ–ƒâ–ƒâ–…â–ƒâ–„â–‚â–‚â–‚â–‚â–‚â–„â–†
wandb:         valid_loss â–‚â–â–‚â–â–â–â–†â–ˆâ–…â–„â–…â–„â–„â–ƒâ–‚â–‚â–‚â–ƒâ–„â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 1171
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.8468
wandb:   test_error_force 12.58646
wandb:          test_loss 4.93735
wandb: train_error_energy 10.37785
wandb:  train_error_force 7.38454
wandb:         train_loss 3.16538
wandb: valid_error_energy 11.24419
wandb:  valid_error_force 7.5136
wandb:         valid_loss 3.26654
wandb: 
wandb: ğŸš€ View run al_77_4 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/6rikmvux
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_123739-6rikmvux/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.247064471244812, Uncertainty Bias: -0.5489121675491333
0.00022888184 0.0110759735
-5.4405136 41.57166
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 4 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_124526-tdm33u08
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_5
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/tdm33u08
Training model 5. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.0156117475557154, Training Loss Force: 3.2895736859419316, time: 0.5664145946502686
Validation Loss Energy: 3.7525010534059065, Validation Loss Force: 3.513263096329632, time: 0.05356907844543457
Test Loss Energy: 9.064459995687185, Test Loss Force: 10.483600812145404, time: 9.389715433120728


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.9655396462398795, Training Loss Force: 2.95629790588866, time: 0.5633788108825684
Validation Loss Energy: 3.1485038696641126, Validation Loss Force: 3.455813788672825, time: 0.05388212203979492
Test Loss Energy: 9.339896739545699, Test Loss Force: 10.517317346242422, time: 9.311192035675049


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.1630525102121902, Training Loss Force: 2.9401943285202172, time: 0.5801446437835693
Validation Loss Energy: 4.55668937813237, Validation Loss Force: 3.445777199589276, time: 0.06258916854858398
Test Loss Energy: 9.091572568616835, Test Loss Force: 10.508091970002662, time: 9.498185157775879


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.2761502184312556, Training Loss Force: 2.9412901334878856, time: 0.6099841594696045
Validation Loss Energy: 5.379195357479437, Validation Loss Force: 3.4662801292080148, time: 0.054498910903930664
Test Loss Energy: 9.162379398466033, Test Loss Force: 10.49129345522853, time: 9.359223127365112


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.2062822071677646, Training Loss Force: 2.935456101663462, time: 0.593069314956665
Validation Loss Energy: 5.419756769101643, Validation Loss Force: 3.452671481685492, time: 0.052164316177368164
Test Loss Energy: 9.228001380749722, Test Loss Force: 10.4849600265812, time: 9.691452741622925


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6305017160485895, Training Loss Force: 2.9073753235041524, time: 0.555652379989624
Validation Loss Energy: 3.3313653390286597, Validation Loss Force: 3.457700241677125, time: 0.05313611030578613
Test Loss Energy: 9.40152469020339, Test Loss Force: 10.559342055553556, time: 9.600427389144897


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.8238050492537585, Training Loss Force: 4.259434201309328, time: 0.5856029987335205
Validation Loss Energy: 5.00582061621968, Validation Loss Force: 4.4551811945782696, time: 0.05574536323547363
Test Loss Energy: 11.758564541075344, Test Loss Force: 11.042643618130827, time: 9.450092554092407


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.717906631750888, Training Loss Force: 3.92471809701908, time: 0.563652515411377
Validation Loss Energy: 5.348878062584785, Validation Loss Force: 4.075758313976821, time: 0.05463099479675293
Test Loss Energy: 12.17582876297108, Test Loss Force: 11.042532978378864, time: 9.315491437911987


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.9316084681007855, Training Loss Force: 3.806209380242911, time: 0.5745401382446289
Validation Loss Energy: 7.65294705007257, Validation Loss Force: 4.430467146071744, time: 0.053913116455078125
Test Loss Energy: 13.875292080723867, Test Loss Force: 10.973808652973403, time: 9.600759744644165


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 7.773345515356399, Training Loss Force: 3.738488779089289, time: 0.5791819095611572
Validation Loss Energy: 7.122819464815736, Validation Loss Force: 3.902063465342683, time: 0.05236101150512695
Test Loss Energy: 12.786113223390837, Test Loss Force: 11.2040158699596, time: 8.756779193878174


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.5109547405734505, Training Loss Force: 3.911576609364714, time: 0.5716619491577148
Validation Loss Energy: 13.78247909001242, Validation Loss Force: 5.4798057390371575, time: 0.05620718002319336
Test Loss Energy: 16.142205781048226, Test Loss Force: 11.897634997364497, time: 8.718502521514893


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 20.181501317998755, Training Loss Force: 6.8073933222387, time: 0.6211519241333008
Validation Loss Energy: 10.88076789620762, Validation Loss Force: 9.326576447401344, time: 0.05263400077819824
Test Loss Energy: 10.04941531599351, Test Loss Force: 14.34139810594239, time: 9.416932582855225


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 16.180198923768494, Training Loss Force: 8.092075411185025, time: 0.582679033279419
Validation Loss Energy: 3.5715928377894377, Validation Loss Force: 7.065652724305111, time: 0.05378270149230957
Test Loss Energy: 9.226148185888851, Test Loss Force: 11.812040119635869, time: 9.333873271942139


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.713551406958587, Training Loss Force: 4.789205376764462, time: 0.5469279289245605
Validation Loss Energy: 9.480518316100229, Validation Loss Force: 4.256351452751803, time: 0.05267691612243652
Test Loss Energy: 10.289181530553646, Test Loss Force: 10.376819708966643, time: 9.386254072189331


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.136485833747611, Training Loss Force: 3.799900395130223, time: 0.5603065490722656
Validation Loss Energy: 3.7890628818289573, Validation Loss Force: 4.245132490503377, time: 0.052573442459106445
Test Loss Energy: 9.229870854940195, Test Loss Force: 10.867814110482621, time: 9.572253942489624


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.097267005740777, Training Loss Force: 3.7371039217365634, time: 0.5908083915710449
Validation Loss Energy: 11.58937059132337, Validation Loss Force: 4.163775864655741, time: 0.052521467208862305
Test Loss Energy: 11.110193234386934, Test Loss Force: 10.739865537357494, time: 9.688575983047485


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.282805776548488, Training Loss Force: 3.659061759526434, time: 0.6047878265380859
Validation Loss Energy: 6.9444075929942635, Validation Loss Force: 3.9671999799243554, time: 0.05312991142272949
Test Loss Energy: 9.58555589421862, Test Loss Force: 10.763101135015097, time: 9.350856065750122


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 19.307895775694732, Training Loss Force: 5.443327122073939, time: 0.6252360343933105
Validation Loss Energy: 4.382223203576738, Validation Loss Force: 8.230899959830067, time: 0.05913972854614258
Test Loss Energy: 9.416231513974259, Test Loss Force: 13.383148398641838, time: 9.598666191101074


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.831956610027907, Training Loss Force: 6.0957243616489505, time: 0.5947656631469727
Validation Loss Energy: 39.11235920004716, Validation Loss Force: 6.433712789440839, time: 0.0571599006652832
Test Loss Energy: 33.367989164239795, Test Loss Force: 11.27361300712159, time: 9.379860877990723


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 14.743730514790439, Training Loss Force: 6.837394337878476, time: 0.5685925483703613
Validation Loss Energy: 3.4479836281562743, Validation Loss Force: 6.287707739779405, time: 0.05561947822570801
Test Loss Energy: 8.656938173798363, Test Loss Force: 11.873190555727216, time: 9.395326852798462

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–â–â–â–â–‚â–â–â–ˆâ–
wandb:   test_error_force â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–„â–ˆâ–„â–â–‚â–‚â–‚â–†â–ƒâ–„
wandb:          test_loss â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–…â–†â–ƒâ–â–‚â–‚â–â–…â–ˆâ–ƒ
wandb: train_error_energy â–â–â–â–‚â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–…â–†
wandb:  train_error_force â–‚â–â–â–â–â–â–ƒâ–‚â–‚â–‚â–‚â–†â–ˆâ–„â–‚â–‚â–‚â–„â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–„â–ƒâ–ƒâ–ƒâ–†â–…â–‡
wandb: valid_error_energy â–â–â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–â–‚â–â–ƒâ–‚â–â–ˆâ–
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ˆâ–…â–‚â–‚â–‚â–‚â–‡â–…â–„
wandb:         valid_loss â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–„â–†â–„â–‚â–‚â–ƒâ–‚â–„â–ˆâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1261
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.65694
wandb:   test_error_force 11.87319
wandb:          test_loss 4.55214
wandb: train_error_energy 14.74373
wandb:  train_error_force 6.83739
wandb:         train_loss 3.27447
wandb: valid_error_energy 3.44798
wandb:  valid_error_force 6.28771
wandb:         valid_loss 2.33463
wandb: 
wandb: ğŸš€ View run al_77_5 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/tdm33u08
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_124526-tdm33u08/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.568834662437439, Uncertainty Bias: -1.0996606349945068
/home/ws/fq0795/git/gnn_uncertainty/uncertainty/base_uncertainty.py:974: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(10, 8))
0.00030136108 0.0008621216
-16.285624 5.7938333
(48745, 22, 3)
Found uncertainty sample 0 after 12 steps.
Found uncertainty sample 1 after 7 steps.
Found uncertainty sample 2 after 12 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 26 steps.
Found uncertainty sample 6 after 10 steps.
Found uncertainty sample 7 after 10 steps.
Found uncertainty sample 8 after 16 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 31 steps.
Found uncertainty sample 11 after 5 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 6 steps.
Found uncertainty sample 16 after 5 steps.
Found uncertainty sample 17 after 12 steps.
Found uncertainty sample 18 after 10 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 9 steps.
Found uncertainty sample 22 after 5 steps.
Found uncertainty sample 23 after 6 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 7 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 6 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 7 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 7 steps.
Found uncertainty sample 33 after 4 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 7 steps.
Found uncertainty sample 36 after 4 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 4 steps.
Found uncertainty sample 39 after 6 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 17 steps.
Found uncertainty sample 43 after 31 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 5 steps.
Found uncertainty sample 46 after 27 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 4 steps.
Found uncertainty sample 50 after 2 steps.
Found uncertainty sample 51 after 4 steps.
Found uncertainty sample 52 after 11 steps.
Found uncertainty sample 53 after 6 steps.
Found uncertainty sample 54 after 11 steps.
Found uncertainty sample 55 after 4 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 13 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 6 steps.
Found uncertainty sample 60 after 8 steps.
Found uncertainty sample 61 after 4 steps.
Found uncertainty sample 62 after 5 steps.
Found uncertainty sample 63 after 10 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 7 steps.
Found uncertainty sample 66 after 14 steps.
Found uncertainty sample 67 after 12 steps.
Found uncertainty sample 68 after 27 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 18 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 9 steps.
Found uncertainty sample 73 after 68 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 7 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 9 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 8 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 4 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 32 steps.
Found uncertainty sample 93 after 17 steps.
Found uncertainty sample 94 after 8 steps.
Found uncertainty sample 95 after 6 steps.
Found uncertainty sample 96 after 8 steps.
Found uncertainty sample 97 after 5 steps.
Found uncertainty sample 98 after 12 steps.
Found uncertainty sample 99 after 6 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_125325-d80cfm65
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_6
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/d80cfm65
Training model 6. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.008263303336539, Training Loss Force: 3.2129434867809046, time: 0.6177046298980713
Validation Loss Energy: 3.2520857709742383, Validation Loss Force: 3.035949960457118, time: 0.0599675178527832
Test Loss Energy: 10.158327719635755, Test Loss Force: 10.478588849101701, time: 9.545387029647827


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.0902953574268865, Training Loss Force: 2.8965803529556395, time: 0.6059441566467285
Validation Loss Energy: 2.033487342088704, Validation Loss Force: 3.0814540704460556, time: 0.05977344512939453
Test Loss Energy: 9.203896617980037, Test Loss Force: 10.50656897832971, time: 9.635443210601807


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.933118458165623, Training Loss Force: 2.8842638596652876, time: 0.6206481456756592
Validation Loss Energy: 3.5072544807613437, Validation Loss Force: 3.7602341301748328, time: 0.05883026123046875
Test Loss Energy: 9.080849876438522, Test Loss Force: 10.532197931221992, time: 10.093549489974976


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.137330287905462, Training Loss Force: 2.9532025690157773, time: 0.5944762229919434
Validation Loss Energy: 2.9885265537648555, Validation Loss Force: 3.1842323588085284, time: 0.06069183349609375
Test Loss Energy: 10.105395952756707, Test Loss Force: 10.589298120471547, time: 9.611689567565918


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.4092434457159357, Training Loss Force: 2.856239911979829, time: 0.6049652099609375
Validation Loss Energy: 3.388202195199593, Validation Loss Force: 3.6349485558765524, time: 0.0628359317779541
Test Loss Energy: 10.42571380106709, Test Loss Force: 10.6345327162729, time: 9.702188968658447


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.442798615156747, Training Loss Force: 2.9901370575571247, time: 0.6113147735595703
Validation Loss Energy: 3.0699181386684016, Validation Loss Force: 3.328371329692515, time: 0.06394290924072266
Test Loss Energy: 10.281637928906605, Test Loss Force: 10.65839480464774, time: 9.735835313796997


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.233185748781818, Training Loss Force: 5.283528396668585, time: 0.6215498447418213
Validation Loss Energy: 2.1930745416442954, Validation Loss Force: 5.763694649269747, time: 0.06210446357727051
Test Loss Energy: 9.046232705102389, Test Loss Force: 11.959685296599083, time: 9.59280776977539


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.809263273533772, Training Loss Force: 6.169665346826424, time: 0.581275224685669
Validation Loss Energy: 8.812943301798256, Validation Loss Force: 7.18826704819427, time: 0.05929279327392578
Test Loss Energy: 13.777407339720131, Test Loss Force: 12.618024412086895, time: 9.584824085235596


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.11909951758922, Training Loss Force: 6.510791091745437, time: 0.58931565284729
Validation Loss Energy: 8.776313336616552, Validation Loss Force: 6.289917731052737, time: 0.05965089797973633
Test Loss Energy: 10.183961419694333, Test Loss Force: 12.077538703499787, time: 9.72501516342163


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 17.152019220973706, Training Loss Force: 7.284490966679312, time: 0.6270871162414551
Validation Loss Energy: 21.504378758489572, Validation Loss Force: 8.290460393659416, time: 0.059969186782836914
Test Loss Energy: 24.389726917747705, Test Loss Force: 14.791654233580715, time: 9.582478284835815


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 13.251495525720996, Training Loss Force: 6.765200336626832, time: 0.605506181716919
Validation Loss Energy: 15.60710577925433, Validation Loss Force: 5.480990338893493, time: 0.06082463264465332
Test Loss Energy: 20.24371410998429, Test Loss Force: 11.442874569019043, time: 9.562901735305786


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 12.014353697127705, Training Loss Force: 5.120056278167288, time: 0.5959103107452393
Validation Loss Energy: 21.514785423693652, Validation Loss Force: 6.008474396049873, time: 0.06619977951049805
Test Loss Energy: 27.815170599300853, Test Loss Force: 12.875304876999444, time: 9.72960638999939


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 12.129793905944444, Training Loss Force: 5.482184145860096, time: 0.6017916202545166
Validation Loss Energy: 7.164236593965741, Validation Loss Force: 4.808396362042309, time: 0.05883383750915527
Test Loss Energy: 14.743599494574552, Test Loss Force: 11.727226062161321, time: 9.626634359359741


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.80369945348567, Training Loss Force: 5.055007743790018, time: 0.6247217655181885
Validation Loss Energy: 6.480035319875634, Validation Loss Force: 6.926780212726036, time: 0.06165766716003418
Test Loss Energy: 12.056831116453996, Test Loss Force: 11.964154907551732, time: 9.581799507141113


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.293684689271437, Training Loss Force: 5.9006712728867585, time: 0.6599295139312744
Validation Loss Energy: 14.244924161673044, Validation Loss Force: 7.049952768164035, time: 0.059247732162475586
Test Loss Energy: 14.120958892417471, Test Loss Force: 12.392990529155808, time: 10.087804794311523


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.96977385192273, Training Loss Force: 5.548482206273972, time: 0.6607627868652344
Validation Loss Energy: 32.78936775274233, Validation Loss Force: 6.727421186339699, time: 0.06308245658874512
Test Loss Energy: 26.92149377851231, Test Loss Force: 11.575820639854038, time: 9.63230276107788


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 19.898178648154566, Training Loss Force: 6.664957752714662, time: 0.6628906726837158
Validation Loss Energy: 10.396520397792623, Validation Loss Force: 6.984514091551338, time: 0.05921292304992676
Test Loss Energy: 10.549796003101067, Test Loss Force: 13.6068008934871, time: 9.57734751701355


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.71524164706761, Training Loss Force: 6.73225087493652, time: 0.6351346969604492
Validation Loss Energy: 2.259767290886857, Validation Loss Force: 4.679365158705805, time: 0.059119224548339844
Test Loss Energy: 9.509823627524728, Test Loss Force: 11.246099149416766, time: 9.74870228767395


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 12.976953533497612, Training Loss Force: 6.096679423792959, time: 0.6633646488189697
Validation Loss Energy: 39.75176166866805, Validation Loss Force: 5.862536815139655, time: 0.06594586372375488
Test Loss Energy: 40.377490656904946, Test Loss Force: 12.099373579327501, time: 9.61257815361023


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 14.383543798777628, Training Loss Force: 6.7635484836919995, time: 0.6210393905639648
Validation Loss Energy: 4.770874829600309, Validation Loss Force: 4.8858184240814015, time: 0.06384825706481934
Test Loss Energy: 10.484057412133332, Test Loss Force: 10.702099679737868, time: 9.611417531967163

wandb: - 0.039 MB of 0.055 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–‚â–â–„â–„â–…â–‚â–‚â–‚â–…â–â–â–ˆâ–
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–„â–„â–ˆâ–ƒâ–…â–ƒâ–ƒâ–„â–ƒâ–†â–‚â–„â–
wandb:          test_loss â–â–â–â–â–â–â–‚â–„â–ƒâ–ˆâ–„â–†â–ƒâ–ƒâ–„â–…â–„â–‚â–ˆâ–
wandb: train_error_energy â–‚â–â–â–â–â–‚â–…â–ƒâ–„â–‡â–…â–…â–…â–„â–„â–…â–ˆâ–„â–…â–†
wandb:  train_error_force â–‚â–â–â–â–â–â–…â–†â–‡â–ˆâ–‡â–…â–…â–„â–†â–…â–‡â–‡â–†â–‡
wandb:         train_loss â–‚â–â–â–â–â–‚â–…â–…â–†â–ˆâ–‡â–…â–…â–…â–…â–…â–ˆâ–†â–†â–‡
wandb: valid_error_energy â–â–â–â–â–â–â–â–‚â–‚â–…â–„â–…â–‚â–‚â–ƒâ–‡â–ƒâ–â–ˆâ–‚
wandb:  valid_error_force â–â–â–‚â–â–‚â–â–…â–‡â–…â–ˆâ–„â–…â–ƒâ–†â–†â–†â–†â–ƒâ–…â–ƒ
wandb:         valid_loss â–â–â–‚â–â–‚â–â–ƒâ–…â–„â–‡â–„â–†â–ƒâ–„â–…â–ˆâ–…â–‚â–ˆâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1351
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.48406
wandb:   test_error_force 10.7021
wandb:          test_loss 4.28256
wandb: train_error_energy 14.38354
wandb:  train_error_force 6.76355
wandb:         train_loss 3.22566
wandb: valid_error_energy 4.77087
wandb:  valid_error_force 4.88582
wandb:         valid_loss 1.95408
wandb: 
wandb: ğŸš€ View run al_77_6 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/d80cfm65
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_125325-d80cfm65/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.7686539888381958, Uncertainty Bias: -1.182350754737854
0.00016212463 0.02648735
-12.70104 33.817116
(48745, 22, 3)
Found uncertainty sample 0 after 7 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 7 steps.
Found uncertainty sample 3 after 15 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 10 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 14 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 15 steps.
Found uncertainty sample 11 after 27 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 17 steps.
Found uncertainty sample 14 after 7 steps.
Found uncertainty sample 15 after 10 steps.
Found uncertainty sample 16 after 2 steps.
Found uncertainty sample 17 after 7 steps.
Found uncertainty sample 18 after 2 steps.
Found uncertainty sample 19 after 4 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 8 steps.
Found uncertainty sample 22 after 7 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 10 steps.
Found uncertainty sample 27 after 15 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 16 steps.
Found uncertainty sample 31 after 12 steps.
Found uncertainty sample 32 after 7 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 3 steps.
Found uncertainty sample 35 after 3 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 7 steps.
Found uncertainty sample 39 after 19 steps.
Found uncertainty sample 40 after 12 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 8 steps.
Found uncertainty sample 45 after 3 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 5 steps.
Found uncertainty sample 48 after 4 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 6 steps.
Found uncertainty sample 51 after 2 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 4 steps.
Found uncertainty sample 54 after 3 steps.
Found uncertainty sample 55 after 5 steps.
Found uncertainty sample 56 after 12 steps.
Found uncertainty sample 57 after 8 steps.
Found uncertainty sample 58 after 5 steps.
Found uncertainty sample 59 after 7 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 5 steps.
Found uncertainty sample 62 after 4 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 16 steps.
Found uncertainty sample 68 after 38 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 15 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 5 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 13 steps.
Found uncertainty sample 78 after 8 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 14 steps.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 9 steps.
Found uncertainty sample 85 after 6 steps.
Found uncertainty sample 86 after 19 steps.
Found uncertainty sample 87 after 3 steps.
Found uncertainty sample 88 after 8 steps.
Found uncertainty sample 89 after 6 steps.
Found uncertainty sample 90 after 2 steps.
Found uncertainty sample 91 after 27 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 12 steps.
Found uncertainty sample 94 after 5 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 8 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 5 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_130132-var8j7jy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_7
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/var8j7jy
Training model 7. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 4.337403542237629, Training Loss Force: 3.2177177225425706, time: 0.6456382274627686
Validation Loss Energy: 3.3212964629189514, Validation Loss Force: 3.2936602158758426, time: 0.05890655517578125
Test Loss Energy: 9.094919465414414, Test Loss Force: 10.481079749743595, time: 9.496173858642578


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.9540313491492785, Training Loss Force: 2.9442710416491735, time: 0.6610417366027832
Validation Loss Energy: 2.073193926859693, Validation Loss Force: 3.229787014693592, time: 0.061699628829956055
Test Loss Energy: 9.33096861146026, Test Loss Force: 10.556731706321651, time: 10.110482692718506


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.8251305686921255, Training Loss Force: 2.970418866467594, time: 0.6496882438659668
Validation Loss Energy: 3.5534018819128668, Validation Loss Force: 3.2840504835450375, time: 0.06082510948181152
Test Loss Energy: 9.227379306109581, Test Loss Force: 10.572691348314006, time: 9.906388759613037


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.983848315422057, Training Loss Force: 2.927813264954367, time: 0.6412999629974365
Validation Loss Energy: 5.538455901397953, Validation Loss Force: 3.162787316033369, time: 0.06371927261352539
Test Loss Energy: 11.521398845662093, Test Loss Force: 10.703808631624458, time: 9.898086071014404


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.9114340461701285, Training Loss Force: 2.9297955770259883, time: 0.6271531581878662
Validation Loss Energy: 5.433134731388985, Validation Loss Force: 3.239505520902183, time: 0.06332540512084961
Test Loss Energy: 9.289144546868325, Test Loss Force: 10.6693144109627, time: 9.850170373916626


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 4.099893306528728, Training Loss Force: 2.8994139936764007, time: 0.6508448123931885
Validation Loss Energy: 4.822135048346054, Validation Loss Force: 3.1604678828748125, time: 0.06203866004943848
Test Loss Energy: 11.359519672950023, Test Loss Force: 10.73129963018471, time: 9.985534906387329


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.380564815051704, Training Loss Force: 5.475352022114883, time: 0.6206512451171875
Validation Loss Energy: 12.456864199575396, Validation Loss Force: 4.747665579181964, time: 0.06287860870361328
Test Loss Energy: 16.14033120528612, Test Loss Force: 11.266913808844961, time: 9.851190090179443


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.260172119095478, Training Loss Force: 4.075453620294422, time: 0.6491265296936035
Validation Loss Energy: 7.411266189796322, Validation Loss Force: 4.7089180304224865, time: 0.06309318542480469
Test Loss Energy: 10.064048101690453, Test Loss Force: 10.954810488800245, time: 9.714340209960938


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.466140396549344, Training Loss Force: 3.901555634470959, time: 0.670351505279541
Validation Loss Energy: 13.964592168377504, Validation Loss Force: 6.21667249685296, time: 0.06321096420288086
Test Loss Energy: 14.26580506925248, Test Loss Force: 10.958309584039391, time: 9.89335823059082


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.70051232554594, Training Loss Force: 4.595125829051186, time: 0.6489672660827637
Validation Loss Energy: 35.361730794158504, Validation Loss Force: 9.636352872539499, time: 0.06626605987548828
Test Loss Energy: 27.420271779377202, Test Loss Force: 13.435911757698143, time: 9.724141120910645


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 18.22051956710342, Training Loss Force: 7.119689375587627, time: 0.6628518104553223
Validation Loss Energy: 21.236037498702245, Validation Loss Force: 6.345297106329, time: 0.06282210350036621
Test Loss Energy: 23.191847297383696, Test Loss Force: 12.318293848861538, time: 9.809700965881348


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 16.262356249954543, Training Loss Force: 5.953412876138789, time: 0.7023367881774902
Validation Loss Energy: 26.036337165260328, Validation Loss Force: 7.078866579431904, time: 0.06118893623352051
Test Loss Energy: 27.140015038926528, Test Loss Force: 12.252106986431418, time: 9.88104772567749


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.599788876468427, Training Loss Force: 7.0033744392869055, time: 0.6784181594848633
Validation Loss Energy: 25.256636211849834, Validation Loss Force: 5.674869254929401, time: 0.0632171630859375
Test Loss Energy: 21.159208757838297, Test Loss Force: 11.044490932660956, time: 9.737687110900879


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 15.868090132750973, Training Loss Force: 7.00262914629295, time: 0.6795432567596436
Validation Loss Energy: 15.86222182778842, Validation Loss Force: 7.85880957420402, time: 0.06325387954711914
Test Loss Energy: 12.124628392095888, Test Loss Force: 11.582353612698714, time: 10.009142398834229


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.98043369588578, Training Loss Force: 6.19323837964103, time: 0.6911406517028809
Validation Loss Energy: 10.524772481787526, Validation Loss Force: 5.736356444878464, time: 0.05914139747619629
Test Loss Energy: 10.958278918650835, Test Loss Force: 11.341247927810924, time: 9.332514762878418


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.969873127576795, Training Loss Force: 5.345915507685819, time: 0.6313385963439941
Validation Loss Energy: 22.183823374284373, Validation Loss Force: 7.821490670684815, time: 0.061411142349243164
Test Loss Energy: 22.102479784491134, Test Loss Force: 12.486151336068037, time: 9.397855520248413


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.773818276545295, Training Loss Force: 5.226888490231498, time: 0.6569044589996338
Validation Loss Energy: 11.126421842054658, Validation Loss Force: 8.790329468676124, time: 0.06097984313964844
Test Loss Energy: 15.026317370281603, Test Loss Force: 13.122195800081808, time: 9.763048648834229


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.293188606448103, Training Loss Force: 5.299299489635424, time: 0.6957132816314697
Validation Loss Energy: 8.167020635246303, Validation Loss Force: 4.919686401177017, time: 0.06146407127380371
Test Loss Energy: 9.89252891247793, Test Loss Force: 10.99108212220161, time: 9.865209817886353


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.57470231942298, Training Loss Force: 5.2470728771150865, time: 0.6557927131652832
Validation Loss Energy: 11.912157129319795, Validation Loss Force: 6.073072273967545, time: 0.06092548370361328
Test Loss Energy: 11.292258491346965, Test Loss Force: 12.146505390863812, time: 9.730423927307129


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 13.082883173064735, Training Loss Force: 6.0004823163391645, time: 0.6425082683563232
Validation Loss Energy: 14.399124771919496, Validation Loss Force: 7.489609211422971, time: 0.060645103454589844
Test Loss Energy: 19.884560417020325, Test Loss Force: 12.729320224298098, time: 9.896746397018433

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–â–‚â–„â–â–ƒâ–ˆâ–†â–ˆâ–†â–‚â–‚â–†â–ƒâ–â–‚â–…
wandb:   test_error_force â–â–â–â–‚â–â–‚â–ƒâ–‚â–‚â–ˆâ–…â–…â–‚â–„â–ƒâ–†â–‡â–‚â–…â–†
wandb:          test_loss â–â–â–â–‚â–â–‚â–ƒâ–‚â–ƒâ–ˆâ–†â–‡â–„â–ƒâ–‚â–†â–…â–‚â–ƒâ–†
wandb: train_error_energy â–‚â–â–â–â–â–‚â–„â–ƒâ–ƒâ–…â–ˆâ–‡â–…â–‡â–…â–…â–…â–…â–…â–†
wandb:  train_error_force â–‚â–â–â–â–â–â–…â–ƒâ–ƒâ–„â–ˆâ–†â–ˆâ–ˆâ–†â–…â–…â–…â–…â–†
wandb:         train_loss â–‚â–â–â–â–â–â–…â–ƒâ–ƒâ–„â–ˆâ–†â–‡â–‡â–†â–…â–…â–…â–…â–†
wandb: valid_error_energy â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–„â–ˆâ–…â–†â–†â–„â–ƒâ–…â–ƒâ–‚â–ƒâ–„
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–„â–ˆâ–„â–…â–„â–†â–„â–†â–‡â–ƒâ–„â–†
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–‚â–„â–ˆâ–…â–†â–…â–…â–ƒâ–†â–…â–ƒâ–„â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 1441
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 19.88456
wandb:   test_error_force 12.72932
wandb:          test_loss 5.58996
wandb: train_error_energy 13.08288
wandb:  train_error_force 6.00048
wandb:         train_loss 2.8833
wandb: valid_error_energy 14.39912
wandb:  valid_error_force 7.48961
wandb:         valid_loss 3.46965
wandb: 
wandb: ğŸš€ View run al_77_7 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/var8j7jy
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_130132-var8j7jy/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.2942274808883667, Uncertainty Bias: -0.6690505146980286
5.054474e-05 0.011341095
-7.944306 35.67032
(48745, 22, 3)
Found uncertainty sample 0 after 8 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 17 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 7 steps.
Found uncertainty sample 6 after 5 steps.
Found uncertainty sample 7 after 8 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 2 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 2 steps.
Found uncertainty sample 31 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 3 steps.
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 10 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 5 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 4 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 6 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 5 steps.
Found uncertainty sample 50 after 7 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 5 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 4 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 18 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 10 steps.
Found uncertainty sample 72 after 21 steps.
Found uncertainty sample 73 after 8 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 6 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 5 steps.
Found uncertainty sample 82 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 4 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 3 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_130939-zwlr1u32
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_8
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/zwlr1u32
Training model 8. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.880722402100261, Training Loss Force: 3.1903156845142164, time: 0.658505916595459
Validation Loss Energy: 4.416910603129368, Validation Loss Force: 4.300628544412105, time: 0.06253385543823242
Test Loss Energy: 9.735750901038328, Test Loss Force: 10.506375295810194, time: 9.79909610748291


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7678692494135537, Training Loss Force: 2.8605963287425094, time: 0.6938552856445312
Validation Loss Energy: 1.7230930100705997, Validation Loss Force: 3.246724343278589, time: 0.062178850173950195
Test Loss Energy: 9.518582900117513, Test Loss Force: 10.657969508910629, time: 10.158379554748535


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6768131068604275, Training Loss Force: 2.8475761215496256, time: 0.7356829643249512
Validation Loss Energy: 2.0365599536333616, Validation Loss Force: 3.092866532182293, time: 0.06347918510437012
Test Loss Energy: 9.100490316356327, Test Loss Force: 10.639117229168075, time: 9.973181962966919


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6758732980977207, Training Loss Force: 2.8072239957298883, time: 0.712090015411377
Validation Loss Energy: 2.1384592555422843, Validation Loss Force: 3.0722784316152363, time: 0.06203818321228027
Test Loss Energy: 9.167217982685745, Test Loss Force: 10.669893238824638, time: 9.758163690567017


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.0043934970176767, Training Loss Force: 2.773777429270822, time: 0.7068352699279785
Validation Loss Energy: 2.6019756997985395, Validation Loss Force: 3.2362848343754838, time: 0.06573700904846191
Test Loss Energy: 9.101218470111263, Test Loss Force: 10.656600963355231, time: 9.851686954498291


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8159153782060464, Training Loss Force: 2.752175061466183, time: 0.6883575916290283
Validation Loss Energy: 3.744272537787291, Validation Loss Force: 4.21486195612937, time: 0.06415987014770508
Test Loss Energy: 9.276481564716295, Test Loss Force: 10.786245501276447, time: 9.964944839477539


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.703272367570158, Training Loss Force: 5.598762095947478, time: 0.6853554248809814
Validation Loss Energy: 16.245461702889365, Validation Loss Force: 4.712274322459056, time: 0.06177544593811035
Test Loss Energy: 20.58705395678517, Test Loss Force: 11.594032807589823, time: 9.754237174987793


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 18.728925395975104, Training Loss Force: 6.442967819100063, time: 0.718599796295166
Validation Loss Energy: 18.145309592332012, Validation Loss Force: 8.471290452405663, time: 0.06444525718688965
Test Loss Energy: 15.274125736877478, Test Loss Force: 13.983971052489528, time: 9.771016359329224


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.693767586958986, Training Loss Force: 7.422507808630345, time: 0.6839437484741211
Validation Loss Energy: 8.377960461243198, Validation Loss Force: 5.52897620784837, time: 0.06284022331237793
Test Loss Energy: 10.507679139074499, Test Loss Force: 11.323241352479444, time: 9.979932308197021


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.757028960921721, Training Loss Force: 6.154777126525769, time: 0.6948015689849854
Validation Loss Energy: 6.336873755207698, Validation Loss Force: 5.6502436060738095, time: 0.06310796737670898
Test Loss Energy: 11.74670498184442, Test Loss Force: 11.773845189064302, time: 9.805712938308716


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.894987437518563, Training Loss Force: 5.019895249958402, time: 0.6649391651153564
Validation Loss Energy: 10.547464694449026, Validation Loss Force: 5.8869716365371865, time: 0.06258821487426758
Test Loss Energy: 11.15584512382583, Test Loss Force: 11.45227124894625, time: 9.764587879180908


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.351479889604446, Training Loss Force: 5.993265193428541, time: 0.6830878257751465
Validation Loss Energy: 11.089478358715485, Validation Loss Force: 7.434234066860482, time: 0.06384944915771484
Test Loss Energy: 11.601144762182964, Test Loss Force: 13.911416528653962, time: 9.925429582595825


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.171168797081404, Training Loss Force: 5.510002903927582, time: 0.6963846683502197
Validation Loss Energy: 11.977698456408763, Validation Loss Force: 4.066179755326446, time: 0.06281328201293945
Test Loss Energy: 18.836310946645735, Test Loss Force: 11.521191215427068, time: 9.868211269378662


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.336769083815108, Training Loss Force: 5.663579409669523, time: 0.662794828414917
Validation Loss Energy: 12.169412583007961, Validation Loss Force: 8.720735117792977, time: 0.06250667572021484
Test Loss Energy: 11.985852372034454, Test Loss Force: 14.168797929140617, time: 10.16370177268982


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.682982046221879, Training Loss Force: 5.606748714011627, time: 0.8682982921600342
Validation Loss Energy: 31.442367272192108, Validation Loss Force: 5.403683396802846, time: 0.06303954124450684
Test Loss Energy: 33.775032877000676, Test Loss Force: 12.81664292599779, time: 9.825239896774292


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.375175723463846, Training Loss Force: 5.748181503264223, time: 0.7058777809143066
Validation Loss Energy: 4.969074417743801, Validation Loss Force: 5.878254139474114, time: 0.06830096244812012
Test Loss Energy: 9.368509188526177, Test Loss Force: 11.60803147352822, time: 9.758099555969238


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.976494998930278, Training Loss Force: 6.940533975586535, time: 0.7011799812316895
Validation Loss Energy: 4.288017009325047, Validation Loss Force: 4.7483652056384695, time: 0.06278085708618164
Test Loss Energy: 11.176514211259713, Test Loss Force: 10.945850319673937, time: 9.941731452941895


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.434955494018055, Training Loss Force: 5.094860654338819, time: 0.6866858005523682
Validation Loss Energy: 16.37182288049705, Validation Loss Force: 7.045624747974636, time: 0.06419944763183594
Test Loss Energy: 14.69700548848182, Test Loss Force: 12.772359348952886, time: 9.778688192367554


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 12.085739376950201, Training Loss Force: 5.203405400934443, time: 0.6878838539123535
Validation Loss Energy: 9.620837286451252, Validation Loss Force: 5.006849793012158, time: 0.06371021270751953
Test Loss Energy: 10.818383005687393, Test Loss Force: 11.32810083728789, time: 9.81058382987976


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.975331854502793, Training Loss Force: 4.961271265139074, time: 0.6626732349395752
Validation Loss Energy: 10.987854832543796, Validation Loss Force: 5.50681372508503, time: 0.06364893913269043
Test Loss Energy: 11.17525277539254, Test Loss Force: 12.489864646020406, time: 9.901659488677979

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–„â–ƒâ–â–‚â–‚â–‚â–„â–‚â–ˆâ–â–‚â–ƒâ–â–‚
wandb:   test_error_force â–â–â–â–â–â–‚â–ƒâ–ˆâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ˆâ–…â–ƒâ–‚â–…â–ƒâ–…
wandb:          test_loss â–â–â–â–â–â–â–„â–†â–‚â–ƒâ–‚â–…â–„â–…â–ˆâ–‚â–‚â–„â–‚â–ƒ
wandb: train_error_energy â–‚â–â–â–â–â–â–†â–ˆâ–…â–…â–„â–…â–ƒâ–„â–„â–„â–†â–ƒâ–…â–…
wandb:  train_error_force â–‚â–â–â–â–â–â–…â–‡â–ˆâ–†â–„â–†â–…â–…â–…â–…â–‡â–…â–…â–„
wandb:         train_loss â–‚â–â–â–â–â–â–†â–ˆâ–ˆâ–†â–„â–†â–…â–…â–…â–…â–‡â–„â–…â–…
wandb: valid_error_energy â–‚â–â–â–â–â–â–„â–…â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–‚â–‚â–„â–ƒâ–ƒ
wandb:  valid_error_force â–ƒâ–â–â–â–â–‚â–ƒâ–ˆâ–„â–„â–„â–†â–‚â–ˆâ–„â–„â–ƒâ–†â–ƒâ–„
wandb:         valid_loss â–‚â–â–â–â–â–‚â–…â–ˆâ–„â–„â–…â–†â–ƒâ–‡â–ˆâ–„â–ƒâ–‡â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1531
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.17525
wandb:   test_error_force 12.48986
wandb:          test_loss 4.927
wandb: train_error_energy 11.97533
wandb:  train_error_force 4.96127
wandb:         train_loss 2.46145
wandb: valid_error_energy 10.98785
wandb:  valid_error_force 5.50681
wandb:         valid_loss 2.57791
wandb: 
wandb: ğŸš€ View run al_77_8 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/zwlr1u32
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_130939-zwlr1u32/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.6899036169052124, Uncertainty Bias: -0.9268074035644531
0.00022125244 0.0053186417
-8.401182 103.351234
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 2 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 4 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 4 steps.
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 3 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 3 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_131744-enrfrls3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_9
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/enrfrls3
Training model 9. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.265030262799997, Training Loss Force: 3.0435583381382627, time: 0.7414789199829102
Validation Loss Energy: 2.6853321172457534, Validation Loss Force: 3.167219768052208, time: 0.06443357467651367
Test Loss Energy: 9.228496853407817, Test Loss Force: 10.845492869281465, time: 9.526681900024414


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.7022479113230045, Training Loss Force: 2.9044964268032976, time: 0.7189266681671143
Validation Loss Energy: 2.2853510850773757, Validation Loss Force: 3.1151296613995907, time: 0.06787562370300293
Test Loss Energy: 9.234150411364853, Test Loss Force: 10.788107780105829, time: 10.066667079925537


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.3707123467490514, Training Loss Force: 2.8933000720222766, time: 0.7261269092559814
Validation Loss Energy: 2.2893382544716454, Validation Loss Force: 3.1256561226041812, time: 0.06328988075256348
Test Loss Energy: 9.195452034014993, Test Loss Force: 10.736004794782765, time: 9.771653175354004


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.638652503873098, Training Loss Force: 2.7893381268727926, time: 0.751507043838501
Validation Loss Energy: 2.8078932232090343, Validation Loss Force: 3.549827151444142, time: 0.0637979507446289
Test Loss Energy: 9.253553063950946, Test Loss Force: 10.972337367174589, time: 9.202067136764526


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.08293135230508, Training Loss Force: 2.8127544814271586, time: 0.7237200736999512
Validation Loss Energy: 1.5913832082431565, Validation Loss Force: 2.9834085034457054, time: 0.06537461280822754
Test Loss Energy: 9.528144672312225, Test Loss Force: 10.854000285583101, time: 9.17153024673462


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.3819669849621175, Training Loss Force: 2.8386522085510038, time: 0.7224225997924805
Validation Loss Energy: 2.6995025590234514, Validation Loss Force: 3.595805658048479, time: 0.06527924537658691
Test Loss Energy: 9.376261679479853, Test Loss Force: 10.945069056413178, time: 9.714669466018677


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.983755591803652, Training Loss Force: 6.984630276818836, time: 0.7741303443908691
Validation Loss Energy: 8.56453226658634, Validation Loss Force: 6.42922622881954, time: 0.07076454162597656
Test Loss Energy: 10.386918643079438, Test Loss Force: 12.191024619481917, time: 9.508299589157104


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 16.155774849650104, Training Loss Force: 6.752100799717715, time: 0.703087568283081
Validation Loss Energy: 9.315793026002353, Validation Loss Force: 8.950549271275294, time: 0.06658244132995605
Test Loss Energy: 12.174116215735559, Test Loss Force: 12.759472396614262, time: 9.581503868103027


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.165197542746075, Training Loss Force: 6.352156730627563, time: 0.7087478637695312
Validation Loss Energy: 7.176140486507042, Validation Loss Force: 4.566915013320812, time: 0.06450200080871582
Test Loss Energy: 10.182203149506435, Test Loss Force: 11.046886852671134, time: 9.79552435874939


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 13.824247115173975, Training Loss Force: 6.549089025706406, time: 0.7373697757720947
Validation Loss Energy: 20.22696060468862, Validation Loss Force: 8.436107391334197, time: 0.06383681297302246
Test Loss Energy: 15.331576764099676, Test Loss Force: 12.906175806665749, time: 9.580126523971558


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 11.353310245209213, Training Loss Force: 5.251097849532425, time: 0.746143102645874
Validation Loss Energy: 15.473807121736117, Validation Loss Force: 5.781456967040407, time: 0.06307673454284668
Test Loss Energy: 21.653108014762324, Test Loss Force: 12.570849405067888, time: 9.761693954467773


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.526057379029787, Training Loss Force: 4.9728924513659765, time: 0.7009656429290771
Validation Loss Energy: 17.345241537481797, Validation Loss Force: 5.513609142080455, time: 0.0646812915802002
Test Loss Energy: 13.252975964636946, Test Loss Force: 11.819751355009773, time: 9.828227758407593


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 15.109412007150869, Training Loss Force: 5.091592727570448, time: 0.73000168800354
Validation Loss Energy: 34.70083451821949, Validation Loss Force: 6.107623177333723, time: 0.06726646423339844
Test Loss Energy: 28.230968394481547, Test Loss Force: 12.346945804711217, time: 10.076075792312622


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 17.041296223491102, Training Loss Force: 7.754026337923121, time: 0.7152812480926514
Validation Loss Energy: 2.521539922203888, Validation Loss Force: 4.293443901749599, time: 0.06424546241760254
Test Loss Energy: 10.806547345500924, Test Loss Force: 11.226188703923707, time: 9.59644103050232


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 14.030874966258166, Training Loss Force: 6.199992975478329, time: 0.7716116905212402
Validation Loss Energy: 5.8212425993582, Validation Loss Force: 7.617551339966743, time: 0.07205724716186523
Test Loss Energy: 10.538035500163287, Test Loss Force: 12.186313761082245, time: 9.764399528503418


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.433729108247093, Training Loss Force: 6.299997190495971, time: 0.6981766223907471
Validation Loss Energy: 29.872382346203466, Validation Loss Force: 6.619039874258245, time: 0.06283187866210938
Test Loss Energy: 33.21338572802126, Test Loss Force: 12.570661882672805, time: 9.548835277557373


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.59944928888884, Training Loss Force: 5.544070368564608, time: 0.7396101951599121
Validation Loss Energy: 14.07521241872126, Validation Loss Force: 6.682152771641725, time: 0.06277346611022949
Test Loss Energy: 11.428341002828233, Test Loss Force: 11.576864941827141, time: 9.628388404846191


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.157051546127109, Training Loss Force: 4.78365890271303, time: 0.7002801895141602
Validation Loss Energy: 14.656892364643298, Validation Loss Force: 5.45741078968547, time: 0.06342530250549316
Test Loss Energy: 19.034074098599977, Test Loss Force: 12.413553975818344, time: 9.71388053894043


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.276514905128613, Training Loss Force: 4.769484003186351, time: 0.7429249286651611
Validation Loss Energy: 15.729508790124001, Validation Loss Force: 5.662373062459935, time: 0.06477022171020508
Test Loss Energy: 12.693599350788212, Test Loss Force: 11.653576589031317, time: 9.203129291534424


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.225961019327025, Training Loss Force: 4.754364174393465, time: 0.7356626987457275
Validation Loss Energy: 20.896757411728657, Validation Loss Force: 6.673007972487617, time: 0.06308937072753906
Test Loss Energy: 24.50206349720505, Test Loss Force: 13.202877673132093, time: 9.05196762084961

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–‚â–â–ƒâ–…â–‚â–‡â–â–â–ˆâ–‚â–„â–‚â–…
wandb:   test_error_force â–â–â–â–‚â–â–‚â–…â–‡â–‚â–‡â–†â–„â–†â–‚â–…â–†â–ƒâ–†â–„â–ˆ
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–„â–‚â–…â–†â–ƒâ–‡â–‚â–ƒâ–ˆâ–‚â–…â–ƒâ–‡
wandb: train_error_energy â–â–‚â–â–â–â–â–…â–ˆâ–„â–‡â–…â–†â–‡â–ˆâ–‡â–…â–†â–…â–…â–†
wandb:  train_error_force â–â–â–â–â–â–â–‡â–‡â–†â–†â–„â–„â–„â–ˆâ–†â–†â–…â–„â–„â–„
wandb:         train_loss â–â–â–â–â–â–â–†â–‡â–…â–†â–…â–…â–…â–ˆâ–†â–…â–…â–„â–„â–…
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–ƒâ–‚â–…â–„â–„â–ˆâ–â–‚â–‡â–„â–„â–„â–…
wandb:  valid_error_force â–â–â–â–‚â–â–‚â–…â–ˆâ–ƒâ–‡â–„â–„â–…â–ƒâ–†â–…â–…â–„â–„â–…
wandb:         valid_loss â–â–â–â–‚â–â–‚â–„â–†â–ƒâ–ˆâ–…â–…â–ˆâ–‚â–…â–ˆâ–…â–…â–…â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 1621
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 24.50206
wandb:   test_error_force 13.20288
wandb:          test_loss 6.05742
wandb: train_error_energy 12.22596
wandb:  train_error_force 4.75436
wandb:         train_loss 2.40899
wandb: valid_error_energy 20.89676
wandb:  valid_error_force 6.67301
wandb:         valid_loss 3.63123
wandb: 
wandb: ğŸš€ View run al_77_9 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/enrfrls3
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_131744-enrfrls3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8526284694671631, Uncertainty Bias: -0.41456273198127747
0.00011444092 0.11377525
0.81514984 48.607227
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 2 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 5 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 6 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 3 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 2 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 5 steps.
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 4 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 5 steps.
Found uncertainty sample 44 after 10 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 6 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 4 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 3 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 9 steps.
Found uncertainty sample 60 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 3 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 6 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 5 steps.
Found uncertainty sample 85 after 7 steps.
Found uncertainty sample 86 after 3 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 11 steps.
Found uncertainty sample 91 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 6 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 2 steps.
Found uncertainty sample 98 after 5 steps.
Found uncertainty sample 99 after 3 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_132544-jvgzdeys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_10
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/jvgzdeys
Training model 10. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.156937102759641, Training Loss Force: 3.045490450930546, time: 0.7493102550506592
Validation Loss Energy: 3.49596459838994, Validation Loss Force: 3.1291907726266386, time: 0.06539726257324219
Test Loss Energy: 10.964627423514889, Test Loss Force: 10.81176498955027, time: 10.000457525253296


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6443057178104397, Training Loss Force: 2.8175123983610284, time: 0.7620041370391846
Validation Loss Energy: 3.7301287049725307, Validation Loss Force: 3.1401958611604006, time: 0.0641787052154541
Test Loss Energy: 11.009534996449949, Test Loss Force: 10.823421311307124, time: 9.72259783744812


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.4295814595840146, Training Loss Force: 2.863064209029227, time: 0.8260726928710938
Validation Loss Energy: 1.7691917023464514, Validation Loss Force: 3.1546611168873078, time: 0.06864738464355469
Test Loss Energy: 9.884617848449755, Test Loss Force: 10.913836554022936, time: 9.806691646575928


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7490898720208254, Training Loss Force: 2.789942947938196, time: 0.781470775604248
Validation Loss Energy: 1.3071461548009082, Validation Loss Force: 3.110108583493492, time: 0.06948566436767578
Test Loss Energy: 9.531920898539292, Test Loss Force: 10.838773903580552, time: 9.629647970199585


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6910629678156115, Training Loss Force: 2.7435541858029944, time: 0.8491611480712891
Validation Loss Energy: 1.3927961757857508, Validation Loss Force: 3.167205628561441, time: 0.0646371841430664
Test Loss Energy: 9.55599695447626, Test Loss Force: 10.865842796051774, time: 9.680986642837524


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8101377738873714, Training Loss Force: 2.7489376866086097, time: 0.7705333232879639
Validation Loss Energy: 4.017602922458277, Validation Loss Force: 3.1162594846460285, time: 0.06384873390197754
Test Loss Energy: 10.89760530696806, Test Loss Force: 10.993762708976464, time: 9.833730936050415


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.2096225140584735, Training Loss Force: 3.9229966659744027, time: 0.7586495876312256
Validation Loss Energy: 9.223254400984516, Validation Loss Force: 4.366806705258428, time: 0.06427407264709473
Test Loss Energy: 9.832387654924982, Test Loss Force: 11.032514279770252, time: 9.728426456451416


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.071323975499804, Training Loss Force: 3.6237474071598, time: 0.7507970333099365
Validation Loss Energy: 12.084305318066317, Validation Loss Force: 4.286637573966025, time: 0.06723737716674805
Test Loss Energy: 16.85253997942208, Test Loss Force: 11.646674027373454, time: 9.653688430786133


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.445022250905095, Training Loss Force: 3.6806642386656456, time: 0.7727444171905518
Validation Loss Energy: 5.6282828615566824, Validation Loss Force: 4.578714696701818, time: 0.06862258911132812
Test Loss Energy: 9.062968802567239, Test Loss Force: 10.996800042034081, time: 9.809761047363281


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 19.113000846324017, Training Loss Force: 6.083492942586805, time: 0.794081449508667
Validation Loss Energy: 27.066722605862658, Validation Loss Force: 7.957177677462248, time: 0.06420540809631348
Test Loss Energy: 22.519986811352112, Test Loss Force: 14.747242543667832, time: 9.624170064926147


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.810219862979203, Training Loss Force: 6.616709526188034, time: 0.758385419845581
Validation Loss Energy: 3.218025185067394, Validation Loss Force: 6.949408296673509, time: 0.07299685478210449
Test Loss Energy: 9.934020050556684, Test Loss Force: 11.915749493785906, time: 9.707798957824707


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.216751791196524, Training Loss Force: 6.660024571679854, time: 0.7815642356872559
Validation Loss Energy: 3.7465422043917327, Validation Loss Force: 5.130021012435281, time: 0.06912517547607422
Test Loss Energy: 11.056344313114534, Test Loss Force: 10.851032038251072, time: 9.837359189987183


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.353129869477778, Training Loss Force: 4.626503185952839, time: 0.7638733386993408
Validation Loss Energy: 29.10250733686447, Validation Loss Force: 4.951170259727185, time: 0.06506228446960449
Test Loss Energy: 23.823422192413876, Test Loss Force: 10.711414459388426, time: 10.067795038223267


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 13.666839077159924, Training Loss Force: 5.925360234386424, time: 0.7512223720550537
Validation Loss Energy: 2.5821748459969114, Validation Loss Force: 4.624828140808587, time: 0.06564903259277344
Test Loss Energy: 10.764788388900936, Test Loss Force: 10.990040189148425, time: 9.681952714920044


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.766987923920578, Training Loss Force: 5.359631592712685, time: 0.9424769878387451
Validation Loss Energy: 18.52365653562034, Validation Loss Force: 5.936476777074728, time: 0.07962560653686523
Test Loss Energy: 15.061626788515639, Test Loss Force: 11.713542080187441, time: 9.76753854751587


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.605681873234083, Training Loss Force: 5.19341389152875, time: 0.7640838623046875
Validation Loss Energy: 11.71998118691194, Validation Loss Force: 6.049717843266055, time: 0.06845331192016602
Test Loss Energy: 11.042776496818016, Test Loss Force: 11.521239932171936, time: 9.654650211334229


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.657851984086133, Training Loss Force: 5.023089993983287, time: 0.756887674331665
Validation Loss Energy: 10.280281314220524, Validation Loss Force: 7.152801197307475, time: 0.06990337371826172
Test Loss Energy: 10.375847999466727, Test Loss Force: 11.872303698327391, time: 9.975829124450684


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.31241489317601, Training Loss Force: 4.943138088813247, time: 0.794558048248291
Validation Loss Energy: 6.701191651625988, Validation Loss Force: 6.114637478099594, time: 0.06492185592651367
Test Loss Energy: 12.861599891845804, Test Loss Force: 11.6980612211595, time: 9.657910108566284


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.858894741775165, Training Loss Force: 4.805032437402237, time: 0.7655653953552246
Validation Loss Energy: 15.714020815009848, Validation Loss Force: 5.267796655801517, time: 0.06608915328979492
Test Loss Energy: 19.085045122586482, Test Loss Force: 11.30718410445791, time: 9.76641845703125


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.121617596476412, Training Loss Force: 5.852571760580987, time: 0.7754690647125244
Validation Loss Energy: 6.283070654815459, Validation Loss Force: 4.509156640889458, time: 0.06765627861022949
Test Loss Energy: 10.087079118864263, Test Loss Force: 10.831406175584583, time: 9.828273057937622

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–â–â–â–‚â–â–…â–â–‡â–â–‚â–ˆâ–‚â–„â–‚â–‚â–ƒâ–†â–
wandb:   test_error_force â–â–â–â–â–â–â–‚â–ƒâ–â–ˆâ–ƒâ–â–â–â–ƒâ–‚â–ƒâ–ƒâ–‚â–
wandb:          test_loss â–â–â–â–â–â–â–â–ƒâ–â–ˆâ–‚â–â–„â–â–ƒâ–‚â–‚â–ƒâ–„â–
wandb: train_error_energy â–‚â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–„â–…â–ƒâ–†â–„â–…â–…â–…â–…â–…
wandb:  train_error_force â–‚â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–‡â–ˆâ–ˆâ–„â–‡â–†â–…â–…â–…â–…â–‡
wandb:         train_loss â–‚â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–‡â–‡â–„â–‡â–…â–†â–…â–…â–…â–†
wandb: valid_error_energy â–‚â–‚â–â–â–â–‚â–ƒâ–„â–‚â–‡â–â–‚â–ˆâ–â–…â–„â–ƒâ–‚â–…â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–‡â–„â–„â–ƒâ–…â–…â–‡â–…â–„â–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–„â–ƒâ–†â–‚â–…â–…â–…â–„â–…â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1711
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.08708
wandb:   test_error_force 10.83141
wandb:          test_loss 4.29926
wandb: train_error_energy 11.12162
wandb:  train_error_force 5.85257
wandb:         train_loss 2.70255
wandb: valid_error_energy 6.28307
wandb:  valid_error_force 4.50916
wandb:         valid_loss 1.92925
wandb: 
wandb: ğŸš€ View run al_77_10 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/jvgzdeys
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_132544-jvgzdeys/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.3565683364868164, Uncertainty Bias: -0.8681283593177795
6.67572e-06 0.002108574
-0.6314897 71.95844
(48745, 22, 3)
Found uncertainty sample 0 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 4 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 3 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 5 steps.
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 4 steps.
Found uncertainty sample 57 after 5 steps.
Found uncertainty sample 58 after 3 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 3 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 7 steps.
Found uncertainty sample 78 after 3 steps.
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 7 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 9 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 4 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_133354-x0y8q0ka
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_11
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/x0y8q0ka
Training model 11. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.795072265836993, Training Loss Force: 2.9955571953208566, time: 0.8065488338470459
Validation Loss Energy: 2.9764843836839345, Validation Loss Force: 3.63386927852624, time: 0.06787276268005371
Test Loss Energy: 10.357879121671912, Test Loss Force: 10.643740913180018, time: 9.294214248657227


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.850289822011713, Training Loss Force: 2.739343048033906, time: 0.8358650207519531
Validation Loss Energy: 1.6583420471151014, Validation Loss Force: 3.448669449125732, time: 0.06998515129089355
Test Loss Energy: 9.601203202513465, Test Loss Force: 10.709778734174053, time: 9.965501070022583


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.787769426716546, Training Loss Force: 2.7906996473615506, time: 0.8504090309143066
Validation Loss Energy: 2.1669739880333645, Validation Loss Force: 3.161073542951736, time: 0.06826949119567871
Test Loss Energy: 9.359492799520845, Test Loss Force: 10.712764277419426, time: 9.642870426177979


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.3903948689042807, Training Loss Force: 2.736289976683423, time: 0.8122191429138184
Validation Loss Energy: 1.388282505195935, Validation Loss Force: 3.060827336782181, time: 0.06668663024902344
Test Loss Energy: 9.716352748467434, Test Loss Force: 10.846798908926242, time: 9.586947917938232


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.310174162199808, Training Loss Force: 2.693322640565372, time: 0.8119401931762695
Validation Loss Energy: 1.439931504270469, Validation Loss Force: 3.025854586719044, time: 0.06865882873535156
Test Loss Energy: 9.632938326290612, Test Loss Force: 10.828935462057508, time: 9.584029197692871


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7097808933935947, Training Loss Force: 2.6471519861140043, time: 0.7772011756896973
Validation Loss Energy: 1.629017689609428, Validation Loss Force: 2.9937564953298343, time: 0.06963229179382324
Test Loss Energy: 9.656712526803807, Test Loss Force: 10.794735915448111, time: 9.751363515853882


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.653560724106162, Training Loss Force: 5.343667545972931, time: 0.7862133979797363
Validation Loss Energy: 5.201734282911076, Validation Loss Force: 3.947071134470775, time: 0.06621003150939941
Test Loss Energy: 9.468171318953376, Test Loss Force: 10.796203834396177, time: 9.51074504852295


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 20.5520268215881, Training Loss Force: 6.71983508683299, time: 0.8111608028411865
Validation Loss Energy: 20.178558567100005, Validation Loss Force: 8.0892765936222, time: 0.06571626663208008
Test Loss Energy: 17.69625454439844, Test Loss Force: 15.111170575556779, time: 9.128705978393555


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 13.05502702979445, Training Loss Force: 8.419602154180112, time: 0.8143854141235352
Validation Loss Energy: 39.1566258262576, Validation Loss Force: 7.167085423018742, time: 0.06499147415161133
Test Loss Energy: 30.01645279036634, Test Loss Force: 11.73481879558054, time: 9.385662078857422


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 12.08021824933167, Training Loss Force: 6.8412112413570725, time: 0.8418350219726562
Validation Loss Energy: 9.934730418759319, Validation Loss Force: 4.6790445151356685, time: 0.0679936408996582
Test Loss Energy: 14.28481327801915, Test Loss Force: 11.005660362118448, time: 9.485158205032349


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 10.192695136180886, Training Loss Force: 5.606839041508588, time: 0.797142744064331
Validation Loss Energy: 15.85663149621076, Validation Loss Force: 5.532840853545092, time: 0.06878852844238281
Test Loss Energy: 18.629174230386404, Test Loss Force: 11.46532170805479, time: 9.571517705917358


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 13.32430957651847, Training Loss Force: 5.987391428931898, time: 0.7985000610351562
Validation Loss Energy: 2.979039623468029, Validation Loss Force: 7.451280280657837, time: 0.06589889526367188
Test Loss Energy: 9.142281626433475, Test Loss Force: 12.829446798263033, time: 9.682412147521973


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.263802720370496, Training Loss Force: 6.785958767961608, time: 0.8076071739196777
Validation Loss Energy: 5.419109692563577, Validation Loss Force: 5.320207406024269, time: 0.06794023513793945
Test Loss Energy: 12.549394587000643, Test Loss Force: 11.510109585248747, time: 9.502060890197754


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 12.996927782152488, Training Loss Force: 5.542365069204433, time: 0.8205618858337402
Validation Loss Energy: 12.047816525123132, Validation Loss Force: 5.255097166401817, time: 0.06947994232177734
Test Loss Energy: 11.112912283467882, Test Loss Force: 10.506063302528345, time: 9.978278636932373


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.77253991136552, Training Loss Force: 5.707204877653411, time: 0.8111245632171631
Validation Loss Energy: 11.270846831456588, Validation Loss Force: 6.763036225554265, time: 0.0658273696899414
Test Loss Energy: 16.14944080289645, Test Loss Force: 12.812633179292803, time: 9.68212080001831


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.216669134014728, Training Loss Force: 5.741831320550724, time: 0.8261642456054688
Validation Loss Energy: 15.021620417628387, Validation Loss Force: 4.872625945192617, time: 0.06728506088256836
Test Loss Energy: 12.444443735600073, Test Loss Force: 10.533054444996006, time: 9.459939956665039


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.935759313468344, Training Loss Force: 5.117804297089287, time: 0.8060238361358643
Validation Loss Energy: 8.649042778216899, Validation Loss Force: 4.110951840498146, time: 0.07063460350036621
Test Loss Energy: 14.033505233280604, Test Loss Force: 11.146604834582332, time: 9.579580068588257


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.139172166429724, Training Loss Force: 4.912037598820111, time: 0.847703218460083
Validation Loss Energy: 7.3891593743194655, Validation Loss Force: 5.857709793394232, time: 0.06646466255187988
Test Loss Energy: 9.865874893428817, Test Loss Force: 11.84672237564781, time: 9.663021564483643


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.364462608701661, Training Loss Force: 4.963513150349396, time: 0.817875862121582
Validation Loss Energy: 4.845887611480462, Validation Loss Force: 5.755760056923757, time: 0.07169222831726074
Test Loss Energy: 12.987515442041486, Test Loss Force: 11.760066571537592, time: 9.57489562034607


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.139624115615458, Training Loss Force: 6.017770567251974, time: 0.7763323783874512
Validation Loss Energy: 3.696159970088382, Validation Loss Force: 6.493176584860855, time: 0.07056832313537598
Test Loss Energy: 9.54216908393344, Test Loss Force: 11.59461662044269, time: 9.536181688308716

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–„â–ˆâ–ƒâ–„â–â–‚â–‚â–ƒâ–‚â–ƒâ–â–‚â–
wandb:   test_error_force â–â–â–â–‚â–â–â–â–ˆâ–ƒâ–‚â–‚â–…â–ƒâ–â–…â–â–‚â–ƒâ–ƒâ–ƒ
wandb:          test_loss â–â–â–â–â–â–â–â–ˆâ–‡â–‚â–„â–ƒâ–ƒâ–â–…â–‚â–ƒâ–‚â–ƒâ–‚
wandb: train_error_energy â–‚â–â–â–â–â–â–…â–ˆâ–…â–…â–„â–…â–…â–…â–„â–„â–…â–…â–…â–…
wandb:  train_error_force â–â–â–â–â–â–â–„â–†â–ˆâ–†â–…â–…â–†â–…â–…â–…â–„â–„â–„â–…
wandb:         train_loss â–‚â–â–â–â–â–â–…â–ˆâ–ˆâ–†â–…â–†â–†â–…â–…â–…â–…â–…â–…â–†
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–„â–ˆâ–ƒâ–„â–â–‚â–ƒâ–ƒâ–„â–‚â–‚â–‚â–
wandb:  valid_error_force â–‚â–‚â–â–â–â–â–‚â–ˆâ–‡â–ƒâ–„â–‡â–„â–„â–†â–„â–ƒâ–…â–…â–†
wandb:         valid_loss â–‚â–â–â–â–â–â–‚â–†â–ˆâ–ƒâ–„â–„â–ƒâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 1801
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.54217
wandb:   test_error_force 11.59462
wandb:          test_loss 4.51816
wandb: train_error_energy 12.13962
wandb:  train_error_force 6.01777
wandb:         train_loss 2.82596
wandb: valid_error_energy 3.69616
wandb:  valid_error_force 6.49318
wandb:         valid_loss 2.41999
wandb: 
wandb: ğŸš€ View run al_77_11 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/x0y8q0ka
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_133354-x0y8q0ka/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.6843514442443848, Uncertainty Bias: -0.9497418403625488
0.00024414062 0.016304016
4.3275213 181.78276
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_134155-9qklrjk2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_12
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/9qklrjk2
Training model 12. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.15549062254548, Training Loss Force: 3.003243099045247, time: 0.9320971965789795
Validation Loss Energy: 4.9735489542312195, Validation Loss Force: 3.4781842444184603, time: 0.07652401924133301
Test Loss Energy: 11.087154630140866, Test Loss Force: 10.542528338655533, time: 9.723238468170166


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.8159769234940868, Training Loss Force: 2.7611953282614907, time: 0.8391399383544922
Validation Loss Energy: 2.017643207675639, Validation Loss Force: 3.0957320614212374, time: 0.06942605972290039
Test Loss Energy: 9.35755918728989, Test Loss Force: 10.529752501718148, time: 9.794934749603271


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.4321861284135275, Training Loss Force: 2.7443071501793046, time: 0.8353245258331299
Validation Loss Energy: 4.76513998574694, Validation Loss Force: 3.222149351653696, time: 0.0697317123413086
Test Loss Energy: 11.627978357486331, Test Loss Force: 10.823744188563559, time: 10.716157913208008


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.7384878364565717, Training Loss Force: 2.8583584685699877, time: 0.9216132164001465
Validation Loss Energy: 2.521696903951767, Validation Loss Force: 3.1987990403314055, time: 0.07159614562988281
Test Loss Energy: 10.393043961179236, Test Loss Force: 10.653351961706981, time: 11.532620429992676


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.7077542229553617, Training Loss Force: 2.7571330933869764, time: 0.9222719669342041
Validation Loss Energy: 1.385297736629346, Validation Loss Force: 3.030701982691319, time: 0.07542943954467773
Test Loss Energy: 9.664844124219398, Test Loss Force: 10.641072077740192, time: 11.360339403152466


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.388747485689826, Training Loss Force: 2.6877858310327616, time: 0.8726744651794434
Validation Loss Energy: 2.1324403415259967, Validation Loss Force: 3.0152670028338964, time: 0.08197903633117676
Test Loss Energy: 9.334821281987, Test Loss Force: 10.612734712590465, time: 11.146698951721191


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.630851521408124, Training Loss Force: 6.005685836112369, time: 0.94154953956604
Validation Loss Energy: 11.997340992447636, Validation Loss Force: 4.052081347017455, time: 0.07809305191040039
Test Loss Energy: 11.196082930728, Test Loss Force: 10.754492521998579, time: 11.259471654891968


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 13.116637024316699, Training Loss Force: 5.393872416097952, time: 1.0246868133544922
Validation Loss Energy: 44.96738738891698, Validation Loss Force: 6.773075263998418, time: 0.07732081413269043
Test Loss Energy: 38.27567170025803, Test Loss Force: 12.011844002835751, time: 11.301408529281616


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.767814010124948, Training Loss Force: 6.386546509944837, time: 0.895066499710083
Validation Loss Energy: 3.6861870643805688, Validation Loss Force: 4.3150548330384755, time: 0.07516980171203613
Test Loss Energy: 9.654461488630167, Test Loss Force: 10.854837820018393, time: 11.01956057548523


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.346098935410199, Training Loss Force: 5.151499622584189, time: 0.9366648197174072
Validation Loss Energy: 19.999206586144425, Validation Loss Force: 6.670193657152236, time: 0.07270288467407227
Test Loss Energy: 22.33388689672212, Test Loss Force: 12.529110257861234, time: 11.336344242095947


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 17.512644409214843, Training Loss Force: 6.279268130099297, time: 0.9480657577514648
Validation Loss Energy: 36.831142301667654, Validation Loss Force: 8.985726988287258, time: 0.07653427124023438
Test Loss Energy: 26.390449232386093, Test Loss Force: 15.326925618336945, time: 11.031229257583618


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 14.936356740600763, Training Loss Force: 8.810357719631877, time: 0.9220061302185059
Validation Loss Energy: 2.7610576630620214, Validation Loss Force: 8.961343826131582, time: 0.07755470275878906
Test Loss Energy: 10.515754264796747, Test Loss Force: 13.638688793946107, time: 11.314997673034668


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.803346189899008, Training Loss Force: 5.337104078934373, time: 0.9328184127807617
Validation Loss Energy: 3.620788356879566, Validation Loss Force: 4.64505473801723, time: 0.07960391044616699
Test Loss Energy: 11.532887384468777, Test Loss Force: 11.057346930490874, time: 11.331707239151001


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.272447981655209, Training Loss Force: 5.753237334257273, time: 0.8760583400726318
Validation Loss Energy: 4.531370337556359, Validation Loss Force: 7.336552007240858, time: 0.0813450813293457
Test Loss Energy: 9.762132503623215, Test Loss Force: 12.154655342745546, time: 11.069323539733887


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 13.58456913504934, Training Loss Force: 6.95575067102463, time: 0.8830862045288086
Validation Loss Energy: 3.1703017317009943, Validation Loss Force: 4.678769518740447, time: 0.07132863998413086
Test Loss Energy: 9.240509305901309, Test Loss Force: 11.242604237093301, time: 11.694122552871704


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.220051731445162, Training Loss Force: 4.931293319878394, time: 0.9175331592559814
Validation Loss Energy: 8.755471504557711, Validation Loss Force: 5.1209512527833505, time: 0.08108782768249512
Test Loss Energy: 10.164470081834578, Test Loss Force: 11.003834339888224, time: 11.231714725494385


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.476263319599529, Training Loss Force: 4.908165046806117, time: 0.9272058010101318
Validation Loss Energy: 8.938994969293102, Validation Loss Force: 4.93022310516232, time: 0.07945013046264648
Test Loss Energy: 10.429259785065657, Test Loss Force: 11.3870909303306, time: 11.234585762023926


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.493434153895553, Training Loss Force: 4.757179764182252, time: 0.8995769023895264
Validation Loss Energy: 2.9262906388126115, Validation Loss Force: 4.865159848729659, time: 0.07155966758728027
Test Loss Energy: 8.851215112681327, Test Loss Force: 11.455735300025571, time: 11.356903791427612


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 12.778427454518106, Training Loss Force: 5.290980781717896, time: 0.9076952934265137
Validation Loss Energy: 36.77422628661534, Validation Loss Force: 5.645125110416368, time: 0.07155251502990723
Test Loss Energy: 37.5639937460498, Test Loss Force: 11.995881777414985, time: 11.181998491287231


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.63398535146559, Training Loss Force: 5.761919181990082, time: 0.9206726551055908
Validation Loss Energy: 32.56381281468332, Validation Loss Force: 4.358891707797278, time: 0.06992769241333008
Test Loss Energy: 29.54036758739809, Test Loss Force: 10.490050518387573, time: 10.11470651626587

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‚â–â–â–â–‚â–ˆâ–â–„â–…â–â–‚â–â–â–â–â–â–ˆâ–†
wandb:   test_error_force â–â–â–â–â–â–â–â–ƒâ–‚â–„â–ˆâ–†â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–
wandb:          test_loss â–â–â–‚â–â–â–â–‚â–‡â–â–…â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–‡â–„
wandb: train_error_energy â–‚â–â–â–‚â–‚â–â–…â–†â–…â–„â–ˆâ–‡â–ƒâ–„â–†â–…â–…â–…â–†â–…
wandb:  train_error_force â–â–â–â–â–â–â–…â–„â–…â–„â–…â–ˆâ–„â–…â–†â–„â–„â–ƒâ–„â–…
wandb:         train_loss â–‚â–â–â–‚â–â–â–…â–…â–…â–„â–†â–ˆâ–„â–„â–†â–„â–„â–„â–…â–…
wandb: valid_error_energy â–‚â–â–‚â–â–â–â–ƒâ–ˆâ–â–„â–‡â–â–â–‚â–â–‚â–‚â–â–‡â–†
wandb:  valid_error_force â–‚â–â–â–â–â–â–‚â–…â–ƒâ–…â–ˆâ–ˆâ–ƒâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒ
wandb:         valid_loss â–‚â–â–â–â–â–â–ƒâ–ˆâ–‚â–…â–ˆâ–„â–‚â–„â–‚â–ƒâ–ƒâ–‚â–†â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 1891
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 29.54037
wandb:   test_error_force 10.49005
wandb:          test_loss 5.48686
wandb: train_error_energy 11.63399
wandb:  train_error_force 5.76192
wandb:         train_loss 2.70651
wandb: valid_error_energy 32.56381
wandb:  valid_error_force 4.35889
wandb:         valid_loss 3.63769
wandb: 
wandb: ğŸš€ View run al_77_12 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/9qklrjk2
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_134155-9qklrjk2/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.0970458984375, Uncertainty Bias: -0.739048957824707
0.0003528595 0.91023827
-9.546106 12.476275
(48745, 22, 3)
Found uncertainty sample 0 after 10 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 11 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 3 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 4 steps.
Found uncertainty sample 11 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 6 steps.
Found uncertainty sample 16 after 37 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 3 steps.
Found uncertainty sample 20 after 4 steps.
Found uncertainty sample 21 after 2 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 8 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 4 steps.
Found uncertainty sample 26 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 18 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 4 steps.
Found uncertainty sample 33 after 4 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 10 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 5 steps.
Found uncertainty sample 38 after 23 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 6 steps.
Found uncertainty sample 41 after 7 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 26 steps.
Found uncertainty sample 45 after 8 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 13 steps.
Found uncertainty sample 52 after 3 steps.
Found uncertainty sample 53 after 4 steps.
Found uncertainty sample 54 after 4 steps.
Found uncertainty sample 55 after 27 steps.
Found uncertainty sample 56 after 13 steps.
Found uncertainty sample 57 after 7 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 43 steps.
Found uncertainty sample 60 after 25 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 4 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 4 steps.
Found uncertainty sample 69 after 6 steps.
Found uncertainty sample 70 after 14 steps.
Found uncertainty sample 71 after 9 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 13 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 6 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 8 steps.
Found uncertainty sample 81 after 5 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 6 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 6 steps.
Found uncertainty sample 89 after 9 steps.
Found uncertainty sample 90 after 2 steps.
Found uncertainty sample 91 after 3 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 7 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 9 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_135039-7sdg6o77
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_13
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/7sdg6o77
Training model 13. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.5809999071083585, Training Loss Force: 2.9166953476533504, time: 0.8700008392333984
Validation Loss Energy: 1.7904377902397242, Validation Loss Force: 3.065133424060706, time: 0.07666969299316406
Test Loss Energy: 10.220613698422913, Test Loss Force: 10.603620064704005, time: 9.829123258590698


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.4290002350478794, Training Loss Force: 2.7142307654630726, time: 0.9321026802062988
Validation Loss Energy: 2.675270700552433, Validation Loss Force: 3.1069927741345245, time: 0.07036185264587402
Test Loss Energy: 10.548341789380645, Test Loss Force: 10.790187099564712, time: 9.685828447341919


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.348236286199816, Training Loss Force: 2.74123867609075, time: 0.9339601993560791
Validation Loss Energy: 3.0287400448465487, Validation Loss Force: 3.5918399205595053, time: 0.07214212417602539
Test Loss Energy: 9.64293330772066, Test Loss Force: 10.647177894219716, time: 10.037307739257812


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5725647419818585, Training Loss Force: 2.6688069169269273, time: 0.8806757926940918
Validation Loss Energy: 1.791173656099729, Validation Loss Force: 2.993646385437173, time: 0.0732574462890625
Test Loss Energy: 9.552347749921775, Test Loss Force: 10.728290920021708, time: 9.861063718795776


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.2256026951445187, Training Loss Force: 2.6760676657486493, time: 0.9057981967926025
Validation Loss Energy: 2.0921535366280066, Validation Loss Force: 2.997547660208401, time: 0.07116317749023438
Test Loss Energy: 9.597828475570065, Test Loss Force: 10.732281939202297, time: 9.80222487449646


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5355923287130464, Training Loss Force: 2.642897307495786, time: 0.9054625034332275
Validation Loss Energy: 1.712962009269177, Validation Loss Force: 2.9831202582363017, time: 0.0725245475769043
Test Loss Energy: 9.446622033209517, Test Loss Force: 10.741020535041299, time: 10.3968346118927


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 16.759326345392584, Training Loss Force: 6.728093923701115, time: 0.9591560363769531
Validation Loss Energy: 18.761917865822575, Validation Loss Force: 8.077896418912122, time: 0.06747579574584961
Test Loss Energy: 22.291009119762595, Test Loss Force: 13.737849128377787, time: 9.829833745956421


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.143099777275689, Training Loss Force: 6.304369561985008, time: 0.867485523223877
Validation Loss Energy: 6.677349352825771, Validation Loss Force: 5.912429177321965, time: 0.06895232200622559
Test Loss Energy: 9.58102752192759, Test Loss Force: 11.199761634863997, time: 9.963961601257324


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.410363025694542, Training Loss Force: 6.146832017551931, time: 1.0150325298309326
Validation Loss Energy: 11.390225549986269, Validation Loss Force: 6.100278435555294, time: 0.1089165210723877
Test Loss Energy: 11.490010265024432, Test Loss Force: 11.469173217437714, time: 9.627724409103394


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.420042918529651, Training Loss Force: 6.3594864534354745, time: 0.9374885559082031
Validation Loss Energy: 3.6963588136356957, Validation Loss Force: 4.936688526483479, time: 0.07440781593322754
Test Loss Energy: 11.358817609956796, Test Loss Force: 11.55578433849544, time: 10.51225996017456


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 11.217218168995837, Training Loss Force: 6.289520036775234, time: 0.9834496974945068
Validation Loss Energy: 11.249023473802499, Validation Loss Force: 5.648795588617842, time: 0.07763791084289551
Test Loss Energy: 17.12769584772044, Test Loss Force: 12.223449644553183, time: 11.186784267425537


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.794897993003042, Training Loss Force: 6.564906878751999, time: 0.9679901599884033
Validation Loss Energy: 2.4115019516767315, Validation Loss Force: 4.650776667060136, time: 0.07161068916320801
Test Loss Energy: 9.73214768990345, Test Loss Force: 11.030168517442451, time: 10.880635976791382


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.303038027280294, Training Loss Force: 5.861263391842721, time: 0.9986305236816406
Validation Loss Energy: 39.14588248530488, Validation Loss Force: 5.823458157360182, time: 0.07612729072570801
Test Loss Energy: 37.475231603736596, Test Loss Force: 13.028665520937102, time: 11.091454982757568


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.27743228769758, Training Loss Force: 6.338752192122744, time: 0.9377031326293945
Validation Loss Energy: 8.913658337854562, Validation Loss Force: 9.168842077809018, time: 0.08112645149230957
Test Loss Energy: 10.491519314022534, Test Loss Force: 13.058938354282606, time: 11.078577041625977


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 16.633024947267703, Training Loss Force: 7.748142778825889, time: 0.9116003513336182
Validation Loss Energy: 12.234935205888672, Validation Loss Force: 6.958288564882256, time: 0.07553410530090332
Test Loss Energy: 11.486838015448912, Test Loss Force: 12.048986969750304, time: 10.93981409072876


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.984895581143043, Training Loss Force: 6.1223384258656255, time: 0.8994946479797363
Validation Loss Energy: 26.912694341535154, Validation Loss Force: 6.4265163453135985, time: 0.0714867115020752
Test Loss Energy: 21.08483050141754, Test Loss Force: 12.034825864921247, time: 11.128896474838257


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.176660812077383, Training Loss Force: 5.460195619316205, time: 0.8867063522338867
Validation Loss Energy: 20.237333424182516, Validation Loss Force: 6.203688881623057, time: 0.07614779472351074
Test Loss Energy: 15.540461095070498, Test Loss Force: 11.653919163763614, time: 10.995702028274536


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.499815248016857, Training Loss Force: 4.652410645575893, time: 0.9392721652984619
Validation Loss Energy: 16.101718971750557, Validation Loss Force: 5.0040439391043225, time: 0.07759785652160645
Test Loss Energy: 21.509942096970878, Test Loss Force: 12.196895138793579, time: 11.58638882637024


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.25170641254606, Training Loss Force: 4.930138358003669, time: 0.9140207767486572
Validation Loss Energy: 5.678668658566368, Validation Loss Force: 5.412227211932095, time: 0.07895851135253906
Test Loss Energy: 9.894523370510267, Test Loss Force: 12.442079805335249, time: 11.073052167892456


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.51710235032429, Training Loss Force: 6.2580979437729995, time: 0.950446367263794
Validation Loss Energy: 4.704854763847596, Validation Loss Force: 6.510477532161152, time: 0.07608962059020996
Test Loss Energy: 11.088158240784807, Test Loss Force: 12.346412289437573, time: 10.876534700393677

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–„â–â–‚â–â–ƒâ–â–ˆâ–â–‚â–„â–ƒâ–„â–â–
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–‚â–ƒâ–ƒâ–…â–‚â–†â–†â–„â–„â–ƒâ–…â–…â–…
wandb:          test_loss â–â–â–â–â–â–â–†â–â–‚â–‚â–„â–â–ˆâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒ
wandb: train_error_energy â–‚â–â–â–â–â–â–ˆâ–…â–…â–†â–…â–†â–†â–„â–ˆâ–…â–†â–†â–…â–†
wandb:  train_error_force â–â–â–â–â–â–â–‡â–†â–†â–†â–†â–†â–…â–†â–ˆâ–†â–…â–„â–„â–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–†â–†â–†â–†â–†â–…â–…â–ˆâ–†â–…â–„â–…â–†
wandb: valid_error_energy â–â–â–â–â–â–â–„â–‚â–ƒâ–â–ƒâ–â–ˆâ–‚â–ƒâ–†â–„â–„â–‚â–‚
wandb:  valid_error_force â–â–â–‚â–â–â–â–‡â–„â–…â–ƒâ–„â–ƒâ–„â–ˆâ–…â–…â–…â–ƒâ–„â–…
wandb:         valid_loss â–â–â–‚â–â–â–â–‡â–„â–„â–ƒâ–„â–‚â–ˆâ–†â–…â–‡â–†â–„â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 1981
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.08816
wandb:   test_error_force 12.34641
wandb:          test_loss 4.87318
wandb: train_error_energy 11.5171
wandb:  train_error_force 6.2581
wandb:         train_loss 2.86471
wandb: valid_error_energy 4.70485
wandb:  valid_error_force 6.51048
wandb:         valid_loss 2.49328
wandb: 
wandb: ğŸš€ View run al_77_13 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/7sdg6o77
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_135039-7sdg6o77/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7836844325065613, Uncertainty Bias: -0.4255831241607666
1.4305115e-05 0.002210617
2.2481256 44.211845
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 2 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 6 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 5 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 3 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 3 steps.
Found uncertainty sample 20 after 6 steps.
Found uncertainty sample 21 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 2 steps.
Found uncertainty sample 32 after 3 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 6 steps.
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 20 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 3 steps.
Found uncertainty sample 43 after 5 steps.
Found uncertainty sample 44 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 9 steps.
Found uncertainty sample 49 after 9 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 3 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 9 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 3 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 8 steps.
Found uncertainty sample 74 after 3 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 5 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 9 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 4 steps.
Found uncertainty sample 94 after 3 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 8 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_135908-8fs10jgb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_14
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/8fs10jgb
Training model 14. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.504335391885025, Training Loss Force: 3.0299391096331725, time: 0.9403214454650879
Validation Loss Energy: 1.3438984210110139, Validation Loss Force: 3.0414442102983132, time: 0.07371282577514648
Test Loss Energy: 9.772253312939435, Test Loss Force: 10.554967975493549, time: 9.272145986557007


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.4730381704176396, Training Loss Force: 2.684348873672401, time: 1.01324462890625
Validation Loss Energy: 1.2683100950559238, Validation Loss Force: 3.2011675720366393, time: 0.08958911895751953
Test Loss Energy: 9.61927799621009, Test Loss Force: 10.610432472171329, time: 11.031517744064331


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.9987867605226344, Training Loss Force: 2.7149052398648377, time: 0.9946191310882568
Validation Loss Energy: 1.5299308103481462, Validation Loss Force: 3.2649512602222273, time: 0.07747340202331543
Test Loss Energy: 9.494361090320334, Test Loss Force: 10.61908161796725, time: 11.178624153137207


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.276563403171099, Training Loss Force: 2.690159466143536, time: 1.0304558277130127
Validation Loss Energy: 2.6464715638947167, Validation Loss Force: 3.2800708292344427, time: 0.07465887069702148
Test Loss Energy: 9.333800855742215, Test Loss Force: 10.712211204566767, time: 9.907705307006836


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.0176813680287324, Training Loss Force: 2.636682428745876, time: 0.938441276550293
Validation Loss Energy: 3.949463269105477, Validation Loss Force: 2.939485121723709, time: 0.07388424873352051
Test Loss Energy: 11.081177821869137, Test Loss Force: 10.762114801631588, time: 9.818922996520996


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.2098923380060795, Training Loss Force: 2.6654744125854903, time: 0.95829176902771
Validation Loss Energy: 2.9360905305940395, Validation Loss Force: 3.1925296791932354, time: 0.10435914993286133
Test Loss Energy: 10.54716952376286, Test Loss Force: 10.760944517885784, time: 9.901961088180542


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.8385421411868546, Training Loss Force: 5.907576641118097, time: 0.8997607231140137
Validation Loss Energy: 11.545703390453763, Validation Loss Force: 5.008223661642532, time: 0.07366609573364258
Test Loss Energy: 17.16828559311711, Test Loss Force: 11.811719140313304, time: 9.888875007629395


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 15.219200130839246, Training Loss Force: 5.626768622296715, time: 0.9871668815612793
Validation Loss Energy: 21.276044909489382, Validation Loss Force: 6.217519975622361, time: 0.0746614933013916
Test Loss Energy: 15.881487514367823, Test Loss Force: 12.449367405081283, time: 10.428695440292358


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.16197321253939, Training Loss Force: 5.993732187864939, time: 0.916327953338623
Validation Loss Energy: 1.5276011344088767, Validation Loss Force: 3.9214204282019045, time: 0.07209968566894531
Test Loss Energy: 11.760573473767971, Test Loss Force: 10.934273044298081, time: 9.804718732833862


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.939697592449223, Training Loss Force: 3.8269811695808507, time: 0.9314723014831543
Validation Loss Energy: 8.394274538888745, Validation Loss Force: 4.101088042671837, time: 0.0700075626373291
Test Loss Energy: 10.141783528000483, Test Loss Force: 10.896445077142232, time: 9.846208572387695


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.014833041049029, Training Loss Force: 3.5550070249952395, time: 0.969146728515625
Validation Loss Energy: 7.7345853523898045, Validation Loss Force: 3.8081256987464083, time: 0.07835245132446289
Test Loss Energy: 10.38551110360994, Test Loss Force: 10.835979347508806, time: 10.059858798980713


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.059295314648122, Training Loss Force: 3.5099474431250335, time: 1.0029571056365967
Validation Loss Energy: 7.767612753459543, Validation Loss Force: 3.875554035512542, time: 0.07346463203430176
Test Loss Energy: 13.431125317696015, Test Loss Force: 11.179083514757117, time: 9.818302392959595


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.139486517591406, Training Loss Force: 3.4429475954477673, time: 0.9559295177459717
Validation Loss Energy: 10.257606469752405, Validation Loss Force: 3.5771956388392727, time: 0.07488393783569336
Test Loss Energy: 14.746241245985068, Test Loss Force: 10.9848337321835, time: 9.911624670028687


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.547750503615713, Training Loss Force: 4.4633058778613, time: 0.9418160915374756
Validation Loss Energy: 11.408127258642281, Validation Loss Force: 4.6687814715307105, time: 0.07193565368652344
Test Loss Energy: 11.300907259421402, Test Loss Force: 11.04501170799519, time: 9.970561504364014


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.372597022115519, Training Loss Force: 6.220158087724052, time: 0.9628818035125732
Validation Loss Energy: 43.47447092636369, Validation Loss Force: 6.959455573876063, time: 0.0795130729675293
Test Loss Energy: 34.78627450216065, Test Loss Force: 12.051602103454327, time: 9.855823040008545


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 17.120919897021334, Training Loss Force: 6.0255715531221385, time: 0.9503152370452881
Validation Loss Energy: 38.914643249288616, Validation Loss Force: 8.716682984675382, time: 0.07813811302185059
Test Loss Energy: 36.32054352097744, Test Loss Force: 14.219985093753586, time: 9.850467681884766


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 15.386646854120379, Training Loss Force: 9.182236318923767, time: 0.9599173069000244
Validation Loss Energy: 4.878016319877237, Validation Loss Force: 7.349294694454867, time: 0.07768654823303223
Test Loss Energy: 11.210271110259917, Test Loss Force: 12.846231640452123, time: 10.046045541763306


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.834663333174094, Training Loss Force: 5.7789628533625494, time: 0.9125792980194092
Validation Loss Energy: 7.099326088567187, Validation Loss Force: 4.922543468988417, time: 0.0765380859375
Test Loss Energy: 12.820050659270294, Test Loss Force: 11.827582845712165, time: 9.829149723052979


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.222763346234506, Training Loss Force: 4.313356559442237, time: 0.9758396148681641
Validation Loss Energy: 2.158170485012677, Validation Loss Force: 3.8068661707224676, time: 0.07518339157104492
Test Loss Energy: 10.280215513739192, Test Loss Force: 10.962941582587195, time: 9.907514095306396


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.448939442354297, Training Loss Force: 5.427461693639167, time: 1.0680062770843506
Validation Loss Energy: 2.9331674413808724, Validation Loss Force: 5.829533462337675, time: 0.07396054267883301
Test Loss Energy: 10.307232471838095, Test Loss Force: 11.901183658056043, time: 9.947304248809814

wandb: - 0.039 MB of 0.055 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–â–â–‚â–‚â–‚â–ˆâ–ˆâ–â–‚â–â–
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–…â–‚â–‚â–‚â–‚â–‚â–‚â–„â–ˆâ–…â–ƒâ–‚â–„
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–â–â–‚â–‚â–‚â–†â–ˆâ–ƒâ–‚â–â–‚
wandb: train_error_energy â–‚â–â–â–â–â–â–„â–‡â–„â–„â–„â–„â–„â–…â–…â–ˆâ–‡â–…â–„â–†
wandb:  train_error_force â–â–â–â–â–â–â–„â–„â–…â–‚â–‚â–‚â–‚â–ƒâ–…â–…â–ˆâ–„â–ƒâ–„
wandb:         train_loss â–‚â–â–â–â–â–â–„â–…â–…â–ƒâ–ƒâ–‚â–‚â–„â–…â–†â–ˆâ–…â–ƒâ–…
wandb: valid_error_energy â–â–â–â–â–â–â–ƒâ–„â–â–‚â–‚â–‚â–‚â–ƒâ–ˆâ–‡â–‚â–‚â–â–
wandb:  valid_error_force â–â–â–â–â–â–â–„â–…â–‚â–‚â–‚â–‚â–‚â–ƒâ–†â–ˆâ–†â–ƒâ–‚â–…
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–…â–â–‚â–‚â–‚â–‚â–ƒâ–ˆâ–ˆâ–„â–ƒâ–â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2071
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.30723
wandb:   test_error_force 11.90118
wandb:          test_loss 4.67194
wandb: train_error_energy 12.44894
wandb:  train_error_force 5.42746
wandb:         train_loss 2.64914
wandb: valid_error_energy 2.93317
wandb:  valid_error_force 5.82953
wandb:         valid_loss 2.14687
wandb: 
wandb: ğŸš€ View run al_77_14 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/8fs10jgb
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_135908-8fs10jgb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.27993905544281, Uncertainty Bias: -0.846946120262146
8.010864e-05 0.10556793
-2.2083292 44.94349
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 2 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 4 steps.
Found uncertainty sample 7 after 6 steps.
Found uncertainty sample 8 after 20 steps.
Found uncertainty sample 9 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 6 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 5 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 3 steps.
Found uncertainty sample 31 after 2 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 2 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 2 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 6 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 4 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 2 steps.
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 7 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 3 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 7 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 6 steps.
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 3 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 3 steps.
Found uncertainty sample 91 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 3 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_140723-k8w7k9g2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_15
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/k8w7k9g2
Training model 15. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7733187701247655, Training Loss Force: 2.8732164753404827, time: 0.9780497550964355
Validation Loss Energy: 1.7821426965464764, Validation Loss Force: 2.9757762957998843, time: 0.0785219669342041
Test Loss Energy: 10.079364129591669, Test Loss Force: 10.544511982661705, time: 9.189642667770386


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6344725450217985, Training Loss Force: 2.645415342624647, time: 0.9598500728607178
Validation Loss Energy: 1.1972207991943047, Validation Loss Force: 2.9899987308231566, time: 0.07342648506164551
Test Loss Energy: 9.28507440203186, Test Loss Force: 10.655435021074872, time: 9.254366874694824


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.0231183374412005, Training Loss Force: 2.6694310613949876, time: 0.948448896408081
Validation Loss Energy: 2.7520051602689, Validation Loss Force: 3.042048979698653, time: 0.0701138973236084
Test Loss Energy: 9.350020248871038, Test Loss Force: 10.558518002510953, time: 9.454563617706299


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.4577738257554422, Training Loss Force: 2.6178122994806516, time: 0.9775428771972656
Validation Loss Energy: 1.3717186484359454, Validation Loss Force: 3.115939286414184, time: 0.06979227066040039
Test Loss Energy: 9.995487870621664, Test Loss Force: 10.636536416823509, time: 9.205094814300537


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5186520360563052, Training Loss Force: 2.6115997222492404, time: 0.9611155986785889
Validation Loss Energy: 2.698594402911785, Validation Loss Force: 3.100564315891589, time: 0.07554006576538086
Test Loss Energy: 9.34110554621955, Test Loss Force: 10.67011792714837, time: 9.19713282585144


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8265307028191846, Training Loss Force: 2.5743725223981473, time: 0.9607827663421631
Validation Loss Energy: 1.5408873570080959, Validation Loss Force: 3.0654980447871623, time: 0.0728302001953125
Test Loss Energy: 9.545437493937404, Test Loss Force: 10.683713191205996, time: 9.360144853591919


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 19.30779391307429, Training Loss Force: 7.779380932430946, time: 1.0001683235168457
Validation Loss Energy: 4.739321498276612, Validation Loss Force: 7.086100431975661, time: 0.07021808624267578
Test Loss Energy: 12.739941985565531, Test Loss Force: 12.774702373965745, time: 9.241602659225464


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.008325637226415, Training Loss Force: 6.34543066197291, time: 0.9631855487823486
Validation Loss Energy: 9.130538531326518, Validation Loss Force: 5.29287877640626, time: 0.07607078552246094
Test Loss Energy: 14.915007582375523, Test Loss Force: 11.382597294426336, time: 9.3188955783844


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.813060942272712, Training Loss Force: 5.6965655994830176, time: 0.9341802597045898
Validation Loss Energy: 16.381908011454414, Validation Loss Force: 4.086136922868516, time: 0.07720828056335449
Test Loss Energy: 19.111875397671376, Test Loss Force: 10.984107135946775, time: 9.523390769958496


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 16.508505421378647, Training Loss Force: 5.706714030694202, time: 0.985518217086792
Validation Loss Energy: 20.103345382988515, Validation Loss Force: 8.324850307233087, time: 0.07007908821105957
Test Loss Energy: 15.615054559472082, Test Loss Force: 13.277784609058857, time: 9.210143327713013


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 10.638428966741499, Training Loss Force: 6.184562531958997, time: 0.9504599571228027
Validation Loss Energy: 10.209548678663394, Validation Loss Force: 4.3018892971163485, time: 0.07034063339233398
Test Loss Energy: 10.740124113360462, Test Loss Force: 10.795485645959875, time: 9.251889705657959


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.899782767218676, Training Loss Force: 5.031258427863219, time: 0.9818730354309082
Validation Loss Energy: 15.652091181467828, Validation Loss Force: 5.742114891661302, time: 0.07116484642028809
Test Loss Energy: 22.035833141751464, Test Loss Force: 12.28129238294309, time: 9.417391538619995


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.227514230273295, Training Loss Force: 4.972970538379209, time: 0.9420654773712158
Validation Loss Energy: 7.144011778110267, Validation Loss Force: 5.551400549527319, time: 0.06963849067687988
Test Loss Energy: 9.873467463635048, Test Loss Force: 11.652815135163893, time: 9.238242626190186


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 13.908136685511716, Training Loss Force: 5.249453335287297, time: 0.9474084377288818
Validation Loss Energy: 12.719789255057176, Validation Loss Force: 7.716801881050282, time: 0.06983113288879395
Test Loss Energy: 17.5161869630982, Test Loss Force: 13.707559311555228, time: 9.193201065063477


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.761582427537572, Training Loss Force: 5.907445046121358, time: 0.9462592601776123
Validation Loss Energy: 7.713906736814929, Validation Loss Force: 5.1875560477132705, time: 0.08049726486206055
Test Loss Energy: 13.458364879655655, Test Loss Force: 11.708948446017935, time: 9.394121646881104


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.736853397219233, Training Loss Force: 4.870964384751158, time: 0.968069314956665
Validation Loss Energy: 1.5764953264772785, Validation Loss Force: 4.440172626135299, time: 0.07089495658874512
Test Loss Energy: 9.857968655841816, Test Loss Force: 11.321384464341849, time: 9.658685207366943


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.167389404598968, Training Loss Force: 7.226670114686812, time: 1.0031859874725342
Validation Loss Energy: 16.137171462488595, Validation Loss Force: 5.3650266970549065, time: 0.0715627670288086
Test Loss Energy: 19.66119171225284, Test Loss Force: 11.722561337078107, time: 9.235231876373291


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.502167430123858, Training Loss Force: 4.965552279762616, time: 0.9666810035705566
Validation Loss Energy: 14.676118064040995, Validation Loss Force: 4.957378943523089, time: 0.07392001152038574
Test Loss Energy: 18.386020595575747, Test Loss Force: 11.369023574482936, time: 9.398311376571655


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.948236089988214, Training Loss Force: 5.32109301033582, time: 1.0173277854919434
Validation Loss Energy: 27.306922048311048, Validation Loss Force: 5.571057559060678, time: 0.07457375526428223
Test Loss Energy: 29.19919061111552, Test Loss Force: 11.630776728078734, time: 9.254088401794434


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 13.154091418954957, Training Loss Force: 6.103952299637604, time: 0.9730544090270996
Validation Loss Energy: 18.207197364074602, Validation Loss Force: 8.172793110185335, time: 0.07555222511291504
Test Loss Energy: 16.354350785727448, Test Loss Force: 12.596601378582971, time: 9.263909816741943

wandb: - 0.039 MB of 0.055 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–ƒâ–„â–ƒâ–‚â–…â–â–„â–‚â–â–…â–„â–ˆâ–ƒ
wandb:   test_error_force â–â–â–â–â–â–â–†â–ƒâ–‚â–‡â–‚â–…â–ƒâ–ˆâ–„â–ƒâ–„â–ƒâ–ƒâ–†
wandb:          test_loss â–â–â–â–â–â–â–…â–„â–„â–‡â–‚â–‡â–ƒâ–ˆâ–„â–‚â–…â–…â–ˆâ–†
wandb: train_error_energy â–‚â–â–â–â–â–â–ˆâ–…â–„â–‡â–…â–…â–…â–†â–„â–„â–…â–…â–„â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–…â–…â–†â–„â–„â–…â–…â–„â–‡â–„â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–…â–†â–…â–„â–„â–…â–…â–„â–†â–„â–…â–†
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–ƒâ–…â–†â–ƒâ–…â–ƒâ–„â–ƒâ–â–…â–…â–ˆâ–†
wandb:  valid_error_force â–â–â–â–â–â–â–†â–„â–‚â–ˆâ–ƒâ–…â–„â–‡â–„â–ƒâ–„â–„â–„â–ˆ
wandb:         valid_loss â–â–â–â–â–â–â–…â–„â–„â–ˆâ–ƒâ–…â–„â–†â–„â–‚â–…â–…â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2161
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 16.35435
wandb:   test_error_force 12.5966
wandb:          test_loss 5.30931
wandb: train_error_energy 13.15409
wandb:  train_error_force 6.10395
wandb:         train_loss 2.92268
wandb: valid_error_energy 18.2072
wandb:  valid_error_force 8.17279
wandb:         valid_loss 3.95308
wandb: 
wandb: ğŸš€ View run al_77_15 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/k8w7k9g2
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_140723-k8w7k9g2/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.2827035188674927, Uncertainty Bias: -0.7641183733940125
5.6028366e-05 0.0027918816
4.321252 37.017323
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 2 steps.
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 3 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 2 steps.
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 3 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 2 steps.
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 5 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_141522-rtp1tpv9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_16
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/rtp1tpv9
Training model 16. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.2296405349212307, Training Loss Force: 2.775037837190709, time: 1.0040068626403809
Validation Loss Energy: 1.4603870610284118, Validation Loss Force: 3.0342401954201117, time: 0.0720064640045166
Test Loss Energy: 9.469756942444105, Test Loss Force: 10.639780797212625, time: 9.048986434936523


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5931338166034685, Training Loss Force: 2.6409846103882826, time: 1.027494192123413
Validation Loss Energy: 2.2948239872180087, Validation Loss Force: 3.1053771981462455, time: 0.0738980770111084
Test Loss Energy: 10.35428271418696, Test Loss Force: 10.743575405847464, time: 9.277365922927856


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.79131310186832, Training Loss Force: 2.6329870403082163, time: 1.0235795974731445
Validation Loss Energy: 1.534091049336088, Validation Loss Force: 2.9864964791614317, time: 0.0800938606262207
Test Loss Energy: 9.3580042364616, Test Loss Force: 10.638284628428204, time: 9.298320531845093


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3251429306374893, Training Loss Force: 2.6194592279580347, time: 1.0225698947906494
Validation Loss Energy: 2.402679243442208, Validation Loss Force: 3.0951778379170234, time: 0.07579183578491211
Test Loss Energy: 10.379358286453522, Test Loss Force: 10.73362114838917, time: 9.201693296432495


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4357042324610476, Training Loss Force: 2.604144646248671, time: 1.0271453857421875
Validation Loss Energy: 1.4453850613252461, Validation Loss Force: 3.0684475202941215, time: 0.07224225997924805
Test Loss Energy: 9.538871332897724, Test Loss Force: 10.630152312875472, time: 9.209079504013062


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.248949567922275, Training Loss Force: 2.58817644591901, time: 0.9719688892364502
Validation Loss Energy: 1.063482648449319, Validation Loss Force: 2.9487952527186683, time: 0.07055139541625977
Test Loss Energy: 9.668697750707915, Test Loss Force: 10.79677616638125, time: 9.388453960418701


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.307118241753452, Training Loss Force: 5.676290856297819, time: 0.9757907390594482
Validation Loss Energy: 19.625486707708475, Validation Loss Force: 5.145897322539932, time: 0.07490324974060059
Test Loss Energy: 16.340652921323255, Test Loss Force: 11.575870312560687, time: 9.185319423675537


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.646799027667978, Training Loss Force: 6.076151164154881, time: 1.017545461654663
Validation Loss Energy: 5.964472523453937, Validation Loss Force: 6.123021639451213, time: 0.07736849784851074
Test Loss Energy: 14.499204200647462, Test Loss Force: 11.931362807147273, time: 9.650546789169312


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.609682165036853, Training Loss Force: 6.279472170880378, time: 1.0396373271942139
Validation Loss Energy: 3.368400896059672, Validation Loss Force: 8.811937727935247, time: 0.07773590087890625
Test Loss Energy: 12.274892666507085, Test Loss Force: 14.468117372842553, time: 9.405439853668213


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 18.21137910631605, Training Loss Force: 7.196700171668511, time: 1.020838975906372
Validation Loss Energy: 2.37689716237724, Validation Loss Force: 7.480693984158314, time: 0.07111263275146484
Test Loss Energy: 9.718323274812716, Test Loss Force: 13.02804581988011, time: 9.156275987625122


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.182093441061217, Training Loss Force: 5.671193153547947, time: 1.0128629207611084
Validation Loss Energy: 14.225688219494634, Validation Loss Force: 4.061290163551018, time: 0.07295608520507812
Test Loss Energy: 13.049797163301307, Test Loss Force: 10.563743602027584, time: 9.12876582145691


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.554262193871391, Training Loss Force: 5.361565483944012, time: 0.988955020904541
Validation Loss Energy: 11.140728334680517, Validation Loss Force: 6.652130513365428, time: 0.07649636268615723
Test Loss Energy: 15.181665752539786, Test Loss Force: 12.294070630769447, time: 9.331904649734497


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.888609511640444, Training Loss Force: 5.339405996237496, time: 1.0062057971954346
Validation Loss Energy: 1.778004112132976, Validation Loss Force: 4.876781619463067, time: 0.07075190544128418
Test Loss Energy: 9.941672837028495, Test Loss Force: 11.665525626284694, time: 9.155852556228638


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.45538665382764, Training Loss Force: 6.29076146208088, time: 0.9999508857727051
Validation Loss Energy: 11.093745590228046, Validation Loss Force: 6.86222731397584, time: 0.07298898696899414
Test Loss Energy: 17.04173069102085, Test Loss Force: 13.147316680273436, time: 9.276111841201782


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.697452815294781, Training Loss Force: 5.261408779794474, time: 1.0741941928863525
Validation Loss Energy: 11.63341276703472, Validation Loss Force: 8.359930049820843, time: 0.07186651229858398
Test Loss Energy: 17.164487664226055, Test Loss Force: 13.963065073063747, time: 9.403738260269165


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.0588967354294425, Training Loss Force: 4.468495297863507, time: 0.9981021881103516
Validation Loss Energy: 9.71276936071558, Validation Loss Force: 4.581437272349076, time: 0.0709371566772461
Test Loss Energy: 10.995262995391371, Test Loss Force: 11.204302928960335, time: 9.184231519699097


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.889522581505217, Training Loss Force: 4.884782814973172, time: 0.9965102672576904
Validation Loss Energy: 2.362824583689177, Validation Loss Force: 4.888786826465765, time: 0.07162237167358398
Test Loss Energy: 9.67552034732883, Test Loss Force: 11.238758267625471, time: 9.185550928115845


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.601753595085414, Training Loss Force: 5.198750193328574, time: 0.9916448593139648
Validation Loss Energy: 30.462886670378616, Validation Loss Force: 6.656915227973124, time: 0.07166147232055664
Test Loss Energy: 32.165775754107315, Test Loss Force: 12.845156775681481, time: 9.764601945877075


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.435261253932982, Training Loss Force: 6.444461590491916, time: 0.9755523204803467
Validation Loss Energy: 4.663720869851209, Validation Loss Force: 8.038333010401965, time: 0.07074999809265137
Test Loss Energy: 9.458604813582056, Test Loss Force: 12.865856696380135, time: 9.098240852355957


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.985808797704621, Training Loss Force: 5.048448711134611, time: 1.0862619876861572
Validation Loss Energy: 15.341702861516664, Validation Loss Force: 6.7893341196957735, time: 0.07249784469604492
Test Loss Energy: 18.735368769407668, Test Loss Force: 12.688002474369792, time: 9.106268405914307

wandb: - 0.039 MB of 0.055 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–â–‚â–ƒâ–â–ƒâ–ƒâ–‚â–â–ˆâ–â–„
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–ˆâ–…â–â–„â–ƒâ–†â–‡â–‚â–‚â–…â–…â–…
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–ƒâ–†â–„â–‚â–„â–‚â–…â–†â–‚â–‚â–ˆâ–ƒâ–…
wandb: train_error_energy â–â–â–â–â–â–â–…â–…â–…â–ˆâ–ƒâ–…â–…â–…â–…â–ƒâ–„â–„â–…â–„
wandb:  train_error_force â–â–â–â–â–â–â–†â–†â–‡â–ˆâ–†â–…â–…â–‡â–…â–„â–„â–…â–‡â–…
wandb:         train_loss â–â–â–â–â–â–â–…â–†â–†â–ˆâ–…â–…â–…â–†â–…â–„â–„â–…â–†â–„
wandb: valid_error_energy â–â–â–â–â–â–â–…â–‚â–‚â–â–„â–ƒâ–â–ƒâ–„â–ƒâ–â–ˆâ–‚â–„
wandb:  valid_error_force â–â–â–â–â–â–â–„â–…â–ˆâ–†â–‚â–…â–ƒâ–†â–‡â–ƒâ–ƒâ–…â–‡â–†
wandb:         valid_loss â–â–â–â–â–â–â–…â–„â–†â–…â–„â–…â–ƒâ–…â–†â–ƒâ–ƒâ–ˆâ–…â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 2251
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 18.73537
wandb:   test_error_force 12.688
wandb:          test_loss 5.49923
wandb: train_error_energy 7.98581
wandb:  train_error_force 5.04845
wandb:         train_loss 2.22364
wandb: valid_error_energy 15.3417
wandb:  valid_error_force 6.78933
wandb:         valid_loss 3.29841
wandb: 
wandb: ğŸš€ View run al_77_16 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/rtp1tpv9
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_141522-rtp1tpv9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8361126780509949, Uncertainty Bias: -0.5971952080726624
0.00017738342 0.00086021423
-1.838788 51.48348
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 4 steps.
Found uncertainty sample 2 after 5 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 6 steps.
Found uncertainty sample 5 after 10 steps.
Found uncertainty sample 6 after 5 steps.
Found uncertainty sample 7 after 2 steps.
Found uncertainty sample 8 after 10 steps.
Found uncertainty sample 9 after 16 steps.
Found uncertainty sample 10 after 17 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 17 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 6 steps.
Found uncertainty sample 16 after 8 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 11 steps.
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 6 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 4 steps.
Found uncertainty sample 24 after 3 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 17 steps.
Found uncertainty sample 27 after 17 steps.
Found uncertainty sample 28 after 4 steps.
Found uncertainty sample 29 after 8 steps.
Found uncertainty sample 30 after 2 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 11 steps.
Found uncertainty sample 33 after 9 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 14 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 4 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 7 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 14 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 13 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 15 steps.
Found uncertainty sample 52 after 5 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 7 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 8 steps.
Found uncertainty sample 58 after 19 steps.
Found uncertainty sample 59 after 17 steps.
Found uncertainty sample 60 after 19 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 5 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 27 steps.
Found uncertainty sample 65 after 5 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 8 steps.
Found uncertainty sample 68 after 20 steps.
Found uncertainty sample 69 after 11 steps.
Found uncertainty sample 70 after 3 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 3 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 46 steps.
Found uncertainty sample 75 after 4 steps.
Found uncertainty sample 76 after 31 steps.
Found uncertainty sample 77 after 11 steps.
Found uncertainty sample 78 after 11 steps.
Found uncertainty sample 79 after 4 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 13 steps.
Found uncertainty sample 82 after 5 steps.
Found uncertainty sample 83 after 7 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 6 steps.
Found uncertainty sample 89 after 10 steps.
Found uncertainty sample 90 after 2 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 6 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 11 steps.
Found uncertainty sample 96 after 10 steps.
Found uncertainty sample 97 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_142330-cyofgaih
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_17
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/cyofgaih
Training model 17. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.0936887447739765, Training Loss Force: 3.013901520405004, time: 1.0268795490264893
Validation Loss Energy: 1.301946217903343, Validation Loss Force: 3.077179936189972, time: 0.07969951629638672
Test Loss Energy: 9.633962422495978, Test Loss Force: 10.662854464271435, time: 9.29965353012085


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.489502164373837, Training Loss Force: 2.693460200052428, time: 1.0525424480438232
Validation Loss Energy: 3.8787575765150053, Validation Loss Force: 3.2206484502755695, time: 0.07825589179992676
Test Loss Energy: 9.478461058373895, Test Loss Force: 10.682526646783508, time: 9.222550392150879


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7647167287956074, Training Loss Force: 2.61613099405623, time: 1.0732109546661377
Validation Loss Energy: 1.9585364622311188, Validation Loss Force: 3.083802784597302, time: 0.07417869567871094
Test Loss Energy: 9.603184263091224, Test Loss Force: 10.681189554702227, time: 9.33281660079956


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7421025007366433, Training Loss Force: 2.5808103655791754, time: 1.1077136993408203
Validation Loss Energy: 2.288212383602548, Validation Loss Force: 3.13808963370907, time: 0.07309675216674805
Test Loss Energy: 10.301614186539066, Test Loss Force: 10.736569852131863, time: 9.345908641815186


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7497923496008345, Training Loss Force: 2.567559785088518, time: 1.0206220149993896
Validation Loss Energy: 1.322942471014885, Validation Loss Force: 3.0825930231274765, time: 0.07428932189941406
Test Loss Energy: 9.98534855295277, Test Loss Force: 10.80506185550905, time: 9.233855485916138


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.924150118056026, Training Loss Force: 2.666175851504033, time: 1.0407204627990723
Validation Loss Energy: 5.195713086536044, Validation Loss Force: 3.2460886228669654, time: 0.08041906356811523
Test Loss Energy: 9.599892872698751, Test Loss Force: 10.772470068734592, time: 9.473668336868286


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 6.851245754650004, Training Loss Force: 4.47750624387239, time: 1.0454914569854736
Validation Loss Energy: 4.045268450494076, Validation Loss Force: 4.087155411541989, time: 0.07585477828979492
Test Loss Energy: 9.173313857162771, Test Loss Force: 11.118758782491154, time: 9.20547103881836


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 6.893580510872987, Training Loss Force: 3.5492760693803107, time: 1.0064470767974854
Validation Loss Energy: 11.798245383660932, Validation Loss Force: 4.0580859545411165, time: 0.08014798164367676
Test Loss Energy: 16.34595201331611, Test Loss Force: 11.403376424718974, time: 9.267975091934204


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 20.293423875322045, Training Loss Force: 7.91806918201722, time: 1.0267305374145508
Validation Loss Energy: 3.4024180614830604, Validation Loss Force: 9.112254722416097, time: 0.07732605934143066
Test Loss Energy: 9.967936702428116, Test Loss Force: 13.992066649783514, time: 9.440535068511963


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.470890252682437, Training Loss Force: 6.3964110495001245, time: 1.0432934761047363
Validation Loss Energy: 10.444619160004484, Validation Loss Force: 7.21429640334579, time: 0.08259177207946777
Test Loss Energy: 11.219458723677814, Test Loss Force: 12.49105962622269, time: 9.250053644180298


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 14.816872030604756, Training Loss Force: 5.639597297714989, time: 1.0445959568023682
Validation Loss Energy: 7.807075070177451, Validation Loss Force: 8.753070240488025, time: 0.08519601821899414
Test Loss Energy: 10.0133202976965, Test Loss Force: 13.182688366593453, time: 9.258301734924316


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.101689230842975, Training Loss Force: 6.405592119652678, time: 1.0446383953094482
Validation Loss Energy: 6.744190845057812, Validation Loss Force: 8.4584932112509, time: 0.07727861404418945
Test Loss Energy: 10.883418064745014, Test Loss Force: 13.176518579949663, time: 9.375733852386475


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 12.91154006639817, Training Loss Force: 5.282057110076935, time: 1.0346078872680664
Validation Loss Energy: 42.31081443921725, Validation Loss Force: 6.01445217194145, time: 0.07332968711853027
Test Loss Energy: 33.28740880500391, Test Loss Force: 11.701963597035485, time: 9.321946859359741


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 13.660544128182966, Training Loss Force: 6.817916153936211, time: 1.0470752716064453
Validation Loss Energy: 2.587060986400578, Validation Loss Force: 5.12420422633621, time: 0.07528424263000488
Test Loss Energy: 10.885544077107735, Test Loss Force: 10.963019411865634, time: 9.790719747543335


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 14.09045403901135, Training Loss Force: 5.449957105522875, time: 1.0602736473083496
Validation Loss Energy: 17.098436837488606, Validation Loss Force: 8.288783249652287, time: 0.07474589347839355
Test Loss Energy: 20.042272333168935, Test Loss Force: 14.765489736910535, time: 9.405753135681152


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.972453749784583, Training Loss Force: 5.642394256893156, time: 1.0590038299560547
Validation Loss Energy: 8.446916642354115, Validation Loss Force: 4.556374523960029, time: 0.07481980323791504
Test Loss Energy: 10.338441061194192, Test Loss Force: 10.919930633157112, time: 9.236544132232666


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.6272561460828765, Training Loss Force: 3.613444198855386, time: 1.0276732444763184
Validation Loss Energy: 2.01165708169109, Validation Loss Force: 4.332047160902516, time: 0.07380437850952148
Test Loss Energy: 9.752123224735543, Test Loss Force: 11.1322369855316, time: 9.320680141448975


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.54039361035431, Training Loss Force: 3.4642238488533494, time: 1.0374302864074707
Validation Loss Energy: 9.66053563246384, Validation Loss Force: 3.7655466819929506, time: 0.07372403144836426
Test Loss Energy: 10.351057392411445, Test Loss Force: 10.665170190746485, time: 9.395137071609497


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.710491539840236, Training Loss Force: 3.418420733318983, time: 1.1067593097686768
Validation Loss Energy: 10.263820720276957, Validation Loss Force: 4.179963020888107, time: 0.07651782035827637
Test Loss Energy: 16.674265689153422, Test Loss Force: 11.242323874488248, time: 9.199363231658936


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.82715495557841, Training Loss Force: 3.417577644742608, time: 1.047156810760498
Validation Loss Energy: 9.229805327257711, Validation Loss Force: 3.406094329082328, time: 0.07440781593322754
Test Loss Energy: 10.483560251386635, Test Loss Force: 10.466498133649546, time: 9.249518394470215

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.040 MB of 0.058 MB uploadedwandb: / 0.040 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–ƒâ–â–‚â–â–â–ˆâ–â–„â–â–â–â–ƒâ–
wandb:   test_error_force â–â–â–â–â–‚â–â–‚â–ƒâ–‡â–„â–…â–…â–ƒâ–‚â–ˆâ–‚â–‚â–â–‚â–
wandb:          test_loss â–â–â–â–â–â–â–â–ƒâ–…â–ƒâ–„â–„â–ˆâ–‚â–ˆâ–â–‚â–â–ƒâ–
wandb: train_error_energy â–‚â–â–â–â–â–â–ƒâ–ƒâ–ˆâ–„â–†â–„â–…â–…â–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb:  train_error_force â–‚â–â–â–â–â–â–ƒâ–‚â–ˆâ–†â–…â–†â–…â–‡â–…â–…â–‚â–‚â–‚â–‚
wandb:         train_loss â–‚â–â–â–â–â–â–ƒâ–ƒâ–ˆâ–…â–…â–…â–…â–†â–…â–„â–ƒâ–‚â–‚â–‚
wandb: valid_error_energy â–â–â–â–â–â–‚â–â–ƒâ–â–ƒâ–‚â–‚â–ˆâ–â–„â–‚â–â–‚â–ƒâ–‚
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–‚â–ˆâ–†â–ˆâ–‡â–„â–ƒâ–‡â–ƒâ–‚â–‚â–‚â–
wandb:         valid_loss â–â–â–â–â–â–‚â–‚â–ƒâ–…â–…â–…â–…â–ˆâ–‚â–†â–ƒâ–‚â–‚â–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 2341
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.48356
wandb:   test_error_force 10.4665
wandb:          test_loss 4.20369
wandb: train_error_energy 6.82715
wandb:  train_error_force 3.41758
wandb:         train_loss 1.60041
wandb: valid_error_energy 9.22981
wandb:  valid_error_force 3.40609
wandb:         valid_loss 1.75735
wandb: 
wandb: ğŸš€ View run al_77_17 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/cyofgaih
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_142330-cyofgaih/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.9530025124549866, Uncertainty Bias: -0.561926007270813
0.0002708435 1.9992876
3.336129 57.407154
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 5 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 6 steps.
Found uncertainty sample 13 after 8 steps.
Found uncertainty sample 14 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 2 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 9 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 6 steps.
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 18 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 8 steps.
Found uncertainty sample 30 after 7 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 3 steps.
Found uncertainty sample 33 after 13 steps.
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 4 steps.
Found uncertainty sample 37 after 2 steps.
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 6 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 5 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 6 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 2 steps.
Found uncertainty sample 52 after 8 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 5 steps.
Found uncertainty sample 62 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 3 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 3 steps.
Found uncertainty sample 69 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 9 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 7 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 4 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 7 steps.
Found uncertainty sample 81 after 9 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 4 steps.
Found uncertainty sample 84 after 7 steps.
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 2 steps.
Found uncertainty sample 89 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 10 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 3 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_143138-3b1k7rlc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_18
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3b1k7rlc
Training model 18. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.2424994934556595, Training Loss Force: 2.7592580539544893, time: 1.1952667236328125
Validation Loss Energy: 2.8165989188073732, Validation Loss Force: 3.115905663373149, time: 0.08733224868774414
Test Loss Energy: 9.472244760692169, Test Loss Force: 10.551557085215558, time: 10.501869201660156


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6354387133397608, Training Loss Force: 2.6144127443111924, time: 1.1900436878204346
Validation Loss Energy: 2.052073981037588, Validation Loss Force: 3.0392049380322903, time: 0.09701132774353027
Test Loss Energy: 10.308896913211644, Test Loss Force: 10.69159029760651, time: 10.638324737548828


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3103999926616363, Training Loss Force: 2.6099596953963546, time: 1.1328487396240234
Validation Loss Energy: 1.0301873207297423, Validation Loss Force: 2.922533987651664, time: 0.08998274803161621
Test Loss Energy: 9.77187464015774, Test Loss Force: 10.557964178878555, time: 10.795233964920044


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.99715908481016, Training Loss Force: 2.611338017409648, time: 1.147428274154663
Validation Loss Energy: 2.2534930310274763, Validation Loss Force: 3.056580479496473, time: 0.09159088134765625
Test Loss Energy: 10.287193661662736, Test Loss Force: 10.65456173366907, time: 9.548909425735474


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.779486596428848, Training Loss Force: 2.5683101348162856, time: 1.1464149951934814
Validation Loss Energy: 1.766278521910278, Validation Loss Force: 3.07495947946678, time: 0.08319640159606934
Test Loss Energy: 9.437590880423746, Test Loss Force: 10.643630665139224, time: 11.315159320831299


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6721938932928353, Training Loss Force: 2.562931535227685, time: 1.1424036026000977
Validation Loss Energy: 2.3850022079544386, Validation Loss Force: 2.9062205780135564, time: 0.09473204612731934
Test Loss Energy: 9.435482476410659, Test Loss Force: 10.656704011371707, time: 8.560503482818604


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.476091737139567, Training Loss Force: 5.702359788455358, time: 1.0968773365020752
Validation Loss Energy: 2.972540245102116, Validation Loss Force: 6.84570415000227, time: 0.07327485084533691
Test Loss Energy: 9.345926088409408, Test Loss Force: 12.331511480037355, time: 8.399227857589722


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.967092561743333, Training Loss Force: 4.470847237622451, time: 1.0649001598358154
Validation Loss Energy: 7.207478051428335, Validation Loss Force: 3.861441376378212, time: 0.07026505470275879
Test Loss Energy: 10.340833502436148, Test Loss Force: 11.007070270507867, time: 8.452452659606934


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.010652859289512, Training Loss Force: 3.4810249343282047, time: 1.1931860446929932
Validation Loss Energy: 4.127947953737668, Validation Loss Force: 3.51746263642183, time: 0.07069063186645508
Test Loss Energy: 9.313138418071725, Test Loss Force: 10.855741255194424, time: 8.484998941421509


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 7.105891916094016, Training Loss Force: 3.4148744811412652, time: 1.1089258193969727
Validation Loss Energy: 8.395683143905883, Validation Loss Force: 3.7457816779238167, time: 0.07152009010314941
Test Loss Energy: 10.12434803490892, Test Loss Force: 10.804077074980528, time: 8.391786098480225


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.198839010080915, Training Loss Force: 3.40252060062476, time: 1.1167047023773193
Validation Loss Energy: 5.8892364044204175, Validation Loss Force: 3.9384144260845404, time: 0.07057881355285645
Test Loss Energy: 10.6869158123867, Test Loss Force: 11.2483012330209, time: 8.930973529815674


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.451418587496031, Training Loss Force: 3.3696477524512956, time: 1.1891334056854248
Validation Loss Energy: 8.352346384048833, Validation Loss Force: 3.340596199807669, time: 0.10468077659606934
Test Loss Energy: 10.099829405974413, Test Loss Force: 10.856759666924635, time: 8.527804136276245


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.942039308034849, Training Loss Force: 5.116084165240946, time: 1.131223201751709
Validation Loss Energy: 10.702084813366408, Validation Loss Force: 5.063699897997273, time: 0.07096576690673828
Test Loss Energy: 14.990359259700936, Test Loss Force: 11.06791925221307, time: 8.373396635055542


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.226613095822882, Training Loss Force: 5.319461136241407, time: 1.0682590007781982
Validation Loss Energy: 23.543388629401004, Validation Loss Force: 5.583604074757125, time: 0.07778477668762207
Test Loss Energy: 23.761846875612733, Test Loss Force: 12.496252904904757, time: 8.382737159729004


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 17.505525761780717, Training Loss Force: 7.544409465268676, time: 1.1198511123657227
Validation Loss Energy: 27.488070523935974, Validation Loss Force: 7.909128347800783, time: 0.07125020027160645
Test Loss Energy: 19.114826204529983, Test Loss Force: 12.325025932223342, time: 8.500663995742798


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 13.578645261986589, Training Loss Force: 7.4804000081482585, time: 1.0774381160736084
Validation Loss Energy: 13.314710444410954, Validation Loss Force: 5.866484071089516, time: 0.07247042655944824
Test Loss Energy: 18.255696499400525, Test Loss Force: 11.940020915667768, time: 10.299991130828857


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.538536395467093, Training Loss Force: 5.688987310860664, time: 1.144052267074585
Validation Loss Energy: 23.168420022394695, Validation Loss Force: 9.101730042564675, time: 0.08355069160461426
Test Loss Energy: 19.342420773312785, Test Loss Force: 13.9288582051861, time: 10.61933946609497


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 14.008798738375404, Training Loss Force: 6.095729823488711, time: 1.3120307922363281
Validation Loss Energy: 5.385983426393047, Validation Loss Force: 7.375537143979041, time: 0.07245135307312012
Test Loss Energy: 9.8723726122684, Test Loss Force: 13.614166089350162, time: 8.363296270370483


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 12.813092779354776, Training Loss Force: 6.518893060563776, time: 1.083322525024414
Validation Loss Energy: 21.41647966779629, Validation Loss Force: 7.820570156435549, time: 0.07452034950256348
Test Loss Energy: 16.979051466863915, Test Loss Force: 12.01796055406656, time: 8.462930917739868


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.998628793433943, Training Loss Force: 5.0533704462494, time: 1.0658717155456543
Validation Loss Energy: 13.403078030987272, Validation Loss Force: 4.806723003410662, time: 0.08141517639160156
Test Loss Energy: 17.199017087597394, Test Loss Force: 11.690863756951092, time: 8.470166444778442

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–â–â–â–‚â–â–„â–ˆâ–†â–…â–†â–â–…â–…
wandb:   test_error_force â–â–â–â–â–â–â–…â–‚â–‚â–‚â–‚â–‚â–‚â–…â–…â–„â–ˆâ–‡â–„â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–‚â–â–‚â–‚â–‚â–ƒâ–‡â–†â–…â–ˆâ–…â–…â–…
wandb: train_error_energy â–â–â–â–â–â–â–…â–„â–ƒâ–„â–„â–„â–…â–…â–ˆâ–†â–…â–†â–†â–…
wandb:  train_error_force â–â–â–â–â–â–â–…â–„â–‚â–‚â–‚â–‚â–…â–…â–ˆâ–ˆâ–…â–†â–‡â–„
wandb:         train_loss â–â–â–â–â–â–â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–ˆâ–‡â–…â–†â–†â–…
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–ƒâ–‚â–ƒâ–‚â–ƒâ–„â–‡â–ˆâ–„â–‡â–‚â–†â–„
wandb:  valid_error_force â–â–â–â–â–â–â–…â–‚â–‚â–‚â–‚â–â–ƒâ–„â–‡â–„â–ˆâ–†â–‡â–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–„â–‚â–‚â–ƒâ–‚â–‚â–„â–†â–ˆâ–…â–ˆâ–…â–‡â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 2431
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 17.19902
wandb:   test_error_force 11.69086
wandb:          test_loss 5.06277
wandb: train_error_energy 10.99863
wandb:  train_error_force 5.05337
wandb:         train_loss 2.42691
wandb: valid_error_energy 13.40308
wandb:  valid_error_force 4.80672
wandb:         valid_loss 2.50529
wandb: 
wandb: ğŸš€ View run al_77_18 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3b1k7rlc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_143138-3b1k7rlc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8710178136825562, Uncertainty Bias: -0.548495888710022
0.00027370453 1.1508408
-5.993526 19.399532
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 2 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 5 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 9 steps.
Found uncertainty sample 23 after 6 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 3 steps.
Found uncertainty sample 26 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 6 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 7 steps.
Found uncertainty sample 31 after 13 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 10 steps.
Found uncertainty sample 35 after 6 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 2 steps.
Found uncertainty sample 38 after 6 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 4 steps.
Found uncertainty sample 45 after 9 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 9 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 7 steps.
Found uncertainty sample 52 after 16 steps.
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 5 steps.
Found uncertainty sample 60 after 4 steps.
Found uncertainty sample 61 after 10 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 5 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 5 steps.
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 4 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 3 steps.
Found uncertainty sample 73 after 16 steps.
Found uncertainty sample 74 after 3 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 3 steps.
Found uncertainty sample 78 after 3 steps.
Found uncertainty sample 79 after 3 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 7 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 3 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 5 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_143938-3887xwfl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_19
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3887xwfl
Training model 19. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.9584175075817496, Training Loss Force: 2.6572302323767514, time: 1.1566636562347412
Validation Loss Energy: 1.5080587994856702, Validation Loss Force: 2.8546244628611803, time: 0.08541321754455566
Test Loss Energy: 9.436806334250342, Test Loss Force: 10.651040795869095, time: 9.346789598464966


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.3646214541479578, Training Loss Force: 2.5612102119040907, time: 1.1296641826629639
Validation Loss Energy: 2.2441125365824037, Validation Loss Force: 3.029822439564904, time: 0.08130502700805664
Test Loss Energy: 9.461440359429565, Test Loss Force: 10.695032827887749, time: 9.331411838531494


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.8096254061669759, Training Loss Force: 2.520777221188553, time: 1.1906170845031738
Validation Loss Energy: 1.624592474337832, Validation Loss Force: 2.9063870232001374, time: 0.0769493579864502
Test Loss Energy: 9.48206767366535, Test Loss Force: 10.70579556786973, time: 9.505226373672485


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.9621795824275587, Training Loss Force: 2.553284185273727, time: 1.1398496627807617
Validation Loss Energy: 1.0726408634194695, Validation Loss Force: 2.9573268210448918, time: 0.07779502868652344
Test Loss Energy: 9.777349563832619, Test Loss Force: 10.762829418035098, time: 9.377904891967773


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6382591553588366, Training Loss Force: 2.5179106772547666, time: 1.1473023891448975
Validation Loss Energy: 1.207348404291252, Validation Loss Force: 3.0008660984901625, time: 0.08245420455932617
Test Loss Energy: 9.84973747985152, Test Loss Force: 10.690488777823555, time: 9.357471227645874


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6057537633460195, Training Loss Force: 2.515652192023766, time: 1.1249470710754395
Validation Loss Energy: 4.511127485500064, Validation Loss Force: 2.8160943505227003, time: 0.07747030258178711
Test Loss Energy: 11.587359448260578, Test Loss Force: 10.788261242282232, time: 9.518475770950317


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 15.09733546980883, Training Loss Force: 6.300758505259088, time: 1.14497709274292
Validation Loss Energy: 7.131938657018063, Validation Loss Force: 5.327392494025068, time: 0.0845956802368164
Test Loss Energy: 10.808531465156372, Test Loss Force: 10.959194067856073, time: 9.382623195648193


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 14.004653173373187, Training Loss Force: 7.760610795328987, time: 1.1648883819580078
Validation Loss Energy: 14.174812327694761, Validation Loss Force: 4.795443029816251, time: 0.07564139366149902
Test Loss Energy: 17.496348685724563, Test Loss Force: 11.124454011055267, time: 9.377593517303467


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.496749512414109, Training Loss Force: 5.156789724983573, time: 1.1109654903411865
Validation Loss Energy: 8.98171716230739, Validation Loss Force: 6.481811328983555, time: 0.08460140228271484
Test Loss Energy: 15.318415996328373, Test Loss Force: 12.18526821402722, time: 9.571873188018799


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.863463488516127, Training Loss Force: 4.309885270044503, time: 1.1597554683685303
Validation Loss Energy: 7.230266679830022, Validation Loss Force: 3.4527712374628674, time: 0.08263134956359863
Test Loss Energy: 12.971680518086993, Test Loss Force: 10.870903196046655, time: 9.886845588684082


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.7422043403234735, Training Loss Force: 3.408631749697555, time: 1.121877908706665
Validation Loss Energy: 11.427559149669106, Validation Loss Force: 3.5594168435518143, time: 0.0803079605102539
Test Loss Energy: 16.89338678060933, Test Loss Force: 11.163057002072563, time: 9.321127653121948


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.050507535657428, Training Loss Force: 3.367230630177281, time: 1.244582176208496
Validation Loss Energy: 5.57664410526286, Validation Loss Force: 3.8756664834312358, time: 0.11328697204589844
Test Loss Energy: 9.597451635437048, Test Loss Force: 11.011093396014523, time: 9.369256734848022


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.97820350279801, Training Loss Force: 3.3935302972161665, time: 1.1233041286468506
Validation Loss Energy: 6.0668188336733095, Validation Loss Force: 3.939375243449219, time: 0.07682991027832031
Test Loss Energy: 9.523861800332325, Test Loss Force: 11.052045698076274, time: 9.298341274261475


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.022393718258943, Training Loss Force: 3.3260566500974336, time: 1.1199586391448975
Validation Loss Energy: 6.8764149012777755, Validation Loss Force: 3.969847961407728, time: 0.07655739784240723
Test Loss Energy: 14.79321927358223, Test Loss Force: 11.592291586164176, time: 9.464643955230713


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.174083201170264, Training Loss Force: 3.3105538266154233, time: 1.215092420578003
Validation Loss Energy: 12.70169649627396, Validation Loss Force: 3.601630905575096, time: 0.07769536972045898
Test Loss Energy: 18.30697883117433, Test Loss Force: 11.572525789346251, time: 9.305428504943848


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 12.607420302171139, Training Loss Force: 4.617964798449267, time: 1.1205499172210693
Validation Loss Energy: 3.8884285428609413, Validation Loss Force: 5.955224628485264, time: 0.08106231689453125
Test Loss Energy: 12.411965661644949, Test Loss Force: 12.438464325247011, time: 9.338102579116821


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.886342857483786, Training Loss Force: 5.371798756338783, time: 1.128835916519165
Validation Loss Energy: 10.492615494409218, Validation Loss Force: 3.9414701147741074, time: 0.07921648025512695
Test Loss Energy: 15.080437267080724, Test Loss Force: 11.242728638031645, time: 9.498926401138306


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.8433284280566395, Training Loss Force: 3.406214450630766, time: 1.1673271656036377
Validation Loss Energy: 9.532207395621029, Validation Loss Force: 3.32956828768645, time: 0.0816810131072998
Test Loss Energy: 11.003837809329996, Test Loss Force: 10.6562351890627, time: 9.377087831497192


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.9018064474374565, Training Loss Force: 3.3476374618610776, time: 1.1285414695739746
Validation Loss Energy: 6.082467353241677, Validation Loss Force: 3.447097961628346, time: 0.08066368103027344
Test Loss Energy: 9.891854022115343, Test Loss Force: 10.950244694103015, time: 9.341030836105347


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.387644759058686, Training Loss Force: 3.3190220723462995, time: 1.111619234085083
Validation Loss Energy: 4.829493818280675, Validation Loss Force: 4.912295566885382, time: 0.08369112014770508
Test Loss Energy: 12.211211267980488, Test Loss Force: 11.601817885182069, time: 9.45796799659729

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–ƒâ–‚â–‡â–†â–„â–‡â–â–â–…â–ˆâ–ƒâ–…â–‚â–â–ƒ
wandb:   test_error_force â–â–â–â–â–â–‚â–‚â–ƒâ–‡â–‚â–ƒâ–‚â–ƒâ–…â–…â–ˆâ–ƒâ–â–‚â–…
wandb:          test_loss â–â–â–â–â–â–‚â–ƒâ–†â–ˆâ–ƒâ–†â–‚â–‚â–†â–ˆâ–‡â–…â–‚â–‚â–…
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–‡â–…â–„â–„â–„â–„â–„â–„â–‡â–…â–„â–„â–„
wandb:  train_error_force â–â–â–â–â–â–â–†â–ˆâ–…â–ƒâ–‚â–‚â–‚â–‚â–‚â–„â–…â–‚â–‚â–‚
wandb:         train_loss â–â–â–â–â–â–â–‡â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–ƒâ–ƒâ–ƒ
wandb: valid_error_energy â–â–‚â–â–â–â–ƒâ–„â–ˆâ–…â–„â–‡â–ƒâ–„â–„â–‡â–ƒâ–†â–†â–„â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–†â–…â–ˆâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‡â–ƒâ–‚â–‚â–…
wandb:         valid_loss â–â–â–â–â–â–‚â–†â–‡â–ˆâ–ƒâ–…â–„â–„â–„â–…â–†â–…â–„â–ƒâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 2521
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.21121
wandb:   test_error_force 11.60182
wandb:          test_loss 4.69919
wandb: train_error_energy 7.38764
wandb:  train_error_force 3.31902
wandb:         train_loss 1.60494
wandb: valid_error_energy 4.82949
wandb:  valid_error_force 4.9123
wandb:         valid_loss 1.96686
wandb: 
wandb: ğŸš€ View run al_77_19 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3887xwfl
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_143938-3887xwfl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8658072352409363, Uncertainty Bias: -0.4649474024772644
0.0001335144 0.0939064
4.9648647 29.383795
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 5 steps.
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_144748-pweeoxkq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_20
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/pweeoxkq
Training model 20. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.4287318790602574, Training Loss Force: 2.692504898703022, time: 1.1733295917510986
Validation Loss Energy: 1.290044329384357, Validation Loss Force: 2.9505429828450556, time: 0.09290170669555664
Test Loss Energy: 9.531404805086156, Test Loss Force: 10.703458530441864, time: 9.940957069396973


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.068839609987256, Training Loss Force: 2.5630438950362837, time: 1.1768381595611572
Validation Loss Energy: 1.474974673451006, Validation Loss Force: 2.978376180747737, time: 0.09760046005249023
Test Loss Energy: 9.737253817513935, Test Loss Force: 10.743118530729074, time: 9.751156091690063


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5209572214882914, Training Loss Force: 2.5184802146077847, time: 1.1823697090148926
Validation Loss Energy: 1.7510846927378954, Validation Loss Force: 2.938306088596021, time: 0.08088183403015137
Test Loss Energy: 9.44929887074891, Test Loss Force: 10.661095084235228, time: 9.85498309135437


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.6063859188462533, Training Loss Force: 2.5046768021849872, time: 1.1943912506103516
Validation Loss Energy: 3.023298456258366, Validation Loss Force: 2.8711510782065135, time: 0.08587503433227539
Test Loss Energy: 9.566838157935717, Test Loss Force: 10.69423728582228, time: 11.4008948802948


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.629948500393808, Training Loss Force: 2.4915819338464424, time: 1.33203125
Validation Loss Energy: 2.625788494936922, Validation Loss Force: 2.9544834473859667, time: 0.09432792663574219
Test Loss Energy: 10.458808941355281, Test Loss Force: 10.77121286707861, time: 11.226468801498413


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.6185536838296475, Training Loss Force: 2.4894033606465036, time: 1.2560560703277588
Validation Loss Energy: 2.8218576960267496, Validation Loss Force: 3.0013198982095632, time: 0.09328222274780273
Test Loss Energy: 10.80209447553747, Test Loss Force: 10.775180225039382, time: 11.032213687896729


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 16.716231445222963, Training Loss Force: 6.371559139876106, time: 1.2298507690429688
Validation Loss Energy: 3.0099046844616635, Validation Loss Force: 7.977316765696541, time: 0.0947268009185791
Test Loss Energy: 12.78512358377854, Test Loss Force: 13.191163233562342, time: 9.861003160476685


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.71481331899975, Training Loss Force: 6.804862767230264, time: 1.1623704433441162
Validation Loss Energy: 4.058845478997877, Validation Loss Force: 7.754054874502293, time: 0.07988953590393066
Test Loss Energy: 11.036295043318345, Test Loss Force: 13.248012755643932, time: 9.550031423568726


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.94462795040609, Training Loss Force: 6.377463956321184, time: 1.1669414043426514
Validation Loss Energy: 23.9822846575266, Validation Loss Force: 9.646143634885943, time: 0.08031535148620605
Test Loss Energy: 19.440257790839247, Test Loss Force: 14.974761190432519, time: 9.365204811096191


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 12.215186536773775, Training Loss Force: 7.047184990680154, time: 1.11967134475708
Validation Loss Energy: 1.6270622091698903, Validation Loss Force: 4.73114419656156, time: 0.09036612510681152
Test Loss Energy: 9.127488029312504, Test Loss Force: 11.078888899629705, time: 9.533511638641357


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.549434065418254, Training Loss Force: 3.655234355923237, time: 1.1624274253845215
Validation Loss Energy: 7.312646915741489, Validation Loss Force: 3.7458326691582338, time: 0.08027005195617676
Test Loss Energy: 14.882232185654432, Test Loss Force: 11.04191661619325, time: 9.521729230880737


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.665231396486391, Training Loss Force: 3.4216622885829846, time: 1.1630468368530273
Validation Loss Energy: 5.396494523426567, Validation Loss Force: 3.844566061150356, time: 0.08070492744445801
Test Loss Energy: 9.723115142812452, Test Loss Force: 10.738953815624978, time: 9.52915096282959


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.160915525188983, Training Loss Force: 4.0298755768290535, time: 1.1530914306640625
Validation Loss Energy: 14.570446858134627, Validation Loss Force: 5.873258334516078, time: 0.08411717414855957
Test Loss Energy: 12.077719538825653, Test Loss Force: 11.542645086858991, time: 9.480373859405518


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.75232591153504, Training Loss Force: 5.500962140143215, time: 1.1403088569641113
Validation Loss Energy: 15.079460064298004, Validation Loss Force: 5.3238070754373386, time: 0.07819437980651855
Test Loss Energy: 20.611425777969895, Test Loss Force: 11.94388474063633, time: 10.133808135986328


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.063979844092382, Training Loss Force: 5.029290449279264, time: 1.1281371116638184
Validation Loss Energy: 10.129877658250432, Validation Loss Force: 4.724595274310678, time: 0.08216357231140137
Test Loss Energy: 14.734796289709019, Test Loss Force: 11.807907403603592, time: 9.37873387336731


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 17.063069495975068, Training Loss Force: 5.571547721737799, time: 1.219285488128662
Validation Loss Energy: 28.615211052503433, Validation Loss Force: 9.668317028437432, time: 0.08575677871704102
Test Loss Energy: 33.63116488222631, Test Loss Force: 13.911747417336425, time: 9.346513271331787


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.267054339819872, Training Loss Force: 6.081375185520198, time: 1.366469144821167
Validation Loss Energy: 7.376744233419116, Validation Loss Force: 3.9716094970211633, time: 0.08555388450622559
Test Loss Energy: 11.880961592339391, Test Loss Force: 10.821405537658741, time: 9.386640071868896


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.629111683083533, Training Loss Force: 3.455243609023471, time: 1.1792192459106445
Validation Loss Energy: 6.047643457502592, Validation Loss Force: 3.8334261464487343, time: 0.08068561553955078
Test Loss Energy: 9.711241794415779, Test Loss Force: 10.875185014136983, time: 9.326692581176758


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.773491218109442, Training Loss Force: 3.3533534908590785, time: 1.175140619277954
Validation Loss Energy: 3.1964721477564604, Validation Loss Force: 3.8190259101124386, time: 0.08249592781066895
Test Loss Energy: 10.812417981393292, Test Loss Force: 10.92208232591746, time: 9.728694677352905


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.7888122606088235, Training Loss Force: 3.2890483767051855, time: 1.1548199653625488
Validation Loss Energy: 6.361667519268465, Validation Loss Force: 3.6235598084043565, time: 0.08816337585449219
Test Loss Energy: 9.667961412987509, Test Loss Force: 10.871029914609407, time: 10.073097705841064

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–‚â–„â–â–ƒâ–â–‚â–„â–ƒâ–ˆâ–‚â–â–â–
wandb:   test_error_force â–â–â–â–â–â–â–…â–…â–ˆâ–‚â–‚â–â–‚â–ƒâ–ƒâ–†â–â–â–â–
wandb:          test_loss â–â–â–â–â–â–â–„â–„â–†â–â–‚â–â–‚â–„â–ƒâ–ˆâ–‚â–â–â–
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–…â–…â–†â–ƒâ–ƒâ–„â–†â–…â–ˆâ–„â–ƒâ–ƒâ–ƒ
wandb:  train_error_force â–â–â–â–â–â–â–‡â–ˆâ–‡â–ˆâ–ƒâ–‚â–ƒâ–†â–…â–†â–‡â–‚â–‚â–‚
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–‡â–ˆâ–ƒâ–ƒâ–„â–†â–…â–‡â–†â–ƒâ–ƒâ–ƒ
wandb: valid_error_energy â–â–â–â–â–â–â–â–‚â–‡â–â–ƒâ–‚â–„â–…â–ƒâ–ˆâ–ƒâ–‚â–â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–†â–†â–ˆâ–ƒâ–‚â–‚â–„â–„â–ƒâ–ˆâ–‚â–‚â–‚â–‚
wandb:         valid_loss â–â–â–â–â–â–â–„â–„â–‡â–‚â–‚â–‚â–„â–„â–ƒâ–ˆâ–‚â–‚â–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 2611
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.66796
wandb:   test_error_force 10.87103
wandb:          test_loss 4.28447
wandb: train_error_energy 6.78881
wandb:  train_error_force 3.28905
wandb:         train_loss 1.55484
wandb: valid_error_energy 6.36167
wandb:  valid_error_force 3.62356
wandb:         valid_loss 1.63818
wandb: 
wandb: ğŸš€ View run al_77_20 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/pweeoxkq
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_144748-pweeoxkq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.2525107860565186, Uncertainty Bias: -0.6158636212348938
0.00016784668 0.0015439987
-6.6612735 18.088017
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 3 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_145613-hjavygl8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_21
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/hjavygl8
Training model 21. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.899640288229105, Training Loss Force: 2.6857259102499635, time: 1.212568759918213
Validation Loss Energy: 1.3043888427668846, Validation Loss Force: 2.9379354558026245, time: 0.08250570297241211
Test Loss Energy: 9.59066405965982, Test Loss Force: 10.646547159964829, time: 9.437327146530151


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.8150300740885268, Training Loss Force: 2.5494980715527165, time: 1.246690034866333
Validation Loss Energy: 1.2718771991064006, Validation Loss Force: 2.792793471750806, time: 0.08683133125305176
Test Loss Energy: 10.045568166728945, Test Loss Force: 10.662917702308878, time: 9.440967798233032


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.2427791924267693, Training Loss Force: 2.505169132036694, time: 1.2852606773376465
Validation Loss Energy: 1.9783865259139972, Validation Loss Force: 2.9995519051908968, time: 0.08329987525939941
Test Loss Energy: 9.380427094068176, Test Loss Force: 10.666681636266444, time: 9.45524525642395


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.596188905923606, Training Loss Force: 2.500394526608222, time: 1.1773676872253418
Validation Loss Energy: 3.754260139658736, Validation Loss Force: 2.854356369209483, time: 0.08368158340454102
Test Loss Energy: 11.488438099610251, Test Loss Force: 10.77288310018157, time: 9.336821794509888


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6411512055583057, Training Loss Force: 2.493564116501607, time: 1.25765061378479
Validation Loss Energy: 3.695885864369745, Validation Loss Force: 2.9240531478997562, time: 0.08925890922546387
Test Loss Energy: 9.495657127340653, Test Loss Force: 10.65012129587196, time: 9.867233991622925


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5217862754880316, Training Loss Force: 2.4918664659759573, time: 1.2253656387329102
Validation Loss Energy: 1.8956486833541712, Validation Loss Force: 2.83770505914243, time: 0.08237957954406738
Test Loss Energy: 9.630657317549412, Test Loss Force: 10.728161184350315, time: 10.64378809928894


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 15.542639540716088, Training Loss Force: 6.256450380284473, time: 1.4646661281585693
Validation Loss Energy: 30.515224607790653, Validation Loss Force: 7.68117570837842, time: 0.11234211921691895
Test Loss Energy: 22.021466183377314, Test Loss Force: 12.050222963044488, time: 10.362175703048706


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.367140251622551, Training Loss Force: 6.459411533303396, time: 1.1806259155273438
Validation Loss Energy: 14.113094675179664, Validation Loss Force: 5.371506591870551, time: 0.08299970626831055
Test Loss Energy: 19.483611627437924, Test Loss Force: 12.030480123298931, time: 10.001248598098755


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.715256188679133, Training Loss Force: 5.9282031696442425, time: 1.1994261741638184
Validation Loss Energy: 4.042316525626511, Validation Loss Force: 5.957621661841177, time: 0.08338093757629395
Test Loss Energy: 9.419923974633406, Test Loss Force: 11.775643413791308, time: 9.580890417098999


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 15.580056599414458, Training Loss Force: 5.784794806353871, time: 1.2114672660827637
Validation Loss Energy: 5.941055522722441, Validation Loss Force: 5.935481668661577, time: 0.08409523963928223
Test Loss Energy: 10.200145605450341, Test Loss Force: 12.244360870110675, time: 9.905969142913818


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.253619029229907, Training Loss Force: 4.979510194973985, time: 1.2620487213134766
Validation Loss Energy: 10.758220150393093, Validation Loss Force: 4.085118879533334, time: 0.10451531410217285
Test Loss Energy: 10.854207659340576, Test Loss Force: 10.644738965255964, time: 10.271568775177002


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.307452545568369, Training Loss Force: 5.443107186519234, time: 1.204003095626831
Validation Loss Energy: 17.44509172657532, Validation Loss Force: 6.383462820610169, time: 0.08384084701538086
Test Loss Energy: 14.361830585841297, Test Loss Force: 11.857993015791005, time: 9.69846510887146


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 12.054988716650923, Training Loss Force: 5.896964543173578, time: 1.266585111618042
Validation Loss Energy: 13.684013457668275, Validation Loss Force: 9.063040387783136, time: 0.08701872825622559
Test Loss Energy: 17.14413477164262, Test Loss Force: 14.160474248692775, time: 10.02682876586914


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.108144693571335, Training Loss Force: 6.03640498419771, time: 1.211536169052124
Validation Loss Energy: 2.6075605759821276, Validation Loss Force: 4.292778789010713, time: 0.0921931266784668
Test Loss Energy: 9.896469783302598, Test Loss Force: 11.20766912685938, time: 10.322286605834961


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.249926714165312, Training Loss Force: 4.681608115491284, time: 1.2657577991485596
Validation Loss Energy: 5.944405134684633, Validation Loss Force: 4.931755291555449, time: 0.08919000625610352
Test Loss Energy: 9.170774706098827, Test Loss Force: 11.562524610485942, time: 10.549881935119629


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.1458057452609, Training Loss Force: 5.117735032737754, time: 1.2469642162322998
Validation Loss Energy: 11.918092598073645, Validation Loss Force: 5.433151208905784, time: 0.08992648124694824
Test Loss Energy: 10.674020167790736, Test Loss Force: 11.358554608550275, time: 10.544967651367188


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.33669613224129, Training Loss Force: 5.849597935450987, time: 1.1928184032440186
Validation Loss Energy: 6.322132167070695, Validation Loss Force: 7.9590008924291045, time: 0.09748196601867676
Test Loss Energy: 9.633926147243912, Test Loss Force: 13.07526478715123, time: 11.19249677658081


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.22216758868775, Training Loss Force: 4.055871279356808, time: 1.2350783348083496
Validation Loss Energy: 4.7664463856260975, Validation Loss Force: 3.9995820858091955, time: 0.0975487232208252
Test Loss Energy: 12.358395354267032, Test Loss Force: 11.337440350700751, time: 10.555317640304565


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.736308247267212, Training Loss Force: 3.351321073629285, time: 1.277414321899414
Validation Loss Energy: 8.865610344207973, Validation Loss Force: 3.503772046653606, time: 0.08996438980102539
Test Loss Energy: 13.91513781738976, Test Loss Force: 11.310929497420208, time: 10.712330341339111


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.60959965143021, Training Loss Force: 3.4958184697981984, time: 1.2529277801513672
Validation Loss Energy: 2.5441917204154034, Validation Loss Force: 5.692555642521301, time: 0.10093522071838379
Test Loss Energy: 10.931076550599215, Test Loss Force: 12.212532766226708, time: 10.673723936080933

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–â–â–ˆâ–‡â–â–‚â–‚â–„â–…â–â–â–‚â–â–ƒâ–„â–‚
wandb:   test_error_force â–â–â–â–â–â–â–„â–„â–ƒâ–„â–â–ƒâ–ˆâ–‚â–ƒâ–‚â–†â–‚â–‚â–„
wandb:          test_loss â–â–â–â–‚â–â–â–†â–†â–ƒâ–ƒâ–â–„â–ˆâ–‚â–‚â–‚â–„â–ƒâ–ƒâ–„
wandb: train_error_energy â–‚â–â–â–â–â–â–ˆâ–†â–…â–ˆâ–„â–…â–†â–†â–†â–†â–†â–„â–„â–…
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–‡â–‡â–…â–†â–‡â–‡â–…â–†â–‡â–„â–ƒâ–ƒ
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–†â–‡â–…â–†â–‡â–‡â–…â–†â–‡â–„â–ƒâ–„
wandb: valid_error_energy â–â–â–â–‚â–‚â–â–ˆâ–„â–‚â–‚â–ƒâ–…â–„â–â–‚â–„â–‚â–‚â–ƒâ–
wandb:  valid_error_force â–â–â–â–â–â–â–†â–„â–…â–…â–‚â–…â–ˆâ–ƒâ–ƒâ–„â–‡â–‚â–‚â–„
wandb:         valid_loss â–â–â–â–â–â–â–ˆâ–„â–ƒâ–„â–ƒâ–…â–‡â–‚â–ƒâ–„â–…â–‚â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2701
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.93108
wandb:   test_error_force 12.21253
wandb:          test_loss 4.81787
wandb: train_error_energy 9.6096
wandb:  train_error_force 3.49582
wandb:         train_loss 1.81279
wandb: valid_error_energy 2.54419
wandb:  valid_error_force 5.69256
wandb:         valid_loss 2.07501
wandb: 
wandb: ğŸš€ View run al_77_21 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/hjavygl8
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_145613-hjavygl8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.0426337718963623, Uncertainty Bias: -0.5374696254730225
0.00016021729 0.051216125
-7.1869216 19.742393
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 2 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 6 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 2 steps.
Found uncertainty sample 33 after 4 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 4 steps.
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 5 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 5 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 5 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_150443-noiqh22g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_22
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/noiqh22g
Training model 22. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.454487614795026, Training Loss Force: 2.706341011891989, time: 1.3166022300720215
Validation Loss Energy: 1.9340918990584528, Validation Loss Force: 2.9228901469505755, time: 0.0935823917388916
Test Loss Energy: 9.487800693030692, Test Loss Force: 10.614557941547005, time: 9.460118055343628


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7346536821615979, Training Loss Force: 2.5104077730887315, time: 1.287797451019287
Validation Loss Energy: 1.2843947121465906, Validation Loss Force: 2.923876125928744, time: 0.09922385215759277
Test Loss Energy: 9.571812521370548, Test Loss Force: 10.686780126436144, time: 10.862576723098755


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3646255733990396, Training Loss Force: 2.509073299184868, time: 1.2984287738800049
Validation Loss Energy: 1.1059188786129504, Validation Loss Force: 2.891222809024163, time: 0.08931279182434082
Test Loss Energy: 9.706994540172772, Test Loss Force: 10.741718900094579, time: 10.147472143173218


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.808174971700206, Training Loss Force: 2.5265854000937846, time: 1.244554042816162
Validation Loss Energy: 1.0858586709236024, Validation Loss Force: 2.826302506183187, time: 0.09046268463134766
Test Loss Energy: 9.665714431822389, Test Loss Force: 10.788574849041034, time: 10.24795150756836


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.4960345062875704, Training Loss Force: 2.5122137337271098, time: 1.287076711654663
Validation Loss Energy: 3.3741262764743403, Validation Loss Force: 2.8886869414557648, time: 0.08919095993041992
Test Loss Energy: 10.949606792466406, Test Loss Force: 10.81266965581854, time: 9.999677658081055


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.589985803197374, Training Loss Force: 2.487831295246158, time: 1.2508540153503418
Validation Loss Energy: 3.892785955377602, Validation Loss Force: 2.8905867546088206, time: 0.08369946479797363
Test Loss Energy: 11.276549647092963, Test Loss Force: 10.879411628932038, time: 10.131027698516846


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.723185395921842, Training Loss Force: 6.079057048755232, time: 1.2509574890136719
Validation Loss Energy: 2.5754005825703503, Validation Loss Force: 5.258309614170117, time: 0.09062910079956055
Test Loss Energy: 11.016620212092667, Test Loss Force: 12.032332632665254, time: 10.08899736404419


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 6.884263085343491, Training Loss Force: 3.757499765714495, time: 1.2677991390228271
Validation Loss Energy: 2.8331182974054636, Validation Loss Force: 3.8404643841477926, time: 0.09234738349914551
Test Loss Energy: 12.046249174492052, Test Loss Force: 11.165840902667904, time: 10.150112628936768


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.836042464911188, Training Loss Force: 3.425557664535023, time: 1.2628066539764404
Validation Loss Energy: 9.122712601523741, Validation Loss Force: 3.7057645810288316, time: 0.09271669387817383
Test Loss Energy: 17.090805935546676, Test Loss Force: 11.464001044526418, time: 9.89631724357605


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 14.840193523335286, Training Loss Force: 5.53440398082818, time: 1.2452428340911865
Validation Loss Energy: 33.53757349147621, Validation Loss Force: 7.057123160001201, time: 0.08734250068664551
Test Loss Energy: 25.585659957443177, Test Loss Force: 13.281463821486724, time: 9.721384286880493


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 14.521818846787534, Training Loss Force: 7.6690495363621975, time: 1.2379224300384521
Validation Loss Energy: 9.872229416188848, Validation Loss Force: 4.1773123673749835, time: 0.09215974807739258
Test Loss Energy: 14.663198096046806, Test Loss Force: 10.995410651848685, time: 10.116241931915283


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.708760469414494, Training Loss Force: 3.5223503582824987, time: 1.2474617958068848
Validation Loss Energy: 4.348963781256942, Validation Loss Force: 3.895964780104867, time: 0.08767080307006836
Test Loss Energy: 11.769293379388404, Test Loss Force: 10.985113516548259, time: 10.179423093795776


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.839976251908053, Training Loss Force: 3.380597580815548, time: 1.3151135444641113
Validation Loss Energy: 8.797429942401468, Validation Loss Force: 3.6452094841246234, time: 0.08978796005249023
Test Loss Energy: 15.524084266694855, Test Loss Force: 11.211427763389729, time: 10.118805170059204


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.873345768938523, Training Loss Force: 3.367075875473186, time: 1.2601840496063232
Validation Loss Energy: 10.171446425088815, Validation Loss Force: 3.858730936199406, time: 0.09661984443664551
Test Loss Energy: 14.328720928923758, Test Loss Force: 11.331329283426294, time: 10.622359991073608


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 12.191801236895616, Training Loss Force: 4.845145531127021, time: 1.2687320709228516
Validation Loss Energy: 11.121218954790908, Validation Loss Force: 5.3472470110908645, time: 0.09183478355407715
Test Loss Energy: 10.661309159051607, Test Loss Force: 11.146178461639513, time: 10.080010890960693


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 12.394329952248404, Training Loss Force: 5.99948938731128, time: 1.2399685382843018
Validation Loss Energy: 2.628774488183814, Validation Loss Force: 7.111091035561428, time: 0.09697794914245605
Test Loss Energy: 8.999900133571497, Test Loss Force: 12.893353332783267, time: 10.076740264892578


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.295557398227377, Training Loss Force: 5.704206744854022, time: 1.251786231994629
Validation Loss Energy: 6.856070957429229, Validation Loss Force: 4.872936791320333, time: 0.08805537223815918
Test Loss Energy: 10.112261511698906, Test Loss Force: 11.173864415780576, time: 9.708904266357422


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 16.92651456556811, Training Loss Force: 6.272426254873463, time: 1.2620444297790527
Validation Loss Energy: 9.906419372869475, Validation Loss Force: 6.206509431694524, time: 0.08668923377990723
Test Loss Energy: 10.748476612558498, Test Loss Force: 11.516052836930443, time: 9.780975580215454


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.723802919363749, Training Loss Force: 5.035765299601934, time: 1.3107964992523193
Validation Loss Energy: 7.129590610874255, Validation Loss Force: 5.128132110070376, time: 0.08976101875305176
Test Loss Energy: 13.66020055935597, Test Loss Force: 11.970315731470944, time: 10.086501121520996


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 14.482298992868385, Training Loss Force: 5.738553587129943, time: 1.2963778972625732
Validation Loss Energy: 19.121989906966313, Validation Loss Force: 7.878953115821233, time: 0.0883474349975586
Test Loss Energy: 19.465529587084692, Test Loss Force: 13.521329903741712, time: 10.219172954559326

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–‚â–‚â–‚â–‚â–„â–ˆâ–ƒâ–‚â–„â–ƒâ–‚â–â–â–‚â–ƒâ–…
wandb:   test_error_force â–â–â–â–â–â–‚â–„â–‚â–ƒâ–‡â–‚â–‚â–‚â–ƒâ–‚â–†â–‚â–ƒâ–„â–ˆ
wandb:          test_loss â–â–â–â–â–‚â–‚â–ƒâ–‚â–„â–ˆâ–ƒâ–‚â–ƒâ–ƒâ–‚â–„â–‚â–‚â–„â–‡
wandb: train_error_energy â–â–â–â–â–‚â–‚â–†â–ƒâ–ƒâ–‡â–‡â–ƒâ–ƒâ–ƒâ–†â–†â–…â–ˆâ–…â–‡
wandb:  train_error_force â–â–â–â–â–â–â–†â–ƒâ–‚â–…â–ˆâ–‚â–‚â–‚â–„â–†â–…â–†â–„â–…
wandb:         train_loss â–â–â–â–â–â–â–†â–ƒâ–ƒâ–†â–ˆâ–ƒâ–ƒâ–ƒâ–…â–†â–†â–‡â–…â–†
wandb: valid_error_energy â–â–â–â–â–â–‚â–â–â–ƒâ–ˆâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–â–‚â–ƒâ–‚â–…
wandb:  valid_error_force â–â–â–â–â–â–â–„â–‚â–‚â–‡â–ƒâ–‚â–‚â–‚â–„â–‡â–„â–†â–„â–ˆ
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–‚â–ƒâ–ˆâ–ƒâ–‚â–ƒâ–ƒâ–„â–„â–ƒâ–„â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 2791
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 19.46553
wandb:   test_error_force 13.52133
wandb:          test_loss 5.82693
wandb: train_error_energy 14.4823
wandb:  train_error_force 5.73855
wandb:         train_loss 2.8893
wandb: valid_error_energy 19.12199
wandb:  valid_error_force 7.87895
wandb:         valid_loss 3.91598
wandb: 
wandb: ğŸš€ View run al_77_22 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/noiqh22g
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_150443-noiqh22g/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.3601438999176025, Uncertainty Bias: -0.8124237656593323
0.00011444092 0.0052087307
-8.09174 34.830326
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_151313-pia971uw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_23
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/pia971uw
Training model 23. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9192355174090814, Training Loss Force: 2.726708741409967, time: 1.3980228900909424
Validation Loss Energy: 1.9795183531827483, Validation Loss Force: 2.9348835524885377, time: 0.09407997131347656
Test Loss Energy: 9.775957874675727, Test Loss Force: 10.655069158181005, time: 10.110388994216919


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.56004179226454, Training Loss Force: 2.527014319010848, time: 1.341135025024414
Validation Loss Energy: 3.2558660250646545, Validation Loss Force: 2.923963968906812, time: 0.09017014503479004
Test Loss Energy: 11.126059182798556, Test Loss Force: 10.779694127333347, time: 10.129489660263062


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.107392103946868, Training Loss Force: 2.5198138007723103, time: 1.3445909023284912
Validation Loss Energy: 1.758296445368918, Validation Loss Force: 2.9144769272754125, time: 0.09648370742797852
Test Loss Energy: 9.56384455196881, Test Loss Force: 10.619390794659276, time: 10.239545345306396


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.616266125989102, Training Loss Force: 2.483179507000286, time: 1.3096909523010254
Validation Loss Energy: 3.484004036551002, Validation Loss Force: 2.816539800149129, time: 0.09424614906311035
Test Loss Energy: 11.052219664467016, Test Loss Force: 10.761376939035562, time: 10.25936484336853


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.5834577028959864, Training Loss Force: 2.4866383491896666, time: 1.4584686756134033
Validation Loss Energy: 2.9544277800994845, Validation Loss Force: 2.9779247190543687, time: 0.11049246788024902
Test Loss Energy: 9.533411252278164, Test Loss Force: 10.751159134573054, time: 10.55959939956665


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.609645200855933, Training Loss Force: 2.4809746566926183, time: 1.3988134860992432
Validation Loss Energy: 1.4730917831530408, Validation Loss Force: 2.842798504520955, time: 0.0888521671295166
Test Loss Energy: 9.578842291230123, Test Loss Force: 10.659990472731986, time: 9.391675233840942


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 18.9985469222975, Training Loss Force: 6.273053797643917, time: 1.3095808029174805
Validation Loss Energy: 15.007316151693985, Validation Loss Force: 12.935349911170366, time: 0.11031866073608398
Test Loss Energy: 14.954573301515248, Test Loss Force: 16.334070872665134, time: 10.852968454360962


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 12.110178611509507, Training Loss Force: 7.223422236471874, time: 1.3197040557861328
Validation Loss Energy: 2.5661545802894175, Validation Loss Force: 4.818494630479281, time: 0.09392333030700684
Test Loss Energy: 9.41795206797258, Test Loss Force: 10.680739972872722, time: 10.050012826919556


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.649871812604693, Training Loss Force: 3.7238003340637764, time: 1.3003723621368408
Validation Loss Energy: 6.6702228268103525, Validation Loss Force: 4.013632286248093, time: 0.09160876274108887
Test Loss Energy: 10.090187505529768, Test Loss Force: 10.582862305986147, time: 10.186688661575317


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.66056984895042, Training Loss Force: 3.466163037047135, time: 1.31178879737854
Validation Loss Energy: 10.020492330989988, Validation Loss Force: 3.8471354404252165, time: 0.09109973907470703
Test Loss Energy: 10.239736459389995, Test Loss Force: 10.634542373818476, time: 10.066690921783447


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.015678352064201, Training Loss Force: 5.632999520153877, time: 1.3003952503204346
Validation Loss Energy: 6.277685178852049, Validation Loss Force: 5.907361004891502, time: 0.0975642204284668
Test Loss Energy: 10.384090432489867, Test Loss Force: 11.918163003492813, time: 9.991487503051758


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 13.50580872198462, Training Loss Force: 5.226612786039896, time: 1.455484390258789
Validation Loss Energy: 35.538416089422455, Validation Loss Force: 7.548991287266432, time: 0.1310579776763916
Test Loss Energy: 29.707679392968604, Test Loss Force: 12.642908409613078, time: 10.115086078643799


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 12.49828746925531, Training Loss Force: 6.040959685838171, time: 1.3392107486724854
Validation Loss Energy: 13.832282161867212, Validation Loss Force: 6.8226262743800214, time: 0.09114265441894531
Test Loss Energy: 11.944404410785486, Test Loss Force: 12.010781071266042, time: 10.002891302108765


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.69535686022837, Training Loss Force: 5.106111298715949, time: 1.3341403007507324
Validation Loss Energy: 4.051797332521824, Validation Loss Force: 4.6620232402826565, time: 0.09051322937011719
Test Loss Energy: 9.41313176360736, Test Loss Force: 11.00350128746788, time: 10.518624305725098


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 12.553964796647593, Training Loss Force: 5.468488657996616, time: 1.2907958030700684
Validation Loss Energy: 14.806734003689067, Validation Loss Force: 8.660451009239361, time: 0.09718132019042969
Test Loss Energy: 19.061180744566602, Test Loss Force: 13.89119728885359, time: 10.133095502853394


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.26443435606906, Training Loss Force: 5.786976702711156, time: 1.3050062656402588
Validation Loss Energy: 18.376074988405737, Validation Loss Force: 6.180122024556351, time: 0.09694480895996094
Test Loss Energy: 14.293346626532731, Test Loss Force: 11.557180313159034, time: 9.978721380233765


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.836935306764344, Training Loss Force: 6.674891941874521, time: 1.3508822917938232
Validation Loss Energy: 10.855665958048888, Validation Loss Force: 7.15814971019266, time: 0.09416484832763672
Test Loss Energy: 15.430263566361889, Test Loss Force: 12.387239920093746, time: 10.190846920013428


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.494500078810088, Training Loss Force: 6.057638059994108, time: 1.280282735824585
Validation Loss Energy: 2.4681136893411315, Validation Loss Force: 5.314706142104978, time: 0.09134507179260254
Test Loss Energy: 11.57466354371755, Test Loss Force: 11.347357129794107, time: 10.016598224639893


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 17.106685164543922, Training Loss Force: 6.217934911011599, time: 1.2918829917907715
Validation Loss Energy: 16.263250295619244, Validation Loss Force: 8.525652452354182, time: 0.09851932525634766
Test Loss Energy: 18.495622835226918, Test Loss Force: 13.764960409593229, time: 10.02678108215332


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.317270537717329, Training Loss Force: 6.153847383435652, time: 1.5524001121520996
Validation Loss Energy: 3.2974161990293527, Validation Loss Force: 6.107280712187285, time: 0.09335064888000488
Test Loss Energy: 9.658929999060959, Test Loss Force: 12.298919963730127, time: 9.839043855667114

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–‚â–â–â–ƒâ–â–â–â–â–ˆâ–‚â–â–„â–ƒâ–ƒâ–‚â–„â–
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–â–â–â–ƒâ–„â–ƒâ–‚â–…â–‚â–ƒâ–‚â–…â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–ˆâ–â–â–â–ƒâ–‡â–ƒâ–â–†â–ƒâ–„â–‚â–†â–ƒ
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–…â–ƒâ–ƒâ–…â–†â–…â–…â–…â–„â–…â–„â–‡â–„
wandb:  train_error_force â–â–â–â–â–â–â–‡â–ˆâ–ƒâ–‚â–†â–…â–†â–…â–…â–†â–‡â–†â–‡â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–ˆâ–ƒâ–ƒâ–†â–†â–†â–…â–†â–†â–‡â–†â–ˆâ–†
wandb: valid_error_energy â–â–â–â–â–â–â–„â–â–‚â–ƒâ–‚â–ˆâ–„â–‚â–„â–„â–ƒâ–â–„â–
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–‚â–‚â–‚â–ƒâ–„â–„â–‚â–…â–ƒâ–„â–ƒâ–…â–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–ˆâ–‚â–‚â–‚â–ƒâ–‡â–…â–‚â–†â–…â–„â–‚â–†â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 2881
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.65893
wandb:   test_error_force 12.29892
wandb:          test_loss 4.76164
wandb: train_error_energy 10.31727
wandb:  train_error_force 6.15385
wandb:         train_loss 2.74954
wandb: valid_error_energy 3.29742
wandb:  valid_error_force 6.10728
wandb:         valid_loss 2.26418
wandb: 
wandb: ğŸš€ View run al_77_23 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/pia971uw
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_151313-pia971uw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.704741358757019, Uncertainty Bias: -0.4866540729999542
0.0001335144 1.0781374
-5.05572 63.26753
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 125 steps.
Found uncertainty sample 2 after 308 steps.
Found uncertainty sample 3 after 30 steps.
Found uncertainty sample 4 after 4 steps.
Found uncertainty sample 5 after 6 steps.
Found uncertainty sample 6 after 89 steps.
Found uncertainty sample 7 after 7 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 102 steps.
Found uncertainty sample 10 after 12 steps.
Found uncertainty sample 11 after 22 steps.
Found uncertainty sample 12 after 51 steps.
Found uncertainty sample 13 after 51 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 60 steps.
Found uncertainty sample 16 after 45 steps.
Found uncertainty sample 17 after 26 steps.
Found uncertainty sample 18 after 23 steps.
Found uncertainty sample 19 after 38 steps.
Found uncertainty sample 20 after 3 steps.
Found uncertainty sample 21 after 29 steps.
Found uncertainty sample 22 after 73 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 64 steps.
Found uncertainty sample 25 after 125 steps.
Found uncertainty sample 26 after 7 steps.
Found uncertainty sample 27 after 38 steps.
Found uncertainty sample 28 after 17 steps.
Found uncertainty sample 29 after 4 steps.
Found uncertainty sample 30 after 30 steps.
Found uncertainty sample 31 after 31 steps.
Found uncertainty sample 32 after 93 steps.
Found uncertainty sample 33 after 11 steps.
Found uncertainty sample 34 after 126 steps.
Found uncertainty sample 35 after 74 steps.
Found uncertainty sample 36 after 7 steps.
Found uncertainty sample 37 after 36 steps.
Found uncertainty sample 38 after 48 steps.
Found uncertainty sample 39 after 6 steps.
Found uncertainty sample 40 after 7 steps.
Found uncertainty sample 41 after 16 steps.
Found uncertainty sample 42 after 20 steps.
Found uncertainty sample 43 after 51 steps.
Found uncertainty sample 44 after 16 steps.
Found uncertainty sample 45 after 15 steps.
Found uncertainty sample 46 after 27 steps.
Found uncertainty sample 47 after 14 steps.
Found uncertainty sample 48 after 48 steps.
Found uncertainty sample 49 after 216 steps.
Found uncertainty sample 50 after 42 steps.
Found uncertainty sample 51 after 17 steps.
Found uncertainty sample 52 after 88 steps.
Found uncertainty sample 53 after 6 steps.
Found uncertainty sample 54 after 37 steps.
Found uncertainty sample 55 after 30 steps.
Found uncertainty sample 56 after 38 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 29 steps.
Found uncertainty sample 60 after 29 steps.
Found uncertainty sample 61 after 120 steps.
Found uncertainty sample 62 after 10 steps.
Found uncertainty sample 63 after 40 steps.
Found uncertainty sample 64 after 166 steps.
Found uncertainty sample 65 after 73 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 68 steps.
Found uncertainty sample 68 after 10 steps.
Found uncertainty sample 69 after 102 steps.
Found uncertainty sample 70 after 5 steps.
Found uncertainty sample 71 after 16 steps.
Found uncertainty sample 72 after 95 steps.
Found uncertainty sample 73 after 4 steps.
Found uncertainty sample 74 after 31 steps.
Found uncertainty sample 75 after 30 steps.
Found uncertainty sample 76 after 51 steps.
Found uncertainty sample 77 after 89 steps.
Found uncertainty sample 78 after 50 steps.
Found uncertainty sample 79 after 27 steps.
Found uncertainty sample 80 after 46 steps.
Found uncertainty sample 81 after 20 steps.
Found uncertainty sample 82 after 25 steps.
Found uncertainty sample 83 after 30 steps.
Found uncertainty sample 84 after 14 steps.
Found uncertainty sample 85 after 16 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 58 steps.
Found uncertainty sample 88 after 25 steps.
Found uncertainty sample 89 after 20 steps.
Found uncertainty sample 90 after 17 steps.
Found uncertainty sample 91 after 152 steps.
Found uncertainty sample 92 after 47 steps.
Found uncertainty sample 93 after 29 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 82 steps.
Found uncertainty sample 96 after 9 steps.
Found uncertainty sample 97 after 28 steps.
Found uncertainty sample 98 after 20 steps.
Found uncertainty sample 99 after 42 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_152304-qqgn7xba
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_24
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/qqgn7xba
Training model 24. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.5685724104942707, Training Loss Force: 2.6443830949063463, time: 1.3339309692382812
Validation Loss Energy: 1.3767461837472759, Validation Loss Force: 2.8666590621911396, time: 0.09447240829467773
Test Loss Energy: 9.624615140144957, Test Loss Force: 10.631659207084724, time: 10.055847644805908


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.0352893340522322, Training Loss Force: 2.550422969215241, time: 1.3354260921478271
Validation Loss Energy: 1.6236395060605477, Validation Loss Force: 2.873240716806473, time: 0.09554266929626465
Test Loss Energy: 10.082623893970576, Test Loss Force: 10.706258472542167, time: 10.08531928062439


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.218460708617351, Training Loss Force: 2.5028709555283077, time: 1.345249891281128
Validation Loss Energy: 1.215663482540347, Validation Loss Force: 2.932102080852181, time: 0.09462213516235352
Test Loss Energy: 9.610935546268799, Test Loss Force: 10.67271728048821, time: 10.262725830078125


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.643624041526517, Training Loss Force: 2.559109156123246, time: 1.304269790649414
Validation Loss Energy: 1.945328953174985, Validation Loss Force: 2.858129275218084, time: 0.10436606407165527
Test Loss Energy: 9.549312762467936, Test Loss Force: 10.69827683988184, time: 10.113191604614258


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.8198466883963746, Training Loss Force: 2.460489261946813, time: 1.3051238059997559
Validation Loss Energy: 1.222659413101679, Validation Loss Force: 2.768188642460358, time: 0.0962069034576416
Test Loss Energy: 9.509908866672756, Test Loss Force: 10.691755282619766, time: 10.241227865219116


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8453007341135634, Training Loss Force: 2.4499234960465284, time: 1.3946003913879395
Validation Loss Energy: 7.804692064977731, Validation Loss Force: 2.986622064559735, time: 0.09408354759216309
Test Loss Energy: 10.116098481136339, Test Loss Force: 10.60595096532358, time: 10.149362564086914


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.505363335306958, Training Loss Force: 6.161425104522936, time: 1.3809964656829834
Validation Loss Energy: 6.856400953103494, Validation Loss Force: 8.481445946813139, time: 0.0952606201171875
Test Loss Energy: 10.366324554344486, Test Loss Force: 13.380009052804404, time: 10.105836868286133


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.52911339410075, Training Loss Force: 5.565152421196512, time: 1.3222246170043945
Validation Loss Energy: 8.864598535608877, Validation Loss Force: 3.9327283467741285, time: 0.09438467025756836
Test Loss Energy: 10.178045732354452, Test Loss Force: 10.82191292952808, time: 10.332188606262207


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 17.169199855412696, Training Loss Force: 5.733506229530457, time: 1.377117395401001
Validation Loss Energy: 11.445148230844232, Validation Loss Force: 10.678081524773683, time: 0.1043999195098877
Test Loss Energy: 12.068164229865209, Test Loss Force: 14.771074167172923, time: 10.79637861251831


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.264573266569998, Training Loss Force: 7.0859738266090115, time: 1.3437154293060303
Validation Loss Energy: 6.796251559383167, Validation Loss Force: 8.345206720553804, time: 0.09634089469909668
Test Loss Energy: 9.998096694977951, Test Loss Force: 13.719730635569821, time: 10.095067501068115


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.818810062754835, Training Loss Force: 6.099467821698149, time: 1.3927063941955566
Validation Loss Energy: 1.96793454753872, Validation Loss Force: 6.343297314729423, time: 0.09320664405822754
Test Loss Energy: 10.384743214487619, Test Loss Force: 12.017242295681834, time: 10.30785059928894


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.511307270365645, Training Loss Force: 5.523921261486317, time: 1.3592894077301025
Validation Loss Energy: 7.014736752058972, Validation Loss Force: 5.241544631504068, time: 0.09722471237182617
Test Loss Energy: 13.330974802717572, Test Loss Force: 11.73240239933539, time: 10.154932975769043


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.133057437672019, Training Loss Force: 4.704041006285197, time: 1.3318724632263184
Validation Loss Energy: 6.392158050758649, Validation Loss Force: 4.935079073896714, time: 0.09356808662414551
Test Loss Energy: 9.747889358620661, Test Loss Force: 11.417979477672999, time: 10.324492454528809


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 12.088295075091544, Training Loss Force: 4.514423592221352, time: 1.3885293006896973
Validation Loss Energy: 7.144508330305687, Validation Loss Force: 4.467152110532494, time: 0.09337043762207031
Test Loss Energy: 10.1623123781731, Test Loss Force: 11.194958677707634, time: 10.192151069641113


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.57833873221197, Training Loss Force: 5.285224847744398, time: 1.3264193534851074
Validation Loss Energy: 15.392154345924396, Validation Loss Force: 5.218130302802324, time: 0.0964505672454834
Test Loss Energy: 20.512518894653407, Test Loss Force: 12.104149447791329, time: 10.167758464813232


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.635817328672482, Training Loss Force: 5.05858204640435, time: 1.3253321647644043
Validation Loss Energy: 27.937768677668814, Validation Loss Force: 6.310820797126709, time: 0.0980067253112793
Test Loss Energy: 21.923337285072165, Test Loss Force: 11.840877186991658, time: 10.26038932800293


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.130942603997621, Training Loss Force: 5.300809298456036, time: 1.3229169845581055
Validation Loss Energy: 8.407616874112737, Validation Loss Force: 5.329608052027049, time: 0.09287619590759277
Test Loss Energy: 13.497621916405546, Test Loss Force: 11.508221947784486, time: 10.16298532485962


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.623277030952078, Training Loss Force: 4.869827070645645, time: 1.325307846069336
Validation Loss Energy: 8.462789835090724, Validation Loss Force: 4.333411900097907, time: 0.1004326343536377
Test Loss Energy: 12.337874021218255, Test Loss Force: 11.589843288774999, time: 10.229779958724976


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.967461877540227, Training Loss Force: 4.8108713559365075, time: 1.4805541038513184
Validation Loss Energy: 31.834952571347635, Validation Loss Force: 5.811995149727819, time: 0.0959925651550293
Test Loss Energy: 31.133519673507458, Test Loss Force: 12.354246941808677, time: 10.160337686538696


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.911190479460064, Training Loss Force: 5.962443885425647, time: 1.3518049716949463
Validation Loss Energy: 7.955376729409984, Validation Loss Force: 6.112209271063252, time: 0.09594869613647461
Test Loss Energy: 11.941390594387613, Test Loss Force: 11.724329306780634, time: 10.092952966690063

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–â–‚â–â–â–‚â–â–â–…â–…â–‚â–‚â–ˆâ–‚
wandb:   test_error_force â–â–â–â–â–â–â–†â–â–ˆâ–†â–ƒâ–ƒâ–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–„â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–„â–â–†â–…â–ƒâ–ƒâ–‚â–‚â–…â–…â–ƒâ–ƒâ–ˆâ–ƒ
wandb: train_error_energy â–â–â–â–‚â–â–â–†â–„â–ˆâ–…â–„â–„â–…â–†â–†â–…â–„â–…â–…â–†
wandb:  train_error_force â–â–â–â–â–â–â–‡â–†â–†â–ˆâ–‡â–†â–„â–„â–…â–…â–…â–…â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–†â–ˆâ–ˆâ–‡â–†â–†â–†â–†â–†â–†â–†â–†â–‡
wandb: valid_error_energy â–â–â–â–â–â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–‚â–‚â–‚â–„â–‡â–ƒâ–ƒâ–ˆâ–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–†â–‚â–ˆâ–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–„â–„
wandb:         valid_loss â–â–â–â–â–â–‚â–†â–ƒâ–ˆâ–†â–„â–„â–ƒâ–ƒâ–…â–‡â–„â–ƒâ–‡â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 2971
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.94139
wandb:   test_error_force 11.72433
wandb:          test_loss 4.72212
wandb: train_error_energy 11.91119
wandb:  train_error_force 5.96244
wandb:         train_loss 2.79216
wandb: valid_error_energy 7.95538
wandb:  valid_error_force 6.11221
wandb:         valid_loss 2.57754
wandb: 
wandb: ğŸš€ View run al_77_24 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/qqgn7xba
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_152304-qqgn7xba/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7603363990783691, Uncertainty Bias: -0.4027678966522217
6.1035156e-05 1.8266964
-4.8234196 43.926136
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 5 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 3 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 2 steps.
Found uncertainty sample 64 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 5 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 5 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 3 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 4 steps.
Found uncertainty sample 94 after 6 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_153141-n044vux8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_25
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/n044vux8
Training model 25. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9963125202546155, Training Loss Force: 2.6215616848327588, time: 1.3614864349365234
Validation Loss Energy: 2.551159037991484, Validation Loss Force: 2.920707543982191, time: 0.09271764755249023
Test Loss Energy: 10.900347588251831, Test Loss Force: 10.7044224010552, time: 9.44638729095459


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5488218585658906, Training Loss Force: 2.5401749383441956, time: 1.383025884628296
Validation Loss Energy: 2.4586203016390735, Validation Loss Force: 2.875241399082634, time: 0.09485411643981934
Test Loss Energy: 10.27530881374535, Test Loss Force: 10.646966477443403, time: 9.544360637664795


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5900241375042494, Training Loss Force: 2.536861507798747, time: 1.3805620670318604
Validation Loss Energy: 1.8784810592049985, Validation Loss Force: 2.8378214037711773, time: 0.09335684776306152
Test Loss Energy: 10.391629091115028, Test Loss Force: 10.77074019760635, time: 9.709128379821777


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.747596202164427, Training Loss Force: 2.517324899647637, time: 1.3773822784423828
Validation Loss Energy: 1.7056683700597395, Validation Loss Force: 2.8398447983002235, time: 0.09695029258728027
Test Loss Energy: 9.38945315460877, Test Loss Force: 10.643495944476902, time: 10.04775595664978


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.0813088023518964, Training Loss Force: 2.4835676146317205, time: 1.4038419723510742
Validation Loss Energy: 3.6452487852288797, Validation Loss Force: 2.934052105897644, time: 0.09169673919677734
Test Loss Energy: 11.70530335443071, Test Loss Force: 10.8858125624706, time: 9.55224871635437


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.886670223270879, Training Loss Force: 2.4847603326340013, time: 1.3594067096710205
Validation Loss Energy: 1.385624837604714, Validation Loss Force: 2.8319167746940375, time: 0.09213733673095703
Test Loss Energy: 10.239288742851766, Test Loss Force: 10.849193560538362, time: 9.606327533721924


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 13.947715409279406, Training Loss Force: 6.309009543668036, time: 1.3694770336151123
Validation Loss Energy: 11.710940819587272, Validation Loss Force: 7.5527824521449975, time: 0.09164977073669434
Test Loss Energy: 12.591913160156146, Test Loss Force: 12.65125724351376, time: 9.45848560333252


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 16.238894128618302, Training Loss Force: 7.668984484499003, time: 1.4169461727142334
Validation Loss Energy: 9.663675267421421, Validation Loss Force: 7.90205993843245, time: 0.09421563148498535
Test Loss Energy: 11.205875266913958, Test Loss Force: 13.020533359270898, time: 9.60867977142334


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.914452635480757, Training Loss Force: 5.972672022499474, time: 1.3655900955200195
Validation Loss Energy: 9.909835117568008, Validation Loss Force: 3.7903800811616932, time: 0.09433770179748535
Test Loss Energy: 10.8446998919915, Test Loss Force: 10.694090007086452, time: 9.525845289230347


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.503260869211567, Training Loss Force: 3.502582652452222, time: 1.4040756225585938
Validation Loss Energy: 3.2726384671470923, Validation Loss Force: 3.5936674775747077, time: 0.09460306167602539
Test Loss Energy: 9.431380267217179, Test Loss Force: 10.705615819076035, time: 9.56114912033081


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.548354902655432, Training Loss Force: 3.4014667284323163, time: 1.4510200023651123
Validation Loss Energy: 10.70234934431177, Validation Loss Force: 3.738438717498556, time: 0.09032177925109863
Test Loss Energy: 10.931190902280816, Test Loss Force: 10.89083186077224, time: 9.617797613143921


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.761330051057236, Training Loss Force: 3.2860542530361276, time: 1.3209004402160645
Validation Loss Energy: 6.442901454416695, Validation Loss Force: 3.384695935493241, time: 0.09620904922485352
Test Loss Energy: 9.671332050279789, Test Loss Force: 10.774874289290944, time: 9.500163793563843


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.937311709500581, Training Loss Force: 3.3653423660190245, time: 1.3357751369476318
Validation Loss Energy: 8.12681282222783, Validation Loss Force: 3.846235745741095, time: 0.09427070617675781
Test Loss Energy: 10.118586140834646, Test Loss Force: 10.919954923912735, time: 9.465002536773682


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.848385612381279, Training Loss Force: 3.8756717416703377, time: 1.3488192558288574
Validation Loss Energy: 8.290589987102345, Validation Loss Force: 6.099703503820509, time: 0.0924382209777832
Test Loss Energy: 10.375476386812506, Test Loss Force: 11.656119386739707, time: 9.596948146820068


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.902022191044418, Training Loss Force: 5.430747766853514, time: 1.3362693786621094
Validation Loss Energy: 3.557969048764992, Validation Loss Force: 4.200553682150343, time: 0.09839749336242676
Test Loss Energy: 11.79290732340173, Test Loss Force: 11.145444194392418, time: 9.49002981185913


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 14.712247932876707, Training Loss Force: 6.035564623555113, time: 1.3949193954467773
Validation Loss Energy: 2.3950961343430643, Validation Loss Force: 7.529646237206517, time: 0.09410262107849121
Test Loss Energy: 10.574419974360318, Test Loss Force: 12.61141450064036, time: 9.574674606323242


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 14.273806126614105, Training Loss Force: 6.97695584253455, time: 1.3711705207824707
Validation Loss Energy: 10.590160612504096, Validation Loss Force: 4.5343868364762425, time: 0.09637641906738281
Test Loss Energy: 10.801620295179276, Test Loss Force: 10.825646114158985, time: 9.618987798690796


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.76384675533543, Training Loss Force: 4.863761697565027, time: 1.3531544208526611
Validation Loss Energy: 1.6693923931527734, Validation Loss Force: 4.563682079991087, time: 0.09585762023925781
Test Loss Energy: 9.65098059863386, Test Loss Force: 11.282083224851975, time: 10.073867082595825


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 12.364684775841729, Training Loss Force: 5.0995993089058915, time: 1.32330322265625
Validation Loss Energy: 6.766919352790099, Validation Loss Force: 5.213184598402122, time: 0.09205150604248047
Test Loss Energy: 9.448795773712732, Test Loss Force: 11.508307415430696, time: 9.661775588989258


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 14.994980960614376, Training Loss Force: 5.107847280241133, time: 1.407686471939087
Validation Loss Energy: 20.951194673848498, Validation Loss Force: 6.7893851109301915, time: 0.09848165512084961
Test Loss Energy: 19.50369157194912, Test Loss Force: 12.824259651726026, time: 9.554819107055664

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.048 MB uploadedwandb: | 0.039 MB of 0.048 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–‚â–â–ƒâ–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–‚â–ƒâ–‚â–‚â–â–â–ˆ
wandb:   test_error_force â–â–â–â–â–‚â–‚â–‡â–ˆâ–â–â–‚â–â–‚â–„â–‚â–‡â–‚â–ƒâ–„â–‡
wandb:          test_loss â–‚â–â–‚â–â–‚â–‚â–…â–†â–‚â–â–‚â–â–‚â–ƒâ–ƒâ–…â–‚â–‚â–‚â–ˆ
wandb: train_error_energy â–‚â–â–â–â–â–â–‡â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–‡â–‡â–…â–†â–‡
wandb:  train_error_force â–â–â–â–â–â–â–†â–ˆâ–†â–‚â–‚â–‚â–‚â–ƒâ–…â–†â–‡â–„â–…â–…
wandb:         train_loss â–â–â–â–â–â–â–†â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–†â–‡â–„â–…â–†
wandb: valid_error_energy â–â–â–â–â–‚â–â–…â–„â–„â–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–â–„â–â–ƒâ–ˆ
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–‚â–‚â–‚â–‚â–‚â–†â–ƒâ–‡â–ƒâ–ƒâ–„â–†
wandb:         valid_loss â–â–â–â–â–â–â–‡â–‡â–ƒâ–‚â–ƒâ–‚â–ƒâ–…â–ƒâ–…â–„â–ƒâ–„â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3061
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 19.50369
wandb:   test_error_force 12.82426
wandb:          test_loss 5.59624
wandb: train_error_energy 14.99498
wandb:  train_error_force 5.10785
wandb:         train_loss 2.71258
wandb: valid_error_energy 20.95119
wandb:  valid_error_force 6.78939
wandb:         valid_loss 3.67382
wandb: 
wandb: ğŸš€ View run al_77_25 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/n044vux8
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_153141-n044vux8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.421541690826416, Uncertainty Bias: -1.0766693353652954
8.392334e-05 0.03412819
-7.5696554 48.518753
(48745, 22, 3)
Found uncertainty sample 0 after 6 steps.
Found uncertainty sample 1 after 4 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 5 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 10 steps.
Found uncertainty sample 9 after 6 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 10 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 7 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 3 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 2 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 7 steps.
Found uncertainty sample 32 after 7 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 7 steps.
Found uncertainty sample 35 after 2 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 3 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 8 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 5 steps.
Found uncertainty sample 45 after 11 steps.
Found uncertainty sample 46 after 8 steps.
Found uncertainty sample 47 after 5 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 6 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 5 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 5 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 26 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 4 steps.
Found uncertainty sample 66 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 12 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 4 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 4 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 9 steps.
Found uncertainty sample 81 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 4 steps.
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 5 steps.
Found uncertainty sample 93 after 6 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_154002-jkep6osy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_26
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/jkep6osy
Training model 26. Added 100 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.8378160822035077, Training Loss Force: 2.6708136469667445, time: 1.4432940483093262
Validation Loss Energy: 1.8724759506757582, Validation Loss Force: 2.8583983708809373, time: 0.09644007682800293
Test Loss Energy: 9.36807112953352, Test Loss Force: 10.655697411598496, time: 9.478666305541992


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.561721783068651, Training Loss Force: 2.483619661295397, time: 1.4801785945892334
Validation Loss Energy: 2.1994948724634042, Validation Loss Force: 2.8405977540171783, time: 0.09676599502563477
Test Loss Energy: 10.464703634841864, Test Loss Force: 10.789997455400806, time: 9.485356330871582


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5151509000639145, Training Loss Force: 2.488040206137887, time: 1.410529613494873
Validation Loss Energy: 1.6543326108062393, Validation Loss Force: 2.8107865860912202, time: 0.09678459167480469
Test Loss Energy: 10.343879737596374, Test Loss Force: 10.866556694318144, time: 9.718643188476562


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.487763879350749, Training Loss Force: 2.469962343695615, time: 1.3906548023223877
Validation Loss Energy: 3.7585260351564207, Validation Loss Force: 2.842888908194202, time: 0.09205245971679688
Test Loss Energy: 11.561364208120667, Test Loss Force: 10.90527391110988, time: 9.453857898712158


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5858245221121188, Training Loss Force: 2.4523331078511714, time: 1.3931593894958496
Validation Loss Energy: 1.7589891690818993, Validation Loss Force: 2.8377599470869015, time: 0.09466314315795898
Test Loss Energy: 10.145282254844602, Test Loss Force: 10.812891299054268, time: 9.467748165130615


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5326571112052327, Training Loss Force: 2.4611448704352035, time: 1.4035472869873047
Validation Loss Energy: 2.3189224668706165, Validation Loss Force: 2.724290199682706, time: 0.09134674072265625
Test Loss Energy: 10.63865445506603, Test Loss Force: 10.808698679192702, time: 9.78414249420166


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.217368464965133, Training Loss Force: 3.8681362897428158, time: 1.4224696159362793
Validation Loss Energy: 3.654326004297199, Validation Loss Force: 3.4001340327187064, time: 0.0948038101196289
Test Loss Energy: 10.885380714897888, Test Loss Force: 11.370272345022352, time: 9.48606824874878


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.945833229761051, Training Loss Force: 4.226111442044254, time: 1.4232077598571777
Validation Loss Energy: 2.24943666719611, Validation Loss Force: 3.985408081325162, time: 0.09304118156433105
Test Loss Energy: 9.478166915929545, Test Loss Force: 10.824487777603796, time: 9.588161706924438


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.759908632724029, Training Loss Force: 3.4184292944704526, time: 1.3758459091186523
Validation Loss Energy: 7.492674474885813, Validation Loss Force: 3.3245254995386233, time: 0.09617090225219727
Test Loss Energy: 13.609507668951453, Test Loss Force: 11.193879942457597, time: 9.586043357849121


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.396836498060216, Training Loss Force: 3.7704547969173823, time: 1.3854765892028809
Validation Loss Energy: 1.9693360271431892, Validation Loss Force: 5.986704478990805, time: 0.09357881546020508
Test Loss Energy: 12.457182705055404, Test Loss Force: 12.289920147837773, time: 9.57141375541687


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.573841397628499, Training Loss Force: 5.49179740370725, time: 1.3797633647918701
Validation Loss Energy: 9.82499817296952, Validation Loss Force: 7.714454948252203, time: 0.09500980377197266
Test Loss Energy: 16.360133794898953, Test Loss Force: 13.325018270162, time: 9.70533037185669


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.400586907642115, Training Loss Force: 6.53662243910431, time: 1.4874722957611084
Validation Loss Energy: 4.422843622522615, Validation Loss Force: 6.6984815454509015, time: 0.1006629467010498
Test Loss Energy: 10.077581470720355, Test Loss Force: 12.87930868741495, time: 9.443881750106812


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 13.429059418931313, Training Loss Force: 6.752549496172531, time: 1.4030871391296387
Validation Loss Energy: 26.169039069467452, Validation Loss Force: 8.058713026652152, time: 0.09636139869689941
Test Loss Energy: 29.634569133577077, Test Loss Force: 13.540940979311625, time: 9.440518379211426


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 14.498233124133671, Training Loss Force: 7.2234475024623, time: 1.4412798881530762
Validation Loss Energy: 13.865936821921029, Validation Loss Force: 8.002326073490565, time: 0.09589266777038574
Test Loss Energy: 16.477051501747802, Test Loss Force: 13.270613296234126, time: 10.244334697723389


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.197481613497203, Training Loss Force: 6.056356971262054, time: 1.4015791416168213
Validation Loss Energy: 18.67397780400644, Validation Loss Force: 7.20821709032367, time: 0.0938560962677002
Test Loss Energy: 15.329852903872576, Test Loss Force: 12.24659499941891, time: 9.534002780914307


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.620060914439334, Training Loss Force: 5.257766994202507, time: 1.420454502105713
Validation Loss Energy: 6.790310746569814, Validation Loss Force: 4.237076098134564, time: 0.0981757640838623
Test Loss Energy: 11.867269053341783, Test Loss Force: 11.262506848363003, time: 9.572556972503662


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 14.147006708153018, Training Loss Force: 5.627507201458982, time: 1.3723475933074951
Validation Loss Energy: 14.09043987093468, Validation Loss Force: 8.531684114179074, time: 0.09401273727416992
Test Loss Energy: 12.14695646492634, Test Loss Force: 13.23871181179141, time: 9.713565111160278


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.845630020056195, Training Loss Force: 6.284596606472502, time: 1.447643518447876
Validation Loss Energy: 2.3090051171147117, Validation Loss Force: 5.7318514027180525, time: 0.09429264068603516
Test Loss Energy: 10.397813823405158, Test Loss Force: 12.276700208364007, time: 9.378738164901733


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.350810358395239, Training Loss Force: 3.6599619538103942, time: 1.3777387142181396
Validation Loss Energy: 6.922206365799533, Validation Loss Force: 3.524820181676149, time: 0.09508943557739258
Test Loss Energy: 9.636498299973738, Test Loss Force: 10.502793157803493, time: 9.647876024246216


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.452203046387286, Training Loss Force: 3.301362936542466, time: 1.4259634017944336
Validation Loss Energy: 6.008111222672865, Validation Loss Force: 3.5379208095422374, time: 0.09533452987670898
Test Loss Energy: 9.436581370796148, Test Loss Force: 10.858574250256115, time: 9.530879974365234

wandb: - 0.039 MB of 0.055 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–â–â–‚â–â–‚â–‚â–ƒâ–â–ˆâ–ƒâ–ƒâ–‚â–‚â–â–â–
wandb:   test_error_force â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–…â–ˆâ–†â–ˆâ–‡â–…â–ƒâ–‡â–…â–â–‚
wandb:          test_loss â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–ƒâ–…â–ƒâ–ˆâ–…â–„â–‚â–„â–ƒâ–â–
wandb: train_error_energy â–‚â–â–â–‚â–â–â–„â–†â–„â–…â–‡â–†â–‡â–ˆâ–†â–†â–ˆâ–‡â–„â–„
wandb:  train_error_force â–â–â–â–â–â–â–ƒâ–„â–‚â–ƒâ–…â–‡â–‡â–ˆâ–†â–…â–†â–‡â–ƒâ–‚
wandb:         train_loss â–â–â–â–â–â–â–ƒâ–„â–ƒâ–„â–†â–‡â–‡â–ˆâ–†â–…â–†â–‡â–ƒâ–ƒ
wandb: valid_error_energy â–â–â–â–‚â–â–â–‚â–â–ƒâ–â–ƒâ–‚â–ˆâ–„â–†â–‚â–…â–â–ƒâ–‚
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–ƒâ–‚â–…â–‡â–†â–‡â–‡â–†â–ƒâ–ˆâ–…â–‚â–‚
wandb:         valid_loss â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–†â–„â–ˆâ–†â–†â–ƒâ–‡â–ƒâ–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 3151
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.43658
wandb:   test_error_force 10.85857
wandb:          test_loss 4.26482
wandb: train_error_energy 6.4522
wandb:  train_error_force 3.30136
wandb:         train_loss 1.53643
wandb: valid_error_energy 6.00811
wandb:  valid_error_force 3.53792
wandb:         valid_loss 1.58587
wandb: 
wandb: ğŸš€ View run al_77_26 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/jkep6osy
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_154002-jkep6osy/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.2455426454544067, Uncertainty Bias: -0.9549895524978638
2.2888184e-05 0.008520126
-10.547312 81.901054
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 5 steps.
Found uncertainty sample 2 after 3 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 11 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 6 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 5 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 3 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 9 steps.
Found uncertainty sample 31 after 2 steps.
Found uncertainty sample 32 after 3 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 2 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 7 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 2 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 12 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 11 steps.
Found uncertainty sample 58 after 7 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 3 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 7 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 3 steps.
Found uncertainty sample 75 after 4 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 4 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 5 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 4 steps.
Found uncertainty sample 88 after 9 steps.
Found uncertainty sample 89 after 4 steps.
Found uncertainty sample 90 after 9 steps.
Found uncertainty sample 91 after 3 steps.
Found uncertainty sample 92 after 4 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 5 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 2 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 5 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_154817-j6ovze6f
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_27
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/j6ovze6f
Training model 27. Added 100 samples to the dataset.
Epoch 0, Batch 100/102, Loss: 0.11989905685186386

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.5365905762069545, Training Loss Force: 2.6514735228166972, time: 1.4419119358062744
Validation Loss Energy: 1.4084334505622484, Validation Loss Force: 2.804262268780963, time: 0.10170102119445801
Test Loss Energy: 9.525384680087798, Test Loss Force: 10.651365086083674, time: 9.581330060958862

Epoch 1, Batch 100/102, Loss: 0.07398335635662079

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.462591112274823, Training Loss Force: 2.4793566278173, time: 1.4105644226074219
Validation Loss Energy: 1.4124080948170596, Validation Loss Force: 2.8400732568439455, time: 0.09684157371520996
Test Loss Energy: 9.418402363275998, Test Loss Force: 10.743404904235264, time: 9.544063329696655

Epoch 2, Batch 100/102, Loss: 0.08596033602952957

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.4334867253601007, Training Loss Force: 2.5057916119296517, time: 1.4692609310150146
Validation Loss Energy: 3.238979465044929, Validation Loss Force: 2.8137775893938946, time: 0.1005103588104248
Test Loss Energy: 9.520164531884914, Test Loss Force: 10.717634025993114, time: 9.785772562026978

Epoch 3, Batch 100/102, Loss: 0.22808152437210083

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5379922697484467, Training Loss Force: 2.446804171303701, time: 1.5273568630218506
Validation Loss Energy: 2.482909720084314, Validation Loss Force: 2.7535080656693394, time: 0.09906435012817383
Test Loss Energy: 9.433128786594938, Test Loss Force: 10.686786221275383, time: 9.586143970489502

Epoch 4, Batch 100/102, Loss: 0.060539186000823975

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.8169675446327658, Training Loss Force: 2.4264868276859697, time: 1.4920382499694824
Validation Loss Energy: 2.291102851785837, Validation Loss Force: 2.7340775106573885, time: 0.10518908500671387
Test Loss Energy: 10.61803543204996, Test Loss Force: 10.76486582180498, time: 9.570542335510254

Epoch 5, Batch 100/102, Loss: 0.06532316654920578

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.442961084456918, Training Loss Force: 2.4172745554145836, time: 1.459390640258789
Validation Loss Energy: 1.5976620328200282, Validation Loss Force: 2.75026522356262, time: 0.13998961448669434
Test Loss Energy: 9.985841003838999, Test Loss Force: 10.826249978893053, time: 9.729506015777588

Epoch 6, Batch 100/102, Loss: 0.8022686839103699

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.96506269710306, Training Loss Force: 5.462469437552368, time: 1.4419949054718018
Validation Loss Energy: 22.744506955760293, Validation Loss Force: 5.369989769648491, time: 0.10204100608825684
Test Loss Energy: 25.519435570015837, Test Loss Force: 12.073531101879766, time: 9.59027910232544

Epoch 7, Batch 100/102, Loss: 1.4232237339019775

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.132174837809485, Training Loss Force: 5.2796952457609, time: 1.4340953826904297
Validation Loss Energy: 10.59859531975184, Validation Loss Force: 5.130980271115938, time: 0.10190105438232422
Test Loss Energy: 15.769627442219539, Test Loss Force: 12.444393344756282, time: 9.748744249343872

Epoch 8, Batch 100/102, Loss: 0.22370420396327972

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 12.089709073638486, Training Loss Force: 5.394179312249658, time: 1.4286930561065674
Validation Loss Energy: 14.497195389198763, Validation Loss Force: 4.114413900379031, time: 0.0967264175415039
Test Loss Energy: 19.227582073486523, Test Loss Force: 11.783184809731413, time: 9.63618516921997

Epoch 9, Batch 100/102, Loss: 0.1668713092803955

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.61646152599416, Training Loss Force: 5.233714095506214, time: 1.4518513679504395
Validation Loss Energy: 9.792456858645341, Validation Loss Force: 3.960081247329044, time: 0.09966921806335449
Test Loss Energy: 15.818913921655778, Test Loss Force: 10.85277863637882, time: 10.317965745925903

Epoch 10, Batch 100/102, Loss: 0.19306249916553497

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.92425815107849, Training Loss Force: 5.688069145207894, time: 1.4241437911987305
Validation Loss Energy: 6.458545965940435, Validation Loss Force: 9.449589571730858, time: 0.09689497947692871
Test Loss Energy: 10.583956737662259, Test Loss Force: 14.627170023053651, time: 9.6494779586792

Epoch 11, Batch 100/102, Loss: 0.2485799491405487

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 16.40465269435372, Training Loss Force: 6.96908752072872, time: 1.4557979106903076
Validation Loss Energy: 6.842472998025711, Validation Loss Force: 5.93553889463208, time: 0.09939002990722656
Test Loss Energy: 14.631200465165335, Test Loss Force: 12.710662084071423, time: 9.551890850067139

Epoch 12, Batch 100/102, Loss: 0.2415781021118164

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.731599465894523, Training Loss Force: 4.346174271701023, time: 1.454176664352417
Validation Loss Energy: 3.455616169805814, Validation Loss Force: 7.083031160468192, time: 0.09962177276611328
Test Loss Energy: 9.59600119426823, Test Loss Force: 12.46670276001076, time: 9.583219528198242

Epoch 13, Batch 100/102, Loss: 0.48767438530921936

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.079616804827982, Training Loss Force: 5.445339070099068, time: 1.4390196800231934
Validation Loss Energy: 5.89296767129873, Validation Loss Force: 5.15570055435855, time: 0.09711861610412598
Test Loss Energy: 13.592378246841943, Test Loss Force: 11.81167495622623, time: 9.715071201324463

Epoch 14, Batch 100/102, Loss: 0.9444408416748047

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 15.029035785912146, Training Loss Force: 6.972759035869846, time: 1.4466633796691895
Validation Loss Energy: 20.49541765243183, Validation Loss Force: 7.338493904862494, time: 0.1009225845336914
Test Loss Energy: 14.786832758745222, Test Loss Force: 12.105950708279881, time: 9.623583555221558

Epoch 15, Batch 100/102, Loss: 0.6222329139709473

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.856313756841308, Training Loss Force: 6.074014253959087, time: 1.4982333183288574
Validation Loss Energy: 17.93539493508297, Validation Loss Force: 5.762975873266693, time: 0.10072875022888184
Test Loss Energy: 20.989534933926514, Test Loss Force: 11.988963831767334, time: 9.79710578918457

Epoch 16, Batch 100/102, Loss: 1.5937473773956299

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.356521346705195, Training Loss Force: 5.771389839538403, time: 1.4094102382659912
Validation Loss Energy: 18.53752882341164, Validation Loss Force: 4.87593837140744, time: 0.10434532165527344
Test Loss Energy: 13.843352208586715, Test Loss Force: 11.187148511947742, time: 9.607773780822754

Epoch 17, Batch 100/102, Loss: 0.3053427040576935

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.789358473587851, Training Loss Force: 4.89962639105196, time: 1.4386279582977295
Validation Loss Energy: 11.959562945149528, Validation Loss Force: 4.683493222002159, time: 0.09712934494018555
Test Loss Energy: 14.940607357449041, Test Loss Force: 11.238414186670465, time: 9.666194915771484

Epoch 18, Batch 100/102, Loss: 0.5985036492347717

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.537349574105466, Training Loss Force: 5.22135095658356, time: 1.4268319606781006
Validation Loss Energy: 19.59974437308606, Validation Loss Force: 6.265529892844997, time: 0.09873366355895996
Test Loss Energy: 24.03680960232604, Test Loss Force: 12.986555068849292, time: 9.690632581710815

Epoch 19, Batch 100/102, Loss: 3.960702419281006

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.53005627042507, Training Loss Force: 4.657732388376381, time: 1.443157434463501
Validation Loss Energy: 33.436992947584045, Validation Loss Force: 6.92697905627335, time: 0.0965120792388916
Test Loss Energy: 33.14847458026301, Test Loss Force: 13.023828046565697, time: 9.725833177566528

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–†â–ƒâ–„â–ƒâ–â–ƒâ–â–‚â–ƒâ–„â–‚â–ƒâ–…â–ˆ
wandb:   test_error_force â–â–â–â–â–â–â–„â–„â–ƒâ–â–ˆâ–…â–„â–ƒâ–„â–ƒâ–‚â–‚â–…â–…
wandb:          test_loss â–â–â–â–â–â–â–†â–„â–„â–‚â–…â–„â–ƒâ–ƒâ–ƒâ–…â–‚â–ƒâ–†â–ˆ
wandb: train_error_energy â–‚â–â–â–‚â–â–â–…â–†â–†â–†â–…â–ˆâ–ƒâ–…â–‡â–†â–…â–„â–…â–†
wandb:  train_error_force â–â–â–â–â–â–â–†â–…â–†â–…â–†â–ˆâ–„â–†â–ˆâ–‡â–†â–…â–…â–„
wandb:         train_loss â–â–â–â–â–â–â–†â–…â–†â–†â–…â–ˆâ–„â–…â–ˆâ–†â–†â–„â–…â–…
wandb: valid_error_energy â–â–â–â–â–â–â–†â–ƒâ–„â–ƒâ–‚â–‚â–â–‚â–…â–…â–…â–ƒâ–…â–ˆ
wandb:  valid_error_force â–â–â–â–â–â–â–„â–ƒâ–‚â–‚â–ˆâ–„â–†â–„â–†â–„â–ƒâ–ƒâ–…â–…
wandb:         valid_loss â–â–â–â–â–â–â–†â–„â–„â–ƒâ–†â–„â–„â–ƒâ–‡â–…â–…â–„â–†â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3241
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 33.14847
wandb:   test_error_force 13.02383
wandb:          test_loss 6.57613
wandb: train_error_energy 12.53006
wandb:  train_error_force 4.65773
wandb:         train_loss 2.39701
wandb: valid_error_energy 33.43699
wandb:  valid_error_force 6.92698
wandb:         valid_loss 4.55541
wandb: 
wandb: ğŸš€ View run al_77_27 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/j6ovze6f
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_154817-j6ovze6f/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7143535614013672, Uncertainty Bias: -0.4680324196815491
3.8146973e-05 0.07317543
-4.8016906 39.55183
(48745, 22, 3)
Found uncertainty sample 0 after 8 steps.
Found uncertainty sample 1 after 7 steps.
Found uncertainty sample 2 after 2 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 8 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 8 steps.
Found uncertainty sample 11 after 7 steps.
Found uncertainty sample 12 after 2 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 6 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 3 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 7 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 6 steps.
Found uncertainty sample 28 after 28 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 6 steps.
Found uncertainty sample 31 after 12 steps.
Found uncertainty sample 32 after 7 steps.
Found uncertainty sample 33 after 7 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 12 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 10 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 10 steps.
Found uncertainty sample 41 after 4 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 18 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 7 steps.
Found uncertainty sample 53 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 9 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 12 steps.
Found uncertainty sample 59 after 9 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 6 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 9 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 6 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 7 steps.
Found uncertainty sample 78 after 5 steps.
Found uncertainty sample 79 after 16 steps.
Found uncertainty sample 80 after 10 steps.
Found uncertainty sample 81 after 3 steps.
Found uncertainty sample 82 after 3 steps.
Found uncertainty sample 83 after 4 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 21 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 2 steps.
Found uncertainty sample 91 after 17 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 15 steps.
Found uncertainty sample 94 after 23 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 11 steps.
Found uncertainty sample 97 after 5 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_155645-tzyb9fs9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_28
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/tzyb9fs9
Training model 28. Added 100 samples to the dataset.
Epoch 0, Batch 100/105, Loss: 0.07698307931423187

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.9343607916044787, Training Loss Force: 2.603683325091087, time: 1.5080811977386475
Validation Loss Energy: 2.86758202580101, Validation Loss Force: 2.930386303748234, time: 0.10537219047546387
Test Loss Energy: 9.33227440395395, Test Loss Force: 10.677798245319394, time: 9.734938383102417

Epoch 1, Batch 100/105, Loss: 0.24266712367534637

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5842634454741895, Training Loss Force: 2.48932346519835, time: 1.4829685688018799
Validation Loss Energy: 1.7981491010996358, Validation Loss Force: 2.763777010006664, time: 0.10072207450866699
Test Loss Energy: 9.277686351687452, Test Loss Force: 10.61245419143565, time: 9.817291021347046

Epoch 2, Batch 100/105, Loss: 0.256560742855072

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5664523610399135, Training Loss Force: 2.4573530130652323, time: 1.4878523349761963
Validation Loss Energy: 2.725927708581859, Validation Loss Force: 2.7468007143211297, time: 0.10255765914916992
Test Loss Energy: 10.28728238424222, Test Loss Force: 10.806954426668735, time: 10.01686406135559

Epoch 3, Batch 100/105, Loss: 0.1132218986749649

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.5679254245334806, Training Loss Force: 2.4647700184038195, time: 1.4516665935516357
Validation Loss Energy: 4.09953603872472, Validation Loss Force: 2.7881491500396174, time: 0.10330510139465332
Test Loss Energy: 11.376271825909525, Test Loss Force: 10.82012998153662, time: 9.86591100692749

Epoch 4, Batch 100/105, Loss: 0.10769422352313995

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6528109142505423, Training Loss Force: 2.502851033583194, time: 1.4900872707366943
Validation Loss Energy: 1.574281271492032, Validation Loss Force: 2.7453682837053766, time: 0.10486865043640137
Test Loss Energy: 9.658974467604951, Test Loss Force: 10.796199851999303, time: 9.922958135604858

Epoch 5, Batch 100/105, Loss: 0.11358294636011124

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.605083324539674, Training Loss Force: 2.4104068902649614, time: 1.6915104389190674
Validation Loss Energy: 1.731919281011458, Validation Loss Force: 2.6846442922539824, time: 0.1034543514251709
Test Loss Energy: 9.975408121966051, Test Loss Force: 10.82704748417759, time: 9.895802021026611

Epoch 6, Batch 100/105, Loss: 0.4218170940876007

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.333877505607111, Training Loss Force: 6.040876444607553, time: 1.4568657875061035
Validation Loss Energy: 11.919481162867651, Validation Loss Force: 4.208189007164206, time: 0.0997471809387207
Test Loss Energy: 11.229322351296295, Test Loss Force: 10.829706638755702, time: 9.797701597213745

Epoch 7, Batch 100/105, Loss: 0.47176000475883484

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 13.907722137684816, Training Loss Force: 6.32091295170513, time: 1.4457826614379883
Validation Loss Energy: 16.78016870180135, Validation Loss Force: 9.157516234370547, time: 0.10073137283325195
Test Loss Energy: 14.070048793507235, Test Loss Force: 15.124576260735692, time: 10.587239742279053

Epoch 8, Batch 100/105, Loss: 0.9437787532806396

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 12.965082834218771, Training Loss Force: 6.931791163427311, time: 1.4715595245361328
Validation Loss Energy: 2.7043989422078307, Validation Loss Force: 9.313227432102973, time: 0.1068880558013916
Test Loss Energy: 8.782310754025254, Test Loss Force: 13.497410112581294, time: 9.810477495193481

Epoch 9, Batch 100/105, Loss: 0.6808109283447266

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.525279567293877, Training Loss Force: 5.1540264174452215, time: 1.4431612491607666
Validation Loss Energy: 12.434649389562765, Validation Loss Force: 5.858439034847146, time: 0.10230612754821777
Test Loss Energy: 16.023789461883386, Test Loss Force: 12.43581758418234, time: 9.934447765350342

Epoch 10, Batch 100/105, Loss: 1.8082339763641357

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.508569386558081, Training Loss Force: 5.990087794579055, time: 1.5144104957580566
Validation Loss Energy: 9.03214103574103, Validation Loss Force: 9.860480946711405, time: 0.10739469528198242
Test Loss Energy: 10.205930652452567, Test Loss Force: 13.761237558675987, time: 10.053082466125488

Epoch 11, Batch 100/105, Loss: 0.8104730844497681

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.729099986326048, Training Loss Force: 5.38269660288869, time: 1.5083882808685303
Validation Loss Energy: 12.576064782788336, Validation Loss Force: 5.135591303789811, time: 0.10437154769897461
Test Loss Energy: 16.590850609461423, Test Loss Force: 12.068490483298858, time: 9.820565462112427

Epoch 12, Batch 100/105, Loss: 0.42166146636009216

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 12.692506567385617, Training Loss Force: 5.141129134224181, time: 1.4519753456115723
Validation Loss Energy: 2.397378047750532, Validation Loss Force: 5.053666203612622, time: 0.10335540771484375
Test Loss Energy: 10.231647278697992, Test Loss Force: 11.480528108124558, time: 9.999839067459106

Epoch 13, Batch 100/105, Loss: 1.5310500860214233

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.392343992430748, Training Loss Force: 5.3771358863074346, time: 1.4755113124847412
Validation Loss Energy: 12.382962536733423, Validation Loss Force: 6.7950288831028764, time: 0.10188961029052734
Test Loss Energy: 17.73874411693785, Test Loss Force: 12.628597293396473, time: 9.654081344604492

Epoch 14, Batch 100/105, Loss: 1.1161166429519653

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.683934287844762, Training Loss Force: 4.932913228372055, time: 1.5011982917785645
Validation Loss Energy: 13.039633888297601, Validation Loss Force: 5.492636380563809, time: 0.09975218772888184
Test Loss Energy: 12.038950331702859, Test Loss Force: 11.8451749719753, time: 9.8198823928833

Epoch 15, Batch 100/105, Loss: 0.33691707253456116

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 13.608481415581135, Training Loss Force: 5.2015087572381065, time: 1.4391000270843506
Validation Loss Energy: 7.058686297405615, Validation Loss Force: 5.998344063255888, time: 0.10170888900756836
Test Loss Energy: 10.643664548155229, Test Loss Force: 11.892569868856237, time: 9.918505668640137

Epoch 16, Batch 100/105, Loss: 0.6221625804901123

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.613049382643013, Training Loss Force: 6.485139382331103, time: 1.4841980934143066
Validation Loss Energy: 11.808217398692161, Validation Loss Force: 5.014293399891378, time: 0.10019850730895996
Test Loss Energy: 11.286385696379945, Test Loss Force: 11.297192956201426, time: 9.732169151306152

Epoch 17, Batch 100/105, Loss: 1.1184074878692627

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.3550618712674, Training Loss Force: 4.6406018640567215, time: 1.4744336605072021
Validation Loss Energy: 16.661703372108263, Validation Loss Force: 6.162844902854668, time: 0.10160303115844727
Test Loss Energy: 13.94639068409749, Test Loss Force: 11.980249480051674, time: 9.94072961807251

Epoch 18, Batch 100/105, Loss: 1.1549663543701172

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 12.043687085519387, Training Loss Force: 4.486113770977608, time: 1.487624168395996
Validation Loss Energy: 13.907642752953304, Validation Loss Force: 5.020736999636236, time: 0.10018730163574219
Test Loss Energy: 18.461076512316627, Test Loss Force: 12.854080081988974, time: 9.924425601959229

Epoch 19, Batch 100/105, Loss: 0.8305540084838867

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.134153084920346, Training Loss Force: 5.427055087544133, time: 1.4974405765533447
Validation Loss Energy: 14.754019755420416, Validation Loss Force: 5.56668411436551, time: 0.1005098819732666
Test Loss Energy: 16.55106184811752, Test Loss Force: 12.271933218686648, time: 9.787736654281616

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–ƒâ–‚â–‚â–ƒâ–…â–â–†â–‚â–‡â–‚â–‡â–ƒâ–‚â–ƒâ–…â–ˆâ–‡
wandb:   test_error_force â–â–â–â–â–â–â–â–ˆâ–…â–„â–†â–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–ƒâ–„â–„
wandb:          test_loss â–â–â–‚â–‚â–â–â–‚â–ˆâ–…â–…â–…â–…â–‚â–†â–ƒâ–ƒâ–‚â–„â–†â–…
wandb: train_error_energy â–‚â–‚â–‚â–‚â–â–â–‡â–ˆâ–‡â–†â–‡â–†â–‡â–‡â–†â–ˆâ–†â–†â–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–‡â–‡â–ˆâ–…â–‡â–†â–…â–†â–…â–…â–‡â–„â–„â–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–ˆâ–ˆâ–†â–‡â–†â–†â–†â–…â–†â–‡â–…â–…â–†
wandb: valid_error_energy â–‚â–â–‚â–‚â–â–â–†â–ˆâ–‚â–†â–„â–†â–â–†â–†â–„â–†â–ˆâ–‡â–‡
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–‡â–‡â–„â–ˆâ–ƒâ–ƒâ–…â–„â–„â–ƒâ–„â–ƒâ–„
wandb:         valid_loss â–â–â–â–â–â–â–„â–ˆâ–†â–…â–‡â–„â–ƒâ–†â–…â–„â–„â–†â–…â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 3331
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 16.55106
wandb:   test_error_force 12.27193
wandb:          test_loss 5.21384
wandb: train_error_energy 12.13415
wandb:  train_error_force 5.42706
wandb:         train_loss 2.62794
wandb: valid_error_energy 14.75402
wandb:  valid_error_force 5.56668
wandb:         valid_loss 2.84998
wandb: 
wandb: ğŸš€ View run al_77_28 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/tzyb9fs9
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_155645-tzyb9fs9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7373957633972168, Uncertainty Bias: -0.4180344343185425
4.9591064e-05 0.00061035156
-4.5352488 36.5552
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 4 steps.
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 2 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 7 steps.
Found uncertainty sample 16 after 10 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 7 steps.
Found uncertainty sample 20 after 4 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 4 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 4 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 12 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 5 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 3 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 6 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 6 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 6 steps.
Found uncertainty sample 69 after 3 steps.
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 6 steps.
Found uncertainty sample 73 after 4 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 8 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 5 steps.
Found uncertainty sample 94 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 12 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 6 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_160511-w1h3xrtr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_29
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/w1h3xrtr
Training model 29. Added 100 samples to the dataset.
Epoch 0, Batch 100/107, Loss: 0.15294186770915985

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.210148999566098, Training Loss Force: 2.6358898765798955, time: 1.5050475597381592
Validation Loss Energy: 1.2704140401485091, Validation Loss Force: 2.796514495848405, time: 0.10146594047546387
Test Loss Energy: 9.604476485590887, Test Loss Force: 10.773165924971106, time: 9.782488584518433

Epoch 1, Batch 100/107, Loss: 0.054615117609500885

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.3424818493288477, Training Loss Force: 2.4719813327472444, time: 1.5681889057159424
Validation Loss Energy: 1.4867776412019138, Validation Loss Force: 2.7679977036677252, time: 0.10574769973754883
Test Loss Energy: 9.842846958648837, Test Loss Force: 10.89629561601561, time: 9.827388525009155

Epoch 2, Batch 100/107, Loss: 0.10120909661054611

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5001358795811703, Training Loss Force: 2.451742484573536, time: 1.5043396949768066
Validation Loss Energy: 1.0818670481464143, Validation Loss Force: 2.7389280239977065, time: 0.10395622253417969
Test Loss Energy: 9.619638109343601, Test Loss Force: 10.798023850388498, time: 9.957418441772461

Epoch 3, Batch 100/107, Loss: 0.05159648135304451

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.1265111958468939, Training Loss Force: 2.4240199386692396, time: 1.5431151390075684
Validation Loss Energy: 1.1123696037704782, Validation Loss Force: 2.7395511636025858, time: 0.10333037376403809
Test Loss Energy: 9.510522682735779, Test Loss Force: 10.866850785466985, time: 9.896790981292725

Epoch 4, Batch 100/107, Loss: 0.14086081087589264

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.8069015431717343, Training Loss Force: 2.4136245068114177, time: 1.4947614669799805
Validation Loss Energy: 1.8286089042476816, Validation Loss Force: 2.7154255183141744, time: 0.10163760185241699
Test Loss Energy: 9.352530954243043, Test Loss Force: 10.832749591003282, time: 9.757432699203491

Epoch 5, Batch 100/107, Loss: 0.054394371807575226

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.639427166271604, Training Loss Force: 2.374944174999729, time: 1.748234510421753
Validation Loss Energy: 1.0465909113719505, Validation Loss Force: 2.757147481524956, time: 0.10675263404846191
Test Loss Energy: 9.59208249709359, Test Loss Force: 10.81011600677816, time: 9.700387239456177

Epoch 6, Batch 100/107, Loss: 0.6514018774032593

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 14.725024313019343, Training Loss Force: 6.650858494018605, time: 1.563413143157959
Validation Loss Energy: 6.905396403966007, Validation Loss Force: 6.370737278920396, time: 0.10517501831054688
Test Loss Energy: 13.423343606954523, Test Loss Force: 11.95893076762552, time: 9.813282012939453

Epoch 7, Batch 100/107, Loss: 1.0338941812515259

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 12.192802148611557, Training Loss Force: 6.240262496943702, time: 1.4600210189819336
Validation Loss Energy: 2.6216568689342323, Validation Loss Force: 5.857308320924124, time: 0.10816645622253418
Test Loss Energy: 8.803548648043698, Test Loss Force: 11.445380513235069, time: 10.641424655914307

Epoch 8, Batch 100/107, Loss: 0.7201172113418579

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.22512144556465, Training Loss Force: 4.836538175219207, time: 1.5675950050354004
Validation Loss Energy: 17.320684693392298, Validation Loss Force: 3.8675758000139497, time: 0.10214614868164062
Test Loss Energy: 22.02375332892681, Test Loss Force: 11.279042879348953, time: 9.800453424453735

Epoch 9, Batch 100/107, Loss: 0.6927143931388855

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.69666522948429, Training Loss Force: 5.104316860368838, time: 1.5019886493682861
Validation Loss Energy: 11.72297118820346, Validation Loss Force: 4.360595126763626, time: 0.10388016700744629
Test Loss Energy: 10.994181849252366, Test Loss Force: 11.310665134351058, time: 9.828281879425049

Epoch 10, Batch 100/107, Loss: 0.4205770492553711

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 10.539050029505438, Training Loss Force: 5.572058966721537, time: 1.5189850330352783
Validation Loss Energy: 8.827703594143518, Validation Loss Force: 4.157719486555651, time: 0.11463522911071777
Test Loss Energy: 15.739306503012058, Test Loss Force: 11.358436269808577, time: 10.046336650848389

Epoch 11, Batch 100/107, Loss: 0.4090285003185272

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 13.189370362994872, Training Loss Force: 5.8587470466255445, time: 1.5020349025726318
Validation Loss Energy: 8.050740580551352, Validation Loss Force: 7.286214084097546, time: 0.10275006294250488
Test Loss Energy: 13.68866544980693, Test Loss Force: 12.665051683373708, time: 9.83376955986023

Epoch 12, Batch 100/107, Loss: 0.7836933135986328

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.02536617464615, Training Loss Force: 5.2202916523128815, time: 1.4792518615722656
Validation Loss Energy: 7.91448933015814, Validation Loss Force: 4.4091031548657025, time: 0.10598134994506836
Test Loss Energy: 9.834546612994844, Test Loss Force: 10.97919030742131, time: 9.974570512771606

Epoch 13, Batch 100/107, Loss: 0.3015654683113098

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.407275864849213, Training Loss Force: 4.8023288879432835, time: 1.5526902675628662
Validation Loss Energy: 10.013077448430593, Validation Loss Force: 4.921805320769666, time: 0.10699176788330078
Test Loss Energy: 10.70864403085932, Test Loss Force: 11.4349416391139, time: 9.786843299865723

Epoch 14, Batch 100/107, Loss: 0.2149714231491089

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.700045383118565, Training Loss Force: 3.4167606754145323, time: 1.5256266593933105
Validation Loss Energy: 1.712414744175773, Validation Loss Force: 3.3826639682020767, time: 0.1065220832824707
Test Loss Energy: 9.864986203380314, Test Loss Force: 11.172094347625366, time: 9.917180061340332

Epoch 15, Batch 100/107, Loss: 0.624335765838623

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 6.668289001465511, Training Loss Force: 3.2354119536347765, time: 1.524688959121704
Validation Loss Energy: 11.184457882910984, Validation Loss Force: 3.5064783674532123, time: 0.10154438018798828
Test Loss Energy: 16.241469032925995, Test Loss Force: 11.471556612016984, time: 9.944134950637817

Epoch 16, Batch 100/107, Loss: 0.25162845849990845

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.9370481123912935, Training Loss Force: 3.2274505202921113, time: 1.5764751434326172
Validation Loss Energy: 2.134856802764892, Validation Loss Force: 3.7590447429318594, time: 0.10359334945678711
Test Loss Energy: 8.867500807588513, Test Loss Force: 11.127350691824308, time: 9.871622085571289

Epoch 17, Batch 100/107, Loss: 0.19657939672470093

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 13.22436442062236, Training Loss Force: 5.00296499679824, time: 1.5529654026031494
Validation Loss Energy: 12.728186108550114, Validation Loss Force: 8.40626883510329, time: 0.1024019718170166
Test Loss Energy: 11.124132383543984, Test Loss Force: 12.644828116097582, time: 9.76740574836731

Epoch 18, Batch 100/107, Loss: 0.42440077662467957

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.223762210042361, Training Loss Force: 5.078039116744526, time: 1.547654151916504
Validation Loss Energy: 6.088943017343548, Validation Loss Force: 3.4880019497315624, time: 0.10599112510681152
Test Loss Energy: 13.620050626325757, Test Loss Force: 11.096548185705187, time: 10.094971179962158

Epoch 19, Batch 100/107, Loss: 0.44659608602523804

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.5541933227680325, Training Loss Force: 3.369588262632172, time: 1.56351900100708
Validation Loss Energy: 7.858324823473335, Validation Loss Force: 3.7090912580689923, time: 0.10596513748168945
Test Loss Energy: 9.608414917547211, Test Loss Force: 11.17877800336245, time: 10.5434250831604

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–â–â–â–ƒâ–â–ˆâ–‚â–…â–„â–‚â–‚â–‚â–…â–â–‚â–„â–
wandb:   test_error_force â–â–â–â–â–â–â–…â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–‚â–ƒâ–‚â–„â–‚â–ˆâ–‚â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–†â–‚â–ˆâ–ƒâ–…â–‡â–‚â–ƒâ–‚â–†â–â–†â–„â–‚
wandb: train_error_energy â–‚â–â–â–â–â–â–ˆâ–‡â–†â–†â–†â–‡â–†â–†â–„â–„â–„â–‡â–†â–„
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–…â–…â–†â–‡â–†â–…â–ƒâ–‚â–‚â–…â–…â–ƒ
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–…â–†â–†â–‡â–†â–…â–ƒâ–ƒâ–ƒâ–†â–†â–ƒ
wandb: valid_error_energy â–â–â–â–â–â–â–„â–‚â–ˆâ–†â–„â–„â–„â–…â–â–…â–â–†â–ƒâ–„
wandb:  valid_error_force â–â–â–â–â–â–â–…â–…â–‚â–ƒâ–ƒâ–‡â–ƒâ–„â–‚â–‚â–‚â–ˆâ–‚â–‚
wandb:         valid_loss â–â–â–â–â–â–â–…â–„â–…â–„â–„â–†â–„â–„â–‚â–ƒâ–‚â–ˆâ–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3421
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.60841
wandb:   test_error_force 11.17878
wandb:          test_loss 4.38346
wandb: train_error_energy 6.55419
wandb:  train_error_force 3.36959
wandb:         train_loss 1.56609
wandb: valid_error_energy 7.85832
wandb:  valid_error_force 3.70909
wandb:         valid_loss 1.76696
wandb: 
wandb: ğŸš€ View run al_77_29 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/w1h3xrtr
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_160511-w1h3xrtr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4061547815799713, Uncertainty Bias: -0.21913394331932068
5.1498413e-05 0.003414154
-1.6573302 50.767776
(48745, 22, 3)
Found uncertainty sample 0 after 4 steps.
Found uncertainty sample 1 after 5 steps.
Found uncertainty sample 2 after 20 steps.
Found uncertainty sample 3 after 37 steps.
Found uncertainty sample 4 after 9 steps.
Found uncertainty sample 5 after 40 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 9 steps.
Found uncertainty sample 8 after 3 steps.
Found uncertainty sample 9 after 6 steps.
Found uncertainty sample 10 after 6 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 10 steps.
Found uncertainty sample 13 after 31 steps.
Found uncertainty sample 14 after 5 steps.
Found uncertainty sample 15 after 19 steps.
Found uncertainty sample 16 after 26 steps.
Found uncertainty sample 17 after 10 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 14 steps.
Found uncertainty sample 20 after 44 steps.
Found uncertainty sample 21 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 16 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 12 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 12 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 45 steps.
Found uncertainty sample 30 after 14 steps.
Found uncertainty sample 31 after 10 steps.
Found uncertainty sample 32 after 11 steps.
Found uncertainty sample 33 after 7 steps.
Found uncertainty sample 34 after 27 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 4 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 4 steps.
Found uncertainty sample 41 after 13 steps.
Found uncertainty sample 42 after 25 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 10 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 11 steps.
Found uncertainty sample 47 after 18 steps.
Found uncertainty sample 48 after 9 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 15 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 19 steps.
Found uncertainty sample 53 after 4 steps.
Found uncertainty sample 54 after 51 steps.
Found uncertainty sample 55 after 7 steps.
Found uncertainty sample 56 after 14 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 8 steps.
Found uncertainty sample 59 after 55 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 19 steps.
Found uncertainty sample 62 after 20 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 13 steps.
Found uncertainty sample 65 after 5 steps.
Found uncertainty sample 66 after 4 steps.
Found uncertainty sample 67 after 6 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 7 steps.
Found uncertainty sample 70 after 32 steps.
Found uncertainty sample 71 after 17 steps.
Found uncertainty sample 72 after 6 steps.
Found uncertainty sample 73 after 7 steps.
Found uncertainty sample 74 after 3 steps.
Found uncertainty sample 75 after 8 steps.
Found uncertainty sample 76 after 5 steps.
Found uncertainty sample 77 after 9 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 68 steps.
Found uncertainty sample 80 after 14 steps.
Found uncertainty sample 81 after 6 steps.
Found uncertainty sample 82 after 91 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 10 steps.
Found uncertainty sample 85 after 11 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 10 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 29 steps.
Found uncertainty sample 91 after 7 steps.
Found uncertainty sample 92 after 18 steps.
Found uncertainty sample 93 after 22 steps.
Found uncertainty sample 94 after 8 steps.
Found uncertainty sample 95 after 26 steps.
Found uncertainty sample 96 after 19 steps.
Found uncertainty sample 97 after 4 steps.
Found uncertainty sample 98 after 13 steps.
Found uncertainty sample 99 after 5 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_161403-k69fj2a8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_30
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/k69fj2a8
Training model 30. Added 100 samples to the dataset.
Epoch 0, Batch 100/110, Loss: 0.09866444766521454

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8048021900012476, Training Loss Force: 2.5508508638935194, time: 1.5637030601501465
Validation Loss Energy: 1.2189494667940897, Validation Loss Force: 2.794890903770873, time: 0.10276460647583008
Test Loss Energy: 9.779474862721425, Test Loss Force: 10.79421065875159, time: 9.576347827911377

Epoch 1, Batch 100/110, Loss: 0.12722262740135193

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7028222267002788, Training Loss Force: 2.4345491414664058, time: 1.5416321754455566
Validation Loss Energy: 1.5857118807636623, Validation Loss Force: 2.7246890001230626, time: 0.10546660423278809
Test Loss Energy: 9.288105840769047, Test Loss Force: 10.789977841863047, time: 9.617003202438354

Epoch 2, Batch 100/110, Loss: 0.08902446180582047

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6640592115443518, Training Loss Force: 2.413380599115335, time: 1.5623281002044678
Validation Loss Energy: 1.0608677055004727, Validation Loss Force: 2.750697535709439, time: 0.1081993579864502
Test Loss Energy: 9.816261848122643, Test Loss Force: 10.78026352614789, time: 9.727415561676025

Epoch 3, Batch 100/110, Loss: 0.08950337022542953

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5478144306785138, Training Loss Force: 2.4360783420687135, time: 1.653080701828003
Validation Loss Energy: 1.1120292539809276, Validation Loss Force: 2.7138200737720353, time: 0.10345005989074707
Test Loss Energy: 9.802143463127003, Test Loss Force: 10.904939893401306, time: 10.197901725769043

Epoch 4, Batch 100/110, Loss: 0.04768022522330284

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5389226980451494, Training Loss Force: 2.410915364224289, time: 1.5300896167755127
Validation Loss Energy: 1.9746450162549727, Validation Loss Force: 2.7398726978493055, time: 0.10377311706542969
Test Loss Energy: 10.453302456952034, Test Loss Force: 10.883655786791525, time: 9.558600664138794

Epoch 5, Batch 100/110, Loss: 0.14515286684036255

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.4122638851689644, Training Loss Force: 2.4100350619985136, time: 1.827723741531372
Validation Loss Energy: 1.2613094327752408, Validation Loss Force: 2.7240352435106185, time: 0.10349369049072266
Test Loss Energy: 9.771412458285674, Test Loss Force: 10.882823162735518, time: 9.702741146087646

Epoch 6, Batch 100/110, Loss: 0.9333213567733765

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 14.030991062160314, Training Loss Force: 6.298475815225712, time: 1.6097631454467773
Validation Loss Energy: 3.5981579349060606, Validation Loss Force: 7.496189084685137, time: 0.11011981964111328
Test Loss Energy: 9.188884497322993, Test Loss Force: 12.576273028569483, time: 9.591951370239258

Epoch 7, Batch 100/110, Loss: 0.5137169361114502

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 6.595716070685646, Training Loss Force: 4.0755822272162225, time: 1.5603249073028564
Validation Loss Energy: 6.744169023481511, Validation Loss Force: 3.556059326834942, time: 0.10585331916809082
Test Loss Energy: 14.61339009982305, Test Loss Force: 11.33776387148268, time: 9.753082513809204

Epoch 8, Batch 100/110, Loss: 0.2648230195045471

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.00160983319971, Training Loss Force: 3.6580982929049863, time: 1.5543732643127441
Validation Loss Energy: 2.0139697234406957, Validation Loss Force: 8.630988318526782, time: 0.10509085655212402
Test Loss Energy: 10.275735154045572, Test Loss Force: 14.447426969792629, time: 9.576802492141724

Epoch 9, Batch 100/110, Loss: 0.13494902849197388

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 13.581864338183873, Training Loss Force: 6.301039002274051, time: 1.5375583171844482
Validation Loss Energy: 21.2579338918362, Validation Loss Force: 5.08475081638414, time: 0.10416030883789062
Test Loss Energy: 15.054943948843489, Test Loss Force: 11.523945387942714, time: 9.608269929885864

Epoch 10, Batch 100/110, Loss: 0.1438213586807251

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 15.046560531997452, Training Loss Force: 6.184576338636791, time: 1.5858561992645264
Validation Loss Energy: 7.343313355856955, Validation Loss Force: 8.11181516457301, time: 0.10548043251037598
Test Loss Energy: 9.84741055696559, Test Loss Force: 12.999207409514598, time: 9.77522587776184

Epoch 11, Batch 100/110, Loss: 0.9911678433418274

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.52955520670293, Training Loss Force: 5.820599350705178, time: 1.5089969635009766
Validation Loss Energy: 13.111897596902132, Validation Loss Force: 6.474783222730636, time: 0.10256719589233398
Test Loss Energy: 17.206142416328976, Test Loss Force: 13.481030411657084, time: 9.601638555526733

Epoch 12, Batch 100/110, Loss: 0.7536922693252563

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.371884115953679, Training Loss Force: 4.7347911872166675, time: 1.6242053508758545
Validation Loss Energy: 23.447940163984185, Validation Loss Force: 5.124417988716301, time: 0.10837244987487793
Test Loss Energy: 25.69996053006311, Test Loss Force: 12.62275629231923, time: 9.652095556259155

Epoch 13, Batch 100/110, Loss: 0.1787826120853424

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.819715200563007, Training Loss Force: 5.714436224121629, time: 1.7577533721923828
Validation Loss Energy: 4.879310695622515, Validation Loss Force: 5.489444863695225, time: 0.10658454895019531
Test Loss Energy: 9.39424312933786, Test Loss Force: 11.865598470496716, time: 9.660240650177002

Epoch 14, Batch 100/110, Loss: 1.3819606304168701

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.10786395778957, Training Loss Force: 5.356766653509492, time: 1.584587812423706
Validation Loss Energy: 11.363430880980196, Validation Loss Force: 8.005701737742832, time: 0.109954833984375
Test Loss Energy: 15.794917933346783, Test Loss Force: 14.095083198795363, time: 9.621554374694824

Epoch 15, Batch 100/110, Loss: 0.7903677821159363

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.861496820034548, Training Loss Force: 6.213062175757656, time: 1.5426459312438965
Validation Loss Energy: 4.18741386451457, Validation Loss Force: 4.699061135339701, time: 0.10558676719665527
Test Loss Energy: 10.567086586376718, Test Loss Force: 11.074747239525765, time: 9.852679967880249

Epoch 16, Batch 100/110, Loss: 0.16035789251327515

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.287405486496292, Training Loss Force: 4.723452214395648, time: 1.5665106773376465
Validation Loss Energy: 11.559368595939352, Validation Loss Force: 3.3709460043976267, time: 0.1113274097442627
Test Loss Energy: 11.255087698949822, Test Loss Force: 10.949712162770517, time: 9.572990417480469

Epoch 17, Batch 100/110, Loss: 0.5821132659912109

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.084629687776618, Training Loss Force: 5.121192826539767, time: 1.6106247901916504
Validation Loss Energy: 1.7891510965799804, Validation Loss Force: 3.5661843155694246, time: 0.10967898368835449
Test Loss Energy: 9.188559106586565, Test Loss Force: 11.02669096631753, time: 9.55147933959961

Epoch 18, Batch 100/110, Loss: 0.16743867099285126

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 15.38192136569054, Training Loss Force: 6.739050191796086, time: 1.5664329528808594
Validation Loss Energy: 20.871437257806917, Validation Loss Force: 5.935554926810587, time: 0.10542511940002441
Test Loss Energy: 16.602890607643815, Test Loss Force: 11.802681351753824, time: 9.80286979675293

Epoch 19, Batch 100/110, Loss: 0.2509067952632904

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.92344972335428, Training Loss Force: 5.583521938181986, time: 1.5661239624023438
Validation Loss Energy: 8.348732463810425, Validation Loss Force: 3.6645184617828974, time: 0.10367369651794434
Test Loss Energy: 10.21631146471958, Test Loss Force: 11.213865759707843, time: 10.220516204833984

wandb: - 0.039 MB of 0.056 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–‚â–â–â–ƒâ–â–ƒâ–â–„â–ˆâ–â–„â–‚â–‚â–â–„â–
wandb:   test_error_force â–â–â–â–â–â–â–„â–‚â–ˆâ–‚â–…â–†â–…â–ƒâ–‡â–‚â–â–â–ƒâ–‚
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–ƒâ–†â–„â–„â–‡â–ˆâ–ƒâ–‡â–‚â–‚â–â–„â–‚
wandb: train_error_energy â–â–â–â–â–â–â–‡â–„â–ƒâ–‡â–ˆâ–…â–„â–†â–†â–…â–„â–…â–ˆâ–†
wandb:  train_error_force â–â–â–â–â–â–â–‡â–„â–ƒâ–‡â–‡â–‡â–…â–†â–†â–‡â–…â–…â–ˆâ–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–„â–ƒâ–‡â–‡â–†â–„â–†â–†â–†â–„â–…â–ˆâ–†
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–ƒâ–â–‡â–ƒâ–…â–ˆâ–‚â–„â–‚â–„â–â–‡â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–‡â–‚â–ˆâ–„â–‡â–…â–„â–„â–‡â–ƒâ–‚â–‚â–…â–‚
wandb:         valid_loss â–â–â–â–â–â–â–†â–ƒâ–‡â–‡â–‡â–‡â–ˆâ–„â–ˆâ–ƒâ–„â–‚â–ˆâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3511
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.21631
wandb:   test_error_force 11.21387
wandb:          test_loss 4.43588
wandb: train_error_energy 10.92345
wandb:  train_error_force 5.58352
wandb:         train_loss 2.59927
wandb: valid_error_energy 8.34873
wandb:  valid_error_force 3.66452
wandb:         valid_loss 1.78486
wandb: 
wandb: ğŸš€ View run al_77_30 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/k69fj2a8
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_161403-k69fj2a8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6881707906723022, Uncertainty Bias: -0.6546024084091187
1.9073486e-05 0.011482716
3.7419033 58.951736
(48745, 22, 3)
Found uncertainty sample 0 after 44 steps.
Found uncertainty sample 1 after 8 steps.
Found uncertainty sample 2 after 102 steps.
Found uncertainty sample 3 after 98 steps.
Found uncertainty sample 4 after 103 steps.
Found uncertainty sample 5 after 52 steps.
Found uncertainty sample 6 after 22 steps.
Found uncertainty sample 7 after 109 steps.
Found uncertainty sample 8 after 170 steps.
Found uncertainty sample 9 after 46 steps.
Found uncertainty sample 10 after 10 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 18 steps.
Found uncertainty sample 13 after 15 steps.
Found uncertainty sample 14 after 9 steps.
Found uncertainty sample 15 after 12 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 5 steps.
Found uncertainty sample 18 after 24 steps.
Found uncertainty sample 19 after 117 steps.
Found uncertainty sample 20 after 69 steps.
Found uncertainty sample 21 after 4 steps.
Found uncertainty sample 22 after 53 steps.
Found uncertainty sample 23 after 17 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 16 steps.
Found uncertainty sample 26 after 24 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 17 steps.
Found uncertainty sample 29 after 6 steps.
Found uncertainty sample 30 after 40 steps.
Found uncertainty sample 31 after 15 steps.
Found uncertainty sample 32 after 17 steps.
Found uncertainty sample 33 after 88 steps.
Found uncertainty sample 34 after 82 steps.
Found uncertainty sample 35 after 55 steps.
Found uncertainty sample 36 after 30 steps.
Found uncertainty sample 37 after 15 steps.
Found uncertainty sample 38 after 32 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 20 steps.
Found uncertainty sample 41 after 18 steps.
Found uncertainty sample 42 after 17 steps.
Found uncertainty sample 43 after 17 steps.
Found uncertainty sample 44 after 14 steps.
Found uncertainty sample 45 after 84 steps.
Found uncertainty sample 46 after 60 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 64 steps.
Found uncertainty sample 49 after 78 steps.
Found uncertainty sample 50 after 14 steps.
Found uncertainty sample 51 after 15 steps.
Found uncertainty sample 52 after 6 steps.
Found uncertainty sample 53 after 25 steps.
Found uncertainty sample 54 after 56 steps.
Found uncertainty sample 55 after 21 steps.
Found uncertainty sample 56 after 11 steps.
Found uncertainty sample 57 after 8 steps.
Found uncertainty sample 58 after 47 steps.
Found uncertainty sample 59 after 31 steps.
Found uncertainty sample 60 after 45 steps.
Found uncertainty sample 61 after 70 steps.
Found uncertainty sample 62 after 36 steps.
Found uncertainty sample 63 after 148 steps.
Found uncertainty sample 64 after 52 steps.
Found uncertainty sample 65 after 48 steps.
Found uncertainty sample 66 after 53 steps.
Found uncertainty sample 67 after 49 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 33 steps.
Found uncertainty sample 71 after 13 steps.
Found uncertainty sample 72 after 33 steps.
Found uncertainty sample 73 after 7 steps.
Found uncertainty sample 74 after 97 steps.
Found uncertainty sample 75 after 51 steps.
Found uncertainty sample 76 after 42 steps.
Found uncertainty sample 77 after 20 steps.
Found uncertainty sample 78 after 98 steps.
Found uncertainty sample 79 after 37 steps.
Found uncertainty sample 80 after 213 steps.
Found uncertainty sample 81 after 34 steps.
Found uncertainty sample 82 after 21 steps.
Found uncertainty sample 83 after 98 steps.
Found uncertainty sample 84 after 26 steps.
Found uncertainty sample 85 after 8 steps.
Found uncertainty sample 86 after 10 steps.
Found uncertainty sample 87 after 12 steps.
Found uncertainty sample 88 after 4 steps.
Found uncertainty sample 89 after 5 steps.
Found uncertainty sample 90 after 2 steps.
Found uncertainty sample 91 after 42 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 28 steps.
Found uncertainty sample 94 after 80 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 22 steps.
Found uncertainty sample 97 after 7 steps.
Found uncertainty sample 98 after 25 steps.
Found uncertainty sample 99 after 82 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_162342-acrfglo8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_31
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/acrfglo8
Training model 31. Added 100 samples to the dataset.
Epoch 0, Batch 100/113, Loss: 0.10735442489385605

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.0512684217054313, Training Loss Force: 2.6079550688582986, time: 1.6117279529571533
Validation Loss Energy: 1.2735999346211573, Validation Loss Force: 2.7947753384841363, time: 0.11385965347290039
Test Loss Energy: 9.516270946194064, Test Loss Force: 10.939631499185761, time: 9.781354427337646

Epoch 1, Batch 100/113, Loss: 0.10096421092748642

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.4205747914214057, Training Loss Force: 2.443695446942354, time: 1.6163392066955566
Validation Loss Energy: 3.1818082711513855, Validation Loss Force: 2.759991523191319, time: 0.10814428329467773
Test Loss Energy: 9.460869510351056, Test Loss Force: 10.906176824004357, time: 9.828627109527588

Epoch 2, Batch 100/113, Loss: 0.22544996440410614

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.1570722867782015, Training Loss Force: 2.537279771591507, time: 1.593735933303833
Validation Loss Energy: 5.478019709817671, Validation Loss Force: 2.9444224760309385, time: 0.11117053031921387
Test Loss Energy: 12.032200327555136, Test Loss Force: 11.070481546878504, time: 10.00118899345398

Epoch 3, Batch 100/113, Loss: 0.3696296811103821

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.622085347509417, Training Loss Force: 2.523671880749976, time: 1.5812180042266846
Validation Loss Energy: 1.2092311832546443, Validation Loss Force: 2.948046639050059, time: 0.10747194290161133
Test Loss Energy: 9.288723452699816, Test Loss Force: 10.890090197645245, time: 9.931671857833862

Epoch 4, Batch 100/113, Loss: 0.10091216117143631

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6926137253428084, Training Loss Force: 2.424577726316245, time: 1.6248977184295654
Validation Loss Energy: 5.0915526907847966, Validation Loss Force: 2.720941589731671, time: 0.10968160629272461
Test Loss Energy: 9.527419815459613, Test Loss Force: 10.846723924216999, time: 10.111484289169312

Epoch 5, Batch 100/113, Loss: 0.06152334436774254

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.1504976609382203, Training Loss Force: 2.4341385857253046, time: 1.6612474918365479
Validation Loss Energy: 1.1998152287480823, Validation Loss Force: 2.725711608175736, time: 0.11080408096313477
Test Loss Energy: 9.453752412217225, Test Loss Force: 10.940324538832451, time: 10.531764030456543

Epoch 6, Batch 100/113, Loss: 0.6455598473548889

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.088806821085004, Training Loss Force: 5.578103003949128, time: 1.6098079681396484
Validation Loss Energy: 17.49802641729844, Validation Loss Force: 6.128834417505658, time: 0.1074380874633789
Test Loss Energy: 21.736493203161643, Test Loss Force: 12.680096717095294, time: 9.863083600997925

Epoch 7, Batch 100/113, Loss: 1.0203309059143066

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.362752584454398, Training Loss Force: 5.181358534814609, time: 1.5652954578399658
Validation Loss Energy: 2.921979096140012, Validation Loss Force: 5.378127436255568, time: 0.10869097709655762
Test Loss Energy: 9.718667654214325, Test Loss Force: 12.087772694027334, time: 9.893967151641846

Epoch 8, Batch 100/113, Loss: 0.9758471250534058

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.717669473278864, Training Loss Force: 5.441524416117737, time: 1.6256766319274902
Validation Loss Energy: 13.100295198384417, Validation Loss Force: 6.064860681204021, time: 0.10862398147583008
Test Loss Energy: 11.921122670484973, Test Loss Force: 11.741178165766275, time: 9.919688940048218

Epoch 9, Batch 100/113, Loss: 0.5434675216674805

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 14.490138576360732, Training Loss Force: 6.7367597391791385, time: 1.5602927207946777
Validation Loss Energy: 16.08041487854477, Validation Loss Force: 7.811964884622024, time: 0.11082720756530762
Test Loss Energy: 21.93711520311465, Test Loss Force: 13.704695921572215, time: 9.747276306152344

Epoch 10, Batch 100/113, Loss: 0.23991762101650238

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.9344771914765175, Training Loss Force: 4.162870208153027, time: 1.5838336944580078
Validation Loss Energy: 8.077363794315062, Validation Loss Force: 3.680213964541055, time: 0.1455249786376953
Test Loss Energy: 12.453464997678857, Test Loss Force: 11.32739398515931, time: 9.973294258117676

Epoch 11, Batch 100/113, Loss: 0.44632506370544434

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.614742189291563, Training Loss Force: 3.2813389871656806, time: 1.6352860927581787
Validation Loss Energy: 7.275432221382577, Validation Loss Force: 3.574185263320902, time: 0.11078310012817383
Test Loss Energy: 9.80772544053137, Test Loss Force: 11.010571641402368, time: 9.798454761505127

Epoch 12, Batch 100/113, Loss: 0.6328167915344238

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.7841338433755505, Training Loss Force: 3.2341039605413964, time: 1.6202735900878906
Validation Loss Energy: 10.088833499919854, Validation Loss Force: 3.966951035819208, time: 0.10994935035705566
Test Loss Energy: 10.702105233642639, Test Loss Force: 11.162914750296716, time: 9.967096328735352

Epoch 13, Batch 100/113, Loss: 1.50788414478302

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 14.077961779693306, Training Loss Force: 5.916171301784296, time: 1.5990955829620361
Validation Loss Energy: 11.108236007568443, Validation Loss Force: 4.891618509994528, time: 0.10697078704833984
Test Loss Energy: 10.529447722450303, Test Loss Force: 11.492743802755353, time: 9.819160461425781

Epoch 14, Batch 100/113, Loss: 0.9101182222366333

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.785779900731962, Training Loss Force: 5.589857637223813, time: 1.627723217010498
Validation Loss Energy: 5.203033780046711, Validation Loss Force: 6.814932387380779, time: 0.11093306541442871
Test Loss Energy: 10.843327102898451, Test Loss Force: 12.163942823858303, time: 9.823239088058472

Epoch 15, Batch 100/113, Loss: 0.4523622691631317

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.585988007496463, Training Loss Force: 4.739727575859942, time: 1.5656020641326904
Validation Loss Energy: 5.925874163022231, Validation Loss Force: 3.9723231516338617, time: 0.1115579605102539
Test Loss Energy: 9.655473926765065, Test Loss Force: 11.242425364986504, time: 10.093926668167114

Epoch 16, Batch 100/113, Loss: 0.17804458737373352

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.07081407022449, Training Loss Force: 5.188268188963986, time: 1.6174876689910889
Validation Loss Energy: 14.794869746255742, Validation Loss Force: 6.433824346682949, time: 0.10679864883422852
Test Loss Energy: 12.912510932728136, Test Loss Force: 12.053690898592894, time: 9.840906143188477

Epoch 17, Batch 100/113, Loss: 0.9269979596138

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.703733671576842, Training Loss Force: 5.746670432425277, time: 1.5679218769073486
Validation Loss Energy: 14.457519310100766, Validation Loss Force: 6.802874853128789, time: 0.10910820960998535
Test Loss Energy: 14.421354082240176, Test Loss Force: 12.479034601666, time: 9.701489925384521

Epoch 18, Batch 100/113, Loss: 0.7484971284866333

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.628788203680145, Training Loss Force: 5.223188591087334, time: 1.7771785259246826
Validation Loss Energy: 13.052649345215373, Validation Loss Force: 5.383159313615244, time: 0.10689544677734375
Test Loss Energy: 11.245795209529996, Test Loss Force: 11.41913850438596, time: 9.734883785247803

Epoch 19, Batch 100/113, Loss: 0.5568618774414062

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.607468733331547, Training Loss Force: 4.799141696355181, time: 1.624101161956787
Validation Loss Energy: 3.4478314337950327, Validation Loss Force: 3.6638630351518593, time: 0.10829544067382812
Test Loss Energy: 10.778756207584875, Test Loss Force: 11.401041042737116, time: 9.794429302215576

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–ƒâ–â–â–â–ˆâ–â–‚â–ˆâ–ƒâ–â–‚â–‚â–‚â–â–ƒâ–„â–‚â–‚
wandb:   test_error_force â–â–â–‚â–â–â–â–…â–„â–ƒâ–ˆâ–‚â–â–‚â–ƒâ–„â–‚â–„â–…â–‚â–‚
wandb:          test_loss â–â–â–‚â–â–â–â–‡â–ƒâ–ƒâ–ˆâ–‚â–â–‚â–‚â–ƒâ–‚â–ƒâ–„â–‚â–‚
wandb: train_error_energy â–â–â–‚â–‚â–â–â–†â–†â–‡â–ˆâ–„â–„â–„â–ˆâ–†â–†â–†â–‡â–†â–…
wandb:  train_error_force â–â–â–â–â–â–â–†â–…â–†â–ˆâ–„â–‚â–‚â–‡â–†â–…â–…â–†â–†â–…
wandb:         train_loss â–â–â–â–‚â–â–â–†â–†â–†â–ˆâ–„â–ƒâ–ƒâ–‡â–†â–…â–†â–†â–†â–…
wandb: valid_error_energy â–â–‚â–ƒâ–â–ƒâ–â–ˆâ–‚â–†â–‡â–„â–„â–…â–…â–ƒâ–ƒâ–‡â–‡â–†â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–†â–…â–†â–ˆâ–‚â–‚â–ƒâ–„â–‡â–ƒâ–†â–‡â–…â–‚
wandb:         valid_loss â–â–â–‚â–â–‚â–â–‡â–„â–†â–ˆâ–ƒâ–ƒâ–„â–…â–…â–ƒâ–‡â–‡â–…â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 3601
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.77876
wandb:   test_error_force 11.40104
wandb:          test_loss 4.53615
wandb: train_error_energy 8.60747
wandb:  train_error_force 4.79914
wandb:         train_loss 2.18183
wandb: valid_error_energy 3.44783
wandb:  valid_error_force 3.66386
wandb:         valid_loss 1.45667
wandb: 
wandb: ğŸš€ View run al_77_31 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/acrfglo8
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_162342-acrfglo8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.2957570552825928, Uncertainty Bias: -0.8592705130577087
9.536743e-05 0.009192467
-6.357735 55.113754
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 3 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 5 steps.
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 4 steps.
Found uncertainty sample 39 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_163211-tsjaqnar
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_32
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/tsjaqnar
Training model 32. Added 100 samples to the dataset.
Epoch 0, Batch 100/116, Loss: 0.04760310798883438

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.903990546685982, Training Loss Force: 2.5657738026129504, time: 1.630467176437378
Validation Loss Energy: 1.0195089435003426, Validation Loss Force: 2.6195982926801302, time: 0.11975932121276855
Test Loss Energy: 9.404496434956556, Test Loss Force: 10.815086019421098, time: 9.759927034378052

Epoch 1, Batch 100/116, Loss: 0.2209503948688507

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.852587103690366, Training Loss Force: 2.4651594514018353, time: 1.6755433082580566
Validation Loss Energy: 1.6981564779757683, Validation Loss Force: 2.6907632032725535, time: 0.11269640922546387
Test Loss Energy: 10.263775396030594, Test Loss Force: 10.99647674042388, time: 9.784775257110596

Epoch 2, Batch 100/116, Loss: 0.124701589345932

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6821653489739172, Training Loss Force: 2.416088285351461, time: 1.679497480392456
Validation Loss Energy: 1.3417523543383238, Validation Loss Force: 2.828051720770547, time: 0.11775660514831543
Test Loss Energy: 10.070177672654358, Test Loss Force: 10.938661286582091, time: 10.685304641723633

Epoch 3, Batch 100/116, Loss: 0.06876825541257858

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7343871922599938, Training Loss Force: 2.376690688572556, time: 1.6541814804077148
Validation Loss Energy: 1.721412192022564, Validation Loss Force: 2.600874860652597, time: 0.11286497116088867
Test Loss Energy: 9.328050158078817, Test Loss Force: 10.932719881538763, time: 10.59777545928955

Epoch 4, Batch 100/116, Loss: 0.05292893201112747

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6183499486708697, Training Loss Force: 2.3739698616459166, time: 1.7626774311065674
Validation Loss Energy: 1.883679437418542, Validation Loss Force: 2.5798283214256, time: 0.12236475944519043
Test Loss Energy: 9.336149896319434, Test Loss Force: 10.918766332333071, time: 10.569763898849487

Epoch 5, Batch 100/116, Loss: 0.04811336472630501

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5350272852747708, Training Loss Force: 2.4147566221220473, time: 1.6484086513519287
Validation Loss Energy: 4.158761578158593, Validation Loss Force: 4.421011203897726, time: 0.11268043518066406
Test Loss Energy: 10.798608260134575, Test Loss Force: 11.054511296042026, time: 10.2810218334198

Epoch 6, Batch 100/116, Loss: 0.591773509979248

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.304678528215915, Training Loss Force: 4.65236905090513, time: 1.663543462753296
Validation Loss Energy: 3.5191770403993767, Validation Loss Force: 5.262136406112032, time: 0.12087774276733398
Test Loss Energy: 11.084273860306398, Test Loss Force: 11.9478819106149, time: 10.8352210521698

Epoch 7, Batch 100/116, Loss: 1.0414471626281738

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.370489868180401, Training Loss Force: 6.252177420416181, time: 1.8726353645324707
Validation Loss Energy: 9.149376934856399, Validation Loss Force: 4.792384223758642, time: 0.13991189002990723
Test Loss Energy: 10.895913380527581, Test Loss Force: 11.430182824075551, time: 12.759205341339111

Epoch 8, Batch 100/116, Loss: 0.19525712728500366

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.713709836796948, Training Loss Force: 4.726249368222078, time: 1.6567003726959229
Validation Loss Energy: 31.35774408997371, Validation Loss Force: 7.100015323796882, time: 0.14020538330078125
Test Loss Energy: 35.725984407976235, Test Loss Force: 14.60486371350915, time: 10.214218378067017

Epoch 9, Batch 100/116, Loss: 0.27156370878219604

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.352011894032072, Training Loss Force: 5.603353309627692, time: 1.7075650691986084
Validation Loss Energy: 8.993664401109097, Validation Loss Force: 4.911601729826671, time: 0.12050938606262207
Test Loss Energy: 12.926195721440495, Test Loss Force: 11.864885826638726, time: 9.790831327438354

Epoch 10, Batch 100/116, Loss: 1.5314373970031738

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.063663133851104, Training Loss Force: 5.643014295212806, time: 1.6782491207122803
Validation Loss Energy: 8.54991745141642, Validation Loss Force: 4.622815137506351, time: 0.11642980575561523
Test Loss Energy: 10.015471258330022, Test Loss Force: 11.147729805739838, time: 10.199709415435791

Epoch 11, Batch 100/116, Loss: 1.0710651874542236

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.724730339721384, Training Loss Force: 4.838147443850977, time: 1.7067880630493164
Validation Loss Energy: 20.38663041449735, Validation Loss Force: 6.436804773201192, time: 0.1188364028930664
Test Loss Energy: 26.22487990731284, Test Loss Force: 13.345289001328009, time: 10.008883714675903

Epoch 12, Batch 100/116, Loss: 1.931255578994751

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 15.681551594620567, Training Loss Force: 7.023415754233662, time: 1.756432056427002
Validation Loss Energy: 16.953303760202626, Validation Loss Force: 6.593846196411391, time: 0.11347317695617676
Test Loss Energy: 21.875825005411308, Test Loss Force: 13.047940867393692, time: 11.290400743484497

Epoch 13, Batch 100/116, Loss: 0.2911088466644287

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.554023033278245, Training Loss Force: 5.566695326524833, time: 1.6972341537475586
Validation Loss Energy: 13.038223501927297, Validation Loss Force: 5.203943531953923, time: 0.11801719665527344
Test Loss Energy: 15.172750900665, Test Loss Force: 11.222385174495601, time: 10.313096523284912

Epoch 14, Batch 100/116, Loss: 0.6982147693634033

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.425226978664323, Training Loss Force: 3.381546938833722, time: 1.689502239227295
Validation Loss Energy: 7.437727819983137, Validation Loss Force: 4.816464704321954, time: 0.11543822288513184
Test Loss Energy: 13.12479682285734, Test Loss Force: 11.989432663402793, time: 9.92744779586792

Epoch 15, Batch 100/116, Loss: 1.6888837814331055

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.123601915529713, Training Loss Force: 5.575510918944494, time: 1.7071659564971924
Validation Loss Energy: 6.8187533157018505, Validation Loss Force: 8.828692542043632, time: 0.11353635787963867
Test Loss Energy: 10.338604675218363, Test Loss Force: 12.5360663720316, time: 11.903633832931519

Epoch 16, Batch 100/116, Loss: 0.26746666431427

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.951056289880354, Training Loss Force: 5.07441617505096, time: 1.9400851726531982
Validation Loss Energy: 16.269385869712234, Validation Loss Force: 5.8957686264853555, time: 0.1859419345855713
Test Loss Energy: 19.906079032712398, Test Loss Force: 12.760563736570084, time: 11.134325981140137

Epoch 17, Batch 100/116, Loss: 0.5322599411010742

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.76332124683317, Training Loss Force: 4.4836665193855945, time: 1.7171072959899902
Validation Loss Energy: 12.468827915893902, Validation Loss Force: 4.255189416702254, time: 0.1157679557800293
Test Loss Energy: 15.055130655173642, Test Loss Force: 11.993046236230704, time: 10.14680552482605

Epoch 18, Batch 100/116, Loss: 0.20119796693325043

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 15.354104734407212, Training Loss Force: 7.1699800816050105, time: 1.6451473236083984
Validation Loss Energy: 2.0935553502361945, Validation Loss Force: 5.130262088897273, time: 0.12239336967468262
Test Loss Energy: 9.550541476462923, Test Loss Force: 11.80239835442484, time: 11.478282451629639

Epoch 19, Batch 100/116, Loss: 0.3145493268966675

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 14.45201747607168, Training Loss Force: 6.964098114234085, time: 1.8280279636383057
Validation Loss Energy: 11.583395042123252, Validation Loss Force: 4.542253440509676, time: 0.14210796356201172
Test Loss Energy: 11.382849435174405, Test Loss Force: 11.292436146351204, time: 12.339978218078613

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–â–ˆâ–‚â–â–…â–„â–ƒâ–‚â–â–„â–ƒâ–â–‚
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–‚â–ˆâ–ƒâ–‚â–†â–…â–‚â–ƒâ–„â–…â–ƒâ–ƒâ–‚
wandb:          test_loss â–â–â–â–â–â–â–‚â–‚â–ˆâ–‚â–â–†â–…â–‚â–‚â–‚â–„â–ƒâ–‚â–‚
wandb: train_error_energy â–â–â–â–â–â–â–…â–†â–†â–…â–†â–†â–ˆâ–…â–ƒâ–…â–…â–…â–ˆâ–‡
wandb:  train_error_force â–â–â–â–â–â–â–„â–‡â–„â–†â–†â–…â–ˆâ–†â–‚â–†â–…â–„â–ˆâ–ˆ
wandb:         train_loss â–â–â–â–â–â–â–…â–†â–…â–…â–†â–…â–ˆâ–…â–ƒâ–†â–…â–„â–ˆâ–ˆ
wandb: valid_error_energy â–â–â–â–â–â–‚â–‚â–ƒâ–ˆâ–ƒâ–ƒâ–…â–…â–„â–‚â–‚â–…â–„â–â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–ƒâ–„â–ƒâ–†â–„â–ƒâ–…â–…â–„â–„â–ˆâ–…â–ƒâ–„â–ƒ
wandb:         valid_loss â–â–â–â–â–â–ƒâ–ƒâ–„â–ˆâ–„â–ƒâ–†â–†â–„â–ƒâ–†â–…â–„â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 3691
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.38285
wandb:   test_error_force 11.29244
wandb:          test_loss 4.54023
wandb: train_error_energy 14.45202
wandb:  train_error_force 6.9641
wandb:         train_loss 3.29735
wandb: valid_error_energy 11.5834
wandb:  valid_error_force 4.54225
wandb:         valid_loss 2.29502
wandb: 
wandb: ğŸš€ View run al_77_32 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/tsjaqnar
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_163211-tsjaqnar/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7190541625022888, Uncertainty Bias: -0.4591374397277832
8.9645386e-05 0.87291527
-6.41196 54.852203
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 5 steps.
Found uncertainty sample 4 after 2 steps.
Found uncertainty sample 5 after 6 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 5 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 4 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 2 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 9 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 6 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 7 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 4 steps.
Found uncertainty sample 47 after 4 steps.
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 4 steps.
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 6 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 2 steps.
Found uncertainty sample 78 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 4 steps.
Found uncertainty sample 82 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 5 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 3 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 4 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_164104-jzg6hnxa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_33
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/jzg6hnxa
Training model 33. Added 100 samples to the dataset.
Epoch 0, Batch 100/119, Loss: 0.26943373680114746

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.169123965293947, Training Loss Force: 2.5391376145001043, time: 1.7504279613494873
Validation Loss Energy: 1.090835975786764, Validation Loss Force: 2.6640920414194205, time: 0.12132501602172852
Test Loss Energy: 9.451024157923841, Test Loss Force: 10.904411890063379, time: 9.203617811203003

Epoch 1, Batch 100/119, Loss: 0.08379149436950684

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.1043345783447993, Training Loss Force: 2.447029588573598, time: 1.774658203125
Validation Loss Energy: 1.384487128159358, Validation Loss Force: 2.722761575995911, time: 0.10494732856750488
Test Loss Energy: 9.936796699832191, Test Loss Force: 10.943929503341126, time: 10.227143049240112

Epoch 2, Batch 100/119, Loss: 0.05505237728357315

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6198149501659385, Training Loss Force: 2.418504220824046, time: 1.8746531009674072
Validation Loss Energy: 2.2082174166937847, Validation Loss Force: 3.464411712404535, time: 0.1435842514038086
Test Loss Energy: 9.435440689222876, Test Loss Force: 10.892917139836749, time: 12.728831052780151

Epoch 3, Batch 100/119, Loss: 0.07375699281692505

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7021887483194504, Training Loss Force: 2.362723399274046, time: 1.9418845176696777
Validation Loss Energy: 2.362247981935797, Validation Loss Force: 2.72408326582309, time: 0.12314343452453613
Test Loss Energy: 9.322599324640752, Test Loss Force: 10.920604395018474, time: 9.876928091049194

Epoch 4, Batch 100/119, Loss: 0.12764349579811096

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.704325022539504, Training Loss Force: 2.3635855573665547, time: 1.7303071022033691
Validation Loss Energy: 1.8627400391622604, Validation Loss Force: 2.6416901151011607, time: 0.11738300323486328
Test Loss Energy: 9.446697171805296, Test Loss Force: 10.920831806667472, time: 9.22648310661316

Epoch 5, Batch 100/119, Loss: 0.12393022328615189

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6830467210634665, Training Loss Force: 2.388009207883404, time: 1.803692102432251
Validation Loss Energy: 1.758559306295686, Validation Loss Force: 2.7407459691281097, time: 0.11353302001953125
Test Loss Energy: 9.513080510028614, Test Loss Force: 10.989352913251773, time: 9.04848837852478

Epoch 6, Batch 100/119, Loss: 0.11754487454891205

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.292345681047186, Training Loss Force: 5.338549499987016, time: 1.772303581237793
Validation Loss Energy: 21.191039627016572, Validation Loss Force: 4.932894541129064, time: 0.12917065620422363
Test Loss Energy: 24.22957670217116, Test Loss Force: 12.571341123827292, time: 12.22221851348877

Epoch 7, Batch 100/119, Loss: 0.1529364138841629

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.802365981927393, Training Loss Force: 5.461648030096592, time: 1.6918253898620605
Validation Loss Energy: 26.00457593517738, Validation Loss Force: 9.244507428732836, time: 0.11446475982666016
Test Loss Energy: 27.78376801032572, Test Loss Force: 14.733387687023495, time: 9.664405584335327

Epoch 8, Batch 100/119, Loss: 0.38442736864089966

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 14.535339913423266, Training Loss Force: 6.664454275713327, time: 1.8204162120819092
Validation Loss Energy: 12.145760220989219, Validation Loss Force: 6.855920507309631, time: 0.10993790626525879
Test Loss Energy: 11.630163075228696, Test Loss Force: 11.915470245692388, time: 9.660476446151733

Epoch 9, Batch 100/119, Loss: 1.021121859550476

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.771371467828521, Training Loss Force: 6.632150603631764, time: 1.8155951499938965
Validation Loss Energy: 1.7607810619222288, Validation Loss Force: 4.76481511146303, time: 0.10526013374328613
Test Loss Energy: 9.86907949181827, Test Loss Force: 11.268903417119985, time: 10.552064180374146

Epoch 10, Batch 100/119, Loss: 0.6059015989303589

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 10.562079039331733, Training Loss Force: 4.7123710937214085, time: 1.9732236862182617
Validation Loss Energy: 12.815284480994643, Validation Loss Force: 5.161352045728294, time: 0.1405808925628662
Test Loss Energy: 17.939166064401633, Test Loss Force: 12.919247007813269, time: 12.443801164627075

Epoch 11, Batch 100/119, Loss: 0.7371942400932312

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.777313746581074, Training Loss Force: 5.100668164973738, time: 1.8402214050292969
Validation Loss Energy: 12.40834414733944, Validation Loss Force: 4.61669811917552, time: 0.12546324729919434
Test Loss Energy: 11.002156230592862, Test Loss Force: 10.570454421068856, time: 10.536790609359741

Epoch 12, Batch 100/119, Loss: 0.16790789365768433

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.328439619439239, Training Loss Force: 5.258758505812461, time: 1.9604582786560059
Validation Loss Energy: 1.5923806917938643, Validation Loss Force: 4.514075847661294, time: 0.1176002025604248
Test Loss Energy: 9.339606201776736, Test Loss Force: 11.434411066337242, time: 9.918127059936523

Epoch 13, Batch 100/119, Loss: 0.5445933938026428

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.654760199734916, Training Loss Force: 4.6516470919785835, time: 1.7197582721710205
Validation Loss Energy: 18.035324988176836, Validation Loss Force: 4.658116213225159, time: 0.11270284652709961
Test Loss Energy: 23.638851499529697, Test Loss Force: 12.729321665235608, time: 10.758601427078247

Epoch 14, Batch 100/119, Loss: 0.632487416267395

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.862822375058187, Training Loss Force: 4.7450080959009195, time: 1.7109100818634033
Validation Loss Energy: 11.359524670376233, Validation Loss Force: 4.7319225736726755, time: 0.12864446640014648
Test Loss Energy: 17.45827442550879, Test Loss Force: 11.416578504028788, time: 10.023029565811157

Epoch 15, Batch 100/119, Loss: 0.3155115246772766

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 16.47369665591447, Training Loss Force: 5.640044464771393, time: 1.7087421417236328
Validation Loss Energy: 16.573697994009322, Validation Loss Force: 6.026498572736144, time: 0.1123814582824707
Test Loss Energy: 15.64145652171123, Test Loss Force: 11.984732581723463, time: 10.958876848220825

Epoch 16, Batch 100/119, Loss: 0.5625412464141846

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.95394524999894, Training Loss Force: 5.970457261966454, time: 1.7673468589782715
Validation Loss Energy: 8.564502874259079, Validation Loss Force: 6.138469831011293, time: 0.12410354614257812
Test Loss Energy: 15.23917322813919, Test Loss Force: 12.51516027602511, time: 10.043091297149658

Epoch 17, Batch 100/119, Loss: 0.11938288062810898

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.168100352443341, Training Loss Force: 4.893686256849338, time: 1.7243359088897705
Validation Loss Energy: 7.534122629270153, Validation Loss Force: 6.016824043683872, time: 0.1165921688079834
Test Loss Energy: 9.752600697332834, Test Loss Force: 11.869934484625036, time: 9.797723770141602

Epoch 18, Batch 100/119, Loss: 0.3874833583831787

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 13.026463254374734, Training Loss Force: 5.998899628388486, time: 1.8152072429656982
Validation Loss Energy: 27.038128918603586, Validation Loss Force: 6.726250911531751, time: 0.1142568588256836
Test Loss Energy: 29.803000486970728, Test Loss Force: 12.257398076324709, time: 11.164584875106812

Epoch 19, Batch 100/119, Loss: 0.5221319794654846

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 13.236972023779407, Training Loss Force: 5.538992644438626, time: 1.8301587104797363
Validation Loss Energy: 8.46419962767664, Validation Loss Force: 3.7364151374118886, time: 0.14347553253173828
Test Loss Energy: 12.63739659980324, Test Loss Force: 11.060371411691603, time: 11.40366530418396

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–†â–‡â–‚â–â–„â–‚â–â–†â–„â–ƒâ–ƒâ–â–ˆâ–‚
wandb:   test_error_force â–‚â–‚â–‚â–‚â–‚â–‚â–„â–ˆâ–ƒâ–‚â–…â–â–‚â–…â–‚â–ƒâ–„â–ƒâ–„â–‚
wandb:          test_loss â–â–â–â–â–â–â–…â–ˆâ–‚â–â–„â–â–â–…â–ƒâ–ƒâ–„â–‚â–†â–‚
wandb: train_error_energy â–â–â–â–â–â–â–…â–†â–‡â–†â–…â–…â–†â–…â–…â–ˆâ–…â–…â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–†â–†â–ˆâ–ˆâ–…â–…â–†â–…â–…â–†â–‡â–…â–‡â–†
wandb:         train_loss â–â–â–â–â–â–â–†â–†â–ˆâ–‡â–…â–…â–†â–…â–…â–‡â–‡â–…â–‡â–‡
wandb: valid_error_energy â–â–â–â–â–â–â–†â–ˆâ–„â–â–„â–„â–â–†â–„â–…â–ƒâ–ƒâ–ˆâ–ƒ
wandb:  valid_error_force â–â–â–‚â–â–â–â–ƒâ–ˆâ–…â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–‚
wandb:         valid_loss â–â–â–‚â–â–â–â–…â–ˆâ–…â–‚â–„â–„â–‚â–„â–ƒâ–…â–„â–„â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 3781
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.6374
wandb:   test_error_force 11.06037
wandb:          test_loss 4.54654
wandb: train_error_energy 13.23697
wandb:  train_error_force 5.53899
wandb:         train_loss 2.73919
wandb: valid_error_energy 8.4642
wandb:  valid_error_force 3.73642
wandb:         valid_loss 1.81665
wandb: 
wandb: ğŸš€ View run al_77_33 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/jzg6hnxa
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_164104-jzg6hnxa/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.0389716625213623, Uncertainty Bias: -0.6602853536605835
3.0517578e-05 0.0028825402
-9.742468 75.43037
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 3 steps.
Found uncertainty sample 4 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_164950-hmwi5o4q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_34
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/hmwi5o4q
Training model 34. Added 100 samples to the dataset.
Epoch 0, Batch 100/121, Loss: 0.1120394766330719

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.055563755866178, Training Loss Force: 2.5820841984917484, time: 1.698488712310791
Validation Loss Energy: 1.2543115538588818, Validation Loss Force: 2.6112642318863446, time: 0.12141776084899902
Test Loss Energy: 9.246662309586855, Test Loss Force: 10.911340584451999, time: 9.68374752998352

Epoch 1, Batch 100/121, Loss: 0.1494930535554886

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6629207375555886, Training Loss Force: 2.444409455532714, time: 1.7325656414031982
Validation Loss Energy: 1.158299809725911, Validation Loss Force: 2.7127635086746085, time: 0.11680483818054199
Test Loss Energy: 9.257952693838194, Test Loss Force: 10.893834163659717, time: 9.725247859954834

Epoch 2, Batch 100/121, Loss: 0.15286487340927124

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.808380133254505, Training Loss Force: 2.44824070637827, time: 1.7379772663116455
Validation Loss Energy: 1.5129874140289223, Validation Loss Force: 3.0733873981850053, time: 0.11474037170410156
Test Loss Energy: 9.75119610502355, Test Loss Force: 10.945686225337264, time: 9.932831525802612

Epoch 3, Batch 100/121, Loss: 0.1824587881565094

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5597900899916117, Training Loss Force: 2.3968099782739714, time: 1.7402124404907227
Validation Loss Energy: 1.6368827906261272, Validation Loss Force: 2.6783905179560272, time: 0.11683535575866699
Test Loss Energy: 9.83814794484876, Test Loss Force: 11.025519572723416, time: 9.735923290252686

Epoch 4, Batch 100/121, Loss: 0.06383422017097473

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5641495686387155, Training Loss Force: 2.389526665576206, time: 1.730750560760498
Validation Loss Energy: 1.0445034809076883, Validation Loss Force: 2.592473924891985, time: 0.11280965805053711
Test Loss Energy: 9.672605703804813, Test Loss Force: 10.94640608320926, time: 9.832556247711182

Epoch 5, Batch 100/121, Loss: 0.29225432872772217

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 3.3595792349222067, Training Loss Force: 2.485299587542258, time: 1.709683895111084
Validation Loss Energy: 2.6105533235264455, Validation Loss Force: 2.6570432269359303, time: 0.12477684020996094
Test Loss Energy: 9.315197284611783, Test Loss Force: 10.896239764531613, time: 9.779170751571655

Epoch 6, Batch 100/121, Loss: 0.11777102947235107

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 13.568623117111061, Training Loss Force: 6.174024603564017, time: 1.768923282623291
Validation Loss Energy: 9.79072523492051, Validation Loss Force: 5.871939613610844, time: 0.11384010314941406
Test Loss Energy: 10.285472566729947, Test Loss Force: 11.797677213607017, time: 9.733743667602539

Epoch 7, Batch 100/121, Loss: 0.08549904823303223

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 12.252645089676067, Training Loss Force: 5.354793325493825, time: 1.7413532733917236
Validation Loss Energy: 22.048682203806653, Validation Loss Force: 9.316412936904616, time: 0.12215542793273926
Test Loss Energy: 29.0035144776099, Test Loss Force: 14.922265677876817, time: 9.806269645690918

Epoch 8, Batch 100/121, Loss: 0.2888335883617401

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.9333941199823, Training Loss Force: 6.483532901926138, time: 1.7405405044555664
Validation Loss Energy: 13.279859308813018, Validation Loss Force: 6.023013652156276, time: 0.11606383323669434
Test Loss Energy: 11.39416189981121, Test Loss Force: 12.224630448541777, time: 10.315137386322021

Epoch 9, Batch 100/121, Loss: 1.3159555196762085

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.59110605164372, Training Loss Force: 5.087914008265169, time: 1.7318110466003418
Validation Loss Energy: 12.210255299984523, Validation Loss Force: 6.102542758100234, time: 0.1168367862701416
Test Loss Energy: 11.104570015392486, Test Loss Force: 12.432074317652095, time: 9.698669910430908

Epoch 10, Batch 100/121, Loss: 1.7538830041885376

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.390373394686023, Training Loss Force: 5.091601459008237, time: 1.772594690322876
Validation Loss Energy: 15.824104581158016, Validation Loss Force: 9.250806887317232, time: 0.1627802848815918
Test Loss Energy: 12.719019456342911, Test Loss Force: 13.181963635648003, time: 9.812244653701782

Epoch 11, Batch 100/121, Loss: 0.6339614391326904

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.621203662603919, Training Loss Force: 3.8059549920151636, time: 1.7108900547027588
Validation Loss Energy: 7.933002191396674, Validation Loss Force: 3.6370137750935085, time: 0.1220705509185791
Test Loss Energy: 9.902920091101201, Test Loss Force: 11.286529272512594, time: 9.759396076202393

Epoch 12, Batch 100/121, Loss: 0.3281400799751282

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.259774233522727, Training Loss Force: 4.306031011319518, time: 1.7209503650665283
Validation Loss Energy: 8.534094878798987, Validation Loss Force: 5.952259491915041, time: 0.11725592613220215
Test Loss Energy: 11.760933325760703, Test Loss Force: 12.732911954565177, time: 9.846755981445312

Epoch 13, Batch 100/121, Loss: 0.4062111973762512

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.710493144441688, Training Loss Force: 5.1357081167598615, time: 1.6962051391601562
Validation Loss Energy: 2.001947630694433, Validation Loss Force: 7.187806494177441, time: 0.1204071044921875
Test Loss Energy: 9.382442448026023, Test Loss Force: 12.845498515695937, time: 9.605152368545532

Epoch 14, Batch 100/121, Loss: 0.2633957862854004

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 12.893066756383295, Training Loss Force: 5.56645132941549, time: 1.707869529724121
Validation Loss Energy: 30.961816972276093, Validation Loss Force: 6.4598110978045495, time: 0.11927318572998047
Test Loss Energy: 21.519350784710657, Test Loss Force: 12.512157954484724, time: 9.730064868927002

Epoch 15, Batch 100/121, Loss: 0.16295376420021057

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 6.4246672399978655, Training Loss Force: 4.075539097122776, time: 1.7355988025665283
Validation Loss Energy: 7.6035486422789775, Validation Loss Force: 3.336035156357058, time: 0.11337423324584961
Test Loss Energy: 12.504524221963289, Test Loss Force: 11.227593142346462, time: 10.030583620071411

Epoch 16, Batch 100/121, Loss: 0.3773238956928253

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.396136509832795, Training Loss Force: 3.197688201009839, time: 1.7107524871826172
Validation Loss Energy: 4.5338075998046365, Validation Loss Force: 3.422786090710891, time: 0.12121272087097168
Test Loss Energy: 9.098390161593903, Test Loss Force: 11.1969741016139, time: 9.719814777374268

Epoch 17, Batch 100/121, Loss: 0.37023282051086426

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.530121154862264, Training Loss Force: 3.1969866827857136, time: 1.7277662754058838
Validation Loss Energy: 3.444337643560852, Validation Loss Force: 3.1868853503476853, time: 0.11747479438781738
Test Loss Energy: 8.864753587946094, Test Loss Force: 10.929626193368504, time: 9.719460248947144

Epoch 18, Batch 100/121, Loss: 0.13980267941951752

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.933762224373329, Training Loss Force: 4.184787055922588, time: 1.8979172706604004
Validation Loss Energy: 4.613155453063809, Validation Loss Force: 5.67718011532584, time: 0.11697959899902344
Test Loss Energy: 9.892386306296745, Test Loss Force: 12.130908249577804, time: 9.692567110061646

Epoch 19, Batch 100/121, Loss: 1.2173113822937012

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.39255338200418, Training Loss Force: 5.8165393724567105, time: 1.7338557243347168
Validation Loss Energy: 12.2713669955305, Validation Loss Force: 8.511145705943084, time: 0.11309218406677246
Test Loss Energy: 11.244280070733309, Test Loss Force: 13.948993968538694, time: 9.711712121963501

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–ˆâ–‚â–‚â–‚â–â–‚â–â–…â–‚â–â–â–â–‚
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–ˆâ–ƒâ–„â–…â–‚â–„â–„â–„â–‚â–‚â–â–ƒâ–†
wandb:          test_loss â–â–â–â–â–â–â–‚â–ˆâ–ƒâ–ƒâ–„â–â–ƒâ–ƒâ–…â–‚â–â–â–‚â–„
wandb: train_error_energy â–â–â–â–â–â–‚â–ˆâ–‡â–‡â–‡â–‡â–„â–…â–†â–ˆâ–„â–„â–„â–…â–‡
wandb:  train_error_force â–â–â–â–â–â–â–‡â–†â–ˆâ–†â–†â–ƒâ–„â–†â–†â–„â–‚â–‚â–„â–‡
wandb:         train_loss â–â–â–â–â–â–‚â–ˆâ–‡â–ˆâ–†â–‡â–„â–…â–†â–‡â–„â–ƒâ–ƒâ–„â–‡
wandb: valid_error_energy â–â–â–â–â–â–â–ƒâ–†â–„â–„â–„â–ƒâ–ƒâ–â–ˆâ–ƒâ–‚â–‚â–‚â–„
wandb:  valid_error_force â–â–â–‚â–â–â–â–„â–ˆâ–…â–…â–ˆâ–‚â–„â–†â–…â–‚â–‚â–‚â–„â–‡
wandb:         valid_loss â–â–â–â–â–â–â–„â–ˆâ–…â–…â–‡â–ƒâ–„â–„â–‡â–‚â–‚â–‚â–ƒâ–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 3871
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.24428
wandb:   test_error_force 13.94899
wandb:          test_loss 5.41985
wandb: train_error_energy 11.39255
wandb:  train_error_force 5.81654
wandb:         train_loss 2.70863
wandb: valid_error_energy 12.27137
wandb:  valid_error_force 8.51115
wandb:         valid_loss 3.66906
wandb: 
wandb: ğŸš€ View run al_77_34 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/hmwi5o4q
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_164950-hmwi5o4q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.47911107540130615, Uncertainty Bias: -0.34120455384254456
4.196167e-05 0.9648628
-4.512501 26.978617
(48745, 22, 3)
Found uncertainty sample 0 after 7 steps.
Found uncertainty sample 1 after 33 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 9 steps.
Found uncertainty sample 4 after 5 steps.
Found uncertainty sample 5 after 8 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 7 steps.
Found uncertainty sample 8 after 8 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 60 steps.
Found uncertainty sample 11 after 8 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 73 steps.
Found uncertainty sample 14 after 7 steps.
Found uncertainty sample 15 after 14 steps.
Found uncertainty sample 16 after 14 steps.
Found uncertainty sample 17 after 60 steps.
Found uncertainty sample 18 after 13 steps.
Found uncertainty sample 19 after 19 steps.
Found uncertainty sample 20 after 18 steps.
Found uncertainty sample 21 after 14 steps.
Found uncertainty sample 22 after 18 steps.
Found uncertainty sample 23 after 19 steps.
Found uncertainty sample 24 after 58 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 11 steps.
Found uncertainty sample 27 after 48 steps.
Found uncertainty sample 28 after 4 steps.
Found uncertainty sample 29 after 56 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 65 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 17 steps.
Found uncertainty sample 34 after 9 steps.
Found uncertainty sample 35 after 98 steps.
Found uncertainty sample 36 after 11 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 14 steps.
Found uncertainty sample 40 after 32 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 6 steps.
Found uncertainty sample 44 after 14 steps.
Found uncertainty sample 45 after 63 steps.
Found uncertainty sample 46 after 42 steps.
Found uncertainty sample 47 after 18 steps.
Found uncertainty sample 48 after 21 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 19 steps.
Found uncertainty sample 51 after 21 steps.
Found uncertainty sample 52 after 77 steps.
Found uncertainty sample 53 after 33 steps.
Found uncertainty sample 54 after 8 steps.
Found uncertainty sample 55 after 27 steps.
Found uncertainty sample 56 after 11 steps.
Found uncertainty sample 57 after 13 steps.
Found uncertainty sample 58 after 4 steps.
Found uncertainty sample 59 after 43 steps.
Found uncertainty sample 60 after 75 steps.
Found uncertainty sample 61 after 12 steps.
Found uncertainty sample 62 after 16 steps.
Found uncertainty sample 63 after 16 steps.
Found uncertainty sample 64 after 20 steps.
Found uncertainty sample 65 after 49 steps.
Found uncertainty sample 66 after 44 steps.
Found uncertainty sample 67 after 5 steps.
Found uncertainty sample 68 after 87 steps.
Found uncertainty sample 69 after 7 steps.
Found uncertainty sample 70 after 37 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 6 steps.
Found uncertainty sample 73 after 20 steps.
Found uncertainty sample 74 after 14 steps.
Found uncertainty sample 75 after 39 steps.
Found uncertainty sample 76 after 88 steps.
Found uncertainty sample 77 after 81 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 22 steps.
Found uncertainty sample 81 after 18 steps.
Found uncertainty sample 82 after 13 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 73 steps.
Found uncertainty sample 86 after 52 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 144 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 6 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 4 steps.
Found uncertainty sample 94 after 22 steps.
Found uncertainty sample 95 after 15 steps.
Found uncertainty sample 96 after 5 steps.
Found uncertainty sample 97 after 21 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 23 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_165910-9907eg5y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_35
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/9907eg5y
Training model 35. Added 100 samples to the dataset.
Epoch 0, Batch 100/124, Loss: 0.10514256358146667

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.6593457138950507, Training Loss Force: 2.4904601740349204, time: 1.7703769207000732
Validation Loss Energy: 1.740307821522947, Validation Loss Force: 2.671621969704212, time: 0.12099266052246094
Test Loss Energy: 9.389613322507307, Test Loss Force: 10.883128473611375, time: 9.656235694885254

Epoch 1, Batch 100/124, Loss: 0.04990951716899872

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7119791148871817, Training Loss Force: 2.4265686942297955, time: 1.7701056003570557
Validation Loss Energy: 1.0348105816508786, Validation Loss Force: 2.77630281747254, time: 0.1177668571472168
Test Loss Energy: 9.572230253351536, Test Loss Force: 11.0558037517031, time: 9.861774444580078

Epoch 2, Batch 100/124, Loss: 0.03001880645751953

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5153735092119711, Training Loss Force: 2.4156417643619728, time: 1.7703440189361572
Validation Loss Energy: 1.3120055192397768, Validation Loss Force: 2.9803987588232475, time: 0.12060260772705078
Test Loss Energy: 9.749644546414725, Test Loss Force: 11.019458989557307, time: 10.644144296646118

Epoch 3, Batch 100/124, Loss: 0.04928719997406006

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3840849758599962, Training Loss Force: 2.3587643464921033, time: 1.7659668922424316
Validation Loss Energy: 1.4838526036337203, Validation Loss Force: 2.8786684256845603, time: 0.12163090705871582
Test Loss Energy: 9.750381089317173, Test Loss Force: 10.998708039152602, time: 10.375824451446533

Epoch 4, Batch 100/124, Loss: 0.05739157274365425

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.576149330049588, Training Loss Force: 2.3911662681352115, time: 1.9143011569976807
Validation Loss Energy: 1.1266017805688853, Validation Loss Force: 2.5386628084104914, time: 0.13698840141296387
Test Loss Energy: 9.68831859107765, Test Loss Force: 10.994273309146797, time: 10.716444253921509

Epoch 5, Batch 100/124, Loss: 0.1546197235584259

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.145968485622602, Training Loss Force: 2.354164018098705, time: 1.789581060409546
Validation Loss Energy: 3.5263433499688683, Validation Loss Force: 2.6455565421510507, time: 0.12096095085144043
Test Loss Energy: 11.322813193520227, Test Loss Force: 11.111281608529636, time: 10.12861967086792

Epoch 6, Batch 100/124, Loss: 0.9412802457809448

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 16.843473680952556, Training Loss Force: 5.748829998719521, time: 1.8277249336242676
Validation Loss Energy: 2.278754103402498, Validation Loss Force: 8.768662276316375, time: 0.11819267272949219
Test Loss Energy: 9.75101259300471, Test Loss Force: 13.445166087921713, time: 10.25227165222168

Epoch 7, Batch 100/124, Loss: 0.8566568493843079

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.659190595866853, Training Loss Force: 5.8131159800139125, time: 1.766716480255127
Validation Loss Energy: 11.871353156774624, Validation Loss Force: 6.126114216995956, time: 0.1193699836730957
Test Loss Energy: 17.332350355595214, Test Loss Force: 12.231861716491307, time: 10.547199726104736

Epoch 8, Batch 100/124, Loss: 0.6383589506149292

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.380529206570143, Training Loss Force: 5.928939465990887, time: 1.9373557567596436
Validation Loss Energy: 17.31856488312307, Validation Loss Force: 4.767525588753361, time: 0.12056469917297363
Test Loss Energy: 12.346523997068791, Test Loss Force: 11.076419459163926, time: 9.899079322814941

Epoch 9, Batch 100/124, Loss: 0.2980557978153229

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 12.864942197483694, Training Loss Force: 5.901655731141568, time: 1.7622005939483643
Validation Loss Energy: 15.582162975311887, Validation Loss Force: 5.046287170785721, time: 0.1188819408416748
Test Loss Energy: 11.48738682611101, Test Loss Force: 11.207461086260272, time: 9.864749670028687

Epoch 10, Batch 100/124, Loss: 0.07208561897277832

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.786823692916018, Training Loss Force: 5.03482759656793, time: 1.735023021697998
Validation Loss Energy: 8.672959671181003, Validation Loss Force: 6.733770300143635, time: 0.11781597137451172
Test Loss Energy: 9.770545466237923, Test Loss Force: 12.134513834295825, time: 9.929510831832886

Epoch 11, Batch 100/124, Loss: 0.5662094950675964

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.760784708162658, Training Loss Force: 5.094694900803309, time: 1.7767333984375
Validation Loss Energy: 15.81538188848154, Validation Loss Force: 7.636509316827986, time: 0.11760735511779785
Test Loss Energy: 12.190547254138876, Test Loss Force: 12.86936032427708, time: 9.796640157699585

Epoch 12, Batch 100/124, Loss: 0.3569481074810028

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.883627859645348, Training Loss Force: 4.817174733976729, time: 1.7555630207061768
Validation Loss Energy: 6.596293478771234, Validation Loss Force: 4.959496378654766, time: 0.11623501777648926
Test Loss Energy: 9.495007894059551, Test Loss Force: 11.650089371034362, time: 9.754231214523315

Epoch 13, Batch 100/124, Loss: 0.7291674613952637

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.948800068600256, Training Loss Force: 4.9618199521425765, time: 1.7943081855773926
Validation Loss Energy: 5.4681219920581325, Validation Loss Force: 8.08300311310483, time: 0.11878299713134766
Test Loss Energy: 11.251852677659674, Test Loss Force: 13.698948180395485, time: 10.78213882446289

Epoch 14, Batch 100/124, Loss: 1.9028578996658325

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.387868242451704, Training Loss Force: 5.021969649339401, time: 1.8514750003814697
Validation Loss Energy: 3.309018708881639, Validation Loss Force: 7.38745907272116, time: 0.11747550964355469
Test Loss Energy: 12.751586411427278, Test Loss Force: 12.950685774112676, time: 10.176730155944824

Epoch 15, Batch 100/124, Loss: 0.7023743987083435

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.83826592659966, Training Loss Force: 5.463335719364986, time: 1.9831914901733398
Validation Loss Energy: 25.19203840815521, Validation Loss Force: 7.9583270955935275, time: 0.12551045417785645
Test Loss Energy: 25.133864922534094, Test Loss Force: 14.372312384977926, time: 10.673827886581421

Epoch 16, Batch 100/124, Loss: 0.13976049423217773

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.701309130971676, Training Loss Force: 5.928984111154646, time: 1.9062821865081787
Validation Loss Energy: 25.742308495906574, Validation Loss Force: 9.7433017540052, time: 0.11624598503112793
Test Loss Energy: 25.69708944575359, Test Loss Force: 15.790612122947431, time: 10.242653608322144

Epoch 17, Batch 100/124, Loss: 1.852968692779541

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 14.368517925429929, Training Loss Force: 6.270020975086535, time: 1.792445421218872
Validation Loss Energy: 16.638393178343765, Validation Loss Force: 6.890827607086243, time: 0.11702370643615723
Test Loss Energy: 19.863841582268144, Test Loss Force: 13.075520831021295, time: 9.92343544960022

Epoch 18, Batch 100/124, Loss: 1.0953854322433472

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.967235122025319, Training Loss Force: 4.980487946137076, time: 1.8335535526275635
Validation Loss Energy: 8.310762178816333, Validation Loss Force: 5.001417111189812, time: 0.13150572776794434
Test Loss Energy: 14.587368615034903, Test Loss Force: 11.840484244731712, time: 12.322522640228271

Epoch 19, Batch 100/124, Loss: 0.28136080503463745

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.816525207930216, Training Loss Force: 5.07395856004807, time: 2.0458736419677734
Validation Loss Energy: 16.67656935807479, Validation Loss Force: 5.524013729039173, time: 0.12625384330749512
Test Loss Energy: 22.32477774678169, Test Loss Force: 12.036363116745996, time: 10.632716655731201

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–‚â–â–„â–‚â–‚â–â–‚â–â–‚â–‚â–ˆâ–ˆâ–…â–ƒâ–‡
wandb:   test_error_force â–â–â–â–â–â–â–…â–ƒâ–â–â–ƒâ–„â–‚â–…â–„â–†â–ˆâ–„â–‚â–ƒ
wandb:          test_loss â–â–â–â–â–â–‚â–ƒâ–„â–‚â–‚â–‚â–ƒâ–‚â–„â–ƒâ–‡â–ˆâ–…â–ƒâ–„
wandb: train_error_energy â–‚â–â–â–â–â–â–ˆâ–…â–…â–†â–„â–…â–…â–…â–…â–…â–†â–‡â–„â–…
wandb:  train_error_force â–â–â–â–â–â–â–‡â–‡â–‡â–‡â–†â–†â–…â–†â–†â–‡â–‡â–ˆâ–†â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–‡â–‡â–…â–†â–…â–†â–†â–†â–‡â–ˆâ–…â–†
wandb: valid_error_energy â–â–â–â–â–â–‚â–â–„â–†â–…â–ƒâ–…â–ƒâ–‚â–‚â–ˆâ–ˆâ–…â–ƒâ–…
wandb:  valid_error_force â–â–â–â–â–â–â–‡â–„â–ƒâ–ƒâ–…â–†â–ƒâ–†â–†â–†â–ˆâ–…â–ƒâ–„
wandb:         valid_loss â–â–â–â–â–â–â–…â–„â–„â–„â–„â–†â–ƒâ–…â–„â–‡â–ˆâ–…â–ƒâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 3961
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 22.32478
wandb:   test_error_force 12.03636
wandb:          test_loss 5.52139
wandb: train_error_energy 9.81653
wandb:  train_error_force 5.07396
wandb:         train_loss 2.35469
wandb: valid_error_energy 16.67657
wandb:  valid_error_force 5.52401
wandb:         valid_loss 2.96436
wandb: 
wandb: ğŸš€ View run al_77_35 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/9907eg5y
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_165910-9907eg5y/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.9951526522636414, Uncertainty Bias: -0.832405686378479
7.6293945e-06 0.0022830963
-11.284575 64.07822
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 2 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 4 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 6 steps.
Found uncertainty sample 10 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 4 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 3 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 3 steps.
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 4 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 10 steps.
Found uncertainty sample 47 after 5 steps.
Found uncertainty sample 48 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 5 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 8 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 5 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 3 steps.
Found uncertainty sample 68 after 11 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 2 steps.
Found uncertainty sample 73 after 9 steps.
Found uncertainty sample 74 after 38 steps.
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 6 steps.
Found uncertainty sample 78 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 6 steps.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 4 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 3 steps.
Found uncertainty sample 89 after 4 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 3 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 6 steps.
Found uncertainty sample 96 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 5 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_170753-4dh4zirm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_36
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/4dh4zirm
Training model 36. Added 100 samples to the dataset.
Epoch 0, Batch 100/127, Loss: 0.07823805510997772

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.59594814376477, Training Loss Force: 2.57511007209688, time: 1.8771581649780273
Validation Loss Energy: 3.2502141625796668, Validation Loss Force: 2.831958005597558, time: 0.12555336952209473
Test Loss Energy: 11.169746510161021, Test Loss Force: 11.07412614882677, time: 10.00062346458435

Epoch 1, Batch 100/127, Loss: 0.32686370611190796

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 3.439903024024442, Training Loss Force: 2.501172671733635, time: 1.8901941776275635
Validation Loss Energy: 2.6851570621855294, Validation Loss Force: 2.7426655998351532, time: 0.12369918823242188
Test Loss Energy: 10.271200598313966, Test Loss Force: 11.001004958828378, time: 10.029745817184448

Epoch 2, Batch 100/127, Loss: 0.045800354331731796

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 3.152950980588949, Training Loss Force: 2.463750684672967, time: 1.9892354011535645
Validation Loss Energy: 3.6649708512643744, Validation Loss Force: 2.6393086685854628, time: 0.13876128196716309
Test Loss Energy: 11.379636503596183, Test Loss Force: 10.976617469196944, time: 12.556720972061157

Epoch 3, Batch 100/127, Loss: 0.05757880210876465

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.0048823759945122, Training Loss Force: 2.390633636015928, time: 1.9839978218078613
Validation Loss Energy: 3.8851913788174426, Validation Loss Force: 2.811552159726444, time: 0.13576006889343262
Test Loss Energy: 9.348751762202845, Test Loss Force: 10.941362312286094, time: 10.682920932769775

Epoch 4, Batch 100/127, Loss: 0.21332955360412598

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.101471308688466, Training Loss Force: 2.4592103082503676, time: 1.9968090057373047
Validation Loss Energy: 2.6938713676575112, Validation Loss Force: 2.6523206370199475, time: 0.12478804588317871
Test Loss Energy: 9.558482016756464, Test Loss Force: 10.880410977385418, time: 10.970203638076782

Epoch 5, Batch 100/127, Loss: 0.16730865836143494

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.2011644382680453, Training Loss Force: 2.391350914700475, time: 1.8186473846435547
Validation Loss Energy: 3.5027191927086108, Validation Loss Force: 2.649473218204872, time: 0.12717175483703613
Test Loss Energy: 9.52827706602368, Test Loss Force: 10.908811319431814, time: 10.150632858276367

Epoch 6, Batch 100/127, Loss: 0.6688817143440247

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.68347211091352, Training Loss Force: 6.271588485363443, time: 1.9562528133392334
Validation Loss Energy: 9.228937808265178, Validation Loss Force: 4.656569404891447, time: 0.1468815803527832
Test Loss Energy: 15.930315416607385, Test Loss Force: 11.959424990538459, time: 12.320933818817139

Epoch 7, Batch 100/127, Loss: 0.11792074888944626

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 14.696373130464261, Training Loss Force: 6.17390617852847, time: 1.8191585540771484
Validation Loss Energy: 2.127759186626919, Validation Loss Force: 5.67138952996294, time: 0.12897872924804688
Test Loss Energy: 10.475444644180495, Test Loss Force: 12.312569092736544, time: 10.340601444244385

Epoch 8, Batch 100/127, Loss: 0.6489226222038269

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.316900229373254, Training Loss Force: 3.6036479373771293, time: 1.810652732849121
Validation Loss Energy: 6.486653418007852, Validation Loss Force: 3.817073175925701, time: 0.12398719787597656
Test Loss Energy: 9.548138309796137, Test Loss Force: 11.075371142151354, time: 9.943252563476562

Epoch 9, Batch 100/127, Loss: 0.5609754323959351

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.495772982494289, Training Loss Force: 3.263919659491999, time: 1.8359358310699463
Validation Loss Energy: 6.357113861011193, Validation Loss Force: 3.411257173011334, time: 0.12471675872802734
Test Loss Energy: 12.903812398682488, Test Loss Force: 11.337569410204056, time: 10.743272066116333

Epoch 10, Batch 100/127, Loss: 0.6289694309234619

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.6122280463714365, Training Loss Force: 3.2065847020847777, time: 2.1382851600646973
Validation Loss Energy: 8.657372237181667, Validation Loss Force: 3.7461664873195044, time: 0.14725661277770996
Test Loss Energy: 15.08754439989592, Test Loss Force: 11.601682175915373, time: 11.800161838531494

Epoch 11, Batch 100/127, Loss: 0.38869500160217285

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.84924604203187, Training Loss Force: 3.189872067707626, time: 1.9181458950042725
Validation Loss Energy: 7.281535656895507, Validation Loss Force: 3.1540760165948583, time: 0.14601659774780273
Test Loss Energy: 9.54358999287815, Test Loss Force: 11.025556286318933, time: 10.448913335800171

Epoch 12, Batch 100/127, Loss: 0.19288742542266846

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.1339062106838895, Training Loss Force: 3.282870948096796, time: 1.851728916168213
Validation Loss Energy: 11.908869048264927, Validation Loss Force: 3.184403925385466, time: 0.1192784309387207
Test Loss Energy: 11.355961555231717, Test Loss Force: 10.910917172659081, time: 10.071461200714111

Epoch 13, Batch 100/127, Loss: 0.6914262771606445

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.856650971981765, Training Loss Force: 3.1944169588786564, time: 1.816885232925415
Validation Loss Energy: 5.415730614051074, Validation Loss Force: 3.103944136634579, time: 0.11922049522399902
Test Loss Energy: 12.01256732082306, Test Loss Force: 11.48068281538289, time: 10.911720514297485

Epoch 14, Batch 100/127, Loss: 0.47302791476249695

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.008477805524949, Training Loss Force: 3.2405329329978474, time: 2.2572290897369385
Validation Loss Energy: 7.140946959985716, Validation Loss Force: 3.2392151974474177, time: 0.1481027603149414
Test Loss Energy: 12.827940151204878, Test Loss Force: 11.376984502410112, time: 11.810518503189087

Epoch 15, Batch 100/127, Loss: 0.8086167573928833

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.775714674838024, Training Loss Force: 3.594484560252389, time: 1.9342830181121826
Validation Loss Energy: 37.29516520689632, Validation Loss Force: 8.170679237759993, time: 0.12211179733276367
Test Loss Energy: 38.60232247825032, Test Loss Force: 14.573059270845008, time: 10.109410524368286

Epoch 16, Batch 100/127, Loss: 2.0115113258361816

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.8104303425233, Training Loss Force: 6.257849802566298, time: 1.8280715942382812
Validation Loss Energy: 4.6065714233109185, Validation Loss Force: 5.747494726673015, time: 0.1188359260559082
Test Loss Energy: 10.094755944277379, Test Loss Force: 11.868340859034735, time: 10.07370138168335

Epoch 17, Batch 100/127, Loss: 0.6834570169448853

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.96764994207028, Training Loss Force: 5.687886512486645, time: 1.8071856498718262
Validation Loss Energy: 5.9405999416498725, Validation Loss Force: 4.606231630194233, time: 0.1205587387084961
Test Loss Energy: 9.467141407702456, Test Loss Force: 11.274894513522879, time: 11.59909987449646

Epoch 18, Batch 100/127, Loss: 0.2629949152469635

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 14.857752819274518, Training Loss Force: 5.28613656681691, time: 2.149667501449585
Validation Loss Energy: 14.197029067506657, Validation Loss Force: 4.450284403167124, time: 0.1544358730316162
Test Loss Energy: 11.33321335352099, Test Loss Force: 10.986966212443715, time: 11.374191999435425

Epoch 19, Batch 100/127, Loss: 0.5347210168838501

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.712232900444901, Training Loss Force: 5.66769146241141, time: 1.8555009365081787
Validation Loss Energy: 6.467776121600483, Validation Loss Force: 5.575006151025415, time: 0.1340024471282959
Test Loss Energy: 12.806487767342743, Test Loss Force: 11.753285085936374, time: 9.830076694488525

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–ƒâ–â–â–‚â–‚â–â–â–‚â–‚â–ˆâ–â–â–â–‚
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–„â–â–‚â–‚â–â–â–‚â–‚â–ˆâ–ƒâ–‚â–â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–‚â–â–‚â–‚â–â–â–‚â–‚â–ˆâ–‚â–â–â–‚
wandb: train_error_energy â–â–‚â–‚â–â–‚â–â–†â–ˆâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–†â–†â–ˆâ–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–ˆâ–‡â–†â–‡
wandb:         train_loss â–â–â–â–â–â–â–‡â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–‡â–‡â–‡â–‡
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–ˆâ–â–‚â–ƒâ–‚
wandb:  valid_error_force â–â–â–â–â–â–â–„â–…â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–…â–ƒâ–ƒâ–…
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–ƒâ–‚â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 4051
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.80649
wandb:   test_error_force 11.75329
wandb:          test_loss 4.7897
wandb: train_error_energy 12.71223
wandb:  train_error_force 5.66769
wandb:         train_loss 2.74714
wandb: valid_error_energy 6.46778
wandb:  valid_error_force 5.57501
wandb:         valid_loss 2.29824
wandb: 
wandb: ğŸš€ View run al_77_36 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/4dh4zirm
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_170753-4dh4zirm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.9100809097290039, Uncertainty Bias: -0.6946537494659424
0.00024032593 0.0038642883
-2.1922462 62.725792
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 14 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 5 steps.
Found uncertainty sample 6 after 9 steps.
Found uncertainty sample 7 after 6 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 2 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 21 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 9 steps.
Found uncertainty sample 19 after 7 steps.
Found uncertainty sample 20 after 3 steps.
Found uncertainty sample 21 after 4 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 15 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 2 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 12 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 3 steps.
Found uncertainty sample 36 after 9 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 6 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 5 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 5 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 15 steps.
Found uncertainty sample 54 after 4 steps.
Found uncertainty sample 55 after 7 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 3 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 14 steps.
Found uncertainty sample 61 after 5 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 12 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 13 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 4 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 5 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 6 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 7 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 6 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 23 steps.
Found uncertainty sample 92 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 3 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 5 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_171653-35m5hpke
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_37
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/35m5hpke
Training model 37. Added 100 samples to the dataset.
Epoch 0, Batch 100/130, Loss: 0.1461751013994217

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.100767313546486, Training Loss Force: 2.4694324867531994, time: 1.9089012145996094
Validation Loss Energy: 1.2341782179108585, Validation Loss Force: 2.593405053037207, time: 0.13361811637878418
Test Loss Energy: 9.494703884224357, Test Loss Force: 10.942426251113092, time: 9.838589429855347

Epoch 1, Batch 100/130, Loss: 0.2806306779384613

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.6782118186868518, Training Loss Force: 2.4196584372523193, time: 1.8612682819366455
Validation Loss Energy: 4.883301668948028, Validation Loss Force: 2.7258382698082446, time: 0.12665700912475586
Test Loss Energy: 9.403264964993713, Test Loss Force: 10.938568493004398, time: 9.978248834609985

Epoch 2, Batch 100/130, Loss: 0.07585692405700684

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.937174194954736, Training Loss Force: 2.429324456917536, time: 1.8960151672363281
Validation Loss Energy: 0.992002957907602, Validation Loss Force: 2.5364165220663724, time: 0.12007927894592285
Test Loss Energy: 9.804711941233016, Test Loss Force: 10.969886248532356, time: 9.970577001571655

Epoch 3, Batch 100/130, Loss: 0.05472439527511597

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3625677100365579, Training Loss Force: 2.3472625539556073, time: 1.851341724395752
Validation Loss Energy: 1.265048827077379, Validation Loss Force: 2.742277190621607, time: 0.12006044387817383
Test Loss Energy: 9.680154052618146, Test Loss Force: 11.0029852033412, time: 9.868978500366211

Epoch 4, Batch 100/130, Loss: 0.1107628345489502

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.1871832791390595, Training Loss Force: 2.3365272582104093, time: 1.8432598114013672
Validation Loss Energy: 3.0394688751932635, Validation Loss Force: 2.5484303389427114, time: 0.12355709075927734
Test Loss Energy: 10.917051840531393, Test Loss Force: 11.088677887617823, time: 10.009957790374756

Epoch 5, Batch 100/130, Loss: 0.21970432996749878

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.5088209758029336, Training Loss Force: 2.314164707219455, time: 1.8085777759552002
Validation Loss Energy: 2.716219259596359, Validation Loss Force: 2.7080258514797513, time: 0.12416410446166992
Test Loss Energy: 9.518363080204008, Test Loss Force: 10.938633661618315, time: 9.861867666244507

Epoch 6, Batch 100/130, Loss: 0.5170652866363525

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.178595831600227, Training Loss Force: 5.3894727426902325, time: 1.8888988494873047
Validation Loss Energy: 21.672718711056344, Validation Loss Force: 6.3645842624109745, time: 0.12034082412719727
Test Loss Energy: 17.229594126633675, Test Loss Force: 18.158185823909037, time: 10.031496286392212

Epoch 7, Batch 100/130, Loss: 0.6635132431983948

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 12.469782387758869, Training Loss Force: 7.7102751709031265, time: 1.8069422245025635
Validation Loss Energy: 4.6009834668708125, Validation Loss Force: 7.089912082415651, time: 0.11974787712097168
Test Loss Energy: 9.665679000614624, Test Loss Force: 12.422916395008059, time: 10.010204315185547

Epoch 8, Batch 100/130, Loss: 0.5921932458877563

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.762904472903507, Training Loss Force: 5.548346775792407, time: 1.8804256916046143
Validation Loss Energy: 11.430500015302423, Validation Loss Force: 8.450717456229006, time: 0.11928105354309082
Test Loss Energy: 16.19254602881079, Test Loss Force: 13.199381804846851, time: 9.909160137176514

Epoch 9, Batch 100/130, Loss: 0.4638896882534027

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 7.4124385064517915, Training Loss Force: 4.049386628993191, time: 1.8510518074035645
Validation Loss Energy: 8.524138302161873, Validation Loss Force: 3.424446905647318, time: 0.12061071395874023
Test Loss Energy: 16.834153710061933, Test Loss Force: 11.561051879086385, time: 10.716532230377197

Epoch 10, Batch 100/130, Loss: 0.8832892179489136

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.368787817312391, Training Loss Force: 4.14804269141661, time: 1.8506808280944824
Validation Loss Energy: 17.18361134851232, Validation Loss Force: 5.859467914747429, time: 0.12184262275695801
Test Loss Energy: 22.025346371613583, Test Loss Force: 12.701999438229898, time: 9.821460247039795

Epoch 11, Batch 100/130, Loss: 0.5893503427505493

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 12.841328130648911, Training Loss Force: 5.401881135002622, time: 1.856731653213501
Validation Loss Energy: 2.0458706784127103, Validation Loss Force: 4.974062800396973, time: 0.12353992462158203
Test Loss Energy: 10.485905332883123, Test Loss Force: 11.061510251289745, time: 9.845819473266602

Epoch 12, Batch 100/130, Loss: 1.2460923194885254

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.868617892597845, Training Loss Force: 5.849948240667221, time: 1.80515456199646
Validation Loss Energy: 9.29701281334261, Validation Loss Force: 7.86396540405493, time: 0.1217195987701416
Test Loss Energy: 10.339833939474271, Test Loss Force: 13.708128346637968, time: 9.975195407867432

Epoch 13, Batch 100/130, Loss: 0.9744797348976135

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.375609031298028, Training Loss Force: 5.012057256983276, time: 1.8540709018707275
Validation Loss Energy: 6.862224864615236, Validation Loss Force: 4.2121116952849675, time: 0.12619805335998535
Test Loss Energy: 9.54112390527344, Test Loss Force: 11.673829348115893, time: 9.858918905258179

Epoch 14, Batch 100/130, Loss: 0.5563206672668457

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.95784512120228, Training Loss Force: 5.672497342078829, time: 1.8431291580200195
Validation Loss Energy: 4.540307015282074, Validation Loss Force: 6.3944683915937475, time: 0.12175321578979492
Test Loss Energy: 12.143220090647356, Test Loss Force: 12.846805357415963, time: 9.77489948272705

Epoch 15, Batch 100/130, Loss: 1.0227141380310059

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.813889189153318, Training Loss Force: 4.516205695170489, time: 2.0423269271850586
Validation Loss Energy: 2.3229185244749613, Validation Loss Force: 4.338306761709791, time: 0.12431669235229492
Test Loss Energy: 10.262881758296983, Test Loss Force: 11.712796061605317, time: 9.8914213180542

Epoch 16, Batch 100/130, Loss: 2.9196901321411133

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.309743207991106, Training Loss Force: 5.304397588352271, time: 1.8287732601165771
Validation Loss Energy: 26.140888048470483, Validation Loss Force: 6.444767273413473, time: 0.12276315689086914
Test Loss Energy: 19.501490640162345, Test Loss Force: 12.438915110189393, time: 9.898484945297241

Epoch 17, Batch 100/130, Loss: 0.6145198345184326

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.728265994147065, Training Loss Force: 5.350017241361031, time: 1.80012845993042
Validation Loss Energy: 15.208658937501395, Validation Loss Force: 5.945913718148293, time: 0.12770771980285645
Test Loss Energy: 12.3152463546297, Test Loss Force: 11.86187005168502, time: 9.9975266456604

Epoch 18, Batch 100/130, Loss: 0.8905800580978394

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.705634096562259, Training Loss Force: 4.49648568054587, time: 1.8110620975494385
Validation Loss Energy: 6.876883619829954, Validation Loss Force: 6.760338885743544, time: 0.11977505683898926
Test Loss Energy: 15.726035794973017, Test Loss Force: 13.352374249613323, time: 9.849379301071167

Epoch 19, Batch 100/130, Loss: 0.20016838610172272

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.79187102849711, Training Loss Force: 5.538176641745008, time: 1.8645482063293457
Validation Loss Energy: 17.484592342386325, Validation Loss Force: 6.037203466149671, time: 0.12128591537475586
Test Loss Energy: 21.50185615255923, Test Loss Force: 13.029631578604457, time: 9.774547815322876

wandb: - 0.039 MB of 0.056 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–‚â–â–…â–â–…â–…â–ˆâ–‚â–‚â–â–ƒâ–â–‡â–ƒâ–…â–ˆ
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–‚â–ƒâ–‚â–ƒâ–â–„â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–ƒ
wandb:          test_loss â–â–â–â–â–â–â–ˆâ–‚â–„â–ƒâ–„â–â–ƒâ–‚â–ƒâ–‚â–„â–‚â–„â–…
wandb: train_error_energy â–â–‚â–‚â–â–‚â–‚â–‡â–ˆâ–‡â–…â–…â–ˆâ–‡â–†â–‡â–†â–‡â–‡â–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–…â–ˆâ–…â–ƒâ–ƒâ–…â–†â–„â–…â–„â–…â–…â–„â–…
wandb:         train_loss â–â–â–â–â–â–â–†â–ˆâ–†â–„â–„â–†â–†â–…â–†â–…â–†â–†â–…â–†
wandb: valid_error_energy â–â–‚â–â–â–‚â–â–‡â–‚â–„â–ƒâ–†â–â–ƒâ–ƒâ–‚â–â–ˆâ–…â–ƒâ–†
wandb:  valid_error_force â–â–â–â–â–â–â–†â–†â–ˆâ–‚â–…â–„â–‡â–ƒâ–†â–ƒâ–†â–…â–†â–…
wandb:         valid_loss â–â–‚â–â–â–â–â–‡â–…â–‡â–ƒâ–†â–ƒâ–†â–ƒâ–…â–ƒâ–ˆâ–†â–…â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 4141
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 21.50186
wandb:   test_error_force 13.02963
wandb:          test_loss 5.79867
wandb: train_error_energy 10.79187
wandb:  train_error_force 5.53818
wandb:         train_loss 2.57529
wandb: valid_error_energy 17.48459
wandb:  valid_error_force 6.0372
wandb:         valid_loss 3.19015
wandb: 
wandb: ğŸš€ View run al_77_37 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/35m5hpke
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_171653-35m5hpke/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8920289874076843, Uncertainty Bias: -0.6320948004722595
4.7683716e-05 0.43285322
-8.520746 17.235266
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 18 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 6 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 10 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 3 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 5 steps.
Found uncertainty sample 24 after 3 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 14 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 5 steps.
Found uncertainty sample 33 after 28 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 6 steps.
Found uncertainty sample 38 after 6 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 4 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 12 steps.
Found uncertainty sample 43 after 7 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 2 steps.
Found uncertainty sample 52 after 4 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 11 steps.
Found uncertainty sample 65 after 11 steps.
Found uncertainty sample 66 after 7 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 5 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 7 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 11 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 3 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 6 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_172537-rcjs15hm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_38
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/rcjs15hm
Training model 38. Added 100 samples to the dataset.
Epoch 0, Batch 100/133, Loss: 0.09404797852039337

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.9379895412253243, Training Loss Force: 2.497508468475511, time: 1.8857553005218506
Validation Loss Energy: 3.3732832510878583, Validation Loss Force: 2.633646860211957, time: 0.12317848205566406
Test Loss Energy: 9.502785934840464, Test Loss Force: 10.869998860867586, time: 9.690935611724854

Epoch 1, Batch 100/133, Loss: 0.12951228022575378

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.837789347785184, Training Loss Force: 2.4467413413453016, time: 1.914337396621704
Validation Loss Energy: 1.432449542630863, Validation Loss Force: 2.5861071460024716, time: 0.1231842041015625
Test Loss Energy: 10.13693226446333, Test Loss Force: 11.015945867327776, time: 9.695422410964966

Epoch 2, Batch 100/133, Loss: 0.13434571027755737

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6151101238263341, Training Loss Force: 2.3460416376501865, time: 1.9391264915466309
Validation Loss Energy: 1.3458910686421597, Validation Loss Force: 2.5275527313744255, time: 0.12298917770385742
Test Loss Energy: 9.668587208883743, Test Loss Force: 10.96624172585897, time: 9.802716732025146

Epoch 3, Batch 100/133, Loss: 0.11750945448875427

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6701265722521426, Training Loss Force: 2.3063027357173644, time: 1.913841962814331
Validation Loss Energy: 0.9359363514239578, Validation Loss Force: 2.5357563822717455, time: 0.11923384666442871
Test Loss Energy: 9.588576542245782, Test Loss Force: 10.97591950518129, time: 9.7078275680542

Epoch 4, Batch 100/133, Loss: 0.14464589953422546

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6728617408441264, Training Loss Force: 2.3499333753177316, time: 1.920353651046753
Validation Loss Energy: 1.3782985588096357, Validation Loss Force: 2.8011032616076856, time: 0.1228020191192627
Test Loss Energy: 9.357288645443397, Test Loss Force: 11.056703302410048, time: 9.907461166381836

Epoch 5, Batch 100/133, Loss: 0.07230928540229797

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6656120168454578, Training Loss Force: 2.28476655596756, time: 1.8914618492126465
Validation Loss Energy: 1.7754565173228916, Validation Loss Force: 2.683132628533803, time: 0.12276029586791992
Test Loss Energy: 10.48195174543519, Test Loss Force: 11.066642791424677, time: 9.80234670639038

Epoch 6, Batch 100/133, Loss: 0.1207861602306366

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 14.51079399317086, Training Loss Force: 6.386011225456006, time: 1.9689044952392578
Validation Loss Energy: 4.004217908981315, Validation Loss Force: 4.972611888242107, time: 0.11940288543701172
Test Loss Energy: 12.118572215680414, Test Loss Force: 12.07407398091561, time: 9.694008827209473

Epoch 7, Batch 100/133, Loss: 0.5099992156028748

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.213171225816458, Training Loss Force: 6.210991619294702, time: 1.912123680114746
Validation Loss Energy: 1.6366443490823628, Validation Loss Force: 5.643530353993232, time: 0.12403059005737305
Test Loss Energy: 9.673197164351906, Test Loss Force: 11.955436582765653, time: 10.068150043487549

Epoch 8, Batch 100/133, Loss: 0.3979431986808777

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.057190704151413, Training Loss Force: 5.881592559381865, time: 1.8624398708343506
Validation Loss Energy: 7.421227442485499, Validation Loss Force: 9.495835281310308, time: 0.15618896484375
Test Loss Energy: 12.866181187113531, Test Loss Force: 14.924947752221465, time: 9.84341287612915

Epoch 9, Batch 100/133, Loss: 0.36610814929008484

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.42457719402451, Training Loss Force: 5.534554143618841, time: 1.8899617195129395
Validation Loss Energy: 2.8312200800479537, Validation Loss Force: 5.227979849803293, time: 0.13230133056640625
Test Loss Energy: 8.398573485016204, Test Loss Force: 11.88529333028746, time: 9.868156671524048

Epoch 10, Batch 100/133, Loss: 0.4301213324069977

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.225442460961369, Training Loss Force: 3.4977140780596168, time: 2.0601322650909424
Validation Loss Energy: 2.310846739397632, Validation Loss Force: 3.36947850339139, time: 0.1230459213256836
Test Loss Energy: 10.20246198944057, Test Loss Force: 11.45869908464822, time: 9.707191467285156

Epoch 11, Batch 100/133, Loss: 0.5462477207183838

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.444808171987479, Training Loss Force: 3.1431379266399926, time: 1.9363586902618408
Validation Loss Energy: 9.207825507417082, Validation Loss Force: 3.4389170600954357, time: 0.12543916702270508
Test Loss Energy: 10.030830029380798, Test Loss Force: 11.040704284118886, time: 9.703648328781128

Epoch 12, Batch 100/133, Loss: 0.34118378162384033

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.555968411752152, Training Loss Force: 3.107547396099316, time: 1.8986284732818604
Validation Loss Energy: 3.577755577518232, Validation Loss Force: 3.1947332501728614, time: 0.12090682983398438
Test Loss Energy: 9.068555433873337, Test Loss Force: 11.219369600060709, time: 10.019099712371826

Epoch 13, Batch 100/133, Loss: 0.6174511313438416

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.58846144537836, Training Loss Force: 3.082506424083058, time: 1.8595457077026367
Validation Loss Energy: 9.510313822962923, Validation Loss Force: 3.3430168732045744, time: 0.12156295776367188
Test Loss Energy: 16.847191443220755, Test Loss Force: 11.96835353831443, time: 9.662929058074951

Epoch 14, Batch 100/133, Loss: 0.16464251279830933

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.754124381856104, Training Loss Force: 3.077849278805529, time: 1.8417112827301025
Validation Loss Energy: 3.7733912046694136, Validation Loss Force: 3.337075466606832, time: 0.12077569961547852
Test Loss Energy: 11.258973809485886, Test Loss Force: 11.714542426571652, time: 9.839154243469238

Epoch 15, Batch 100/133, Loss: 0.40580227971076965

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 6.955382257891035, Training Loss Force: 3.126534241820005, time: 1.9681730270385742
Validation Loss Energy: 7.731658218020487, Validation Loss Force: 3.244497429150165, time: 0.17550325393676758
Test Loss Energy: 9.900474885432311, Test Loss Force: 11.362012760645941, time: 9.99534559249878

Epoch 16, Batch 100/133, Loss: 0.8843860626220703

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 14.110163492026754, Training Loss Force: 4.57190473756954, time: 1.8821897506713867
Validation Loss Energy: 39.811904901875344, Validation Loss Force: 6.3449785410042265, time: 0.12198710441589355
Test Loss Energy: 41.09901011171389, Test Loss Force: 13.094291844686234, time: 9.74614667892456

Epoch 17, Batch 100/133, Loss: 0.9001786708831787

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 13.443031112177009, Training Loss Force: 6.579426572791562, time: 1.8422839641571045
Validation Loss Energy: 15.35764062830559, Validation Loss Force: 4.682878952051594, time: 0.1254713535308838
Test Loss Energy: 21.527280231184353, Test Loss Force: 12.16340873967082, time: 10.616907119750977

Epoch 18, Batch 100/133, Loss: 0.8940299153327942

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.79832904196584, Training Loss Force: 5.586417819188908, time: 1.8335230350494385
Validation Loss Energy: 8.02704071578148, Validation Loss Force: 3.739120715981009, time: 0.12148904800415039
Test Loss Energy: 14.100043847676094, Test Loss Force: 11.46186655907808, time: 9.648991107940674

Epoch 19, Batch 100/133, Loss: 0.8892568349838257

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.148632114715069, Training Loss Force: 5.237400817295495, time: 1.9351656436920166
Validation Loss Energy: 13.200644017918721, Validation Loss Force: 5.396400260154792, time: 0.12028360366821289
Test Loss Energy: 11.978827661777466, Test Loss Force: 11.215929471341987, time: 9.69173812866211

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–â–‚â–â–â–â–â–ƒâ–‚â–â–ˆâ–„â–‚â–‚
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–ˆâ–ƒâ–‚â–â–‚â–ƒâ–‚â–‚â–…â–ƒâ–‚â–‚
wandb:          test_loss â–â–â–â–â–â–â–‚â–‚â–…â–‚â–‚â–â–â–ƒâ–‚â–â–ˆâ–„â–‚â–‚
wandb: train_error_energy â–â–‚â–â–â–â–â–ˆâ–†â–†â–†â–„â–„â–„â–„â–„â–„â–ˆâ–‡â–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–‡â–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–…â–ˆâ–†â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–‡â–†â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–ˆâ–‡â–†
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–â–‚â–â–â–‚â–â–ƒâ–‚â–‚â–ˆâ–„â–‚â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–„â–ˆâ–„â–‚â–‚â–‚â–‚â–‚â–‚â–…â–ƒâ–‚â–„
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–ƒâ–†â–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ˆâ–„â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 4231
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.97883
wandb:   test_error_force 11.21593
wandb:          test_loss 4.55452
wandb: train_error_energy 12.14863
wandb:  train_error_force 5.2374
wandb:         train_loss 2.56545
wandb: valid_error_energy 13.20064
wandb:  valid_error_force 5.3964
wandb:         valid_loss 2.68905
wandb: 
wandb: ğŸš€ View run al_77_38 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/rcjs15hm
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_172537-rcjs15hm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.651373565196991, Uncertainty Bias: -0.4860057830810547
3.0517578e-05 0.054121017
-6.923232 36.390167
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 5 steps.
Found uncertainty sample 3 after 6 steps.
Found uncertainty sample 4 after 9 steps.
Found uncertainty sample 5 after 5 steps.
Found uncertainty sample 6 after 7 steps.
Found uncertainty sample 7 after 11 steps.
Found uncertainty sample 8 after 9 steps.
Found uncertainty sample 9 after 7 steps.
Found uncertainty sample 10 after 8 steps.
Found uncertainty sample 11 after 19 steps.
Found uncertainty sample 12 after 5 steps.
Found uncertainty sample 13 after 8 steps.
Found uncertainty sample 14 after 11 steps.
Found uncertainty sample 15 after 17 steps.
Found uncertainty sample 16 after 16 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 6 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 4 steps.
Found uncertainty sample 29 after 17 steps.
Found uncertainty sample 30 after 5 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 8 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 5 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 6 steps.
Found uncertainty sample 42 after 6 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 12 steps.
Found uncertainty sample 49 after 11 steps.
Found uncertainty sample 50 after 18 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 8 steps.
Found uncertainty sample 53 after 4 steps.
Found uncertainty sample 54 after 26 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 19 steps.
Found uncertainty sample 57 after 17 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 13 steps.
Found uncertainty sample 61 after 9 steps.
Found uncertainty sample 62 after 26 steps.
Found uncertainty sample 63 after 2 steps.
Found uncertainty sample 64 after 14 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 3 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 26 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 4 steps.
Found uncertainty sample 71 after 43 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 4 steps.
Found uncertainty sample 74 after 20 steps.
Found uncertainty sample 75 after 8 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 4 steps.
Found uncertainty sample 78 after 4 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 21 steps.
Found uncertainty sample 81 after 14 steps.
Found uncertainty sample 82 after 13 steps.
Found uncertainty sample 83 after 8 steps.
Found uncertainty sample 84 after 12 steps.
Found uncertainty sample 85 after 4 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 5 steps.
Found uncertainty sample 89 after 10 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 5 steps.
Found uncertainty sample 92 after 4 steps.
Found uncertainty sample 93 after 21 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 34 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 6 steps.
Found uncertainty sample 98 after 9 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_173423-810x3ekd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_39
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/810x3ekd
Training model 39. Added 100 samples to the dataset.
Epoch 0, Batch 100/136, Loss: 0.3883460760116577

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7598847488333456, Training Loss Force: 2.4916389973275708, time: 1.961771011352539
Validation Loss Energy: 1.406298610125666, Validation Loss Force: 2.6823305000471223, time: 0.12618803977966309
Test Loss Energy: 9.262760887799809, Test Loss Force: 10.950714668229383, time: 9.893594741821289

Epoch 1, Batch 100/136, Loss: 0.11410640925168991

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.428268709146856, Training Loss Force: 2.3496722458257664, time: 1.927452802658081
Validation Loss Energy: 1.9827841302114981, Validation Loss Force: 2.581087812338016, time: 0.12329530715942383
Test Loss Energy: 9.340758289606152, Test Loss Force: 10.895326569218987, time: 9.722348928451538

Epoch 2, Batch 100/136, Loss: 0.22288914024829865

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.9071711465137242, Training Loss Force: 2.360097754406322, time: 1.9612152576446533
Validation Loss Energy: 1.970585609278612, Validation Loss Force: 2.5851798774557797, time: 0.12526893615722656
Test Loss Energy: 10.175249068503565, Test Loss Force: 11.085694717955601, time: 9.806800127029419

Epoch 3, Batch 100/136, Loss: 0.10587828606367111

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.809168321347117, Training Loss Force: 2.324983532483683, time: 1.9439697265625
Validation Loss Energy: 3.0030901550312135, Validation Loss Force: 2.666339292663172, time: 0.12520313262939453
Test Loss Energy: 9.517991798638988, Test Loss Force: 10.98246498480396, time: 9.731031656265259

Epoch 4, Batch 100/136, Loss: 0.05379798635840416

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.528183121972224, Training Loss Force: 2.2930495302450176, time: 1.9884865283966064
Validation Loss Energy: 1.3906319430219056, Validation Loss Force: 2.7685004905992305, time: 0.12793636322021484
Test Loss Energy: 10.169515433592101, Test Loss Force: 11.065559584138809, time: 9.877500295639038

Epoch 5, Batch 100/136, Loss: 0.08339989930391312

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7267577500390647, Training Loss Force: 2.331509914395359, time: 1.916001796722412
Validation Loss Energy: 4.027840359111454, Validation Loss Force: 2.844108095991205, time: 0.12300944328308105
Test Loss Energy: 9.501143489914275, Test Loss Force: 11.130616718918668, time: 9.70367693901062

Epoch 6, Batch 100/136, Loss: 0.8885748982429504

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.429567107931087, Training Loss Force: 5.444500285249995, time: 1.943774938583374
Validation Loss Energy: 5.853785101251158, Validation Loss Force: 5.129850893207792, time: 0.13165998458862305
Test Loss Energy: 10.54292724984756, Test Loss Force: 12.055528756096255, time: 9.909930229187012

Epoch 7, Batch 100/136, Loss: 0.2903943955898285

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 14.521803537241848, Training Loss Force: 6.383365952982598, time: 1.8995168209075928
Validation Loss Energy: 15.222947765041676, Validation Loss Force: 5.777744181701206, time: 0.12700629234313965
Test Loss Energy: 18.224279473070926, Test Loss Force: 12.35429829327747, time: 10.592743396759033

Epoch 8, Batch 100/136, Loss: 1.3035368919372559

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.582611646224233, Training Loss Force: 5.67900268848163, time: 1.9562747478485107
Validation Loss Energy: 3.2176629765900184, Validation Loss Force: 4.414460797185872, time: 0.12249755859375
Test Loss Energy: 12.030589965638871, Test Loss Force: 11.783402605803882, time: 9.80081820487976

Epoch 9, Batch 100/136, Loss: 1.445279598236084

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 14.952169138220592, Training Loss Force: 5.81625511993404, time: 1.9254615306854248
Validation Loss Energy: 4.291777816976719, Validation Loss Force: 6.758146485332738, time: 0.12625575065612793
Test Loss Energy: 11.192469162044508, Test Loss Force: 12.937389469614116, time: 9.833643436431885

Epoch 10, Batch 100/136, Loss: 0.5932252407073975

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.163480480487669, Training Loss Force: 6.223766917579536, time: 1.9799761772155762
Validation Loss Energy: 14.41438681207358, Validation Loss Force: 5.923529234246465, time: 0.12581396102905273
Test Loss Energy: 18.246649221171683, Test Loss Force: 13.032104707683729, time: 9.792316198348999

Epoch 11, Batch 100/136, Loss: 1.4715726375579834

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 12.013942137284566, Training Loss Force: 5.804109171295659, time: 1.9166746139526367
Validation Loss Energy: 2.886568875428175, Validation Loss Force: 4.40710366015832, time: 0.12355375289916992
Test Loss Energy: 11.123974701146068, Test Loss Force: 11.207087151318417, time: 9.692694664001465

Epoch 12, Batch 100/136, Loss: 0.4090040922164917

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.979059669197712, Training Loss Force: 4.2449109821841, time: 1.91752028465271
Validation Loss Energy: 10.325113668507722, Validation Loss Force: 3.562656605402015, time: 0.12752056121826172
Test Loss Energy: 10.563483151406887, Test Loss Force: 11.24507092625445, time: 9.971187353134155

Epoch 13, Batch 100/136, Loss: 2.059575319290161

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.661212474627444, Training Loss Force: 4.051384045488796, time: 1.974801778793335
Validation Loss Energy: 3.2104029975331407, Validation Loss Force: 5.485002985349209, time: 0.1266493797302246
Test Loss Energy: 9.884951605017555, Test Loss Force: 12.166743283576931, time: 9.687329530715942

Epoch 14, Batch 100/136, Loss: 1.0254456996917725

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.355379894124603, Training Loss Force: 4.935715483600326, time: 1.9677746295928955
Validation Loss Energy: 19.124723393401723, Validation Loss Force: 5.372092508616536, time: 0.12402558326721191
Test Loss Energy: 22.932506814113726, Test Loss Force: 13.328153484379259, time: 9.798426151275635

Epoch 15, Batch 100/136, Loss: 0.553626537322998

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.39011231055797, Training Loss Force: 4.952334848831304, time: 2.1445326805114746
Validation Loss Energy: 6.145105520006038, Validation Loss Force: 4.993332885177757, time: 0.12462115287780762
Test Loss Energy: 13.432960852911744, Test Loss Force: 11.824684257682796, time: 9.68528938293457

Epoch 16, Batch 100/136, Loss: 0.15370941162109375

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.110984151747543, Training Loss Force: 4.596685254330294, time: 2.0286643505096436
Validation Loss Energy: 9.267528746392019, Validation Loss Force: 4.664203987674824, time: 0.1248939037322998
Test Loss Energy: 14.443590284812114, Test Loss Force: 11.805223846352378, time: 9.765787839889526

Epoch 17, Batch 100/136, Loss: 1.217052936553955

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.592501251680233, Training Loss Force: 5.408443933798204, time: 1.9409172534942627
Validation Loss Energy: 4.9079213057366795, Validation Loss Force: 4.72783986065904, time: 0.13478779792785645
Test Loss Energy: 9.910392084216204, Test Loss Force: 12.010586437447973, time: 9.799835920333862

Epoch 18, Batch 100/136, Loss: 0.21076376736164093

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.43692339107173, Training Loss Force: 5.114940683948216, time: 1.9990863800048828
Validation Loss Energy: 18.131507371098632, Validation Loss Force: 6.453376256379431, time: 0.12203502655029297
Test Loss Energy: 15.710471893177525, Test Loss Force: 12.662275570369442, time: 9.689918279647827

Epoch 19, Batch 100/136, Loss: 0.10913075506687164

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.000298460415342, Training Loss Force: 5.63230619667716, time: 1.9921886920928955
Validation Loss Energy: 7.015930110235047, Validation Loss Force: 7.58206626531035, time: 0.12372803688049316
Test Loss Energy: 13.555906141260824, Test Loss Force: 13.444408798317063, time: 9.827709197998047

wandb: - 0.039 MB of 0.040 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–†â–‚â–‚â–†â–‚â–‚â–â–ˆâ–ƒâ–„â–â–„â–ƒ
wandb:   test_error_force â–â–â–‚â–â–â–‚â–„â–…â–ƒâ–‡â–‡â–‚â–‚â–„â–ˆâ–„â–ƒâ–„â–†â–ˆ
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–…â–ƒâ–„â–†â–‚â–‚â–ƒâ–ˆâ–ƒâ–„â–ƒâ–…â–†
wandb: train_error_energy â–‚â–â–â–â–â–â–‡â–ˆâ–†â–ˆâ–‡â–†â–„â–…â–†â–…â–…â–…â–…â–…
wandb:  train_error_force â–â–â–â–â–â–â–†â–ˆâ–‡â–‡â–ˆâ–‡â–„â–„â–†â–†â–…â–†â–†â–‡
wandb:         train_loss â–â–â–â–â–â–â–‡â–ˆâ–‡â–‡â–‡â–‡â–„â–…â–†â–…â–…â–†â–…â–†
wandb: valid_error_energy â–â–â–â–‚â–â–‚â–ƒâ–†â–‚â–‚â–†â–‚â–…â–‚â–ˆâ–ƒâ–„â–‚â–ˆâ–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–…â–…â–„â–‡â–†â–„â–‚â–…â–…â–„â–„â–„â–†â–ˆ
wandb:         valid_loss â–â–â–â–â–â–‚â–„â–‡â–ƒâ–†â–‡â–ƒâ–„â–„â–‡â–„â–„â–„â–ˆâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 4321
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 13.55591
wandb:   test_error_force 13.44441
wandb:          test_loss 5.40571
wandb: train_error_energy 10.0003
wandb:  train_error_force 5.63231
wandb:         train_loss 2.55381
wandb: valid_error_energy 7.01593
wandb:  valid_error_force 7.58207
wandb:         valid_loss 3.00649
wandb: 
wandb: ğŸš€ View run al_77_39 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/810x3ekd
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_173423-810x3ekd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5168967247009277, Uncertainty Bias: -0.35130298137664795
3.0517578e-05 0.012958527
1.7711148 16.251081
(48745, 22, 3)
Found uncertainty sample 0 after 21 steps.
Found uncertainty sample 1 after 7 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 35 steps.
Found uncertainty sample 4 after 10 steps.
Found uncertainty sample 5 after 12 steps.
Found uncertainty sample 6 after 34 steps.
Found uncertainty sample 7 after 4 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 21 steps.
Found uncertainty sample 10 after 17 steps.
Found uncertainty sample 11 after 32 steps.
Found uncertainty sample 12 after 4 steps.
Found uncertainty sample 13 after 32 steps.
Found uncertainty sample 14 after 5 steps.
Found uncertainty sample 15 after 4 steps.
Found uncertainty sample 16 after 10 steps.
Found uncertainty sample 17 after 20 steps.
Found uncertainty sample 18 after 42 steps.
Found uncertainty sample 19 after 21 steps.
Found uncertainty sample 20 after 16 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 22 steps.
Found uncertainty sample 23 after 22 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 17 steps.
Found uncertainty sample 26 after 41 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 23 steps.
Found uncertainty sample 30 after 6 steps.
Found uncertainty sample 31 after 14 steps.
Found uncertainty sample 32 after 5 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 8 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 7 steps.
Found uncertainty sample 41 after 12 steps.
Found uncertainty sample 42 after 11 steps.
Found uncertainty sample 43 after 5 steps.
Found uncertainty sample 44 after 10 steps.
Found uncertainty sample 45 after 4 steps.
Found uncertainty sample 46 after 6 steps.
Found uncertainty sample 47 after 35 steps.
Found uncertainty sample 48 after 25 steps.
Found uncertainty sample 49 after 14 steps.
Found uncertainty sample 50 after 20 steps.
Found uncertainty sample 51 after 23 steps.
Found uncertainty sample 52 after 14 steps.
Found uncertainty sample 53 after 45 steps.
Found uncertainty sample 54 after 4 steps.
Found uncertainty sample 55 after 33 steps.
Found uncertainty sample 56 after 38 steps.
Found uncertainty sample 57 after 14 steps.
Found uncertainty sample 58 after 19 steps.
Found uncertainty sample 59 after 5 steps.
Found uncertainty sample 60 after 5 steps.
Found uncertainty sample 61 after 15 steps.
Found uncertainty sample 62 after 12 steps.
Found uncertainty sample 63 after 21 steps.
Found uncertainty sample 64 after 21 steps.
Found uncertainty sample 65 after 15 steps.
Found uncertainty sample 66 after 8 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 125 steps.
Found uncertainty sample 69 after 18 steps.
Found uncertainty sample 70 after 39 steps.
Found uncertainty sample 71 after 52 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 21 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 5 steps.
Found uncertainty sample 76 after 3 steps.
Found uncertainty sample 77 after 18 steps.
Found uncertainty sample 78 after 6 steps.
Found uncertainty sample 79 after 3 steps.
Found uncertainty sample 80 after 14 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 26 steps.
Found uncertainty sample 83 after 35 steps.
Found uncertainty sample 84 after 14 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 49 steps.
Found uncertainty sample 88 after 11 steps.
Found uncertainty sample 89 after 6 steps.
Found uncertainty sample 90 after 29 steps.
Found uncertainty sample 91 after 15 steps.
Found uncertainty sample 92 after 10 steps.
Found uncertainty sample 93 after 18 steps.
Found uncertainty sample 94 after 9 steps.
Found uncertainty sample 95 after 6 steps.
Found uncertainty sample 96 after 7 steps.
Found uncertainty sample 97 after 6 steps.
Found uncertainty sample 98 after 14 steps.
Found uncertainty sample 99 after 12 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_174331-93bwhvux
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_40
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/93bwhvux
Training model 40. Added 100 samples to the dataset.
Epoch 0, Batch 100/138, Loss: 0.0677461177110672

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.9063248423632206, Training Loss Force: 2.4683418913312725, time: 2.0094451904296875
Validation Loss Energy: 2.9424396815095, Validation Loss Force: 2.574930713560918, time: 0.12699222564697266
Test Loss Energy: 9.369453445601756, Test Loss Force: 10.953122610041833, time: 9.86025357246399

Epoch 1, Batch 100/138, Loss: 0.2593227028846741

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.443556569738902, Training Loss Force: 2.364434037889488, time: 1.924628496170044
Validation Loss Energy: 3.2529426760708176, Validation Loss Force: 2.692756908582142, time: 0.12643218040466309
Test Loss Energy: 10.968171434967935, Test Loss Force: 11.09801355853066, time: 9.966129302978516

Epoch 2, Batch 100/138, Loss: 0.1024196594953537

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.460050060225076, Training Loss Force: 2.3408011029469566, time: 1.978898048400879
Validation Loss Energy: 1.7126869757624321, Validation Loss Force: 2.601670457510997, time: 0.13307929039001465
Test Loss Energy: 10.15864991139765, Test Loss Force: 11.04370834038016, time: 10.180105209350586

Epoch 3, Batch 100/138, Loss: 0.3026934862136841

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 3.6416311998240594, Training Loss Force: 2.4349456404348486, time: 1.9642574787139893
Validation Loss Energy: 6.123433280478984, Validation Loss Force: 3.005251381761589, time: 0.12443828582763672
Test Loss Energy: 9.691384439775629, Test Loss Force: 10.982948948614888, time: 9.886321306228638

Epoch 4, Batch 100/138, Loss: 0.0932547077536583

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.58086864975322, Training Loss Force: 2.404945445766532, time: 1.9978296756744385
Validation Loss Energy: 1.2252743838841686, Validation Loss Force: 2.5049633918583427, time: 0.12682843208312988
Test Loss Energy: 9.743933127122345, Test Loss Force: 11.02583665358722, time: 10.198314428329468

Epoch 5, Batch 100/138, Loss: 0.1415529102087021

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.4903047700899044, Training Loss Force: 2.3170144746603087, time: 2.030353546142578
Validation Loss Energy: 3.2871353047811964, Validation Loss Force: 2.7625911483585015, time: 0.1254262924194336
Test Loss Energy: 9.625444355381164, Test Loss Force: 10.9983038212069, time: 9.915145874023438

Epoch 6, Batch 100/138, Loss: 1.1337389945983887

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.975253250680737, Training Loss Force: 5.177500652768157, time: 2.0874407291412354
Validation Loss Energy: 28.70315972419321, Validation Loss Force: 6.849505112323256, time: 0.12964749336242676
Test Loss Energy: 30.632878311944257, Test Loss Force: 14.321064892181907, time: 9.924834728240967

Epoch 7, Batch 100/138, Loss: 0.16712519526481628

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.435322711867189, Training Loss Force: 5.834942972046945, time: 1.9180104732513428
Validation Loss Energy: 1.6568765130196537, Validation Loss Force: 4.4070931204854125, time: 0.1272590160369873
Test Loss Energy: 9.0272588223584, Test Loss Force: 11.778878365133028, time: 10.184801578521729

Epoch 8, Batch 100/138, Loss: 0.9809725880622864

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 13.29768424905388, Training Loss Force: 5.56025329550692, time: 1.9877042770385742
Validation Loss Energy: 2.844214086504667, Validation Loss Force: 6.4671452256870205, time: 0.12364387512207031
Test Loss Energy: 9.871095200181458, Test Loss Force: 13.077268772158758, time: 9.999829769134521

Epoch 9, Batch 100/138, Loss: 0.34735336899757385

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.402389210872643, Training Loss Force: 5.426733087029144, time: 1.9678702354431152
Validation Loss Energy: 16.203479959008334, Validation Loss Force: 4.650986495618649, time: 0.12767243385314941
Test Loss Energy: 20.29118124026984, Test Loss Force: 11.804376892196151, time: 10.141968727111816

Epoch 10, Batch 100/138, Loss: 0.7186213731765747

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.62088573738339, Training Loss Force: 5.935704499161957, time: 1.9261078834533691
Validation Loss Energy: 1.6484468900507407, Validation Loss Force: 3.9896251380568395, time: 0.1321558952331543
Test Loss Energy: 8.802623636110845, Test Loss Force: 11.453806211125956, time: 10.022468328475952

Epoch 11, Batch 100/138, Loss: 0.26538556814193726

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.674494736286642, Training Loss Force: 5.1779713495259365, time: 1.9542315006256104
Validation Loss Energy: 1.7050344681128076, Validation Loss Force: 5.042161259957775, time: 0.12524151802062988
Test Loss Energy: 10.487802422710919, Test Loss Force: 11.522245188935429, time: 9.977602005004883

Epoch 12, Batch 100/138, Loss: 0.5632756352424622

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.515010514756865, Training Loss Force: 4.389180233192858, time: 1.9166293144226074
Validation Loss Energy: 2.6036390754301446, Validation Loss Force: 5.498415238685023, time: 0.13029193878173828
Test Loss Energy: 10.497017302024025, Test Loss Force: 12.29791391888107, time: 10.918014764785767

Epoch 13, Batch 100/138, Loss: 0.41140180826187134

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.437667938850569, Training Loss Force: 3.3710680690417534, time: 1.9538724422454834
Validation Loss Energy: 4.690873219728693, Validation Loss Force: 3.3239636681719227, time: 0.1242828369140625
Test Loss Energy: 12.270028793600916, Test Loss Force: 11.781734116748272, time: 9.965723991394043

Epoch 14, Batch 100/138, Loss: 1.223867654800415

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.661725176220635, Training Loss Force: 3.892722756065227, time: 1.9680907726287842
Validation Loss Energy: 4.813921525783264, Validation Loss Force: 4.03019723779805, time: 0.12445807456970215
Test Loss Energy: 9.557534005782195, Test Loss Force: 11.126349576007446, time: 10.152286767959595

Epoch 15, Batch 100/138, Loss: 0.2761300802230835

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 13.070590268651006, Training Loss Force: 5.818653367031373, time: 2.062800168991089
Validation Loss Energy: 4.194932176892249, Validation Loss Force: 6.0632325986320454, time: 0.12435436248779297
Test Loss Energy: 9.864686656254495, Test Loss Force: 13.034152629627231, time: 9.954305171966553

Epoch 16, Batch 100/138, Loss: 0.34778088331222534

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.6225635839401, Training Loss Force: 5.165449725981042, time: 2.010218620300293
Validation Loss Energy: 18.782143646577683, Validation Loss Force: 5.571004786473093, time: 0.12771344184875488
Test Loss Energy: 14.127172452784254, Test Loss Force: 11.766503761675308, time: 9.914804458618164

Epoch 17, Batch 100/138, Loss: 1.2062233686447144

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.903995716028724, Training Loss Force: 4.43891434699483, time: 1.9143972396850586
Validation Loss Energy: 1.5324101756207655, Validation Loss Force: 6.213738533963166, time: 0.12329244613647461
Test Loss Energy: 9.41011028552558, Test Loss Force: 12.540948002510707, time: 10.172367095947266

Epoch 18, Batch 100/138, Loss: 0.5874335765838623

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.451809521801067, Training Loss Force: 4.646292584843243, time: 1.980879306793213
Validation Loss Energy: 21.39065687538914, Validation Loss Force: 4.820540737261211, time: 0.1290431022644043
Test Loss Energy: 14.939295582033592, Test Loss Force: 11.580361052600248, time: 9.960024356842041

Epoch 19, Batch 100/138, Loss: 0.7431269884109497

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.125205608583167, Training Loss Force: 4.56805375140081, time: 1.9457039833068848
Validation Loss Energy: 20.416450860304387, Validation Loss Force: 5.961097675655206, time: 0.12223052978515625
Test Loss Energy: 16.41728699572654, Test Loss Force: 11.818143054240373, time: 10.026737928390503

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.039 MB of 0.056 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–â–â–â–ˆâ–â–â–…â–â–‚â–‚â–‚â–â–â–ƒâ–â–ƒâ–ƒ
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–ƒâ–…â–ƒâ–‚â–‚â–„â–ƒâ–â–…â–ƒâ–„â–‚â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–ˆâ–‚â–ƒâ–„â–â–‚â–‚â–‚â–â–ƒâ–ƒâ–‚â–ƒâ–ƒ
wandb: train_error_energy â–â–‚â–‚â–‚â–â–â–‡â–†â–ˆâ–…â–ˆâ–†â–†â–„â–†â–ˆâ–†â–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–…â–ƒâ–„â–ˆâ–‡â–…â–†â–…
wandb:         train_loss â–â–â–â–‚â–â–â–‡â–‡â–ˆâ–†â–ˆâ–‡â–†â–ƒâ–…â–ˆâ–‡â–†â–†â–†
wandb: valid_error_energy â–â–‚â–â–‚â–â–‚â–ˆâ–â–â–…â–â–â–â–‚â–‚â–‚â–…â–â–†â–†
wandb:  valid_error_force â–â–â–â–‚â–â–â–ˆâ–„â–‡â–„â–ƒâ–…â–†â–‚â–ƒâ–‡â–†â–‡â–…â–‡
wandb:         valid_loss â–â–â–â–‚â–â–â–ˆâ–‚â–„â–…â–‚â–ƒâ–ƒâ–‚â–ƒâ–„â–†â–„â–†â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 4411
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 16.41729
wandb:   test_error_force 11.81814
wandb:          test_loss 5.05304
wandb: train_error_energy 10.12521
wandb:  train_error_force 4.56805
wandb:         train_loss 2.20607
wandb: valid_error_energy 20.41645
wandb:  valid_error_force 5.9611
wandb:         valid_loss 3.36088
wandb: 
wandb: ğŸš€ View run al_77_40 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/93bwhvux
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_174331-93bwhvux/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6738255023956299, Uncertainty Bias: -0.627101719379425
3.8146973e-05 0.042357683
-7.762519 21.674686
(48745, 22, 3)
Found uncertainty sample 0 after 26 steps.
Found uncertainty sample 1 after 5 steps.
Found uncertainty sample 2 after 354 steps.
Found uncertainty sample 3 after 146 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 64 steps.
Found uncertainty sample 7 after 36 steps.
Found uncertainty sample 8 after 4 steps.
Found uncertainty sample 9 after 8 steps.
Found uncertainty sample 10 after 82 steps.
Found uncertainty sample 11 after 7 steps.
Found uncertainty sample 12 after 61 steps.
Found uncertainty sample 13 after 91 steps.
Found uncertainty sample 14 after 45 steps.
Found uncertainty sample 15 after 24 steps.
Found uncertainty sample 16 after 42 steps.
Found uncertainty sample 17 after 24 steps.
Found uncertainty sample 18 after 100 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 20 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 42 steps.
Found uncertainty sample 25 after 31 steps.
Found uncertainty sample 26 after 7 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 36 steps.
Found uncertainty sample 29 after 48 steps.
Found uncertainty sample 30 after 42 steps.
Found uncertainty sample 31 after 76 steps.
Found uncertainty sample 32 after 28 steps.
Found uncertainty sample 33 after 77 steps.
Found uncertainty sample 34 after 13 steps.
Found uncertainty sample 35 after 145 steps.
Found uncertainty sample 36 after 60 steps.
Found uncertainty sample 37 after 19 steps.
Found uncertainty sample 38 after 15 steps.
Found uncertainty sample 39 after 111 steps.
Found uncertainty sample 40 after 70 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 22 steps.
Found uncertainty sample 43 after 27 steps.
Found uncertainty sample 44 after 58 steps.
Found uncertainty sample 45 after 154 steps.
Found uncertainty sample 46 after 361 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 65 steps.
Found uncertainty sample 49 after 13 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 137 steps.
Found uncertainty sample 52 after 65 steps.
Found uncertainty sample 53 after 84 steps.
Found uncertainty sample 54 after 5 steps.
Found uncertainty sample 55 after 26 steps.
Found uncertainty sample 56 after 198 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 160 steps.
Found uncertainty sample 59 after 21 steps.
Found uncertainty sample 60 after 178 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 7 steps.
Found uncertainty sample 63 after 17 steps.
Found uncertainty sample 64 after 75 steps.
Found uncertainty sample 65 after 29 steps.
Found uncertainty sample 66 after 24 steps.
Found uncertainty sample 67 after 63 steps.
Found uncertainty sample 68 after 26 steps.
Found uncertainty sample 69 after 78 steps.
Found uncertainty sample 70 after 86 steps.
Found uncertainty sample 71 after 7 steps.
Found uncertainty sample 72 after 78 steps.
Found uncertainty sample 73 after 37 steps.
Found uncertainty sample 74 after 124 steps.
Found uncertainty sample 75 after 103 steps.
Found uncertainty sample 76 after 58 steps.
Found uncertainty sample 77 after 11 steps.
Found uncertainty sample 78 after 39 steps.
Found uncertainty sample 79 after 6 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 12 steps.
Found uncertainty sample 82 after 44 steps.
Found uncertainty sample 83 after 40 steps.
Found uncertainty sample 84 after 26 steps.
Found uncertainty sample 85 after 20 steps.
Found uncertainty sample 86 after 52 steps.
Found uncertainty sample 87 after 10 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 19 steps.
Found uncertainty sample 90 after 37 steps.
Found uncertainty sample 91 after 16 steps.
Found uncertainty sample 92 after 162 steps.
Found uncertainty sample 93 after 75 steps.
Found uncertainty sample 94 after 86 steps.
Found uncertainty sample 95 after 31 steps.
Found uncertainty sample 96 after 39 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_175359-r59oviip
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_41
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/r59oviip
Training model 41. Added 100 samples to the dataset.
Epoch 0, Batch 100/141, Loss: 0.17526982724666595

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.2196359903822347, Training Loss Force: 2.4747815871909533, time: 2.0459139347076416
Validation Loss Energy: 2.7953786605350275, Validation Loss Force: 2.532374037945102, time: 0.12556219100952148
Test Loss Energy: 9.406346701310124, Test Loss Force: 10.948719743874223, time: 10.377538442611694

Epoch 1, Batch 100/141, Loss: 0.11507299542427063

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5138348722685442, Training Loss Force: 2.3342671686022896, time: 2.0925400257110596
Validation Loss Energy: 1.0266073760918504, Validation Loss Force: 2.667790279041086, time: 0.12804841995239258
Test Loss Energy: 9.687306791805009, Test Loss Force: 10.98550254169653, time: 9.804114818572998

Epoch 2, Batch 100/141, Loss: 0.0627632886171341

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7512939823896987, Training Loss Force: 2.3505858370223547, time: 2.0467302799224854
Validation Loss Energy: 2.3241094332904315, Validation Loss Force: 2.5866381376924608, time: 0.12470221519470215
Test Loss Energy: 9.578128724052952, Test Loss Force: 11.107163124669784, time: 9.889902114868164

Epoch 3, Batch 100/141, Loss: 0.13366907835006714

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6188840598118306, Training Loss Force: 2.3143696132227762, time: 1.9891047477722168
Validation Loss Energy: 2.5855708102529804, Validation Loss Force: 2.5041479774459554, time: 0.1253373622894287
Test Loss Energy: 9.60736718810139, Test Loss Force: 11.002953409451424, time: 9.717835187911987

Epoch 4, Batch 100/141, Loss: 0.1540679782629013

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5179739845342575, Training Loss Force: 2.2755194234623084, time: 2.0201492309570312
Validation Loss Energy: 1.1117125256766076, Validation Loss Force: 2.545209652416012, time: 0.13292670249938965
Test Loss Energy: 10.1702705641221, Test Loss Force: 11.150552891441631, time: 9.8899667263031

Epoch 5, Batch 100/141, Loss: 0.16945776343345642

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.378165707843438, Training Loss Force: 2.3085760842053507, time: 2.023995876312256
Validation Loss Energy: 1.328835539852199, Validation Loss Force: 2.4831555468214184, time: 0.1242377758026123
Test Loss Energy: 9.684001761470093, Test Loss Force: 11.117183646600614, time: 9.71454930305481

Epoch 6, Batch 100/141, Loss: 0.38457825779914856

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.765616391649434, Training Loss Force: 5.554087343348277, time: 2.08719539642334
Validation Loss Energy: 16.376343657943774, Validation Loss Force: 6.517168294657989, time: 0.12613725662231445
Test Loss Energy: 22.013825082955343, Test Loss Force: 13.919553707953694, time: 9.701566696166992

Epoch 7, Batch 100/141, Loss: 1.0246360301971436

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 12.78962991591685, Training Loss Force: 5.9886996182265255, time: 2.0104475021362305
Validation Loss Energy: 6.605948116046469, Validation Loss Force: 7.048469643206058, time: 0.12627696990966797
Test Loss Energy: 10.33145684174755, Test Loss Force: 13.462060278148561, time: 9.809461116790771

Epoch 8, Batch 100/141, Loss: 0.9918662905693054

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.846175320284313, Training Loss Force: 5.8964461786913525, time: 2.025840997695923
Validation Loss Energy: 5.135299161870353, Validation Loss Force: 4.140203069745145, time: 0.12784266471862793
Test Loss Energy: 10.1180122746475, Test Loss Force: 11.574754728481514, time: 9.568842649459839

Epoch 9, Batch 100/141, Loss: 0.8059141635894775

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.980411139334075, Training Loss Force: 4.725939477409802, time: 1.9704735279083252
Validation Loss Energy: 10.292719761158146, Validation Loss Force: 4.226669804045726, time: 0.12443852424621582
Test Loss Energy: 10.501201863827289, Test Loss Force: 11.590318555665357, time: 9.88339614868164

Epoch 10, Batch 100/141, Loss: 0.6546198129653931

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.265403139091831, Training Loss Force: 3.6481040029913117, time: 1.982694149017334
Validation Loss Energy: 3.986899816156781, Validation Loss Force: 3.4927497754840826, time: 0.12770628929138184
Test Loss Energy: 11.728348524502067, Test Loss Force: 11.323865478939988, time: 9.678343296051025

Epoch 11, Batch 100/141, Loss: 0.4846479296684265

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.413157992124023, Training Loss Force: 3.0896532134775807, time: 1.9957873821258545
Validation Loss Energy: 2.1326828838931986, Validation Loss Force: 4.772587006886397, time: 0.12675023078918457
Test Loss Energy: 11.02064763709571, Test Loss Force: 11.759637950918787, time: 9.61638331413269

Epoch 12, Batch 100/141, Loss: 3.7778444290161133

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 13.999565794904301, Training Loss Force: 4.982121785834846, time: 2.0489213466644287
Validation Loss Energy: 17.601391513083485, Validation Loss Force: 8.274228109813473, time: 0.12504315376281738
Test Loss Energy: 21.820275055077005, Test Loss Force: 13.476784215966877, time: 9.82602858543396

Epoch 13, Batch 100/141, Loss: 0.1598607897758484

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 12.009598301193476, Training Loss Force: 6.002200062757051, time: 2.019705295562744
Validation Loss Energy: 2.5259364502671837, Validation Loss Force: 6.241311654303405, time: 0.12976837158203125
Test Loss Energy: 9.150327481072395, Test Loss Force: 12.596358620250012, time: 9.8365478515625

Epoch 14, Batch 100/141, Loss: 1.091496229171753

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.952932141452234, Training Loss Force: 5.311517946323018, time: 2.0639286041259766
Validation Loss Energy: 14.07099892469592, Validation Loss Force: 9.03238656558594, time: 0.1257767677307129
Test Loss Energy: 19.171974812077803, Test Loss Force: 14.86248980756846, time: 9.693745374679565

Epoch 15, Batch 100/141, Loss: 1.9515131711959839

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.264885650732797, Training Loss Force: 5.453001520525229, time: 2.2219810485839844
Validation Loss Energy: 5.883413606254407, Validation Loss Force: 3.320207981910608, time: 0.12549781799316406
Test Loss Energy: 11.264401499322458, Test Loss Force: 11.316436098404155, time: 9.652895212173462

Epoch 16, Batch 100/141, Loss: 0.39782679080963135

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.262125312398431, Training Loss Force: 3.1404177643876348, time: 2.052901268005371
Validation Loss Energy: 9.562220079124051, Validation Loss Force: 3.1817312276268948, time: 0.12406063079833984
Test Loss Energy: 10.223999915560563, Test Loss Force: 11.098889499313302, time: 9.762341737747192

Epoch 17, Batch 100/141, Loss: 0.13403411209583282

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.412314113962088, Training Loss Force: 3.099042103812827, time: 2.0005693435668945
Validation Loss Energy: 4.568076678255117, Validation Loss Force: 3.556212782988011, time: 0.12808704376220703
Test Loss Energy: 9.40348950875737, Test Loss Force: 11.32260039642837, time: 9.807662010192871

Epoch 18, Batch 100/141, Loss: 0.5173981785774231

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.5699234350888025, Training Loss Force: 3.0403277243020312, time: 2.060155153274536
Validation Loss Energy: 5.251338436995507, Validation Loss Force: 3.144090641414872, time: 0.1257936954498291
Test Loss Energy: 12.357114631288077, Test Loss Force: 11.422231376491283, time: 9.672253608703613

Epoch 19, Batch 100/141, Loss: 0.6138667464256287

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.2957899233422, Training Loss Force: 4.938312651545669, time: 2.0468499660491943
Validation Loss Energy: 10.81938009092069, Validation Loss Force: 3.9119340174664194, time: 0.13730692863464355
Test Loss Energy: 11.090374197492801, Test Loss Force: 11.212421968302419, time: 10.505333662033081

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–‚â–â–ˆâ–‚â–‚â–‚â–‚â–‚â–ˆâ–â–†â–‚â–‚â–â–ƒâ–‚
wandb:   test_error_force â–â–â–â–â–â–â–†â–…â–‚â–‚â–‚â–‚â–†â–„â–ˆâ–‚â–â–‚â–‚â–
wandb:          test_loss â–â–â–â–â–â–â–ˆâ–„â–‚â–‚â–‚â–‚â–‡â–ƒâ–ˆâ–‚â–â–â–‚â–‚
wandb: train_error_energy â–â–â–â–â–â–â–†â–‡â–†â–†â–„â–„â–ˆâ–‡â–…â–…â–„â–„â–„â–†
wandb:  train_error_force â–â–â–â–â–â–â–‡â–ˆâ–ˆâ–†â–„â–ƒâ–†â–ˆâ–‡â–‡â–ƒâ–ƒâ–‚â–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–ˆâ–‡â–†â–„â–ƒâ–‡â–ˆâ–†â–†â–ƒâ–ƒâ–ƒâ–†
wandb: valid_error_energy â–‚â–â–‚â–‚â–â–â–‡â–ƒâ–ƒâ–…â–‚â–â–ˆâ–‚â–‡â–ƒâ–…â–‚â–ƒâ–…
wandb:  valid_error_force â–â–â–â–â–â–â–…â–†â–ƒâ–ƒâ–‚â–ƒâ–‡â–…â–ˆâ–‚â–‚â–‚â–‚â–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–†â–…â–ƒâ–„â–‚â–ƒâ–ˆâ–„â–ˆâ–‚â–ƒâ–‚â–‚â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 4501
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.09037
wandb:   test_error_force 11.21242
wandb:          test_loss 4.49389
wandb: train_error_energy 10.29579
wandb:  train_error_force 4.93831
wandb:         train_loss 2.34138
wandb: valid_error_energy 10.81938
wandb:  valid_error_force 3.91193
wandb:         valid_loss 2.03299
wandb: 
wandb: ğŸš€ View run al_77_41 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/r59oviip
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_175359-r59oviip/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.199563980102539, Uncertainty Bias: -0.8933095335960388
5.340576e-05 0.00023078918
-8.216055 86.88072
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 2 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 5 steps.
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 4 steps.
Found uncertainty sample 87 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_180240-gxvzvsgk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_42
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/gxvzvsgk
Training model 42. Added 100 samples to the dataset.
Epoch 0, Batch 100/144, Loss: 0.05426820367574692

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7182351863398582, Training Loss Force: 2.432033936526387, time: 2.117933750152588
Validation Loss Energy: 0.9272894961474306, Validation Loss Force: 2.542967819454811, time: 0.12651586532592773
Test Loss Energy: 9.91229112327338, Test Loss Force: 11.089185820421635, time: 9.818050622940063

Epoch 1, Batch 100/144, Loss: 0.2239263653755188

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.9150692356505823, Training Loss Force: 2.35921410554465, time: 2.065981149673462
Validation Loss Energy: 3.303476399616448, Validation Loss Force: 2.6668927739368997, time: 0.12913894653320312
Test Loss Energy: 9.49645800413917, Test Loss Force: 11.031278141915994, time: 9.81084418296814

Epoch 2, Batch 100/144, Loss: 0.3309237062931061

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6162249374605184, Training Loss Force: 2.377659496485654, time: 2.0683743953704834
Validation Loss Energy: 3.5445922740464284, Validation Loss Force: 2.76933490611207, time: 0.12808775901794434
Test Loss Energy: 11.182878790271328, Test Loss Force: 11.267394993371887, time: 10.012158870697021

Epoch 3, Batch 100/144, Loss: 0.10214352607727051

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.2430357480236918, Training Loss Force: 2.3460696226810085, time: 2.015080690383911
Validation Loss Energy: 2.739987261124885, Validation Loss Force: 2.484398189101793, time: 0.13457989692687988
Test Loss Energy: 11.104775603133564, Test Loss Force: 11.205661504533868, time: 9.84839129447937

Epoch 4, Batch 100/144, Loss: 0.3657025694847107

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.3161360690659847, Training Loss Force: 2.329217355256069, time: 2.0410518646240234
Validation Loss Energy: 4.983095448524582, Validation Loss Force: 2.7765812281279985, time: 0.12784361839294434
Test Loss Energy: 12.044977852581122, Test Loss Force: 11.32423209261359, time: 9.996235370635986

Epoch 5, Batch 100/144, Loss: 0.3601108193397522

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 3.0777882350509063, Training Loss Force: 2.3642192950793985, time: 2.097975254058838
Validation Loss Energy: 4.043638438122835, Validation Loss Force: 2.718012191559369, time: 0.13470745086669922
Test Loss Energy: 11.57184987773669, Test Loss Force: 11.31220849966796, time: 9.814564228057861

Epoch 6, Batch 100/144, Loss: 1.050049066543579

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.648986993641481, Training Loss Force: 6.370770468993647, time: 2.0646140575408936
Validation Loss Energy: 15.264771562275195, Validation Loss Force: 6.680158546770795, time: 0.1302943229675293
Test Loss Energy: 18.30516632764262, Test Loss Force: 13.181828462652547, time: 9.833327531814575

Epoch 7, Batch 100/144, Loss: 1.3813846111297607

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.397978137694807, Training Loss Force: 5.788188598603404, time: 2.0893771648406982
Validation Loss Energy: 5.760793567190503, Validation Loss Force: 9.549851847190446, time: 0.127516508102417
Test Loss Energy: 11.041789417317997, Test Loss Force: 14.912313071201305, time: 10.740251302719116

Epoch 8, Batch 100/144, Loss: 0.9639501571655273

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.32449099788014, Training Loss Force: 5.577816773764127, time: 2.0689637660980225
Validation Loss Energy: 12.94281957033966, Validation Loss Force: 5.172115426904152, time: 0.13401079177856445
Test Loss Energy: 18.755335919755478, Test Loss Force: 12.435272564724757, time: 9.778240442276001

Epoch 9, Batch 100/144, Loss: 0.32810544967651367

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.07512435947271, Training Loss Force: 5.623540766442042, time: 2.0203447341918945
Validation Loss Energy: 12.331461241525687, Validation Loss Force: 4.519643318539976, time: 0.1287696361541748
Test Loss Energy: 11.147622276361075, Test Loss Force: 11.24800161594666, time: 9.97420072555542

Epoch 10, Batch 100/144, Loss: 0.6482334136962891

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.769810491300295, Training Loss Force: 4.638642151136195, time: 2.0433125495910645
Validation Loss Energy: 3.431251897416017, Validation Loss Force: 4.500732770206814, time: 0.13391923904418945
Test Loss Energy: 10.963734480602279, Test Loss Force: 11.983325867645354, time: 9.746377944946289

Epoch 11, Batch 100/144, Loss: 0.6564196348190308

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.58793912670926, Training Loss Force: 4.754747712082727, time: 2.06046724319458
Validation Loss Energy: 19.740716506265137, Validation Loss Force: 5.0087279330350105, time: 0.1281881332397461
Test Loss Energy: 14.96355725776854, Test Loss Force: 11.559102794264309, time: 9.824928522109985

Epoch 12, Batch 100/144, Loss: 1.4049489498138428

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.445500727641864, Training Loss Force: 5.31703569588548, time: 2.1066017150878906
Validation Loss Energy: 1.8166062208286384, Validation Loss Force: 3.827301037805601, time: 0.14042234420776367
Test Loss Energy: 9.267879195762314, Test Loss Force: 11.177643113518585, time: 9.969806671142578

Epoch 13, Batch 100/144, Loss: 0.6180371046066284

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.469587989505367, Training Loss Force: 4.2701736662726, time: 2.0233757495880127
Validation Loss Energy: 6.698636077838176, Validation Loss Force: 3.523695294262446, time: 0.13866782188415527
Test Loss Energy: 11.853412508608843, Test Loss Force: 11.684856138730733, time: 9.906563520431519

Epoch 14, Batch 100/144, Loss: 0.3610062599182129

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.334547562339617, Training Loss Force: 3.1305918594674758, time: 2.127086877822876
Validation Loss Energy: 7.338277099337408, Validation Loss Force: 3.2766481817923867, time: 0.1328599452972412
Test Loss Energy: 12.16332228342023, Test Loss Force: 11.416468498475858, time: 10.083823919296265

Epoch 15, Batch 100/144, Loss: 1.0448881387710571

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 12.861094999191513, Training Loss Force: 4.2336655892535875, time: 2.127631425857544
Validation Loss Energy: 3.623855958368441, Validation Loss Force: 7.172829767422338, time: 0.13190984725952148
Test Loss Energy: 10.794716810202234, Test Loss Force: 16.371346032772337, time: 9.92418909072876

Epoch 16, Batch 100/144, Loss: 0.2910841107368469

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.854840719577895, Training Loss Force: 5.719172579798459, time: 2.0818240642547607
Validation Loss Energy: 14.220515912126926, Validation Loss Force: 3.8638938172399357, time: 0.12953639030456543
Test Loss Energy: 20.86976491692482, Test Loss Force: 11.901010083376466, time: 9.946088790893555

Epoch 17, Batch 100/144, Loss: 0.7558780908584595

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 13.3168967848191, Training Loss Force: 5.426886205279094, time: 2.114492893218994
Validation Loss Energy: 10.559652267482237, Validation Loss Force: 4.590334092297669, time: 0.13175415992736816
Test Loss Energy: 14.37732889000588, Test Loss Force: 11.665849366238348, time: 9.966080904006958

Epoch 18, Batch 100/144, Loss: 0.8730752468109131

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.13398478711014, Training Loss Force: 4.674774835808816, time: 2.0585427284240723
Validation Loss Energy: 3.0134838312017824, Validation Loss Force: 8.470191209055267, time: 0.12816524505615234
Test Loss Energy: 11.580480613107731, Test Loss Force: 14.527883254516135, time: 9.840974807739258

Epoch 19, Batch 100/144, Loss: 0.6762104034423828

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.095032362420925, Training Loss Force: 5.912338721074058, time: 2.010409355163574
Validation Loss Energy: 5.975821598706633, Validation Loss Force: 5.431308844392377, time: 0.1371920108795166
Test Loss Energy: 9.452271440893465, Test Loss Force: 12.094065221500234, time: 10.041157484054565

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–‚â–ƒâ–‚â–†â–‚â–‡â–‚â–‚â–„â–â–ƒâ–ƒâ–‚â–ˆâ–„â–‚â–
wandb:   test_error_force â–â–â–â–â–â–â–„â–†â–ƒâ–â–‚â–‚â–â–‚â–‚â–ˆâ–‚â–‚â–†â–‚
wandb:          test_loss â–â–â–‚â–‚â–‚â–‚â–†â–†â–…â–‚â–ƒâ–ƒâ–â–‚â–‚â–ˆâ–…â–ƒâ–†â–‚
wandb: train_error_energy â–â–â–‚â–â–â–‚â–ˆâ–†â–‡â–‡â–„â–†â–†â–†â–„â–ˆâ–‡â–ˆâ–…â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–‡â–‡â–…â–…â–†â–„â–‚â–„â–‡â–†â–…â–‡
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–‡â–‡â–…â–†â–†â–…â–ƒâ–†â–‡â–‡â–…â–‡
wandb: valid_error_energy â–â–‚â–‚â–‚â–ƒâ–‚â–†â–ƒâ–…â–…â–‚â–ˆâ–â–ƒâ–ƒâ–‚â–†â–…â–‚â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–…â–ˆâ–„â–ƒâ–ƒâ–„â–‚â–‚â–‚â–†â–‚â–ƒâ–‡â–„
wandb:         valid_loss â–â–‚â–‚â–â–‚â–‚â–‡â–ˆâ–…â–…â–ƒâ–†â–‚â–ƒâ–ƒâ–†â–„â–„â–‡â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 4591
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.45227
wandb:   test_error_force 12.09407
wandb:          test_loss 4.67926
wandb: train_error_energy 12.09503
wandb:  train_error_force 5.91234
wandb:         train_loss 2.78769
wandb: valid_error_energy 5.97582
wandb:  valid_error_force 5.43131
wandb:         valid_loss 2.21724
wandb: 
wandb: ğŸš€ View run al_77_42 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/gxvzvsgk
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_180240-gxvzvsgk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8607749342918396, Uncertainty Bias: -0.5314489603042603
1.5258789e-05 0.011109829
0.631422 37.957943
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 4 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 9 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 3 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_181127-t03w23ge
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_43
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/t03w23ge
Training model 43. Added 100 samples to the dataset.
Epoch 0, Batch 100/147, Loss: 0.054096996784210205

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.288325530959868, Training Loss Force: 2.3876808943012304, time: 2.098860502243042
Validation Loss Energy: 1.470368316829198, Validation Loss Force: 2.5383993908108593, time: 0.13803839683532715
Test Loss Energy: 9.951577724889972, Test Loss Force: 11.158527076926546, time: 9.850631475448608

Epoch 1, Batch 100/147, Loss: 0.08080485463142395

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.9358779305500737, Training Loss Force: 2.344336494014439, time: 2.0952796936035156
Validation Loss Energy: 1.5201546513865216, Validation Loss Force: 2.50551583400939, time: 0.14059925079345703
Test Loss Energy: 10.015558726501125, Test Loss Force: 11.146557162400574, time: 9.875258445739746

Epoch 2, Batch 100/147, Loss: 0.06525292992591858

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3658708655042175, Training Loss Force: 2.2993818171135656, time: 2.0671229362487793
Validation Loss Energy: 1.3828717748402248, Validation Loss Force: 2.6814154783034567, time: 0.13282084465026855
Test Loss Energy: 10.290507402907908, Test Loss Force: 11.209260556069202, time: 10.102235317230225

Epoch 3, Batch 100/147, Loss: 0.10727584362030029

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.698112182720668, Training Loss Force: 2.3171712490183216, time: 2.097015142440796
Validation Loss Energy: 2.528744679312576, Validation Loss Force: 2.8733582861155225, time: 0.13183069229125977
Test Loss Energy: 10.830436219458464, Test Loss Force: 11.336786342273989, time: 9.803752660751343

Epoch 4, Batch 100/147, Loss: 0.22947639226913452

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.4118644899922472, Training Loss Force: 2.2865198989030704, time: 2.0652687549591064
Validation Loss Energy: 2.948502665460919, Validation Loss Force: 2.577614025215446, time: 0.13071250915527344
Test Loss Energy: 9.443147331317016, Test Loss Force: 11.049371364690417, time: 10.045527219772339

Epoch 5, Batch 100/147, Loss: 0.062288548797369

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.046437012013213, Training Loss Force: 2.284721610762509, time: 2.100386381149292
Validation Loss Energy: 0.9717041231168682, Validation Loss Force: 2.5103027010854966, time: 0.13298964500427246
Test Loss Energy: 9.78948953696411, Test Loss Force: 11.14701476964579, time: 9.884179830551147

Epoch 6, Batch 100/147, Loss: 0.8425220847129822

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.617125829792323, Training Loss Force: 5.448518159552234, time: 2.1104507446289062
Validation Loss Energy: 7.772938702537942, Validation Loss Force: 3.3580722763912387, time: 0.1324450969696045
Test Loss Energy: 15.225313918111933, Test Loss Force: 11.318859988457007, time: 9.821382284164429

Epoch 7, Batch 100/147, Loss: 0.4497622549533844

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 6.44830265657788, Training Loss Force: 3.1859642574615914, time: 2.1199545860290527
Validation Loss Energy: 3.7334584622692106, Validation Loss Force: 4.166391336443809, time: 0.1304473876953125
Test Loss Energy: 10.60779409693802, Test Loss Force: 11.785774048529312, time: 10.024535894393921

Epoch 8, Batch 100/147, Loss: 0.27791327238082886

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 13.686695828659268, Training Loss Force: 6.2597696177515525, time: 2.1231698989868164
Validation Loss Energy: 5.710036283822361, Validation Loss Force: 5.448871947500479, time: 0.13456416130065918
Test Loss Energy: 9.805451771711336, Test Loss Force: 12.073628978376181, time: 9.841747999191284

Epoch 9, Batch 100/147, Loss: 0.6212653517723083

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.870762895708157, Training Loss Force: 5.215749896924063, time: 2.077589750289917
Validation Loss Energy: 1.649986016298918, Validation Loss Force: 5.326575224036464, time: 0.13050270080566406
Test Loss Energy: 11.68611183416401, Test Loss Force: 12.044784501150803, time: 10.037639856338501

Epoch 10, Batch 100/147, Loss: 0.5599454045295715

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.2445698484631444, Training Loss Force: 3.696107542865187, time: 2.11680006980896
Validation Loss Energy: 5.78221419058266, Validation Loss Force: 3.5768542128155167, time: 0.1328287124633789
Test Loss Energy: 9.670540220143955, Test Loss Force: 11.265219107783707, time: 9.925100326538086

Epoch 11, Batch 100/147, Loss: 0.44460874795913696

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.319247847282394, Training Loss Force: 3.081801821333885, time: 2.1126389503479004
Validation Loss Energy: 10.712223384918792, Validation Loss Force: 3.2079462888461188, time: 0.1316390037536621
Test Loss Energy: 15.670781446673482, Test Loss Force: 11.774725578566633, time: 9.86135983467102

Epoch 12, Batch 100/147, Loss: 1.0098241567611694

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.63875307590123, Training Loss Force: 4.552815742358225, time: 2.100179672241211
Validation Loss Energy: 6.9797138643264764, Validation Loss Force: 3.9589824493169385, time: 0.1343224048614502
Test Loss Energy: 10.051072370813607, Test Loss Force: 10.895830249158692, time: 9.975146532058716

Epoch 13, Batch 100/147, Loss: 0.4464380741119385

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.214080349279239, Training Loss Force: 3.2399644942710255, time: 2.090494394302368
Validation Loss Energy: 10.629777016270983, Validation Loss Force: 3.1791995536607445, time: 0.13191628456115723
Test Loss Energy: 10.463144008848277, Test Loss Force: 10.914544357450065, time: 9.947166204452515

Epoch 14, Batch 100/147, Loss: 0.22637470066547394

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.493078717226951, Training Loss Force: 3.0288646797354026, time: 2.075352191925049
Validation Loss Energy: 7.165799657147107, Validation Loss Force: 3.363911032512709, time: 0.13204264640808105
Test Loss Energy: 9.755251621313192, Test Loss Force: 11.26942900956894, time: 9.923314094543457

Epoch 15, Batch 100/147, Loss: 0.15744012594223022

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.760813553699617, Training Loss Force: 4.332348418624152, time: 2.157003879547119
Validation Loss Energy: 8.137567148191753, Validation Loss Force: 4.353866287842834, time: 0.13177180290222168
Test Loss Energy: 10.32386354874095, Test Loss Force: 11.367580729712833, time: 9.86203384399414

Epoch 16, Batch 100/147, Loss: 0.7590277194976807

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 13.525260209352782, Training Loss Force: 5.6175322346271415, time: 2.130563735961914
Validation Loss Energy: 3.8180077183311596, Validation Loss Force: 8.443179957193253, time: 0.13399100303649902
Test Loss Energy: 9.93008089579999, Test Loss Force: 14.137708723089332, time: 9.814786672592163

Epoch 17, Batch 100/147, Loss: 0.4033409357070923

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 5.978587019230032, Training Loss Force: 4.071794138201381, time: 2.07588529586792
Validation Loss Energy: 6.421573016050953, Validation Loss Force: 3.3604533517916937, time: 0.12982702255249023
Test Loss Energy: 9.589032741196085, Test Loss Force: 11.053264152969248, time: 10.036955833435059

Epoch 18, Batch 100/147, Loss: 1.6900761127471924

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 12.187370327036184, Training Loss Force: 5.021895369976035, time: 2.1703052520751953
Validation Loss Energy: 52.715235138307385, Validation Loss Force: 6.183492790087406, time: 0.13327765464782715
Test Loss Energy: 42.182557048468674, Test Loss Force: 14.079375995997683, time: 10.51459264755249

Epoch 19, Batch 100/147, Loss: 0.29605966806411743

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 14.406554792951301, Training Loss Force: 6.982490531913878, time: 2.058392286300659
Validation Loss Energy: 16.483322822413317, Validation Loss Force: 6.047444762401376, time: 0.13168787956237793
Test Loss Energy: 20.65727034733425, Test Loss Force: 12.67714312162623, time: 10.056354284286499

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–â–â–â–â–ˆâ–ƒ
wandb:   test_error_force â–‚â–‚â–‚â–‚â–â–‚â–‚â–ƒâ–„â–ƒâ–‚â–ƒâ–â–â–‚â–‚â–ˆâ–â–ˆâ–…
wandb:          test_loss â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–ƒâ–â–ˆâ–„
wandb: train_error_energy â–â–â–â–â–‚â–â–†â–„â–ˆâ–‡â–„â–„â–…â–„â–„â–†â–ˆâ–ƒâ–‡â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–†â–‚â–‡â–…â–ƒâ–‚â–„â–‚â–‚â–„â–†â–„â–…â–ˆ
wandb:         train_loss â–â–â–â–â–â–â–†â–ƒâ–‡â–†â–ƒâ–ƒâ–…â–ƒâ–ƒâ–…â–‡â–„â–†â–ˆ
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–ˆâ–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–ƒâ–„â–„â–‚â–‚â–ƒâ–‚â–‚â–ƒâ–ˆâ–‚â–…â–…
wandb:         valid_loss â–â–â–â–â–â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–„â–‚â–ˆâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 4681
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 20.65727
wandb:   test_error_force 12.67714
wandb:          test_loss 5.62421
wandb: train_error_energy 14.40655
wandb:  train_error_force 6.98249
wandb:         train_loss 3.30046
wandb: valid_error_energy 16.48332
wandb:  valid_error_force 6.04744
wandb:         valid_loss 3.12657
wandb: 
wandb: ğŸš€ View run al_77_43 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/t03w23ge
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_181127-t03w23ge/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6124021410942078, Uncertainty Bias: -0.5046250820159912
4.5776367e-05 0.6411514
-3.9445357 39.414284
(48745, 22, 3)
Found uncertainty sample 0 after 35 steps.
Found uncertainty sample 1 after 26 steps.
Found uncertainty sample 2 after 6 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 16 steps.
Found uncertainty sample 5 after 70 steps.
Found uncertainty sample 6 after 4 steps.
Found uncertainty sample 7 after 34 steps.
Found uncertainty sample 8 after 6 steps.
Found uncertainty sample 9 after 12 steps.
Found uncertainty sample 10 after 6 steps.
Found uncertainty sample 11 after 25 steps.
Found uncertainty sample 12 after 9 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 5 steps.
Found uncertainty sample 15 after 25 steps.
Found uncertainty sample 16 after 10 steps.
Found uncertainty sample 17 after 17 steps.
Found uncertainty sample 18 after 19 steps.
Found uncertainty sample 19 after 28 steps.
Found uncertainty sample 20 after 35 steps.
Found uncertainty sample 21 after 14 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 20 steps.
Found uncertainty sample 24 after 3 steps.
Found uncertainty sample 25 after 13 steps.
Found uncertainty sample 26 after 8 steps.
Found uncertainty sample 27 after 42 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 3 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 17 steps.
Found uncertainty sample 35 after 6 steps.
Found uncertainty sample 36 after 32 steps.
Found uncertainty sample 37 after 36 steps.
Found uncertainty sample 38 after 7 steps.
Found uncertainty sample 39 after 8 steps.
Found uncertainty sample 40 after 18 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 18 steps.
Found uncertainty sample 44 after 12 steps.
Found uncertainty sample 45 after 21 steps.
Found uncertainty sample 46 after 8 steps.
Found uncertainty sample 47 after 98 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 2 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 9 steps.
Found uncertainty sample 52 after 4 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 44 steps.
Found uncertainty sample 55 after 69 steps.
Found uncertainty sample 56 after 28 steps.
Found uncertainty sample 57 after 9 steps.
Found uncertainty sample 58 after 48 steps.
Found uncertainty sample 59 after 84 steps.
Found uncertainty sample 60 after 30 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 11 steps.
Found uncertainty sample 63 after 44 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 22 steps.
Found uncertainty sample 66 after 13 steps.
Found uncertainty sample 67 after 55 steps.
Found uncertainty sample 68 after 3 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 2 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 61 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 43 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 5 steps.
Found uncertainty sample 77 after 12 steps.
Found uncertainty sample 78 after 16 steps.
Found uncertainty sample 79 after 15 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 41 steps.
Found uncertainty sample 82 after 9 steps.
Found uncertainty sample 83 after 25 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 16 steps.
Found uncertainty sample 87 after 32 steps.
Found uncertainty sample 88 after 19 steps.
Found uncertainty sample 89 after 39 steps.
Found uncertainty sample 90 after 7 steps.
Found uncertainty sample 91 after 8 steps.
Found uncertainty sample 92 after 6 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 19 steps.
Found uncertainty sample 95 after 31 steps.
Found uncertainty sample 96 after 73 steps.
Found uncertainty sample 97 after 42 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 72 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_182048-5hyc1jj1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_44
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/5hyc1jj1
Training model 44. Added 100 samples to the dataset.
Epoch 0, Batch 100/150, Loss: 0.09934690594673157

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.6908600542886605, Training Loss Force: 2.358616424816571, time: 2.275392532348633
Validation Loss Energy: 1.770990330928445, Validation Loss Force: 2.526437307399424, time: 0.15189576148986816
Test Loss Energy: 9.56944220714634, Test Loss Force: 11.01247989499844, time: 11.34995985031128

Epoch 1, Batch 100/150, Loss: 0.058204516768455505

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.2183651034437344, Training Loss Force: 2.2950851283967393, time: 2.2571070194244385
Validation Loss Energy: 1.2232991972252159, Validation Loss Force: 2.4797183516618286, time: 0.15302658081054688
Test Loss Energy: 10.04148634983233, Test Loss Force: 11.18254210985946, time: 11.393659830093384

Epoch 2, Batch 100/150, Loss: 0.17522302269935608

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6581810454469077, Training Loss Force: 2.2726736176562086, time: 2.4579479694366455
Validation Loss Energy: 0.925998441883579, Validation Loss Force: 2.4569018896169, time: 0.1467142105102539
Test Loss Energy: 9.927825506834449, Test Loss Force: 11.09453240273035, time: 11.538081169128418

Epoch 3, Batch 100/150, Loss: 0.04589039832353592

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5216352475652308, Training Loss Force: 2.2616536459167333, time: 2.2697737216949463
Validation Loss Energy: 0.9935847809645107, Validation Loss Force: 2.530695112584807, time: 0.14930057525634766
Test Loss Energy: 9.732793607629313, Test Loss Force: 11.128451582071458, time: 11.428086519241333

Epoch 4, Batch 100/150, Loss: 0.21341770887374878

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.1376332431918965, Training Loss Force: 2.2786345226488294, time: 2.236923933029175
Validation Loss Energy: 3.821569756658566, Validation Loss Force: 2.4910607468401467, time: 0.15730690956115723
Test Loss Energy: 9.756830119391255, Test Loss Force: 11.046499744109651, time: 11.757255554199219

Epoch 5, Batch 100/150, Loss: 0.15018919110298157

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.441621598757384, Training Loss Force: 2.234999371291627, time: 2.1885311603546143
Validation Loss Energy: 1.7808521618439652, Validation Loss Force: 2.471472616295928, time: 0.1444990634918213
Test Loss Energy: 10.333028289024591, Test Loss Force: 11.145873607760018, time: 11.49154782295227

Epoch 6, Batch 100/150, Loss: 0.09632856398820877

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 13.217305381497711, Training Loss Force: 5.821050434400804, time: 2.207012891769409
Validation Loss Energy: 10.979474753459852, Validation Loss Force: 4.435614811387302, time: 0.15552234649658203
Test Loss Energy: 16.259429675459835, Test Loss Force: 12.897987271233994, time: 12.306519269943237

Epoch 7, Batch 100/150, Loss: 0.44812479615211487

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.220726195503687, Training Loss Force: 5.06939784111978, time: 2.236865520477295
Validation Loss Energy: 16.443440106347904, Validation Loss Force: 4.869342391743718, time: 0.15119385719299316
Test Loss Energy: 20.607819666307087, Test Loss Force: 12.238649012474847, time: 11.428161144256592

Epoch 8, Batch 100/150, Loss: 1.479356288909912

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.418396795937245, Training Loss Force: 5.053380155405457, time: 2.316772937774658
Validation Loss Energy: 9.26420147556747, Validation Loss Force: 3.920327530922167, time: 0.14362454414367676
Test Loss Energy: 14.182442474193723, Test Loss Force: 11.508508176730603, time: 11.622163772583008

Epoch 9, Batch 100/150, Loss: 0.24379630386829376

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.252615230827283, Training Loss Force: 3.260239506871346, time: 2.1864871978759766
Validation Loss Energy: 8.172707308341105, Validation Loss Force: 3.4812271667436607, time: 0.16207242012023926
Test Loss Energy: 10.158873485209845, Test Loss Force: 11.059775922115357, time: 11.55215048789978

Epoch 10, Batch 100/150, Loss: 0.6646559238433838

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.42760582907533, Training Loss Force: 3.128766414221712, time: 2.2825732231140137
Validation Loss Energy: 13.705664469403418, Validation Loss Force: 3.287053956319884, time: 0.15735888481140137
Test Loss Energy: 19.487829508006577, Test Loss Force: 11.702757250493535, time: 10.039761781692505

Epoch 11, Batch 100/150, Loss: 2.1048402786254883

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 14.021521728765077, Training Loss Force: 5.91894313161017, time: 2.312173843383789
Validation Loss Energy: 6.191014108498554, Validation Loss Force: 5.851487452557378, time: 0.15801763534545898
Test Loss Energy: 9.876711774075744, Test Loss Force: 12.19901801280373, time: 11.898458480834961

Epoch 12, Batch 100/150, Loss: 0.7071828842163086

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.422590599414532, Training Loss Force: 5.427462021134095, time: 2.352155923843384
Validation Loss Energy: 5.902540143659117, Validation Loss Force: 4.613212011026874, time: 0.15939712524414062
Test Loss Energy: 14.725863269897216, Test Loss Force: 11.901986880458335, time: 9.217609882354736

Epoch 13, Batch 100/150, Loss: 0.10973356664180756

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.702962488998658, Training Loss Force: 4.93027068065204, time: 2.365196466445923
Validation Loss Energy: 18.803451599381997, Validation Loss Force: 5.970518658773314, time: 0.12268233299255371
Test Loss Energy: 13.792842037452248, Test Loss Force: 12.008293654055603, time: 9.041815757751465

Epoch 14, Batch 100/150, Loss: 0.7383648753166199

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.310428524072549, Training Loss Force: 5.697305916149966, time: 2.177222728729248
Validation Loss Energy: 12.168850714529738, Validation Loss Force: 4.499556780224121, time: 0.13059020042419434
Test Loss Energy: 16.838418941049603, Test Loss Force: 12.350468491221758, time: 9.0043044090271

Epoch 15, Batch 100/150, Loss: 0.3432445526123047

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.801752480518752, Training Loss Force: 5.335062478233724, time: 2.2317891120910645
Validation Loss Energy: 10.90200073528671, Validation Loss Force: 6.232406818265557, time: 0.12280511856079102
Test Loss Energy: 10.772349183870302, Test Loss Force: 12.351094133231465, time: 9.235535860061646

Epoch 16, Batch 100/150, Loss: 0.5241693258285522

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.982016294924685, Training Loss Force: 4.404786239928841, time: 2.1236350536346436
Validation Loss Energy: 16.073386549622708, Validation Loss Force: 4.833507800751266, time: 0.12207579612731934
Test Loss Energy: 12.381956100173198, Test Loss Force: 11.97744679130997, time: 8.982071876525879

Epoch 17, Batch 100/150, Loss: 0.15335872769355774

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.707076635473392, Training Loss Force: 4.911422494524348, time: 2.1776039600372314
Validation Loss Energy: 8.508507521902132, Validation Loss Force: 3.7215224311478488, time: 0.12623167037963867
Test Loss Energy: 9.804638653938829, Test Loss Force: 11.221602129881676, time: 9.078197717666626

Epoch 18, Batch 100/150, Loss: 0.6218894720077515

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.360371365265719, Training Loss Force: 4.353646470346436, time: 2.155010223388672
Validation Loss Energy: 13.393283112150103, Validation Loss Force: 3.9632928786438093, time: 0.1217496395111084
Test Loss Energy: 11.493669448887237, Test Loss Force: 11.43287207979161, time: 9.1810941696167

Epoch 19, Batch 100/150, Loss: 0.56615149974823

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.486384132503032, Training Loss Force: 4.823922315605992, time: 2.2233939170837402
Validation Loss Energy: 5.857937732376613, Validation Loss Force: 3.277786614912467, time: 0.12226009368896484
Test Loss Energy: 12.334701309983142, Test Loss Force: 11.505191595579468, time: 9.012897729873657

wandb: - 0.039 MB of 0.047 MB uploadedwandb: \ 0.039 MB of 0.047 MB uploadedwandb: | 0.057 MB of 0.057 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–…â–ˆâ–„â–â–‡â–â–„â–„â–†â–‚â–ƒâ–â–‚â–ƒ
wandb:   test_error_force â–â–‚â–â–â–â–â–ˆâ–†â–ƒâ–â–„â–…â–„â–…â–†â–†â–…â–‚â–ƒâ–ƒ
wandb:          test_loss â–â–‚â–â–â–â–‚â–ˆâ–ˆâ–„â–â–†â–„â–…â–…â–‡â–„â–„â–‚â–ƒâ–ƒ
wandb: train_error_energy â–â–â–â–â–‚â–‚â–ˆâ–†â–…â–„â–„â–ˆâ–‡â–†â–‡â–†â–…â–†â–…â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–†â–ƒâ–ƒâ–ˆâ–‡â–†â–ˆâ–‡â–…â–†â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–†â–ƒâ–ƒâ–ˆâ–‡â–†â–‡â–†â–…â–†â–…â–†
wandb: valid_error_energy â–â–â–â–â–‚â–â–…â–‡â–„â–„â–†â–ƒâ–ƒâ–ˆâ–…â–…â–‡â–„â–†â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–…â–…â–„â–ƒâ–ƒâ–‡â–…â–ˆâ–…â–ˆâ–…â–ƒâ–„â–ƒ
wandb:         valid_loss â–â–â–â–â–‚â–â–…â–†â–„â–ƒâ–„â–…â–„â–ˆâ–…â–‡â–†â–„â–…â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 4771
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.3347
wandb:   test_error_force 11.50519
wandb:          test_loss 4.67512
wandb: train_error_energy 10.48638
wandb:  train_error_force 4.82392
wandb:         train_loss 2.31586
wandb: valid_error_energy 5.85794
wandb:  valid_error_force 3.27779
wandb:         valid_loss 1.48877
wandb: 
wandb: ğŸš€ View run al_77_44 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/5hyc1jj1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_182048-5hyc1jj1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.49182215332984924, Uncertainty Bias: -0.3257559537887573
5.9127808e-05 0.00025367737
-0.44250816 22.700336
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 13 steps.
Found uncertainty sample 2 after 7 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 10 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 4 steps.
Found uncertainty sample 10 after 13 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 15 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 8 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 19 steps.
Found uncertainty sample 20 after 4 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 5 steps.
Found uncertainty sample 23 after 4 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 3 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 9 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 7 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 4 steps.
Found uncertainty sample 32 after 13 steps.
Found uncertainty sample 33 after 16 steps.
Found uncertainty sample 34 after 15 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 3 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 9 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 7 steps.
Found uncertainty sample 44 after 8 steps.
Found uncertainty sample 45 after 21 steps.
Found uncertainty sample 46 after 5 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 9 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 25 steps.
Found uncertainty sample 57 after 12 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 5 steps.
Found uncertainty sample 60 after 18 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 9 steps.
Found uncertainty sample 63 after 28 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 15 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 6 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 14 steps.
Found uncertainty sample 70 after 7 steps.
Found uncertainty sample 71 after 16 steps.
Found uncertainty sample 72 after 10 steps.
Found uncertainty sample 73 after 8 steps.
Found uncertainty sample 74 after 4 steps.
Found uncertainty sample 75 after 7 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 8 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 10 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 26 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 24 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 6 steps.
Found uncertainty sample 87 after 12 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 3 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 11 steps.
Found uncertainty sample 94 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 4 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 5 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_183001-fjvormqx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_45
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/fjvormqx
Training model 45. Added 100 samples to the dataset.
Epoch 0, Batch 100/152, Loss: 0.21615831553936005

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.116339754150056, Training Loss Force: 2.408727586408973, time: 2.3353638648986816
Validation Loss Energy: 1.2844302649868904, Validation Loss Force: 2.5800865434118276, time: 0.13491058349609375
Test Loss Energy: 10.085856439086553, Test Loss Force: 11.155391559599453, time: 9.917867660522461

Epoch 1, Batch 100/152, Loss: 0.141862690448761

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.131139457469218, Training Loss Force: 2.3200846943375013, time: 2.149857997894287
Validation Loss Energy: 1.7207070359489347, Validation Loss Force: 2.5004501109395325, time: 0.1361989974975586
Test Loss Energy: 9.631411356884014, Test Loss Force: 11.134177308147919, time: 9.869213581085205

Epoch 2, Batch 100/152, Loss: 0.09079364687204361

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6341063703077088, Training Loss Force: 2.2363892411416835, time: 2.160471200942993
Validation Loss Energy: 1.4945749394636778, Validation Loss Force: 2.5461894708811434, time: 0.13387537002563477
Test Loss Energy: 9.991026900630848, Test Loss Force: 11.107897620415056, time: 10.121280670166016

Epoch 3, Batch 100/152, Loss: 0.10154058039188385

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.27574946755905, Training Loss Force: 2.2457056625400162, time: 2.2316110134124756
Validation Loss Energy: 0.8761877992778248, Validation Loss Force: 2.5066208667575807, time: 0.1334688663482666
Test Loss Energy: 10.114678481521125, Test Loss Force: 11.202961444118108, time: 9.895419597625732

Epoch 4, Batch 100/152, Loss: 0.05431041494011879

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4682582421091532, Training Loss Force: 2.253329336976791, time: 2.1521060466766357
Validation Loss Energy: 0.8918526664726555, Validation Loss Force: 2.4389083670497187, time: 0.13583016395568848
Test Loss Energy: 9.999789432820137, Test Loss Force: 11.172447274724242, time: 10.124659299850464

Epoch 5, Batch 100/152, Loss: 0.059049900621175766

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5112404870496357, Training Loss Force: 2.2277844967024514, time: 2.1033761501312256
Validation Loss Energy: 0.9611886283677148, Validation Loss Force: 2.477917923170905, time: 0.1407923698425293
Test Loss Energy: 10.375833067810072, Test Loss Force: 11.105062951254448, time: 9.882306337356567

Epoch 6, Batch 100/152, Loss: 0.12570588290691376

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.168236929677313, Training Loss Force: 5.25934711761671, time: 2.1597812175750732
Validation Loss Energy: 23.535512970155636, Validation Loss Force: 3.505394488273885, time: 0.13222932815551758
Test Loss Energy: 18.326893678754796, Test Loss Force: 10.780012000945941, time: 9.841591835021973

Epoch 7, Batch 100/152, Loss: 0.8945866823196411

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 15.174093990597271, Training Loss Force: 6.277848663417947, time: 2.2558631896972656
Validation Loss Energy: 13.239396762291603, Validation Loss Force: 5.523994134154332, time: 0.1896510124206543
Test Loss Energy: 15.968771413345769, Test Loss Force: 12.123327267493618, time: 9.891147375106812

Epoch 8, Batch 100/152, Loss: 1.2832942008972168

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.029712774915886, Training Loss Force: 5.3642905103755405, time: 2.1754648685455322
Validation Loss Energy: 3.3170261140383484, Validation Loss Force: 6.098579989533206, time: 0.1397840976715088
Test Loss Energy: 10.946678555632225, Test Loss Force: 12.18417803987844, time: 9.8832426071167

Epoch 9, Batch 100/152, Loss: 0.3315293788909912

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.408497628321541, Training Loss Force: 4.650271682735743, time: 2.1893043518066406
Validation Loss Energy: 19.40708330883228, Validation Loss Force: 4.230709170798953, time: 0.1345076560974121
Test Loss Energy: 14.785567536336762, Test Loss Force: 11.249255665260382, time: 10.101458549499512

Epoch 10, Batch 100/152, Loss: 1.0054407119750977

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 11.935157075135752, Training Loss Force: 5.035661598372335, time: 2.1659903526306152
Validation Loss Energy: 5.005451133883537, Validation Loss Force: 6.069429035624924, time: 0.14453339576721191
Test Loss Energy: 12.209394017282486, Test Loss Force: 12.535372618910996, time: 9.96818494796753

Epoch 11, Batch 100/152, Loss: 1.0031847953796387

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.788104169790925, Training Loss Force: 5.041874026525719, time: 2.1492393016815186
Validation Loss Energy: 15.170365782245705, Validation Loss Force: 4.728011315901405, time: 0.13471293449401855
Test Loss Energy: 18.579520381833596, Test Loss Force: 12.284393644195921, time: 9.950709581375122

Epoch 12, Batch 100/152, Loss: 0.8575854301452637

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.405846801627932, Training Loss Force: 4.50067399531848, time: 2.38557505607605
Validation Loss Energy: 6.477798162966498, Validation Loss Force: 4.214662593020716, time: 0.13433599472045898
Test Loss Energy: 9.693327131311865, Test Loss Force: 11.603117783355328, time: 9.920708656311035

Epoch 13, Batch 100/152, Loss: 0.09434406459331512

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.238842792972395, Training Loss Force: 4.091114776591589, time: 2.2263739109039307
Validation Loss Energy: 3.028370451175833, Validation Loss Force: 3.085196804785383, time: 0.13495612144470215
Test Loss Energy: 11.24200945632774, Test Loss Force: 11.433547263937573, time: 9.833398818969727

Epoch 14, Batch 100/152, Loss: 1.1955657005310059

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.607930612519745, Training Loss Force: 4.793953264825279, time: 2.155036449432373
Validation Loss Energy: 6.52929797371326, Validation Loss Force: 5.605501284344616, time: 0.1365966796875
Test Loss Energy: 9.9116542848528, Test Loss Force: 12.69925026003164, time: 10.149701833724976

Epoch 15, Batch 100/152, Loss: 0.17194746434688568

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.136793038248225, Training Loss Force: 5.474828074289872, time: 2.168972969055176
Validation Loss Energy: 3.205237815578102, Validation Loss Force: 8.266263382909735, time: 0.1422741413116455
Test Loss Energy: 9.314715046195284, Test Loss Force: 13.404455868024835, time: 9.92066240310669

Epoch 16, Batch 100/152, Loss: 0.3935632109642029

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.109833829656083, Training Loss Force: 3.6825853118390426, time: 2.185548782348633
Validation Loss Energy: 8.411984900525354, Validation Loss Force: 3.4033910885392404, time: 0.15482068061828613
Test Loss Energy: 14.495843262869807, Test Loss Force: 11.452604851627514, time: 9.915756702423096

Epoch 17, Batch 100/152, Loss: 0.40880459547042847

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.23868923094086, Training Loss Force: 3.0698084067273173, time: 2.319185256958008
Validation Loss Energy: 9.547292339581054, Validation Loss Force: 3.428686971524075, time: 0.13366103172302246
Test Loss Energy: 16.04121594561809, Test Loss Force: 11.890889898932823, time: 9.979251146316528

Epoch 18, Batch 100/152, Loss: 0.6957904696464539

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.755767988111957, Training Loss Force: 4.742721111181493, time: 2.1401984691619873
Validation Loss Energy: 7.50893756129694, Validation Loss Force: 4.791772328945632, time: 0.13460922241210938
Test Loss Energy: 9.80109537927584, Test Loss Force: 11.785164587921416, time: 9.962028503417969

Epoch 19, Batch 100/152, Loss: 0.6195898652076721

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.672103932984747, Training Loss Force: 4.309568417635387, time: 2.1619319915771484
Validation Loss Energy: 7.890109879154895, Validation Loss Force: 3.3388415298262335, time: 0.13434433937072754
Test Loss Energy: 13.492123849129174, Test Loss Force: 11.237842829300085, time: 10.816949367523193

wandb: - 0.039 MB of 0.056 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‚â–‚â–‚â–‚â–ˆâ–†â–‚â–…â–ƒâ–ˆâ–â–‚â–â–â–…â–†â–â–„
wandb:   test_error_force â–‚â–‚â–‚â–‚â–‚â–‚â–â–…â–…â–‚â–†â–…â–ƒâ–ƒâ–†â–ˆâ–ƒâ–„â–„â–‚
wandb:          test_loss â–â–â–â–â–â–â–„â–†â–„â–„â–†â–ˆâ–‚â–‚â–…â–†â–„â–†â–ƒâ–ƒ
wandb: train_error_energy â–â–â–â–â–â–â–†â–ˆâ–„â–†â–†â–…â–…â–„â–†â–…â–ƒâ–ƒâ–†â–„
wandb:  train_error_force â–â–â–â–â–â–â–†â–ˆâ–†â–…â–†â–†â–…â–„â–…â–‡â–„â–‚â–…â–…
wandb:         train_loss â–â–â–â–â–â–â–†â–ˆâ–…â–…â–†â–†â–…â–„â–†â–†â–ƒâ–ƒâ–†â–„
wandb: valid_error_energy â–â–â–â–â–â–â–ˆâ–…â–‚â–‡â–‚â–…â–ƒâ–‚â–ƒâ–‚â–ƒâ–„â–ƒâ–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–…â–…â–ƒâ–…â–„â–ƒâ–‚â–…â–ˆâ–‚â–‚â–„â–‚
wandb:         valid_loss â–â–â–â–â–â–â–‡â–‡â–†â–‡â–†â–‡â–„â–‚â–†â–ˆâ–„â–„â–…â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 4861
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 13.49212
wandb:   test_error_force 11.23784
wandb:          test_loss 4.66312
wandb: train_error_energy 7.6721
wandb:  train_error_force 4.30957
wandb:         train_loss 1.95542
wandb: valid_error_energy 7.89011
wandb:  valid_error_force 3.33884
wandb:         valid_loss 1.6452
wandb: 
wandb: ğŸš€ View run al_77_45 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/fjvormqx
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_183001-fjvormqx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 1.0096354484558105, Uncertainty Bias: -0.9152620434761047
0.00010681152 0.010050774
7.4303794 134.89929
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 16 steps.
Found uncertainty sample 4 after 11 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 4 steps.
Found uncertainty sample 9 after 12 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 10 steps.
Found uncertainty sample 13 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 6 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 5 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 11 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 5 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 18 steps.
Found uncertainty sample 27 after 5 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 7 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 9 steps.
Found uncertainty sample 32 after 3 steps.
Found uncertainty sample 33 after 2 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 11 steps.
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 8 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 4 steps.
Found uncertainty sample 47 after 4 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 4 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 3 steps.
Found uncertainty sample 55 after 4 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 11 steps.
Found uncertainty sample 58 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 5 steps.
Found uncertainty sample 63 after 7 steps.
Found uncertainty sample 64 after 6 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 13 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 11 steps.
Found uncertainty sample 71 after 14 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 5 steps.
Found uncertainty sample 76 after 4 steps.
Found uncertainty sample 77 after 3 steps.
Found uncertainty sample 78 after 10 steps.
Found uncertainty sample 79 after 13 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 3 steps.
Found uncertainty sample 82 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 7 steps.
Found uncertainty sample 87 after 5 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 3 steps.
Found uncertainty sample 91 after 4 steps.
Found uncertainty sample 92 after 3 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 5 steps.
Found uncertainty sample 96 after 15 steps.
Found uncertainty sample 97 after 6 steps.
Found uncertainty sample 98 after 9 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_183855-u7muxzae
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_46
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/u7muxzae
Training model 46. Added 100 samples to the dataset.
Epoch 0, Batch 100/155, Loss: 0.05511713773012161

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.5321941782889086, Training Loss Force: 2.376210521123381, time: 2.2765023708343506
Validation Loss Energy: 1.0838706436772345, Validation Loss Force: 2.5203577717081576, time: 0.1387178897857666
Test Loss Energy: 9.878833053261854, Test Loss Force: 11.079086960920492, time: 10.077816247940063

Epoch 1, Batch 100/155, Loss: 0.0520152673125267

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5511150357246672, Training Loss Force: 2.2584756970693274, time: 2.213345766067505
Validation Loss Energy: 2.2720775175079506, Validation Loss Force: 2.449953053579932, time: 0.13696599006652832
Test Loss Energy: 9.481485856674144, Test Loss Force: 11.125236524061181, time: 10.101969003677368

Epoch 2, Batch 100/155, Loss: 0.04796217381954193

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6424518054639345, Training Loss Force: 2.2835903029475992, time: 2.406825065612793
Validation Loss Energy: 0.9825416902298438, Validation Loss Force: 2.4275392515738887, time: 0.17829227447509766
Test Loss Energy: 10.061405398981389, Test Loss Force: 11.195343482634394, time: 10.082218170166016

Epoch 3, Batch 100/155, Loss: 0.13986656069755554

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.8484666844938975, Training Loss Force: 2.2492481770457946, time: 2.2623047828674316
Validation Loss Energy: 2.8985193800440228, Validation Loss Force: 2.6860853698549025, time: 0.13934636116027832
Test Loss Energy: 11.171023493230969, Test Loss Force: 11.274123756515223, time: 10.009995222091675

Epoch 4, Batch 100/155, Loss: 0.14207673072814941

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6040250468052566, Training Loss Force: 2.254893436073745, time: 2.245603084564209
Validation Loss Energy: 1.6205062913966735, Validation Loss Force: 2.48951475495997, time: 0.14355182647705078
Test Loss Energy: 9.80628814033958, Test Loss Force: 11.154247301330162, time: 10.1981201171875

Epoch 5, Batch 100/155, Loss: 0.09015893936157227

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6179148604540612, Training Loss Force: 2.195635121178167, time: 2.1694443225860596
Validation Loss Energy: 1.941412870905553, Validation Loss Force: 2.4256048504801657, time: 0.13489747047424316
Test Loss Energy: 10.420841151965472, Test Loss Force: 11.212694603938372, time: 10.062305450439453

Epoch 6, Batch 100/155, Loss: 1.3681871891021729

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.53468460417609, Training Loss Force: 5.310280475030501, time: 2.243434429168701
Validation Loss Energy: 5.401615616890712, Validation Loss Force: 5.041542091286, time: 0.14343690872192383
Test Loss Energy: 13.886868983041467, Test Loss Force: 12.002692077103168, time: 9.970220565795898

Epoch 7, Batch 100/155, Loss: 0.1342768371105194

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.81899210373066, Training Loss Force: 6.158007427151989, time: 2.400106191635132
Validation Loss Energy: 7.489882871803316, Validation Loss Force: 4.793947361163055, time: 0.1362297534942627
Test Loss Energy: 11.492099055499764, Test Loss Force: 12.379443520183463, time: 10.033618211746216

Epoch 8, Batch 100/155, Loss: 0.7344726324081421

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.815690314636425, Training Loss Force: 5.559319954400982, time: 2.2211930751800537
Validation Loss Energy: 5.51407912242446, Validation Loss Force: 5.0904350401183365, time: 0.13601374626159668
Test Loss Energy: 12.111029434761537, Test Loss Force: 12.618547449086801, time: 10.01092004776001

Epoch 9, Batch 100/155, Loss: 0.6430697441101074

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.463992965600163, Training Loss Force: 5.575872460421686, time: 2.1981427669525146
Validation Loss Energy: 19.866810184005512, Validation Loss Force: 4.6444661492414925, time: 0.14602994918823242
Test Loss Energy: 22.44187291744243, Test Loss Force: 12.575661483498585, time: 10.942991971969604

Epoch 10, Batch 100/155, Loss: 1.5908464193344116

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.525261812708454, Training Loss Force: 5.1709420548402125, time: 2.2175590991973877
Validation Loss Energy: 9.38075541331191, Validation Loss Force: 4.757902273604653, time: 0.1479194164276123
Test Loss Energy: 13.031412791875034, Test Loss Force: 11.861623791267691, time: 9.980349779129028

Epoch 11, Batch 100/155, Loss: 0.8253489136695862

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.803114321466069, Training Loss Force: 5.901455633823306, time: 2.176900625228882
Validation Loss Energy: 3.41620376808812, Validation Loss Force: 4.085433585259579, time: 0.13468456268310547
Test Loss Energy: 9.844096284112409, Test Loss Force: 11.772830526569557, time: 10.169959783554077

Epoch 12, Batch 100/155, Loss: 0.5776291489601135

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.561425979506518, Training Loss Force: 5.072363657094271, time: 2.176931142807007
Validation Loss Energy: 17.853568774229622, Validation Loss Force: 5.021130307570993, time: 0.13719773292541504
Test Loss Energy: 14.062643004242803, Test Loss Force: 11.68179575533619, time: 9.919383525848389

Epoch 13, Batch 100/155, Loss: 0.9676703810691833

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.27524259047333, Training Loss Force: 5.32072104340777, time: 2.2710437774658203
Validation Loss Energy: 14.768404924477835, Validation Loss Force: 6.464170514343523, time: 0.14275693893432617
Test Loss Energy: 19.25978665377171, Test Loss Force: 13.004258165953592, time: 9.982369184494019

Epoch 14, Batch 100/155, Loss: 0.24255335330963135

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.648425512319896, Training Loss Force: 5.082914041584584, time: 2.2731053829193115
Validation Loss Energy: 4.1415468038177705, Validation Loss Force: 4.231684164765922, time: 0.14132285118103027
Test Loss Energy: 9.379008064404426, Test Loss Force: 11.279492796930887, time: 10.189520835876465

Epoch 15, Batch 100/155, Loss: 0.6011883616447449

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.203512114137142, Training Loss Force: 4.58769408859488, time: 2.2533109188079834
Validation Loss Energy: 6.0343746773198035, Validation Loss Force: 5.495083514477554, time: 0.14183282852172852
Test Loss Energy: 10.564305782164896, Test Loss Force: 12.727508037865903, time: 10.005085229873657

Epoch 16, Batch 100/155, Loss: 0.6181151866912842

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.182442035158974, Training Loss Force: 4.820149898320252, time: 2.288998603820801
Validation Loss Energy: 9.239872644683397, Validation Loss Force: 3.6350306094568263, time: 0.14654302597045898
Test Loss Energy: 9.945079381177814, Test Loss Force: 10.865441711795413, time: 10.149238348007202

Epoch 17, Batch 100/155, Loss: 1.1864463090896606

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.332436145436274, Training Loss Force: 4.516766455202075, time: 2.219275951385498
Validation Loss Energy: 5.483018261028506, Validation Loss Force: 3.7962328283278324, time: 0.14101910591125488
Test Loss Energy: 11.0926193226931, Test Loss Force: 11.672737802978029, time: 10.0206778049469

Epoch 18, Batch 100/155, Loss: 1.3720747232437134

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.492967270387819, Training Loss Force: 4.667852498727051, time: 2.2298195362091064
Validation Loss Energy: 8.319323659031157, Validation Loss Force: 4.929554058601856, time: 0.13897395133972168
Test Loss Energy: 13.533776087752104, Test Loss Force: 12.309425051028324, time: 9.98449969291687

Epoch 19, Batch 100/155, Loss: 1.4312267303466797

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.445024534542233, Training Loss Force: 4.938187971892707, time: 2.211883068084717
Validation Loss Energy: 12.601687766748539, Validation Loss Force: 5.822783024509729, time: 0.1410367488861084
Test Loss Energy: 15.805522095591039, Test Loss Force: 12.424798651106991, time: 10.182706832885742

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–â–‚â–ƒâ–‚â–‚â–ˆâ–ƒâ–â–„â–†â–â–‚â–â–‚â–ƒâ–„
wandb:   test_error_force â–‚â–‚â–‚â–‚â–‚â–‚â–…â–†â–‡â–‡â–„â–„â–„â–ˆâ–‚â–‡â–â–„â–†â–†
wandb:          test_loss â–â–â–‚â–‚â–â–‚â–„â–„â–…â–ˆâ–„â–‚â–„â–ˆâ–‚â–„â–â–ƒâ–…â–†
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–†â–†â–‡â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–‡â–ˆâ–‡â–‡â–†â–ˆâ–†â–‡â–†â–…â–†â–…â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–‡
wandb: valid_error_energy â–â–â–â–‚â–â–â–ƒâ–ƒâ–ƒâ–ˆâ–„â–‚â–‡â–†â–‚â–ƒâ–„â–ƒâ–„â–…
wandb:  valid_error_force â–â–â–â–â–â–â–†â–…â–†â–…â–…â–„â–…â–ˆâ–„â–†â–ƒâ–ƒâ–…â–‡
wandb:         valid_loss â–â–â–â–‚â–â–â–…â–…â–…â–‡â–…â–ƒâ–‡â–ˆâ–„â–…â–„â–ƒâ–…â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 4951
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 15.80552
wandb:   test_error_force 12.4248
wandb:          test_loss 5.21509
wandb: train_error_energy 11.44502
wandb:  train_error_force 4.93819
wandb:         train_loss 2.41824
wandb: valid_error_energy 12.60169
wandb:  valid_error_force 5.82278
wandb:         valid_loss 2.79163
wandb: 
wandb: ğŸš€ View run al_77_46 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/u7muxzae
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_183855-u7muxzae/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8675327301025391, Uncertainty Bias: -0.7002964019775391
4.5776367e-05 0.0018205643
-9.1546545 25.11409
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 9 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 6 steps.
Found uncertainty sample 7 after 4 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 4 steps.
Found uncertainty sample 12 after 7 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 6 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 6 steps.
Found uncertainty sample 21 after 2 steps.
Found uncertainty sample 22 after 7 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 5 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 28 after 4 steps.
Found uncertainty sample 29 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 5 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 6 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 4 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 5 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 8 steps.
Found uncertainty sample 67 after 5 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 3 steps.
Found uncertainty sample 71 after 4 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 6 steps.
Found uncertainty sample 74 after 5 steps.
Found uncertainty sample 75 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 5 steps.
Found uncertainty sample 79 after 9 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 5 steps.
Found uncertainty sample 85 after 3 steps.
Found uncertainty sample 86 after 6 steps.
Found uncertainty sample 87 after 9 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 3 steps.
Found uncertainty sample 94 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 8 steps.
Found uncertainty sample 99 after 3 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_184752-3jql53q4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_47
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3jql53q4
Training model 47. Added 100 samples to the dataset.
Epoch 0, Batch 100/158, Loss: 0.07704947143793106

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.3115522463865608, Training Loss Force: 2.4379961991352967, time: 2.3298182487487793
Validation Loss Energy: 1.814007040999748, Validation Loss Force: 2.484886354092715, time: 0.14437317848205566
Test Loss Energy: 10.361768258643075, Test Loss Force: 11.1514834153102, time: 10.91045093536377

Epoch 1, Batch 100/158, Loss: 0.12002907693386078

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.2746140615161081, Training Loss Force: 2.2651524610325153, time: 2.261066436767578
Validation Loss Energy: 4.303631683183233, Validation Loss Force: 2.590408074779116, time: 0.1394801139831543
Test Loss Energy: 9.590080708466543, Test Loss Force: 11.135640330706382, time: 9.972692489624023

Epoch 2, Batch 100/158, Loss: 0.13708265125751495

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.4037579789172883, Training Loss Force: 2.2598584583998322, time: 2.4425203800201416
Validation Loss Energy: 2.3499792831104194, Validation Loss Force: 2.4497994118692414, time: 0.166975736618042
Test Loss Energy: 10.549974898523999, Test Loss Force: 11.131546967656776, time: 10.195107460021973

Epoch 3, Batch 100/158, Loss: 0.22269877791404724

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.239647730595732, Training Loss Force: 2.248568654949138, time: 2.233570098876953
Validation Loss Energy: 1.4517717319917953, Validation Loss Force: 2.5919297214993375, time: 0.1431262493133545
Test Loss Energy: 10.141144772993114, Test Loss Force: 11.267927753202567, time: 10.016855001449585

Epoch 4, Batch 100/158, Loss: 0.047383762896060944

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.2232541014382967, Training Loss Force: 2.2441421543974087, time: 2.2651114463806152
Validation Loss Energy: 4.421449936338253, Validation Loss Force: 2.459364461924764, time: 0.1410694122314453
Test Loss Energy: 9.696613662620564, Test Loss Force: 11.097143260254128, time: 10.20220160484314

Epoch 5, Batch 100/158, Loss: 0.2148987501859665

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.1610074033803173, Training Loss Force: 2.2725992472889076, time: 2.2690231800079346
Validation Loss Energy: 2.0312089542760465, Validation Loss Force: 2.4010226221080297, time: 0.14152908325195312
Test Loss Energy: 10.708635469172368, Test Loss Force: 11.266893267324795, time: 10.076187372207642

Epoch 6, Batch 100/158, Loss: 0.1477443277835846

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.941832900312043, Training Loss Force: 5.034695208122599, time: 2.274211883544922
Validation Loss Energy: 16.837316292765426, Validation Loss Force: 6.4844014907120355, time: 0.13919782638549805
Test Loss Energy: 18.86645815728711, Test Loss Force: 12.530939595646702, time: 10.112378597259521

Epoch 7, Batch 100/158, Loss: 0.10892815887928009

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 14.747435013736954, Training Loss Force: 6.761786242708333, time: 2.3076071739196777
Validation Loss Energy: 9.900701526094243, Validation Loss Force: 7.700154096578039, time: 0.1426527500152588
Test Loss Energy: 15.547425188024285, Test Loss Force: 12.612846862473496, time: 10.005847215652466

Epoch 8, Batch 100/158, Loss: 0.4776079058647156

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.033546437000096, Training Loss Force: 5.275188881070664, time: 2.285444736480713
Validation Loss Energy: 18.19126405066859, Validation Loss Force: 6.0734899270622575, time: 0.14101910591125488
Test Loss Energy: 14.115034191057505, Test Loss Force: 11.888525810118246, time: 10.044770240783691

Epoch 9, Batch 100/158, Loss: 0.41645053029060364

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.377751884446948, Training Loss Force: 5.509931427131738, time: 2.2739381790161133
Validation Loss Energy: 10.914522460484912, Validation Loss Force: 4.647323736614234, time: 0.1410822868347168
Test Loss Energy: 18.66544550938243, Test Loss Force: 11.526095723703659, time: 10.286729335784912

Epoch 10, Batch 100/158, Loss: 0.7468462586402893

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.231647762236037, Training Loss Force: 4.259244918512772, time: 2.321084499359131
Validation Loss Energy: 8.366202639661628, Validation Loss Force: 3.512822953651181, time: 0.15003108978271484
Test Loss Energy: 9.815352928990263, Test Loss Force: 10.987949659288978, time: 10.000068187713623

Epoch 11, Batch 100/158, Loss: 0.5543432831764221

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.371771659617595, Training Loss Force: 4.904281411189377, time: 2.296483039855957
Validation Loss Energy: 8.916632752305865, Validation Loss Force: 4.348416683165397, time: 0.14432740211486816
Test Loss Energy: 10.085571245377098, Test Loss Force: 11.926588496201658, time: 10.116352319717407

Epoch 12, Batch 100/158, Loss: 1.0622581243515015

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 13.572699776848394, Training Loss Force: 5.755785483485344, time: 2.306431770324707
Validation Loss Energy: 25.160012053342513, Validation Loss Force: 6.215855375310454, time: 0.14076828956604004
Test Loss Energy: 18.267142607493938, Test Loss Force: 11.868130897185617, time: 10.282180070877075

Epoch 13, Batch 100/158, Loss: 0.5677712559700012

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.487650207596447, Training Loss Force: 6.099269669631426, time: 2.2280750274658203
Validation Loss Energy: 9.0743994831474, Validation Loss Force: 3.9785761465898055, time: 0.13813209533691406
Test Loss Energy: 10.311950726117672, Test Loss Force: 11.370701376005238, time: 9.973444700241089

Epoch 14, Batch 100/158, Loss: 1.133887767791748

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 12.917508918612326, Training Loss Force: 6.321921823648504, time: 2.2305853366851807
Validation Loss Energy: 4.467327944934728, Validation Loss Force: 5.639880658245379, time: 0.14633846282958984
Test Loss Energy: 10.444362269758507, Test Loss Force: 12.56428258651303, time: 10.147504329681396

Epoch 15, Batch 100/158, Loss: 0.8224433660507202

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.68400640194547, Training Loss Force: 6.165221298444929, time: 2.268348217010498
Validation Loss Energy: 4.370997115915642, Validation Loss Force: 4.780138162963099, time: 0.13838505744934082
Test Loss Energy: 8.850431741641428, Test Loss Force: 11.878375720392238, time: 9.972894668579102

Epoch 16, Batch 100/158, Loss: 0.8081116080284119

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.662566540667266, Training Loss Force: 4.362158716552822, time: 2.3048713207244873
Validation Loss Energy: 3.7270617714910945, Validation Loss Force: 3.9557135178086216, time: 0.1453254222869873
Test Loss Energy: 9.217205788325165, Test Loss Force: 11.29033910662521, time: 10.205544471740723

Epoch 17, Batch 100/158, Loss: 3.4404637813568115

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 14.507642884595855, Training Loss Force: 6.339051817416663, time: 2.2407824993133545
Validation Loss Energy: 7.643142185346883, Validation Loss Force: 4.104959888004278, time: 0.1410822868347168
Test Loss Energy: 10.294134168008354, Test Loss Force: 10.796348086696913, time: 10.06576132774353

Epoch 18, Batch 100/158, Loss: 0.48894238471984863

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 5.921349967087374, Training Loss Force: 3.3072790610815295, time: 2.280487298965454
Validation Loss Energy: 7.261426703218962, Validation Loss Force: 3.441134251004487, time: 0.13910484313964844
Test Loss Energy: 13.55763723415485, Test Loss Force: 11.415712463279592, time: 10.082921981811523

Epoch 19, Batch 100/158, Loss: 0.6119799613952637

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.138347746075313, Training Loss Force: 3.055528756748321, time: 2.434486150741577
Validation Loss Energy: 1.1166901459877259, Validation Loss Force: 3.4839526370898173, time: 0.14191389083862305
Test Loss Energy: 8.703904564309584, Test Loss Force: 11.078148323034876, time: 9.991039752960205

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.049 MB uploadedwandb: | 0.039 MB of 0.049 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–†â–…â–ˆâ–‚â–‚â–ˆâ–‚â–‚â–â–â–‚â–„â–
wandb:   test_error_force â–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ˆâ–ˆâ–…â–„â–‚â–…â–…â–ƒâ–ˆâ–…â–ƒâ–â–ƒâ–‚
wandb:          test_loss â–‚â–â–‚â–‚â–â–‚â–ˆâ–‡â–…â–†â–â–ƒâ–†â–‚â–…â–ƒâ–‚â–â–„â–
wandb: train_error_energy â–‚â–â–‚â–‚â–â–â–‡â–ˆâ–†â–†â–…â–…â–‡â–†â–‡â–†â–…â–ˆâ–ƒâ–„
wandb:  train_error_force â–â–â–â–â–â–â–…â–ˆâ–†â–†â–„â–…â–†â–‡â–‡â–‡â–„â–‡â–ƒâ–‚
wandb:         train_loss â–â–â–â–â–â–â–†â–ˆâ–†â–†â–…â–…â–‡â–‡â–‡â–‡â–…â–ˆâ–ƒâ–ƒ
wandb: valid_error_energy â–â–‚â–â–â–‚â–â–†â–„â–†â–„â–ƒâ–ƒâ–ˆâ–ƒâ–‚â–‚â–‚â–ƒâ–ƒâ–
wandb:  valid_error_force â–â–â–â–â–â–â–†â–ˆâ–†â–„â–‚â–„â–†â–ƒâ–…â–„â–ƒâ–ƒâ–‚â–‚
wandb:         valid_loss â–â–‚â–â–â–â–â–‡â–‡â–‡â–„â–ƒâ–„â–ˆâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 5041
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.7039
wandb:   test_error_force 11.07815
wandb:          test_loss 4.28925
wandb: train_error_energy 6.13835
wandb:  train_error_force 3.05553
wandb:         train_loss 1.43317
wandb: valid_error_energy 1.11669
wandb:  valid_error_force 3.48395
wandb:         valid_loss 1.24047
wandb: 
wandb: ğŸš€ View run al_77_47 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3jql53q4
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_184752-3jql53q4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6741304993629456, Uncertainty Bias: -0.4248567223548889
7.6293945e-06 0.001141429
-4.0617294 23.519287
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 3 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 2 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 5 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 5 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 3 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 6 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 5 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 7 steps.
Found uncertainty sample 66 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 4 steps.
Found uncertainty sample 70 after 3 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 8 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 3 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 4 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 3 steps.
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_185653-kameblsz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_48
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/kameblsz
Training model 48. Added 100 samples to the dataset.
Epoch 0, Batch 100/161, Loss: 0.15049587190151215

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.2984924140281118, Training Loss Force: 2.4451463177774246, time: 2.335649013519287
Validation Loss Energy: 3.7837639499402655, Validation Loss Force: 2.8077678233683527, time: 0.1420879364013672
Test Loss Energy: 9.523422743408837, Test Loss Force: 10.99236734985899, time: 9.966598510742188

Epoch 1, Batch 100/161, Loss: 0.06843376159667969

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.643534831701487, Training Loss Force: 2.3279014404392457, time: 2.259869337081909
Validation Loss Energy: 2.8496358575388627, Validation Loss Force: 2.5488093218290806, time: 0.14858293533325195
Test Loss Energy: 10.770381511420664, Test Loss Force: 11.036991309918944, time: 10.065443515777588

Epoch 2, Batch 100/161, Loss: 0.0356498658657074

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7318741521300987, Training Loss Force: 2.2632585556933322, time: 2.493793487548828
Validation Loss Energy: 1.9215204629734235, Validation Loss Force: 2.4634775321652937, time: 0.14422035217285156
Test Loss Energy: 10.254000887365994, Test Loss Force: 11.029931042547028, time: 10.03041386604309

Epoch 3, Batch 100/161, Loss: 0.049537673592567444

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.415132956972179, Training Loss Force: 2.238469081816979, time: 2.3061928749084473
Validation Loss Energy: 1.15568971910887, Validation Loss Force: 2.4425802554891165, time: 0.14472532272338867
Test Loss Energy: 9.612241292130204, Test Loss Force: 11.07115006260816, time: 10.047956228256226

Epoch 4, Batch 100/161, Loss: 0.08098993450403214

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4601394384162394, Training Loss Force: 2.2366193623065658, time: 2.279662847518921
Validation Loss Energy: 1.251964546837754, Validation Loss Force: 2.5320653442857974, time: 0.14414644241333008
Test Loss Energy: 9.94417855274634, Test Loss Force: 11.135583812380759, time: 10.205061197280884

Epoch 5, Batch 100/161, Loss: 0.0666835755109787

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2635026713071789, Training Loss Force: 2.223765124765637, time: 2.318434953689575
Validation Loss Energy: 4.261693879347, Validation Loss Force: 2.4657970766584234, time: 0.1431732177734375
Test Loss Energy: 9.616915264394917, Test Loss Force: 11.05852203135139, time: 10.207761764526367

Epoch 6, Batch 100/161, Loss: 0.5808436870574951

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.441169258412183, Training Loss Force: 5.778498716156237, time: 2.28192400932312
Validation Loss Energy: 1.6107813277828502, Validation Loss Force: 4.8330355937158025, time: 0.14166831970214844
Test Loss Energy: 9.326164451191024, Test Loss Force: 12.08000157347705, time: 10.167436122894287

Epoch 7, Batch 100/161, Loss: 0.5193800926208496

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.18749666269801, Training Loss Force: 4.523546316346261, time: 2.5409891605377197
Validation Loss Energy: 4.755851787662853, Validation Loss Force: 3.416841418299009, time: 0.14179468154907227
Test Loss Energy: 11.916703114623688, Test Loss Force: 11.57691888138758, time: 10.188640832901001

Epoch 8, Batch 100/161, Loss: 0.1705552041530609

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.013420692959764, Training Loss Force: 4.606722422957684, time: 2.3261053562164307
Validation Loss Energy: 2.2848967286851103, Validation Loss Force: 8.127911620239935, time: 0.1434776782989502
Test Loss Energy: 10.510659091109273, Test Loss Force: 14.744834718436087, time: 10.150248527526855

Epoch 9, Batch 100/161, Loss: 1.1157015562057495

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 12.277603822770196, Training Loss Force: 5.931580467882891, time: 2.3050622940063477
Validation Loss Energy: 15.158979966583933, Validation Loss Force: 9.539440728603445, time: 0.153641939163208
Test Loss Energy: 19.00998743381889, Test Loss Force: 14.9187982692732, time: 10.228591203689575

Epoch 10, Batch 100/161, Loss: 0.2515891194343567

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.63034867427297, Training Loss Force: 6.392456102400222, time: 2.356924295425415
Validation Loss Energy: 4.509931974855578, Validation Loss Force: 4.168326628214117, time: 0.1650846004486084
Test Loss Energy: 11.395604893804613, Test Loss Force: 11.70749481053781, time: 10.04448914527893

Epoch 11, Batch 100/161, Loss: 0.442663311958313

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.641739409374632, Training Loss Force: 4.328114372914986, time: 2.2873575687408447
Validation Loss Energy: 6.761981738702199, Validation Loss Force: 3.6265897416960446, time: 0.14990758895874023
Test Loss Energy: 9.570498857347681, Test Loss Force: 11.316079114296762, time: 10.217278957366943

Epoch 12, Batch 100/161, Loss: 0.521752655506134

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.223916126093563, Training Loss Force: 3.02259856495005, time: 2.256868362426758
Validation Loss Energy: 6.087955257012214, Validation Loss Force: 3.2016043004548522, time: 0.14620685577392578
Test Loss Energy: 9.430730241187284, Test Loss Force: 11.256119344921624, time: 10.072381973266602

Epoch 13, Batch 100/161, Loss: 0.12107644975185394

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.331063135384324, Training Loss Force: 2.973671288188222, time: 2.3050482273101807
Validation Loss Energy: 7.750503450070841, Validation Loss Force: 3.0704938126336736, time: 0.14121437072753906
Test Loss Energy: 13.439864016126041, Test Loss Force: 11.643574528820878, time: 9.959378242492676

Epoch 14, Batch 100/161, Loss: 0.16324426233768463

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.451288310536748, Training Loss Force: 3.863761647657902, time: 2.3302977085113525
Validation Loss Energy: 8.11523372974739, Validation Loss Force: 7.718950638308485, time: 0.14865922927856445
Test Loss Energy: 14.530821106926838, Test Loss Force: 13.156091681934898, time: 11.183707237243652

Epoch 15, Batch 100/161, Loss: 0.17780105769634247

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.701548994601964, Training Loss Force: 5.427350706698166, time: 2.2516517639160156
Validation Loss Energy: 6.243365999843735, Validation Loss Force: 3.647658918952878, time: 0.1412525177001953
Test Loss Energy: 9.681593022245623, Test Loss Force: 11.10096263066838, time: 10.094208240509033

Epoch 16, Batch 100/161, Loss: 0.8076550960540771

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.438848876407992, Training Loss Force: 5.559764684486186, time: 2.3384833335876465
Validation Loss Energy: 2.564445743040519, Validation Loss Force: 7.710616280622506, time: 0.14580106735229492
Test Loss Energy: 10.08186483700244, Test Loss Force: 13.46802322730612, time: 10.23593020439148

Epoch 17, Batch 100/161, Loss: 0.29354530572891235

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.809689887788462, Training Loss Force: 5.168993855347141, time: 2.2726075649261475
Validation Loss Energy: 3.421464623552859, Validation Loss Force: 3.4910224566960726, time: 0.14459013938903809
Test Loss Energy: 9.985015859601003, Test Loss Force: 11.260102180136743, time: 9.97569227218628

Epoch 18, Batch 100/161, Loss: 0.42086169123649597

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.0406165033204715, Training Loss Force: 3.0141400542323025, time: 2.307770252227783
Validation Loss Energy: 9.153862679024163, Validation Loss Force: 3.023751847750999, time: 0.14817023277282715
Test Loss Energy: 12.945423628876739, Test Loss Force: 11.391670248389575, time: 10.115455627441406

Epoch 19, Batch 100/161, Loss: 0.5292706489562988

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.245227225079218, Training Loss Force: 3.1316698623289017, time: 2.5064659118652344
Validation Loss Energy: 8.86843972682223, Validation Loss Force: 3.3164474711511307, time: 0.14700579643249512
Test Loss Energy: 13.864431977200569, Test Loss Force: 11.672622770465113, time: 10.146187782287598

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–‚â–â–â–â–â–ƒâ–‚â–ˆâ–‚â–â–â–„â–…â–â–‚â–â–„â–„
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–‚â–ˆâ–ˆâ–‚â–‚â–â–‚â–…â–â–…â–â–‚â–‚
wandb:          test_loss â–â–â–â–â–â–â–‚â–‚â–†â–ˆâ–‚â–â–â–ƒâ–…â–â–„â–â–‚â–ƒ
wandb: train_error_energy â–‚â–‚â–â–â–â–â–‡â–…â–†â–ˆâ–ˆâ–…â–„â–„â–…â–‡â–ˆâ–…â–„â–„
wandb:  train_error_force â–â–â–â–â–â–â–‡â–…â–…â–‡â–ˆâ–…â–‚â–‚â–„â–†â–‡â–†â–‚â–ƒ
wandb:         train_loss â–â–â–â–â–â–â–‡â–…â–…â–‡â–ˆâ–…â–ƒâ–ƒâ–„â–‡â–‡â–†â–ƒâ–ƒ
wandb: valid_error_energy â–‚â–‚â–â–â–â–ƒâ–â–ƒâ–‚â–ˆâ–ƒâ–„â–ƒâ–„â–„â–„â–‚â–‚â–…â–…
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–‚â–‡â–ˆâ–ƒâ–‚â–‚â–‚â–†â–‚â–†â–‚â–‚â–‚
wandb:         valid_loss â–‚â–â–â–â–â–â–ƒâ–‚â–…â–ˆâ–ƒâ–ƒâ–‚â–‚â–†â–ƒâ–…â–‚â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5131
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 13.86443
wandb:   test_error_force 11.67262
wandb:          test_loss 4.83351
wandb: train_error_energy 6.24523
wandb:  train_error_force 3.13167
wandb:         train_loss 1.4658
wandb: valid_error_energy 8.86844
wandb:  valid_error_force 3.31645
wandb:         valid_loss 1.70318
wandb: 
wandb: ğŸš€ View run al_77_48 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/kameblsz
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_185653-kameblsz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6170075535774231, Uncertainty Bias: -0.4774731397628784
1.2397766e-05 0.004667282
-6.854915 28.97287
(48745, 22, 3)
Found uncertainty sample 0 after 12 steps.
Found uncertainty sample 1 after 11 steps.
Found uncertainty sample 2 after 7 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 8 steps.
Found uncertainty sample 5 after 17 steps.
Found uncertainty sample 6 after 11 steps.
Found uncertainty sample 7 after 4 steps.
Found uncertainty sample 8 after 17 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 25 steps.
Found uncertainty sample 11 after 7 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 5 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 8 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 9 steps.
Found uncertainty sample 19 after 8 steps.
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 12 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 14 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 28 after 7 steps.
Found uncertainty sample 29 after 6 steps.
Found uncertainty sample 30 after 14 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 2 steps.
Found uncertainty sample 33 after 2 steps.
Found uncertainty sample 34 after 5 steps.
Found uncertainty sample 35 after 10 steps.
Found uncertainty sample 36 after 8 steps.
Found uncertainty sample 37 after 3 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 8 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 6 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 11 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 5 steps.
Found uncertainty sample 53 after 3 steps.
Found uncertainty sample 54 after 3 steps.
Found uncertainty sample 55 after 4 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 12 steps.
Found uncertainty sample 58 after 5 steps.
Found uncertainty sample 59 after 19 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 11 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 5 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 4 steps.
Found uncertainty sample 68 after 4 steps.
Found uncertainty sample 69 after 8 steps.
Found uncertainty sample 70 after 2 steps.
Found uncertainty sample 71 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 10 steps.
Found uncertainty sample 74 after 13 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 5 steps.
Found uncertainty sample 79 after 12 steps.
Found uncertainty sample 80 after 5 steps.
Found uncertainty sample 81 after 3 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 6 steps.
Found uncertainty sample 85 after 11 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 17 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 5 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 9 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 17 steps.
Found uncertainty sample 97 after 4 steps.
Found uncertainty sample 98 after 10 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_190557-jxbrwdgn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_49
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/jxbrwdgn
Training model 49. Added 100 samples to the dataset.
Epoch 0, Batch 100/164, Loss: 0.19680657982826233

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.014520939553701, Training Loss Force: 2.383118306141146, time: 2.4339916706085205
Validation Loss Energy: 1.623558825606673, Validation Loss Force: 2.499687172222535, time: 0.14049172401428223
Test Loss Energy: 9.514425333742224, Test Loss Force: 11.015228536925461, time: 9.24895429611206

Epoch 1, Batch 100/164, Loss: 0.21940870583057404

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.219416510699312, Training Loss Force: 2.2638244136941537, time: 2.3303325176239014
Validation Loss Energy: 3.560711516189287, Validation Loss Force: 2.505958277602348, time: 0.14305496215820312
Test Loss Energy: 9.453840071580684, Test Loss Force: 10.995407443547693, time: 10.071930646896362

Epoch 2, Batch 100/164, Loss: 0.2032468169927597

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.3830209359106567, Training Loss Force: 2.2382673723868742, time: 2.4634571075439453
Validation Loss Energy: 4.073672052525576, Validation Loss Force: 2.429676875374796, time: 0.15910577774047852
Test Loss Energy: 11.476981499253418, Test Loss Force: 11.137127275625447, time: 12.010382890701294

Epoch 3, Batch 100/164, Loss: 0.14165885746479034

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6670952523664098, Training Loss Force: 2.192021080337524, time: 2.4848854541778564
Validation Loss Energy: 1.49122484505167, Validation Loss Force: 2.3745469637650207, time: 0.14264631271362305
Test Loss Energy: 9.8634383193953, Test Loss Force: 11.008320789757853, time: 9.087815046310425

Epoch 4, Batch 100/164, Loss: 0.08507728576660156

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.8104039103713119, Training Loss Force: 2.1965887454797497, time: 2.458655834197998
Validation Loss Energy: 4.134289719459789, Validation Loss Force: 2.5298387270495684, time: 0.13432908058166504
Test Loss Energy: 9.55685704960952, Test Loss Force: 11.050970105842037, time: 9.287657737731934

Epoch 5, Batch 100/164, Loss: 0.24828937649726868

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.1060055404820783, Training Loss Force: 2.21540193534949, time: 2.37309193611145
Validation Loss Energy: 2.8881585104609537, Validation Loss Force: 2.479110613339543, time: 0.14119553565979004
Test Loss Energy: 9.481273819688482, Test Loss Force: 11.026991240906941, time: 9.930851697921753

Epoch 6, Batch 100/164, Loss: 1.3123414516448975

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 15.714749866379343, Training Loss Force: 6.703459206946761, time: 2.4066078662872314
Validation Loss Energy: 6.393671978281812, Validation Loss Force: 5.908168477437685, time: 0.1385948657989502
Test Loss Energy: 12.140649150356861, Test Loss Force: 12.127810858804368, time: 9.25145173072815

Epoch 7, Batch 100/164, Loss: 0.19375750422477722

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.132553078275086, Training Loss Force: 5.431540292676704, time: 2.522894859313965
Validation Loss Energy: 1.6313876872198725, Validation Loss Force: 4.973979522136395, time: 0.13657927513122559
Test Loss Energy: 9.292457723862885, Test Loss Force: 12.01695089055243, time: 9.207661390304565

Epoch 8, Batch 100/164, Loss: 0.37098944187164307

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.747911379135466, Training Loss Force: 5.171006306004751, time: 2.401493549346924
Validation Loss Energy: 5.070005145979462, Validation Loss Force: 3.7814270372537533, time: 0.14076566696166992
Test Loss Energy: 9.344417221654636, Test Loss Force: 11.442841413466635, time: 9.19839859008789

Epoch 9, Batch 100/164, Loss: 0.4950871765613556

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.149439134757729, Training Loss Force: 3.1624308268039547, time: 2.330857038497925
Validation Loss Energy: 3.076851239419058, Validation Loss Force: 3.1693172742953233, time: 0.1323840618133545
Test Loss Energy: 9.304018696593001, Test Loss Force: 11.217303710699, time: 11.797346353530884

Epoch 10, Batch 100/164, Loss: 0.5906773209571838

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.259872605741345, Training Loss Force: 3.0173614098121515, time: 2.5387067794799805
Validation Loss Energy: 8.422884406772422, Validation Loss Force: 3.2524446394144255, time: 0.17690110206604004
Test Loss Energy: 9.962506699630364, Test Loss Force: 11.392362108239537, time: 11.635839939117432

Epoch 11, Batch 100/164, Loss: 0.09747263044118881

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.809696282360995, Training Loss Force: 3.010256642917054, time: 2.528062343597412
Validation Loss Energy: 13.417318316653743, Validation Loss Force: 4.923251779541613, time: 0.16110801696777344
Test Loss Energy: 18.413668856843373, Test Loss Force: 12.072706838991895, time: 10.21561312675476

Epoch 12, Batch 100/164, Loss: 0.8133354783058167

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.274461332004565, Training Loss Force: 5.406308667391072, time: 2.340705633163452
Validation Loss Energy: 4.788355693570517, Validation Loss Force: 7.68982388111406, time: 0.14724302291870117
Test Loss Energy: 9.651731895990292, Test Loss Force: 13.03485434754158, time: 10.139650344848633

Epoch 13, Batch 100/164, Loss: 0.6892247200012207

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.013881284047393, Training Loss Force: 4.969797767968519, time: 2.3624579906463623
Validation Loss Energy: 4.376503130110048, Validation Loss Force: 8.167570776482842, time: 0.1474153995513916
Test Loss Energy: 9.654484399070247, Test Loss Force: 13.404697814956089, time: 9.969546794891357

Epoch 14, Batch 100/164, Loss: 0.5061273574829102

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 12.57067657369693, Training Loss Force: 5.2384551880728125, time: 2.3388326168060303
Validation Loss Energy: 16.678106665858277, Validation Loss Force: 5.574828906385257, time: 0.1446528434753418
Test Loss Energy: 20.562506294591973, Test Loss Force: 12.557964182786922, time: 10.271135568618774

Epoch 15, Batch 100/164, Loss: 0.4765254855155945

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.070857865909987, Training Loss Force: 5.000181492944285, time: 2.381359100341797
Validation Loss Energy: 7.271310986606697, Validation Loss Force: 5.374029730186106, time: 0.1469426155090332
Test Loss Energy: 9.750107087355362, Test Loss Force: 11.848971492571437, time: 10.078605890274048

Epoch 16, Batch 100/164, Loss: 0.7779141664505005

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 13.766588122779126, Training Loss Force: 5.68093585951734, time: 2.3519232273101807
Validation Loss Energy: 10.922643946467383, Validation Loss Force: 4.418570824281407, time: 0.1499624252319336
Test Loss Energy: 10.847518598922026, Test Loss Force: 11.263831102577084, time: 10.121182918548584

Epoch 17, Batch 100/164, Loss: 0.24604302644729614

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 5.97770048209142, Training Loss Force: 3.3262293366884887, time: 2.3669724464416504
Validation Loss Energy: 7.1143590763931615, Validation Loss Force: 3.4106135107335054, time: 0.144026517868042
Test Loss Energy: 9.780501201939554, Test Loss Force: 11.2009428538799, time: 9.883725643157959

Epoch 18, Batch 100/164, Loss: 0.6376874446868896

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.330809772229205, Training Loss Force: 5.102478287791202, time: 2.3170220851898193
Validation Loss Energy: 1.6210339059379493, Validation Loss Force: 6.239964505970541, time: 0.14904522895812988
Test Loss Energy: 9.77174431225637, Test Loss Force: 12.655790516857614, time: 9.973474502563477

Epoch 19, Batch 100/164, Loss: 0.7607035636901855

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.7439311060672, Training Loss Force: 5.689965018709084, time: 2.5386669635772705
Validation Loss Energy: 8.148782844626039, Validation Loss Force: 7.091781018780472, time: 0.14341044425964355
Test Loss Energy: 9.63825640228542, Test Loss Force: 12.954173602992523, time: 10.75715708732605

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–â–â–â–ƒâ–â–â–â–â–‡â–â–â–ˆâ–â–‚â–â–â–
wandb:   test_error_force â–â–â–â–â–â–â–„â–„â–‚â–‚â–‚â–„â–‡â–ˆâ–†â–ƒâ–‚â–‚â–†â–‡
wandb:          test_loss â–â–â–‚â–â–â–â–„â–ƒâ–‚â–â–‚â–†â–…â–†â–ˆâ–ƒâ–‚â–‚â–„â–…
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–…â–„â–ƒâ–ƒâ–„â–…â–…â–†â–†â–‡â–ƒâ–…â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–†â–ƒâ–‚â–‚â–†â–…â–†â–…â–†â–ƒâ–†â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–…â–ƒâ–ƒâ–ƒâ–†â–…â–†â–…â–‡â–ƒâ–…â–†
wandb: valid_error_energy â–â–‚â–‚â–â–‚â–‚â–ƒâ–â–ƒâ–‚â–„â–†â–ƒâ–‚â–ˆâ–„â–…â–„â–â–„
wandb:  valid_error_force â–â–â–â–â–â–â–…â–„â–ƒâ–‚â–‚â–„â–‡â–ˆâ–…â–…â–ƒâ–‚â–†â–‡
wandb:         valid_loss â–â–‚â–‚â–â–‚â–â–†â–„â–ƒâ–‚â–ƒâ–†â–ˆâ–ˆâ–ˆâ–†â–…â–ƒâ–…â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5221
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.63826
wandb:   test_error_force 12.95417
wandb:          test_loss 4.97951
wandb: train_error_energy 10.74393
wandb:  train_error_force 5.68997
wandb:         train_loss 2.62287
wandb: valid_error_energy 8.14878
wandb:  valid_error_force 7.09178
wandb:         valid_loss 2.91825
wandb: 
wandb: ğŸš€ View run al_77_49 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/jxbrwdgn
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_190557-jxbrwdgn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7189018726348877, Uncertainty Bias: -0.5474068522453308
3.194809e-05 0.03222537
-3.7972188 42.049995
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 9 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 4 steps.
Found uncertainty sample 9 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 4 steps.
Found uncertainty sample 14 after 23 steps.
Found uncertainty sample 15 after 4 steps.
Found uncertainty sample 16 after 9 steps.
Found uncertainty sample 17 after 9 steps.
Found uncertainty sample 18 after 2 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 25 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 6 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 6 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 11 steps.
Found uncertainty sample 30 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 3 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 8 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 4 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 6 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 8 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 9 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 4 steps.
Found uncertainty sample 59 after 4 steps.
Found uncertainty sample 60 after 4 steps.
Found uncertainty sample 61 after 17 steps.
Found uncertainty sample 62 after 21 steps.
Found uncertainty sample 63 after 7 steps.
Found uncertainty sample 64 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 4 steps.
Found uncertainty sample 67 after 14 steps.
Found uncertainty sample 68 after 9 steps.
Found uncertainty sample 69 after 13 steps.
Found uncertainty sample 70 after 3 steps.
Found uncertainty sample 71 after 4 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 3 steps.
Found uncertainty sample 77 after 2 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 3 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 16 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 10 steps.
Found uncertainty sample 86 after 3 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 5 steps.
Found uncertainty sample 89 after 8 steps.
Found uncertainty sample 90 after 3 steps.
Found uncertainty sample 91 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_191457-25p9h0up
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_50
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/25p9h0up
Training model 50. Added 100 samples to the dataset.
Epoch 0, Batch 100/166, Loss: 0.12377682328224182

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7859809767408024, Training Loss Force: 2.3357879310008363, time: 2.412000894546509
Validation Loss Energy: 1.4701014478578036, Validation Loss Force: 2.515731597532362, time: 0.15242791175842285
Test Loss Energy: 9.638784149145796, Test Loss Force: 11.084606772829897, time: 10.092527151107788

Epoch 1, Batch 100/166, Loss: 0.09373979270458221

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.1885517323351262, Training Loss Force: 2.238940655326535, time: 2.3879737854003906
Validation Loss Energy: 0.9358987760055827, Validation Loss Force: 2.3890306268079917, time: 0.15261149406433105
Test Loss Energy: 9.590453650140795, Test Loss Force: 11.115818724373515, time: 10.239540576934814

Epoch 2, Batch 100/166, Loss: 0.06468251347541809

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.97398239881897, Training Loss Force: 2.247103285022803, time: 2.5989201068878174
Validation Loss Energy: 6.572818806730943, Validation Loss Force: 2.52579230910672, time: 0.15134143829345703
Test Loss Energy: 12.667906255192221, Test Loss Force: 11.242814585990361, time: 10.126428127288818

Epoch 3, Batch 100/166, Loss: 0.11571410298347473

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.8541381347724406, Training Loss Force: 2.25224851305018, time: 2.3681862354278564
Validation Loss Energy: 3.5547344083051473, Validation Loss Force: 2.447716564678233, time: 0.15705537796020508
Test Loss Energy: 9.668974051641221, Test Loss Force: 11.024294859778216, time: 9.975362300872803

Epoch 4, Batch 100/166, Loss: 0.11208148300647736

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.646180774191992, Training Loss Force: 2.191061946568827, time: 2.4045910835266113
Validation Loss Energy: 1.9513045765981518, Validation Loss Force: 2.347620400401509, time: 0.15301895141601562
Test Loss Energy: 10.535741886716297, Test Loss Force: 11.067575469704666, time: 10.237073421478271

Epoch 5, Batch 100/166, Loss: 0.0741623193025589

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5826827121705822, Training Loss Force: 2.149732326393171, time: 2.335458517074585
Validation Loss Energy: 2.088614989895495, Validation Loss Force: 2.3595116008014902, time: 0.14834213256835938
Test Loss Energy: 10.460449889385222, Test Loss Force: 11.136247898043635, time: 10.065537929534912

Epoch 6, Batch 100/166, Loss: 0.5393050312995911

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 13.429585645259046, Training Loss Force: 5.806844608492689, time: 2.343301296234131
Validation Loss Energy: 8.848976365222777, Validation Loss Force: 4.6031200515490385, time: 0.1489558219909668
Test Loss Energy: 14.724187734703811, Test Loss Force: 11.518849775911791, time: 10.204910039901733

Epoch 7, Batch 100/166, Loss: 1.423677682876587

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.901953129696414, Training Loss Force: 4.891508744836629, time: 2.3623743057250977
Validation Loss Energy: 33.48891939241441, Validation Loss Force: 4.695666841102226, time: 0.15343499183654785
Test Loss Energy: 26.879995776161667, Test Loss Force: 10.902516530293243, time: 10.057594060897827

Epoch 8, Batch 100/166, Loss: 0.12638801336288452

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.174817993646622, Training Loss Force: 4.665264309084491, time: 2.336275577545166
Validation Loss Energy: 7.546318366838714, Validation Loss Force: 3.3316012198772462, time: 0.15314173698425293
Test Loss Energy: 9.756180587663549, Test Loss Force: 11.05467957795761, time: 10.006103992462158

Epoch 9, Batch 100/166, Loss: 0.31386613845825195

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.076497169563147, Training Loss Force: 3.0616820066918917, time: 2.3727023601531982
Validation Loss Energy: 10.359191251492705, Validation Loss Force: 2.930917369661272, time: 0.14776396751403809
Test Loss Energy: 14.67832783730841, Test Loss Force: 11.36166971959082, time: 11.131277084350586

Epoch 10, Batch 100/166, Loss: 0.7647488117218018

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.267297140056163, Training Loss Force: 3.034406478299664, time: 2.319204568862915
Validation Loss Energy: 5.275710465693887, Validation Loss Force: 3.243121333828331, time: 0.15294957160949707
Test Loss Energy: 9.358432824437807, Test Loss Force: 11.182542440948659, time: 10.022042274475098

Epoch 11, Batch 100/166, Loss: 0.640417754650116

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.514127850105414, Training Loss Force: 4.542191487961781, time: 2.3687186241149902
Validation Loss Energy: 5.012620486598023, Validation Loss Force: 5.6924347331750615, time: 0.15224194526672363
Test Loss Energy: 12.331141154434114, Test Loss Force: 12.261109656219425, time: 10.166412353515625

Epoch 12, Batch 100/166, Loss: 0.5755579471588135

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.296202944203273, Training Loss Force: 6.277016109040991, time: 2.3206756114959717
Validation Loss Energy: 2.6230907840109583, Validation Loss Force: 6.139284651639293, time: 0.16054439544677734
Test Loss Energy: 9.771297337171422, Test Loss Force: 12.557151190921578, time: 10.009578943252563

Epoch 13, Batch 100/166, Loss: 0.556361198425293

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.558708947984204, Training Loss Force: 5.888857150575208, time: 2.3692128658294678
Validation Loss Energy: 7.214674198663381, Validation Loss Force: 6.789678069303183, time: 0.15842771530151367
Test Loss Energy: 10.421075922188544, Test Loss Force: 13.536873930858675, time: 10.014041900634766

Epoch 14, Batch 100/166, Loss: 0.36099642515182495

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.231322987631936, Training Loss Force: 5.045256038260623, time: 2.5579590797424316
Validation Loss Energy: 2.9570788418393765, Validation Loss Force: 3.7880714845682384, time: 0.1470499038696289
Test Loss Energy: 10.739612888317552, Test Loss Force: 11.691413066578606, time: 10.141781568527222

Epoch 15, Batch 100/166, Loss: 0.14902108907699585

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.970392713214374, Training Loss Force: 3.0615092825036707, time: 2.4312660694122314
Validation Loss Energy: 6.224472522811761, Validation Loss Force: 3.0973079280784925, time: 0.15311098098754883
Test Loss Energy: 12.145026298821014, Test Loss Force: 11.414167125417169, time: 10.017616748809814

Epoch 16, Batch 100/166, Loss: 0.5806460380554199

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.26932249970749, Training Loss Force: 2.9515762924305373, time: 2.445202112197876
Validation Loss Energy: 22.917673781626387, Validation Loss Force: 3.458641241487816, time: 0.14960217475891113
Test Loss Energy: 16.535122486504548, Test Loss Force: 11.184175662009498, time: 10.335623025894165

Epoch 17, Batch 100/166, Loss: 0.321175217628479

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 13.379797496261173, Training Loss Force: 5.113205600573202, time: 2.35945987701416
Validation Loss Energy: 13.4506483250928, Validation Loss Force: 4.709851162590021, time: 0.1484510898590088
Test Loss Energy: 11.970669232146886, Test Loss Force: 11.292021133032247, time: 10.102020025253296

Epoch 18, Batch 100/166, Loss: 0.2873424291610718

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.509116106635448, Training Loss Force: 4.33531153082323, time: 2.3274247646331787
Validation Loss Energy: 4.315129724094555, Validation Loss Force: 3.3028729662309835, time: 0.15328335762023926
Test Loss Energy: 8.968852295017461, Test Loss Force: 11.026613957767248, time: 10.228798627853394

Epoch 19, Batch 100/166, Loss: 0.4672071933746338

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.88729397607225, Training Loss Force: 3.4551132648328338, time: 2.390730619430542
Validation Loss Energy: 2.1752050829029206, Validation Loss Force: 6.72766174324035, time: 0.15037083625793457
Test Loss Energy: 9.398903722800904, Test Loss Force: 13.021580906850822, time: 10.138599872589111

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–â–‚â–‚â–ƒâ–ˆâ–â–ƒâ–â–‚â–â–‚â–‚â–‚â–„â–‚â–â–
wandb:   test_error_force â–â–‚â–‚â–â–â–‚â–ƒâ–â–â–‚â–‚â–…â–…â–ˆâ–ƒâ–‚â–‚â–‚â–â–‡
wandb:          test_loss â–â–â–ƒâ–â–‚â–‚â–„â–ˆâ–â–„â–â–…â–„â–‡â–ƒâ–ƒâ–„â–ƒâ–â–…
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–‡â–…â–„â–„â–†â–†â–‡â–‡â–„â–„â–ˆâ–…â–…
wandb:  train_error_force â–â–â–â–â–â–â–‡â–†â–…â–ƒâ–ƒâ–…â–ˆâ–‡â–†â–ƒâ–‚â–†â–…â–ƒ
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–…â–ƒâ–ƒâ–†â–ˆâ–ˆâ–‡â–ƒâ–ƒâ–‡â–…â–„
wandb: valid_error_energy â–â–â–‚â–‚â–â–â–ƒâ–ˆâ–‚â–ƒâ–‚â–‚â–â–‚â–â–‚â–†â–„â–‚â–
wandb:  valid_error_force â–â–â–â–â–â–â–…â–…â–ƒâ–‚â–‚â–†â–‡â–ˆâ–ƒâ–‚â–ƒâ–…â–ƒâ–ˆ
wandb:         valid_loss â–â–â–‚â–â–â–â–„â–ˆâ–ƒâ–ƒâ–‚â–„â–„â–…â–‚â–‚â–…â–…â–‚â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 5311
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.3989
wandb:   test_error_force 13.02158
wandb:          test_loss 4.98604
wandb: train_error_energy 7.88729
wandb:  train_error_force 3.45511
wandb:         train_loss 1.68391
wandb: valid_error_energy 2.17521
wandb:  valid_error_force 6.72766
wandb:         valid_loss 2.39666
wandb: 
wandb: ğŸš€ View run al_77_50 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/25p9h0up
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_191457-25p9h0up/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5326171517372131, Uncertainty Bias: -0.3445577621459961
3.8146973e-06 0.0051903725
-4.21581 16.819212
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 4 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 3 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 3 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 11 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 6 steps.
Found uncertainty sample 38 after 14 steps.
Found uncertainty sample 39 after 15 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 10 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 5 steps.
Found uncertainty sample 46 after 4 steps.
Found uncertainty sample 47 after 11 steps.
Found uncertainty sample 48 after 12 steps.
Found uncertainty sample 49 after 7 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 5 steps.
Found uncertainty sample 53 after 7 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 3 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 5 steps.
Found uncertainty sample 60 after 25 steps.
Found uncertainty sample 61 after 4 steps.
Found uncertainty sample 62 after 8 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 16 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 3 steps.
Found uncertainty sample 73 after 15 steps.
Found uncertainty sample 74 after 5 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 2 steps.
Found uncertainty sample 78 after 5 steps.
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 13 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 3 steps.
Found uncertainty sample 87 after 30 steps.
Found uncertainty sample 88 after 8 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 17 steps.
Found uncertainty sample 92 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 5 steps.
Found uncertainty sample 95 after 7 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 6 steps.
Found uncertainty sample 99 after 11 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_192359-n54ruzwk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_51
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/n54ruzwk
Training model 51. Added 100 samples to the dataset.
Epoch 0, Batch 100/169, Loss: 0.05527079105377197

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7726483766644134, Training Loss Force: 2.3064154815354163, time: 2.424267530441284
Validation Loss Energy: 1.6095214286436665, Validation Loss Force: 2.4002853645658626, time: 0.15061235427856445
Test Loss Energy: 10.172791398025728, Test Loss Force: 11.095879800545921, time: 9.94070553779602

Epoch 1, Batch 100/169, Loss: 0.0653446838259697

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.483803606869241, Training Loss Force: 2.211406485958521, time: 2.4958720207214355
Validation Loss Energy: 1.8143905886036087, Validation Loss Force: 2.3990171153337068, time: 0.14960074424743652
Test Loss Energy: 10.658303605894085, Test Loss Force: 11.14730652451731, time: 9.948472499847412

Epoch 2, Batch 100/169, Loss: 0.06070522591471672

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7185203548851766, Training Loss Force: 2.227295610738399, time: 2.6340458393096924
Validation Loss Energy: 3.3232815583548976, Validation Loss Force: 2.4403511146692334, time: 0.15004301071166992
Test Loss Energy: 11.216599908321655, Test Loss Force: 11.201799973873573, time: 10.9293053150177

Epoch 3, Batch 100/169, Loss: 0.1024513989686966

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6904739331586967, Training Loss Force: 2.1627859954444935, time: 2.436553478240967
Validation Loss Energy: 1.7300582865128424, Validation Loss Force: 2.3526568795902008, time: 0.15214133262634277
Test Loss Energy: 9.837175988197751, Test Loss Force: 11.070451259211431, time: 9.934880256652832

Epoch 4, Batch 100/169, Loss: 0.03560950607061386

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5129376718498322, Training Loss Force: 2.1512210198365533, time: 2.383768320083618
Validation Loss Energy: 2.7689300936047743, Validation Loss Force: 2.5493740107831537, time: 0.15695881843566895
Test Loss Energy: 11.523185351696661, Test Loss Force: 11.26909422709094, time: 10.209253311157227

Epoch 5, Batch 100/169, Loss: 0.05694378539919853

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7715108199472538, Training Loss Force: 2.202647982857022, time: 2.431257963180542
Validation Loss Energy: 3.5332485428532343, Validation Loss Force: 2.5099819090692637, time: 0.1585838794708252
Test Loss Energy: 9.674220229983844, Test Loss Force: 11.119678543629066, time: 9.933215141296387

Epoch 6, Batch 100/169, Loss: 0.49449825286865234

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.559456133900218, Training Loss Force: 5.691469728576384, time: 2.452772378921509
Validation Loss Energy: 11.553958181031598, Validation Loss Force: 5.944866727824141, time: 0.1543266773223877
Test Loss Energy: 17.19608120306899, Test Loss Force: 12.766234912203213, time: 10.177148580551147

Epoch 7, Batch 100/169, Loss: 0.5989144444465637

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.220231222649037, Training Loss Force: 4.720168274391993, time: 2.4548537731170654
Validation Loss Energy: 8.876090638676308, Validation Loss Force: 4.008568492977501, time: 0.14951205253601074
Test Loss Energy: 10.411211441199306, Test Loss Force: 11.81464883207468, time: 9.955859661102295

Epoch 8, Batch 100/169, Loss: 0.4762149751186371

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.537088847566599, Training Loss Force: 5.189500306418395, time: 2.43319034576416
Validation Loss Energy: 7.562658274105186, Validation Loss Force: 5.123011833059767, time: 0.1485283374786377
Test Loss Energy: 9.544432745465784, Test Loss Force: 11.758085930650372, time: 9.934487104415894

Epoch 9, Batch 100/169, Loss: 0.8955697417259216

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 12.446101147132982, Training Loss Force: 5.273004192765952, time: 2.4312002658843994
Validation Loss Energy: 41.52861038771433, Validation Loss Force: 8.555497837105564, time: 0.15210556983947754
Test Loss Energy: 44.87050206165181, Test Loss Force: 17.21985826680429, time: 10.150150537490845

Epoch 10, Batch 100/169, Loss: 0.35115480422973633

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 10.992698939205285, Training Loss Force: 6.314320000592758, time: 2.461019515991211
Validation Loss Energy: 5.41381209668976, Validation Loss Force: 4.805269345002997, time: 0.14761686325073242
Test Loss Energy: 12.423617310847161, Test Loss Force: 11.460346505238887, time: 10.099325180053711

Epoch 11, Batch 100/169, Loss: 2.4260525703430176

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 12.569449667851726, Training Loss Force: 5.543483517739772, time: 2.44914174079895
Validation Loss Energy: 1.5321425644188846, Validation Loss Force: 3.9444094959464517, time: 0.15155363082885742
Test Loss Energy: 10.723686075451637, Test Loss Force: 11.381712484183684, time: 10.164363861083984

Epoch 12, Batch 100/169, Loss: 1.286473274230957

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.671180703416657, Training Loss Force: 4.4287989180534355, time: 2.4924652576446533
Validation Loss Energy: 11.716927353798589, Validation Loss Force: 3.9146022618420715, time: 0.16288137435913086
Test Loss Energy: 11.345054721498759, Test Loss Force: 11.1468389752696, time: 9.892613649368286

Epoch 13, Batch 100/169, Loss: 0.9438098669052124

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.417316719977674, Training Loss Force: 4.661431985186635, time: 2.399961233139038
Validation Loss Energy: 10.965926078160633, Validation Loss Force: 5.204025993760966, time: 0.14882445335388184
Test Loss Energy: 16.3197488491578, Test Loss Force: 12.86747997077525, time: 9.90789532661438

Epoch 14, Batch 100/169, Loss: 0.7027767300605774

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.506526214752908, Training Loss Force: 5.110427181648764, time: 2.606505870819092
Validation Loss Energy: 12.43862655740123, Validation Loss Force: 4.885900366327102, time: 0.15866899490356445
Test Loss Energy: 17.395820621802415, Test Loss Force: 12.497078501475889, time: 10.045679092407227

Epoch 15, Batch 100/169, Loss: 0.6396095752716064

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.445432139164698, Training Loss Force: 4.623233286885353, time: 2.4422614574432373
Validation Loss Energy: 7.175823034527998, Validation Loss Force: 4.522405158179965, time: 0.154191255569458
Test Loss Energy: 9.254752809972434, Test Loss Force: 11.408412501315636, time: 9.916309118270874

Epoch 16, Batch 100/169, Loss: 0.13266530632972717

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.300910828339628, Training Loss Force: 5.019527986545553, time: 2.3596246242523193
Validation Loss Energy: 31.537068163062997, Validation Loss Force: 6.556633431851746, time: 0.14795827865600586
Test Loss Energy: 26.04003601454209, Test Loss Force: 12.312383612835406, time: 10.091415643692017

Epoch 17, Batch 100/169, Loss: 0.10270124673843384

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.399859544030999, Training Loss Force: 5.6596824760682605, time: 2.445477247238159
Validation Loss Energy: 5.490910843128965, Validation Loss Force: 6.843428174438692, time: 0.1557912826538086
Test Loss Energy: 9.552650248871034, Test Loss Force: 13.701186096245012, time: 10.045156002044678

Epoch 18, Batch 100/169, Loss: 0.2767281234264374

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.206963691630301, Training Loss Force: 4.73562911153929, time: 2.3717479705810547
Validation Loss Energy: 9.673305559536702, Validation Loss Force: 3.552790135545955, time: 0.15204429626464844
Test Loss Energy: 15.562757276083378, Test Loss Force: 11.65875092568049, time: 10.16550898551941

Epoch 19, Batch 100/169, Loss: 1.0832252502441406

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.240502085672464, Training Loss Force: 4.671961356900949, time: 2.466818332672119
Validation Loss Energy: 2.8204736959502354, Validation Loss Force: 7.624689147885555, time: 0.15046238899230957
Test Loss Energy: 10.205591673069199, Test Loss Force: 14.812554729050149, time: 10.038419008255005

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–ƒâ–â–â–ˆâ–‚â–â–â–‚â–ƒâ–â–„â–â–‚â–
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–‚â–‚â–ˆâ–â–â–â–ƒâ–ƒâ–â–‚â–„â–‚â–…
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–â–â–ˆâ–â–â–â–ƒâ–ƒâ–â–ƒâ–‚â–‚â–ƒ
wandb: train_error_energy â–â–â–â–â–â–â–‡â–†â–‡â–ˆâ–‡â–ˆâ–…â–†â–‡â–†â–‡â–‡â–…â–‡
wandb:  train_error_force â–â–â–â–â–â–â–‡â–…â–†â–†â–ˆâ–‡â–…â–…â–†â–…â–†â–‡â–…â–…
wandb:         train_loss â–â–â–â–â–â–â–‡â–†â–‡â–‡â–ˆâ–‡â–…â–†â–‡â–†â–‡â–‡â–…â–†
wandb: valid_error_energy â–â–â–â–â–â–â–ƒâ–‚â–‚â–ˆâ–‚â–â–ƒâ–ƒâ–ƒâ–‚â–†â–‚â–‚â–
wandb:  valid_error_force â–â–â–â–â–â–â–…â–ƒâ–„â–ˆâ–„â–ƒâ–ƒâ–„â–„â–ƒâ–†â–†â–‚â–‡
wandb:         valid_loss â–â–â–â–â–â–â–„â–ƒâ–ƒâ–ˆâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–†â–„â–‚â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 5401
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.20559
wandb:   test_error_force 14.81255
wandb:          test_loss 5.63929
wandb: train_error_energy 11.2405
wandb:  train_error_force 4.67196
wandb:         train_loss 2.31547
wandb: valid_error_energy 2.82047
wandb:  valid_error_force 7.62469
wandb:         valid_loss 2.73999
wandb: 
wandb: ğŸš€ View run al_77_51 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/n54ruzwk
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_192359-n54ruzwk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7041581273078918, Uncertainty Bias: -0.47001588344573975
1.7166138e-05 0.10750961
-4.607226 54.53203
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 3 steps.
Found uncertainty sample 4 after 2 steps.
Found uncertainty sample 5 after 2 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 2 steps.
Found uncertainty sample 17 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 3 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 4 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 3 steps.
Found uncertainty sample 62 after 6 steps.
Found uncertainty sample 63 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 2 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_193259-ovrzfekc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_52
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/ovrzfekc
Training model 52. Added 100 samples to the dataset.
Epoch 0, Batch 100/172, Loss: 0.10332606732845306

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.005263539782659, Training Loss Force: 2.3420469308332974, time: 2.5067315101623535
Validation Loss Energy: 1.0386297286187833, Validation Loss Force: 2.44138964356584, time: 0.16041159629821777
Test Loss Energy: 9.691002008432015, Test Loss Force: 11.067194763755484, time: 10.080543279647827

Epoch 1, Batch 100/172, Loss: 0.12472045421600342

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.4433802714830013, Training Loss Force: 2.18751950442085, time: 2.5158305168151855
Validation Loss Energy: 1.6003089009576719, Validation Loss Force: 2.377402918230691, time: 0.15575456619262695
Test Loss Energy: 10.07757075462198, Test Loss Force: 11.101575201649478, time: 10.05361270904541

Epoch 2, Batch 100/172, Loss: 0.07622447609901428

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.4036364674671762, Training Loss Force: 2.175192315243965, time: 2.696695327758789
Validation Loss Energy: 1.3852188931959253, Validation Loss Force: 2.4147697698393196, time: 0.16320300102233887
Test Loss Energy: 9.268892333381446, Test Loss Force: 11.150825937441669, time: 10.133827686309814

Epoch 3, Batch 100/172, Loss: 0.10434939712285995

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5575969882671492, Training Loss Force: 2.1517658458954156, time: 2.514657497406006
Validation Loss Energy: 1.798869213117566, Validation Loss Force: 2.410023576993869, time: 0.1547684669494629
Test Loss Energy: 9.378664449772243, Test Loss Force: 11.039254346578714, time: 10.075076580047607

Epoch 4, Batch 100/172, Loss: 0.30181992053985596

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.6042008840336712, Training Loss Force: 2.236880148217741, time: 2.461947441101074
Validation Loss Energy: 2.40459705568456, Validation Loss Force: 2.5453614385504864, time: 0.1531367301940918
Test Loss Energy: 9.599632967675753, Test Loss Force: 11.059496053812488, time: 10.260756492614746

Epoch 5, Batch 100/172, Loss: 0.14770492911338806

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.9597224277647647, Training Loss Force: 2.25148566738235, time: 2.43050479888916
Validation Loss Energy: 1.363724863208662, Validation Loss Force: 2.3236209714073146, time: 0.1537165641784668
Test Loss Energy: 9.911111363847901, Test Loss Force: 11.029234556774709, time: 10.041073560714722

Epoch 6, Batch 100/172, Loss: 0.11373570561408997

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.007475973573133, Training Loss Force: 6.0294904343507, time: 2.486171245574951
Validation Loss Energy: 17.561446449657208, Validation Loss Force: 6.647631483271957, time: 0.15984582901000977
Test Loss Energy: 13.83802405069257, Test Loss Force: 13.511303459511085, time: 10.225979328155518

Epoch 7, Batch 100/172, Loss: 0.30744481086730957

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.145271568709402, Training Loss Force: 5.274349272806716, time: 2.562135934829712
Validation Loss Energy: 8.404400195630664, Validation Loss Force: 4.213706451708102, time: 0.16327381134033203
Test Loss Energy: 14.913883501894599, Test Loss Force: 11.882458413975755, time: 10.081901550292969

Epoch 8, Batch 100/172, Loss: 1.3558157682418823

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.857492454909563, Training Loss Force: 4.877695234852573, time: 2.511040210723877
Validation Loss Energy: 3.9220073469590457, Validation Loss Force: 4.256521868871485, time: 0.16238975524902344
Test Loss Energy: 8.969825911777706, Test Loss Force: 11.730600541953628, time: 10.048169612884521

Epoch 9, Batch 100/172, Loss: 0.0794634222984314

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.62230661595003, Training Loss Force: 4.680355707579817, time: 2.5448520183563232
Validation Loss Energy: 10.325458954130006, Validation Loss Force: 4.2053303806687845, time: 0.15430569648742676
Test Loss Energy: 10.594847660460646, Test Loss Force: 11.11774771068211, time: 10.271956205368042

Epoch 10, Batch 100/172, Loss: 0.35580047965049744

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.944468623419795, Training Loss Force: 4.980779580661536, time: 2.4810962677001953
Validation Loss Energy: 8.923670284885961, Validation Loss Force: 4.592555588143543, time: 0.15767669677734375
Test Loss Energy: 14.573336327221275, Test Loss Force: 12.148264201986182, time: 10.16413164138794

Epoch 11, Batch 100/172, Loss: 0.8949460983276367

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.353607540438368, Training Loss Force: 4.763984735496459, time: 2.487989664077759
Validation Loss Energy: 8.273249256247976, Validation Loss Force: 3.4333264316255776, time: 0.15714550018310547
Test Loss Energy: 9.791413902013572, Test Loss Force: 11.253563135768967, time: 10.213579416275024

Epoch 12, Batch 100/172, Loss: 0.2970806360244751

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.575229771128111, Training Loss Force: 5.270706705923766, time: 2.445037603378296
Validation Loss Energy: 6.005646646342687, Validation Loss Force: 7.567446402973121, time: 0.15209031105041504
Test Loss Energy: 9.782736580992943, Test Loss Force: 13.130952060229188, time: 10.138707399368286

Epoch 13, Batch 100/172, Loss: 1.684943675994873

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 12.898878399683163, Training Loss Force: 6.194605526207344, time: 2.4780094623565674
Validation Loss Energy: 2.660093571565995, Validation Loss Force: 6.298033353414368, time: 0.15332293510437012
Test Loss Energy: 10.910224557366156, Test Loss Force: 12.51847897894507, time: 10.090345621109009

Epoch 14, Batch 100/172, Loss: 0.36113882064819336

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.854211420205236, Training Loss Force: 3.97314346476746, time: 2.6717517375946045
Validation Loss Energy: 7.415332054177914, Validation Loss Force: 3.241003230689218, time: 0.15396714210510254
Test Loss Energy: 13.32821841516853, Test Loss Force: 11.239273447085884, time: 10.072383165359497

Epoch 15, Batch 100/172, Loss: 0.37982702255249023

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 6.0043188091641175, Training Loss Force: 2.988583314978246, time: 2.408132791519165
Validation Loss Energy: 7.580445085481898, Validation Loss Force: 3.098384681845151, time: 0.15413236618041992
Test Loss Energy: 11.77673528535514, Test Loss Force: 11.257285478394337, time: 10.856790542602539

Epoch 16, Batch 100/172, Loss: 0.5803351402282715

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.120519712745217, Training Loss Force: 2.9044059163899965, time: 2.4696812629699707
Validation Loss Energy: 3.9595144801296094, Validation Loss Force: 3.1536842673441225, time: 0.17209458351135254
Test Loss Energy: 10.95342948309913, Test Loss Force: 11.354772632562849, time: 10.186991214752197

Epoch 17, Batch 100/172, Loss: 0.5717101097106934

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.165109578822147, Training Loss Force: 2.937693678536964, time: 2.467304229736328
Validation Loss Energy: 1.3613486494178935, Validation Loss Force: 4.278842224059288, time: 0.1520535945892334
Test Loss Energy: 9.329510979593579, Test Loss Force: 11.817680448014539, time: 10.08335256576538

Epoch 18, Batch 100/172, Loss: 0.19771167635917664

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.048000024257469, Training Loss Force: 5.035925541710991, time: 2.459717035293579
Validation Loss Energy: 20.336311344008394, Validation Loss Force: 4.680542113588129, time: 0.155348539352417
Test Loss Energy: 23.077282953924595, Test Loss Force: 12.518870718092149, time: 10.200080156326294

Epoch 19, Batch 100/172, Loss: 0.32308369874954224

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.090237581338299, Training Loss Force: 3.552403478773109, time: 2.465210199356079
Validation Loss Energy: 9.115435031604335, Validation Loss Force: 3.12148215235224, time: 0.15289783477783203
Test Loss Energy: 14.87877819315831, Test Loss Force: 11.522904569401067, time: 10.029073238372803

wandb: - 0.039 MB of 0.057 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.057 MB of 0.057 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–â–â–â–ƒâ–„â–â–‚â–„â–â–â–‚â–ƒâ–‚â–‚â–â–ˆâ–„
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–ƒâ–ƒâ–â–„â–‚â–‡â–…â–‚â–‚â–‚â–ƒâ–…â–‚
wandb:          test_loss â–â–â–â–â–â–â–‡â–„â–‚â–‚â–…â–â–…â–„â–ƒâ–‚â–‚â–‚â–ˆâ–„
wandb: train_error_energy â–â–â–â–â–‚â–‚â–‡â–†â–†â–‡â–†â–†â–‡â–ˆâ–„â–„â–„â–„â–†â–„
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–†â–…â–†â–†â–†â–ˆâ–„â–‚â–‚â–‚â–†â–ƒ
wandb:         train_loss â–â–â–â–â–â–â–‡â–†â–†â–†â–†â–†â–†â–ˆâ–„â–ƒâ–ƒâ–ƒâ–†â–„
wandb: valid_error_energy â–â–â–â–â–â–â–‡â–„â–‚â–„â–„â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–â–ˆâ–„
wandb:  valid_error_force â–â–â–â–â–â–â–‡â–„â–„â–„â–„â–‚â–ˆâ–†â–‚â–‚â–‚â–„â–„â–‚
wandb:         valid_loss â–â–â–â–â–â–â–ˆâ–„â–ƒâ–„â–„â–ƒâ–‡â–…â–ƒâ–ƒâ–‚â–ƒâ–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5491
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 14.87878
wandb:   test_error_force 11.5229
wandb:          test_loss 4.8513
wandb: train_error_energy 7.09024
wandb:  train_error_force 3.5524
wandb:         train_loss 1.66313
wandb: valid_error_energy 9.11544
wandb:  valid_error_force 3.12148
wandb:         valid_loss 1.65447
wandb: 
wandb: ğŸš€ View run al_77_52 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/ovrzfekc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_193259-ovrzfekc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7756859064102173, Uncertainty Bias: -0.6450514793395996
4.196167e-05 0.000641346
2.3491354 35.201157
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 5 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 4 steps.
Found uncertainty sample 5 after 13 steps.
Found uncertainty sample 6 after 10 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 17 steps.
Found uncertainty sample 9 after 9 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 4 steps.
Found uncertainty sample 13 after 6 steps.
Found uncertainty sample 14 after 13 steps.
Found uncertainty sample 15 after 3 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 3 steps.
Found uncertainty sample 18 after 10 steps.
Found uncertainty sample 19 after 5 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 6 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 3 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 9 steps.
Found uncertainty sample 28 after 4 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 11 steps.
Found uncertainty sample 32 after 2 steps.
Found uncertainty sample 33 after 23 steps.
Found uncertainty sample 34 after 3 steps.
Found uncertainty sample 35 after 2 steps.
Found uncertainty sample 36 after 13 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 5 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 3 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 13 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 5 steps.
Found uncertainty sample 50 after 12 steps.
Found uncertainty sample 51 after 13 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 4 steps.
Found uncertainty sample 55 after 5 steps.
Found uncertainty sample 56 after 5 steps.
Found uncertainty sample 57 after 14 steps.
Found uncertainty sample 58 after 4 steps.
Found uncertainty sample 59 after 6 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 3 steps.
Found uncertainty sample 62 after 9 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 5 steps.
Found uncertainty sample 65 after 7 steps.
Found uncertainty sample 66 after 5 steps.
Found uncertainty sample 67 after 4 steps.
Found uncertainty sample 68 after 10 steps.
Found uncertainty sample 69 after 2 steps.
Found uncertainty sample 70 after 6 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 5 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 19 steps.
Found uncertainty sample 75 after 18 steps.
Found uncertainty sample 76 after 5 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 5 steps.
Found uncertainty sample 79 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 8 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 5 steps.
Found uncertainty sample 85 after 8 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 9 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 5 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 8 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 3 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_194211-2li6e5i3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_53
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/2li6e5i3
Training model 53. Added 100 samples to the dataset.
Epoch 0, Batch 100/175, Loss: 0.06714048236608505

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.6559900762685467, Training Loss Force: 2.2819600827381414, time: 2.423790454864502
Validation Loss Energy: 0.9723599579746741, Validation Loss Force: 2.381431299972719, time: 0.17917180061340332
Test Loss Energy: 9.55899965840136, Test Loss Force: 11.033814905407109, time: 10.11695671081543

Epoch 1, Batch 100/175, Loss: 0.08289459347724915

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.218636358072013, Training Loss Force: 2.2022542864941657, time: 2.4675257205963135
Validation Loss Energy: 1.1496203148637434, Validation Loss Force: 2.3856090184886165, time: 0.15808606147766113
Test Loss Energy: 10.053558682838966, Test Loss Force: 11.03830131797091, time: 10.113848209381104

Epoch 2, Batch 100/175, Loss: 0.21313446760177612

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.470945163560898, Training Loss Force: 2.224742367534236, time: 2.6819770336151123
Validation Loss Energy: 2.0515359010464542, Validation Loss Force: 2.5248992573854525, time: 0.1553807258605957
Test Loss Energy: 10.35253414817692, Test Loss Force: 11.078956064105283, time: 10.043754577636719

Epoch 3, Batch 100/175, Loss: 0.04851759225130081

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.960170262734695, Training Loss Force: 2.207656255243115, time: 2.445678949356079
Validation Loss Energy: 4.109942035921364, Validation Loss Force: 2.5488342607734245, time: 0.16185259819030762
Test Loss Energy: 11.511507434530072, Test Loss Force: 11.19135574170322, time: 10.050236701965332

Epoch 4, Batch 100/175, Loss: 0.0685645192861557

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.3150935567089266, Training Loss Force: 2.1868465282579317, time: 2.464705467224121
Validation Loss Energy: 1.774863660721859, Validation Loss Force: 2.3521558740118635, time: 0.18683242797851562
Test Loss Energy: 10.067602013176938, Test Loss Force: 11.05307320310145, time: 11.072832584381104

Epoch 5, Batch 100/175, Loss: 0.09250526875257492

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8481263581205218, Training Loss Force: 2.1646596485905705, time: 2.4839091300964355
Validation Loss Energy: 1.784992768835541, Validation Loss Force: 2.3615447185499088, time: 0.16152572631835938
Test Loss Energy: 9.664059326231673, Test Loss Force: 11.03283353369839, time: 10.065726518630981

Epoch 6, Batch 100/175, Loss: 0.3570113182067871

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.111665565855553, Training Loss Force: 3.590272485004378, time: 2.4695167541503906
Validation Loss Energy: 6.790364187164836, Validation Loss Force: 3.7686521372366326, time: 0.1618974208831787
Test Loss Energy: 10.21111227306417, Test Loss Force: 11.243118801007594, time: 10.190424919128418

Epoch 7, Batch 100/175, Loss: 0.13857394456863403

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 6.36319085551734, Training Loss Force: 3.14985781894889, time: 2.5568904876708984
Validation Loss Energy: 8.933574608496832, Validation Loss Force: 3.587013269929326, time: 0.1539769172668457
Test Loss Energy: 10.318430743343056, Test Loss Force: 11.15169995697429, time: 10.109279155731201

Epoch 8, Batch 100/175, Loss: 0.2591182589530945

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.468829288978808, Training Loss Force: 3.064964592497078, time: 2.5142970085144043
Validation Loss Energy: 5.866182131727638, Validation Loss Force: 3.266184810179143, time: 0.1553819179534912
Test Loss Energy: 14.492869153859953, Test Loss Force: 11.257727216674034, time: 10.050225496292114

Epoch 9, Batch 100/175, Loss: 0.7090733051300049

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.58289838665253, Training Loss Force: 3.047670790967772, time: 2.478299856185913
Validation Loss Energy: 7.196060839416984, Validation Loss Force: 3.1526356441129, time: 0.17226791381835938
Test Loss Energy: 14.13907239090805, Test Loss Force: 11.598561221849907, time: 10.231094121932983

Epoch 10, Batch 100/175, Loss: 3.49582576751709

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.743965263653013, Training Loss Force: 5.09291318781486, time: 2.491420269012451
Validation Loss Energy: 7.596797313774446, Validation Loss Force: 7.752932918899011, time: 0.15446805953979492
Test Loss Energy: 13.839400295237702, Test Loss Force: 14.169482477048781, time: 10.05210018157959

Epoch 11, Batch 100/175, Loss: 0.12372034788131714

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.47406041081704, Training Loss Force: 5.790516529717849, time: 2.459807872772217
Validation Loss Energy: 6.351742190534831, Validation Loss Force: 4.522801509259717, time: 0.15791845321655273
Test Loss Energy: 9.329266304673835, Test Loss Force: 11.223189795866352, time: 10.209012508392334

Epoch 12, Batch 100/175, Loss: 2.1593475341796875

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 12.899280383870641, Training Loss Force: 5.10990316429272, time: 2.449956178665161
Validation Loss Energy: 10.172480500602504, Validation Loss Force: 4.191119338883479, time: 0.1574852466583252
Test Loss Energy: 16.77472866373654, Test Loss Force: 11.368992526709667, time: 10.150695323944092

Epoch 13, Batch 100/175, Loss: 0.8816031217575073

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.071188944268366, Training Loss Force: 4.799922846743417, time: 2.5593693256378174
Validation Loss Energy: 7.690511928774977, Validation Loss Force: 4.879098566149495, time: 0.15828633308410645
Test Loss Energy: 14.074556712879437, Test Loss Force: 11.635728950506135, time: 10.147178411483765

Epoch 14, Batch 100/175, Loss: 0.16945655643939972

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.535946917883395, Training Loss Force: 5.154506327560739, time: 2.637458562850952
Validation Loss Energy: 33.859863625723314, Validation Loss Force: 7.486915434318585, time: 0.15715646743774414
Test Loss Energy: 27.67130696930807, Test Loss Force: 12.219303535729551, time: 10.007508754730225

Epoch 15, Batch 100/175, Loss: 0.3697627782821655

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.446717427394207, Training Loss Force: 5.256336406721773, time: 2.5232160091400146
Validation Loss Energy: 13.093650008839447, Validation Loss Force: 3.88435109968376, time: 0.1581554412841797
Test Loss Energy: 11.41155516270831, Test Loss Force: 11.213899353603699, time: 10.083290576934814

Epoch 16, Batch 100/175, Loss: 1.7627023458480835

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.24729163712129, Training Loss Force: 5.381795126639334, time: 2.483469009399414
Validation Loss Energy: 14.190889633815138, Validation Loss Force: 3.9026109345494704, time: 0.16298460960388184
Test Loss Energy: 18.091598329555687, Test Loss Force: 11.650612174872858, time: 10.186697959899902

Epoch 17, Batch 100/175, Loss: 0.5874576568603516

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.806689549809477, Training Loss Force: 4.239908687608652, time: 2.520045042037964
Validation Loss Energy: 9.89206344769322, Validation Loss Force: 5.831497997989149, time: 0.16351532936096191
Test Loss Energy: 14.835368430463992, Test Loss Force: 12.286182547129778, time: 10.099611520767212

Epoch 18, Batch 100/175, Loss: 0.45708364248275757

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.05314152083874, Training Loss Force: 4.594767767936481, time: 2.500044822692871
Validation Loss Energy: 4.748495244419691, Validation Loss Force: 3.688823467068353, time: 0.1519322395324707
Test Loss Energy: 10.472252529252899, Test Loss Force: 11.602419264415516, time: 10.195714712142944

Epoch 19, Batch 100/175, Loss: 0.556185245513916

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.792773272647321, Training Loss Force: 3.2904850079317054, time: 2.519273281097412
Validation Loss Energy: 6.204757099739122, Validation Loss Force: 4.106010886373057, time: 0.16217517852783203
Test Loss Energy: 12.29723510208035, Test Loss Force: 12.16001034684437, time: 10.064575433731079

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.054 MB uploadedwandb: / 0.039 MB of 0.054 MB uploadedwandb: - 0.057 MB of 0.057 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–â–â–â–â–ƒâ–ƒâ–ƒâ–â–„â–ƒâ–ˆâ–‚â–„â–ƒâ–â–‚
wandb:   test_error_force â–â–â–â–â–â–â–â–â–‚â–‚â–ˆâ–â–‚â–‚â–„â–â–‚â–„â–‚â–„
wandb:          test_loss â–â–â–â–‚â–â–â–â–â–ƒâ–ƒâ–‡â–â–„â–ƒâ–ˆâ–‚â–„â–„â–‚â–ƒ
wandb: train_error_energy â–â–â–‚â–â–â–â–„â–„â–„â–„â–ˆâ–†â–ˆâ–‡â–‡â–‡â–†â–†â–†â–„
wandb:  train_error_force â–â–â–â–â–â–â–„â–ƒâ–ƒâ–ƒâ–‡â–ˆâ–‡â–†â–‡â–‡â–‡â–…â–†â–ƒ
wandb:         train_loss â–â–â–â–â–â–â–„â–„â–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–†â–†â–„
wandb: valid_error_energy â–â–â–â–‚â–â–â–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–ˆâ–„â–„â–ƒâ–‚â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–‚â–ˆâ–„â–ƒâ–„â–ˆâ–ƒâ–ƒâ–†â–ƒâ–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–‚â–…â–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–ƒâ–„â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5581
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.29724
wandb:   test_error_force 12.16001
wandb:          test_loss 4.89172
wandb: train_error_energy 6.79277
wandb:  train_error_force 3.29049
wandb:         train_loss 1.55558
wandb: valid_error_energy 6.20476
wandb:  valid_error_force 4.10601
wandb:         valid_loss 1.78911
wandb: 
wandb: ğŸš€ View run al_77_53 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/2li6e5i3
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_194211-2li6e5i3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5581870079040527, Uncertainty Bias: -0.4415009617805481
6.866455e-05 0.006948471
-4.640023 25.90856
(48745, 22, 3)
Found uncertainty sample 0 after 7 steps.
Found uncertainty sample 1 after 59 steps.
Found uncertainty sample 2 after 19 steps.
Found uncertainty sample 3 after 12 steps.
Found uncertainty sample 4 after 7 steps.
Found uncertainty sample 5 after 53 steps.
Found uncertainty sample 6 after 11 steps.
Found uncertainty sample 7 after 6 steps.
Found uncertainty sample 8 after 59 steps.
Found uncertainty sample 9 after 5 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 51 steps.
Found uncertainty sample 12 after 19 steps.
Found uncertainty sample 13 after 27 steps.
Found uncertainty sample 14 after 7 steps.
Found uncertainty sample 15 after 19 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 93 steps.
Found uncertainty sample 18 after 57 steps.
Found uncertainty sample 19 after 3 steps.
Found uncertainty sample 20 after 5 steps.
Found uncertainty sample 21 after 7 steps.
Found uncertainty sample 22 after 12 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 15 steps.
Found uncertainty sample 25 after 8 steps.
Found uncertainty sample 26 after 33 steps.
Found uncertainty sample 27 after 35 steps.
Found uncertainty sample 28 after 6 steps.
Found uncertainty sample 29 after 22 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 22 steps.
Found uncertainty sample 32 after 46 steps.
Found uncertainty sample 33 after 26 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 29 steps.
Found uncertainty sample 37 after 15 steps.
Found uncertainty sample 38 after 5 steps.
Found uncertainty sample 39 after 7 steps.
Found uncertainty sample 40 after 66 steps.
Found uncertainty sample 41 after 100 steps.
Found uncertainty sample 42 after 15 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 34 steps.
Found uncertainty sample 45 after 33 steps.
Found uncertainty sample 46 after 11 steps.
Found uncertainty sample 47 after 7 steps.
Found uncertainty sample 48 after 13 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 57 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 18 steps.
Found uncertainty sample 53 after 4 steps.
Found uncertainty sample 54 after 37 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 9 steps.
Found uncertainty sample 57 after 8 steps.
Found uncertainty sample 58 after 26 steps.
Found uncertainty sample 59 after 35 steps.
Found uncertainty sample 60 after 117 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 11 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 100 steps.
Found uncertainty sample 65 after 24 steps.
Found uncertainty sample 66 after 26 steps.
Found uncertainty sample 67 after 7 steps.
Found uncertainty sample 68 after 13 steps.
Found uncertainty sample 69 after 21 steps.
Found uncertainty sample 70 after 3 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 27 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 124 steps.
Found uncertainty sample 80 after 5 steps.
Found uncertainty sample 81 after 39 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 25 steps.
Found uncertainty sample 84 after 6 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 9 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 21 steps.
Found uncertainty sample 89 after 79 steps.
Found uncertainty sample 90 after 17 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 12 steps.
Found uncertainty sample 93 after 43 steps.
Found uncertainty sample 94 after 18 steps.
Found uncertainty sample 95 after 52 steps.
Found uncertainty sample 96 after 70 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 9 steps.
Found uncertainty sample 99 after 72 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_195157-ofo4pnlr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_54
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/ofo4pnlr
Training model 54. Added 100 samples to the dataset.
Epoch 0, Batch 100/178, Loss: 0.24792800843715668

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.452504480884623, Training Loss Force: 2.2815685967897985, time: 2.613126516342163
Validation Loss Energy: 1.9970947051050338, Validation Loss Force: 2.3954513658538676, time: 0.15607953071594238
Test Loss Energy: 10.373556111414167, Test Loss Force: 10.97010774720801, time: 10.001006841659546

Epoch 1, Batch 100/178, Loss: 0.06548736989498138

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.2399071892663702, Training Loss Force: 2.231801552302437, time: 2.547856569290161
Validation Loss Energy: 2.7484884722242136, Validation Loss Force: 2.4464751841896852, time: 0.1568892002105713
Test Loss Energy: 10.998396195094056, Test Loss Force: 11.052900491119088, time: 10.008321523666382

Epoch 2, Batch 100/178, Loss: 0.0416613444685936

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.113088075359519, Training Loss Force: 2.2532320290127923, time: 2.735752820968628
Validation Loss Energy: 1.5456340508591249, Validation Loss Force: 2.3914552711379993, time: 0.16464686393737793
Test Loss Energy: 10.126083902948608, Test Loss Force: 11.032012222633961, time: 10.050521612167358

Epoch 3, Batch 100/178, Loss: 0.24548186361789703

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.171162131968712, Training Loss Force: 2.173468852345272, time: 2.5251662731170654
Validation Loss Energy: 2.205762712029077, Validation Loss Force: 2.340305867403878, time: 0.16042518615722656
Test Loss Energy: 9.510674144387291, Test Loss Force: 10.918117826515568, time: 10.042684316635132

Epoch 4, Batch 100/178, Loss: 0.1374267041683197

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5729279209477895, Training Loss Force: 2.109595908858774, time: 2.601846694946289
Validation Loss Energy: 1.5546664392060643, Validation Loss Force: 2.3127487050190987, time: 0.15456175804138184
Test Loss Energy: 10.349081158274858, Test Loss Force: 11.045680428906886, time: 10.143731355667114

Epoch 5, Batch 100/178, Loss: 0.044515304267406464

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2899130186994603, Training Loss Force: 2.129649723566828, time: 2.5847346782684326
Validation Loss Energy: 0.9109468426281578, Validation Loss Force: 2.390912329537178, time: 0.15957331657409668
Test Loss Energy: 10.283425288311934, Test Loss Force: 11.049808243886343, time: 10.004099369049072

Epoch 6, Batch 100/178, Loss: 0.4659550189971924

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.35297049121538, Training Loss Force: 5.507907576005849, time: 2.5124335289001465
Validation Loss Energy: 8.024258242133966, Validation Loss Force: 8.117682199676013, time: 0.15551042556762695
Test Loss Energy: 11.170957970211715, Test Loss Force: 13.272877563986134, time: 10.112020254135132

Epoch 7, Batch 100/178, Loss: 0.29873529076576233

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.45960075863372, Training Loss Force: 4.322388191169556, time: 2.6136839389801025
Validation Loss Energy: 7.333995320107316, Validation Loss Force: 3.0791418369232173, time: 0.1570606231689453
Test Loss Energy: 12.132459994715756, Test Loss Force: 11.73800641049294, time: 10.127995729446411

Epoch 8, Batch 100/178, Loss: 0.512444257736206

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.225274342313729, Training Loss Force: 3.1787349090365735, time: 2.591120958328247
Validation Loss Energy: 3.895602942742734, Validation Loss Force: 3.6672485339580696, time: 0.15912580490112305
Test Loss Energy: 9.744530855048122, Test Loss Force: 11.610550945769216, time: 9.986241579055786

Epoch 9, Batch 100/178, Loss: 0.5400136709213257

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.69887516791906, Training Loss Force: 4.513317810584391, time: 2.5652785301208496
Validation Loss Energy: 30.88514515681268, Validation Loss Force: 5.787490410218467, time: 0.15880894660949707
Test Loss Energy: 32.805975989153524, Test Loss Force: 13.441481214334827, time: 10.186161756515503

Epoch 10, Batch 100/178, Loss: 0.2921765148639679

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.317036448474822, Training Loss Force: 4.44103697897273, time: 2.536595106124878
Validation Loss Energy: 22.75242239855973, Validation Loss Force: 6.5129782551160185, time: 0.15618276596069336
Test Loss Energy: 26.70930299025875, Test Loss Force: 13.67709363973451, time: 9.944828271865845

Epoch 11, Batch 100/178, Loss: 0.28409212827682495

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.376841209969765, Training Loss Force: 4.658557309961672, time: 2.5451180934906006
Validation Loss Energy: 6.110614217747922, Validation Loss Force: 3.305761207722813, time: 0.16162490844726562
Test Loss Energy: 9.576285736998582, Test Loss Force: 11.189298903667424, time: 10.127884149551392

Epoch 12, Batch 100/178, Loss: 0.21395984292030334

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.65293511143641, Training Loss Force: 3.2887266544497953, time: 2.5108985900878906
Validation Loss Energy: 2.440814860277152, Validation Loss Force: 6.787611699628974, time: 0.15407514572143555
Test Loss Energy: 9.46150974360176, Test Loss Force: 13.65924855665837, time: 9.928459405899048

Epoch 13, Batch 100/178, Loss: 0.9372844696044922

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 12.018073053621334, Training Loss Force: 5.36618233991467, time: 2.5516297817230225
Validation Loss Energy: 14.385003094684755, Validation Loss Force: 4.280618826951597, time: 0.15540862083435059
Test Loss Energy: 18.398403965902457, Test Loss Force: 12.05919918300222, time: 9.957515954971313

Epoch 14, Batch 100/178, Loss: 3.046814441680908

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 13.199413968433367, Training Loss Force: 5.489018787535443, time: 2.701524257659912
Validation Loss Energy: 2.1052718295796713, Validation Loss Force: 5.055111697484936, time: 0.16867899894714355
Test Loss Energy: 10.458656756903402, Test Loss Force: 11.80472373378224, time: 10.093328475952148

Epoch 15, Batch 100/178, Loss: 1.191699743270874

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.674874772389535, Training Loss Force: 4.648191655910851, time: 2.5601820945739746
Validation Loss Energy: 6.285506135710567, Validation Loss Force: 5.148610471860014, time: 0.15751028060913086
Test Loss Energy: 11.838507241162477, Test Loss Force: 12.5141199098166, time: 10.004021883010864

Epoch 16, Batch 100/178, Loss: 1.3968305587768555

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.569771087780998, Training Loss Force: 5.5294459509288085, time: 2.549208402633667
Validation Loss Energy: 2.7179024156704266, Validation Loss Force: 6.343841889237314, time: 0.1543440818786621
Test Loss Energy: 11.796839767773061, Test Loss Force: 12.3440149145002, time: 11.007206201553345

Epoch 17, Batch 100/178, Loss: 0.8088595271110535

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.988262654452665, Training Loss Force: 4.389001030960584, time: 2.4771406650543213
Validation Loss Energy: 11.886030319305405, Validation Loss Force: 4.486385454904201, time: 0.15890836715698242
Test Loss Energy: 10.6663990259854, Test Loss Force: 11.356478669931828, time: 9.941205263137817

Epoch 18, Batch 100/178, Loss: 0.3477804958820343

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.907123856525104, Training Loss Force: 4.931593833907547, time: 2.5882675647735596
Validation Loss Energy: 1.5736717703723413, Validation Loss Force: 4.895661884792382, time: 0.15700769424438477
Test Loss Energy: 9.334287267747941, Test Loss Force: 11.809951044703615, time: 10.13040280342102

Epoch 19, Batch 100/178, Loss: 0.5007435083389282

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.785020150805149, Training Loss Force: 3.6892876246558903, time: 2.4915201663970947
Validation Loss Energy: 4.619961855070432, Validation Loss Force: 5.243508053809811, time: 0.15423059463500977
Test Loss Energy: 11.519781041233642, Test Loss Force: 12.478542225391426, time: 10.044862508773804

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–‚â–â–ˆâ–†â–â–â–„â–â–‚â–‚â–â–â–‚
wandb:   test_error_force â–â–â–â–â–â–â–‡â–ƒâ–ƒâ–‡â–ˆâ–‚â–ˆâ–„â–ƒâ–…â–…â–‚â–ƒâ–…
wandb:          test_loss â–â–â–â–â–â–â–„â–‚â–‚â–ˆâ–‡â–â–„â–„â–‚â–ƒâ–ƒâ–‚â–‚â–ƒ
wandb: train_error_energy â–‚â–‚â–â–‚â–â–â–‡â–…â–„â–‡â–…â–†â–„â–‡â–ˆâ–†â–†â–†â–‡â–„
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–ƒâ–†â–†â–†â–ƒâ–ˆâ–ˆâ–†â–ˆâ–†â–‡â–„
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–…â–ƒâ–†â–†â–†â–„â–ˆâ–ˆâ–†â–‡â–†â–‡â–„
wandb: valid_error_energy â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–ˆâ–†â–‚â–â–„â–â–‚â–â–„â–â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–‚â–ƒâ–…â–†â–‚â–†â–ƒâ–„â–„â–†â–„â–„â–…
wandb:         valid_loss â–â–â–â–â–â–â–†â–‚â–‚â–ˆâ–‡â–‚â–…â–„â–ƒâ–„â–„â–„â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 5671
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.51978
wandb:   test_error_force 12.47854
wandb:          test_loss 4.94627
wandb: train_error_energy 6.78502
wandb:  train_error_force 3.68929
wandb:         train_loss 1.68851
wandb: valid_error_energy 4.61996
wandb:  valid_error_force 5.24351
wandb:         valid_loss 2.06367
wandb: 
wandb: ğŸš€ View run al_77_54 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/ofo4pnlr
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_195157-ofo4pnlr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.881524384021759, Uncertainty Bias: -0.648036539554596
9.1552734e-05 0.0013771057
-0.12179824 42.954136
(48745, 22, 3)
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 5 steps.
Found uncertainty sample 7 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 4 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 5 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 4 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 3 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 7 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 6 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 4 steps.
Found uncertainty sample 69 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 5 steps.
Found uncertainty sample 73 after 4 steps.
Found uncertainty sample 74 after 3 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 7 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 3 steps.
Found uncertainty sample 86 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 6 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_200100-45uulo25
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_55
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/45uulo25
Training model 55. Added 100 samples to the dataset.
Epoch 0, Batch 100/181, Loss: 0.20738524198532104

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 3.1371912839588925, Training Loss Force: 2.3231796309082835, time: 2.6749660968780518
Validation Loss Energy: 0.9020234693839125, Validation Loss Force: 2.4022525722470864, time: 0.16470098495483398
Test Loss Energy: 9.960745379691232, Test Loss Force: 10.982592267617322, time: 9.921356916427612

Epoch 1, Batch 100/181, Loss: 0.14384397864341736

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5548819098439717, Training Loss Force: 2.1733876064849182, time: 2.6360549926757812
Validation Loss Energy: 1.5048974728421225, Validation Loss Force: 2.3848923949539724, time: 0.16598176956176758
Test Loss Energy: 9.468984828437048, Test Loss Force: 11.076055690059517, time: 9.93681287765503

Epoch 2, Batch 100/181, Loss: 0.06941685080528259

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.4178857210532732, Training Loss Force: 2.165026843846866, time: 2.8371269702911377
Validation Loss Energy: 0.9260388192220406, Validation Loss Force: 2.4075018489162363, time: 0.15376067161560059
Test Loss Energy: 9.965919184732392, Test Loss Force: 11.114851127840861, time: 10.010379552841187

Epoch 3, Batch 100/181, Loss: 0.36285996437072754

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.9384405773471092, Training Loss Force: 2.1913202796845916, time: 2.5721259117126465
Validation Loss Energy: 4.477196047697858, Validation Loss Force: 2.4275224771648958, time: 0.16822457313537598
Test Loss Energy: 9.694849162359809, Test Loss Force: 11.029692695627936, time: 9.865540266036987

Epoch 4, Batch 100/181, Loss: 0.17275884747505188

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.9300935011726796, Training Loss Force: 2.1619846703133048, time: 2.6085574626922607
Validation Loss Energy: 1.4988960940210767, Validation Loss Force: 2.358706503392864, time: 0.15892863273620605
Test Loss Energy: 9.788206113976036, Test Loss Force: 11.037301573143136, time: 10.034454822540283

Epoch 5, Batch 100/181, Loss: 0.03982933238148689

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6696665240908444, Training Loss Force: 2.1267751571314757, time: 2.6117091178894043
Validation Loss Energy: 1.5475840387930633, Validation Loss Force: 2.3578565752628577, time: 0.15795612335205078
Test Loss Energy: 10.515130693260357, Test Loss Force: 11.0517275959655, time: 9.96962833404541

Epoch 6, Batch 100/181, Loss: 0.35869306325912476

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 14.680295660765482, Training Loss Force: 6.2324536005200235, time: 2.5837955474853516
Validation Loss Energy: 6.830962561865266, Validation Loss Force: 6.077914066099648, time: 0.1580812931060791
Test Loss Energy: 10.778545872677501, Test Loss Force: 12.209867330280234, time: 10.92593264579773

Epoch 7, Batch 100/181, Loss: 0.4133419692516327

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.947522344487064, Training Loss Force: 5.294912179974927, time: 2.5674922466278076
Validation Loss Energy: 6.911222319500062, Validation Loss Force: 5.230278389173658, time: 0.1577157974243164
Test Loss Energy: 10.072618956548347, Test Loss Force: 12.015238166114237, time: 9.877892017364502

Epoch 8, Batch 100/181, Loss: 0.6757367253303528

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.406928885779799, Training Loss Force: 3.9245305262818118, time: 2.5381360054016113
Validation Loss Energy: 12.853505788339247, Validation Loss Force: 3.6210978298808816, time: 0.15526509284973145
Test Loss Energy: 17.48223007223074, Test Loss Force: 11.85860873445113, time: 9.82829475402832

Epoch 9, Batch 100/181, Loss: 0.5509358644485474

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.066669064803284, Training Loss Force: 5.450433929321738, time: 2.598128080368042
Validation Loss Energy: 21.580659566932734, Validation Loss Force: 5.02734544877213, time: 0.1568436622619629
Test Loss Energy: 27.08376980126955, Test Loss Force: 12.712146538826637, time: 10.040553569793701

Epoch 10, Batch 100/181, Loss: 1.4928640127182007

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.659066265616941, Training Loss Force: 5.9409023534148, time: 2.6189537048339844
Validation Loss Energy: 2.155253704758645, Validation Loss Force: 7.674413543308788, time: 0.16009950637817383
Test Loss Energy: 10.517765445165361, Test Loss Force: 13.857166555767792, time: 9.994386196136475

Epoch 11, Batch 100/181, Loss: 2.1215550899505615

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.090536045155048, Training Loss Force: 5.37905095100233, time: 2.5839593410491943
Validation Loss Energy: 7.571562664804742, Validation Loss Force: 3.960261609337246, time: 0.15852141380310059
Test Loss Energy: 9.803325293025795, Test Loss Force: 11.144537289774615, time: 9.994834899902344

Epoch 12, Batch 100/181, Loss: 0.31675535440444946

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.257771550629133, Training Loss Force: 4.546555205833389, time: 2.656991958618164
Validation Loss Energy: 21.837135827056255, Validation Loss Force: 6.3434146613693265, time: 0.15632104873657227
Test Loss Energy: 24.14957189721103, Test Loss Force: 13.197598059443367, time: 9.856600999832153

Epoch 13, Batch 100/181, Loss: 0.4864935278892517

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.651674373894501, Training Loss Force: 5.5531381832687305, time: 2.5931904315948486
Validation Loss Energy: 9.456100417557787, Validation Loss Force: 4.014474124065797, time: 0.15985488891601562
Test Loss Energy: 9.971626747535064, Test Loss Force: 11.192571212130153, time: 9.846031665802002

Epoch 14, Batch 100/181, Loss: 0.5651407837867737

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.35583426035578, Training Loss Force: 4.153599772737161, time: 2.745288133621216
Validation Loss Energy: 22.90201212457841, Validation Loss Force: 6.246127097253239, time: 0.15455842018127441
Test Loss Energy: 24.942480451193504, Test Loss Force: 13.294150646724475, time: 9.890775918960571

Epoch 15, Batch 100/181, Loss: 0.20330029726028442

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.13002469369972, Training Loss Force: 5.116478618210294, time: 2.5750644207000732
Validation Loss Energy: 4.85348775476927, Validation Loss Force: 4.70449092246315, time: 0.1579444408416748
Test Loss Energy: 9.21239064149032, Test Loss Force: 11.440070625868955, time: 9.876802444458008

Epoch 16, Batch 100/181, Loss: 0.24066737294197083

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.198346316000775, Training Loss Force: 4.759836800180437, time: 2.6328482627868652
Validation Loss Energy: 2.023399427767018, Validation Loss Force: 5.11776129459879, time: 0.1648263931274414
Test Loss Energy: 9.732934800852359, Test Loss Force: 11.491684335964186, time: 10.093254566192627

Epoch 17, Batch 100/181, Loss: 0.7685546875

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.335968638037787, Training Loss Force: 4.049940401017495, time: 2.572080373764038
Validation Loss Energy: 3.8672447652170034, Validation Loss Force: 3.0705133332954673, time: 0.15708160400390625
Test Loss Energy: 10.603368688725359, Test Loss Force: 11.397957170662192, time: 9.862975358963013

Epoch 18, Batch 100/181, Loss: 0.45919060707092285

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 5.943477020159214, Training Loss Force: 2.955170566848704, time: 2.602938652038574
Validation Loss Energy: 6.349324300502251, Validation Loss Force: 3.0089654288926155, time: 0.16078972816467285
Test Loss Energy: 9.25125565929944, Test Loss Force: 11.039149232752152, time: 10.089170217514038

Epoch 19, Batch 100/181, Loss: 0.20436353981494904

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.006967675928506, Training Loss Force: 5.478915608669473, time: 2.5985002517700195
Validation Loss Energy: 4.706386579125482, Validation Loss Force: 8.52783757890622, time: 0.16639208793640137
Test Loss Energy: 9.131175346430961, Test Loss Force: 14.105018085203763, time: 10.019295692443848

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–‚â–‚â–â–„â–ˆâ–‚â–â–‡â–â–‡â–â–â–‚â–â–
wandb:   test_error_force â–â–â–â–â–â–â–„â–ƒâ–ƒâ–…â–‡â–â–†â–â–†â–‚â–‚â–‚â–â–ˆ
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–‚â–„â–ˆâ–…â–â–ˆâ–â–ˆâ–‚â–‚â–‚â–â–…
wandb: train_error_energy â–‚â–â–â–â–â–â–ˆâ–†â–„â–†â–‡â–†â–†â–†â–…â–†â–…â–„â–ƒâ–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–„â–‡â–ˆâ–‡â–…â–‡â–„â–†â–…â–„â–‚â–‡
wandb:         train_loss â–‚â–â–â–â–â–â–ˆâ–†â–„â–†â–‡â–†â–…â–†â–…â–†â–…â–„â–ƒâ–‡
wandb: valid_error_energy â–â–â–â–‚â–â–â–ƒâ–ƒâ–…â–ˆâ–â–ƒâ–ˆâ–„â–ˆâ–‚â–â–‚â–ƒâ–‚
wandb:  valid_error_force â–â–â–â–â–â–â–…â–„â–‚â–„â–‡â–ƒâ–†â–ƒâ–…â–„â–„â–‚â–‚â–ˆ
wandb:         valid_loss â–â–â–â–‚â–â–â–…â–„â–„â–‡â–†â–ƒâ–ˆâ–„â–ˆâ–„â–ƒâ–‚â–‚â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 5761
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.13118
wandb:   test_error_force 14.10502
wandb:          test_loss 5.33065
wandb: train_error_energy 12.00697
wandb:  train_error_force 5.47892
wandb:         train_loss 2.63678
wandb: valid_error_energy 4.70639
wandb:  valid_error_force 8.52784
wandb:         valid_loss 3.1684
wandb: 
wandb: ğŸš€ View run al_77_55 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/45uulo25
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_200100-45uulo25/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.799875020980835, Uncertainty Bias: -0.6345585584640503
5.531311e-05 0.0019584298
-8.959371 28.536528
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 10 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 4 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 16 steps.
Found uncertainty sample 10 after 5 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 11 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 5 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 5 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 6 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 9 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 4 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 10 steps.
Found uncertainty sample 48 after 6 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 10 steps.
Found uncertainty sample 51 after 3 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 3 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 5 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 4 steps.
Found uncertainty sample 60 after 3 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 4 steps.
Found uncertainty sample 65 after 6 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 16 steps.
Found uncertainty sample 70 after 2 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 8 steps.
Found uncertainty sample 74 after 5 steps.
Found uncertainty sample 75 after 7 steps.
Found uncertainty sample 76 after 5 steps.
Found uncertainty sample 77 after 4 steps.
Found uncertainty sample 78 after 3 steps.
Found uncertainty sample 79 after 12 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 7 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 4 steps.
Found uncertainty sample 86 after 4 steps.
Found uncertainty sample 87 after 4 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 12 steps.
Found uncertainty sample 91 after 9 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 13 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 5 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_201004-q26baze1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_56
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/q26baze1
Training model 56. Added 100 samples to the dataset.
Epoch 0, Batch 100/183, Loss: 0.14713548123836517

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7962865835075585, Training Loss Force: 2.271071173428714, time: 2.6169509887695312
Validation Loss Energy: 1.1255193858505301, Validation Loss Force: 2.4195203418446405, time: 0.1597297191619873
Test Loss Energy: 9.468840827950636, Test Loss Force: 11.031759634215984, time: 9.834302425384521

Epoch 1, Batch 100/183, Loss: 0.07387937605381012

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5608076828448776, Training Loss Force: 2.177449935282075, time: 2.717428684234619
Validation Loss Energy: 1.5066416773736993, Validation Loss Force: 2.334383758798783, time: 0.15899348258972168
Test Loss Energy: 10.116156030035395, Test Loss Force: 11.034166895197119, time: 10.779824256896973

Epoch 2, Batch 100/183, Loss: 0.2167438268661499

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.2837531979017136, Training Loss Force: 2.1542720428850104, time: 2.769167423248291
Validation Loss Energy: 3.085965607123223, Validation Loss Force: 2.394045135974285, time: 0.16935205459594727
Test Loss Energy: 11.223308862071649, Test Loss Force: 11.115677027521825, time: 9.930035829544067

Epoch 3, Batch 100/183, Loss: 0.26611393690109253

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.342484022769008, Training Loss Force: 2.143784431362528, time: 2.6090900897979736
Validation Loss Energy: 3.3990203158738237, Validation Loss Force: 2.3985829104991474, time: 0.1625080108642578
Test Loss Energy: 11.10918832736556, Test Loss Force: 11.09674583663189, time: 9.801161766052246

Epoch 4, Batch 100/183, Loss: 0.13835056126117706

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.36211690676963, Training Loss Force: 2.1344692075713376, time: 2.613368034362793
Validation Loss Energy: 2.094820185208113, Validation Loss Force: 2.341327473445395, time: 0.170318603515625
Test Loss Energy: 10.34902840317438, Test Loss Force: 11.021541349440973, time: 10.112802267074585

Epoch 5, Batch 100/183, Loss: 0.09364653378725052

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.040147797550463, Training Loss Force: 2.170379832168109, time: 2.5898568630218506
Validation Loss Energy: 1.4249693066771378, Validation Loss Force: 2.310314931254108, time: 0.16450858116149902
Test Loss Energy: 10.037308349191125, Test Loss Force: 11.049256574665364, time: 9.763295650482178

Epoch 6, Batch 100/183, Loss: 0.5187948942184448

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.703880284299473, Training Loss Force: 5.254946789288496, time: 2.672673225402832
Validation Loss Energy: 7.735616384758498, Validation Loss Force: 3.9795772670698963, time: 0.15896248817443848
Test Loss Energy: 9.935190343628943, Test Loss Force: 11.09920918225843, time: 10.055763959884644

Epoch 7, Batch 100/183, Loss: 0.7730554342269897

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.82283748623073, Training Loss Force: 5.252950809833368, time: 2.63834810256958
Validation Loss Energy: 16.068534143594647, Validation Loss Force: 4.282260937679765, time: 0.16393184661865234
Test Loss Energy: 12.62279649867266, Test Loss Force: 11.235073538598655, time: 9.908644199371338

Epoch 8, Batch 100/183, Loss: 0.32616496086120605

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.6595316761215, Training Loss Force: 4.757009653837193, time: 2.679450511932373
Validation Loss Energy: 8.225939485043185, Validation Loss Force: 3.2164665010459, time: 0.17288589477539062
Test Loss Energy: 10.338328448896707, Test Loss Force: 11.058607028011556, time: 9.82222580909729

Epoch 9, Batch 100/183, Loss: 0.44256073236465454

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.004162923950008, Training Loss Force: 3.046211923569171, time: 2.591405153274536
Validation Loss Energy: 5.963339953954699, Validation Loss Force: 3.1751014764760406, time: 0.16350221633911133
Test Loss Energy: 11.71639321520678, Test Loss Force: 11.430805742759997, time: 10.058704614639282

Epoch 10, Batch 100/183, Loss: 0.30640292167663574

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.605652782535921, Training Loss Force: 3.1554130085377703, time: 2.623415231704712
Validation Loss Energy: 5.195909258053606, Validation Loss Force: 6.624033007186529, time: 0.15805983543395996
Test Loss Energy: 12.215947237863945, Test Loss Force: 13.251950138373918, time: 9.924408435821533

Epoch 11, Batch 100/183, Loss: 0.3201271891593933

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.14683560702684, Training Loss Force: 5.020519962924606, time: 2.672814130783081
Validation Loss Energy: 2.9513596590492703, Validation Loss Force: 4.407270810463864, time: 0.15761351585388184
Test Loss Energy: 10.620537314594236, Test Loss Force: 11.59074733949715, time: 10.058042764663696

Epoch 12, Batch 100/183, Loss: 1.0465247631072998

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.793389066498325, Training Loss Force: 5.001765275692968, time: 2.6338868141174316
Validation Loss Energy: 14.742299193809265, Validation Loss Force: 6.677526300573735, time: 0.15697646141052246
Test Loss Energy: 12.407648561570157, Test Loss Force: 12.576548191675109, time: 9.987064361572266

Epoch 13, Batch 100/183, Loss: 1.1588029861450195

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.222792673568538, Training Loss Force: 4.018850356383989, time: 2.7193331718444824
Validation Loss Energy: 3.580619770742301, Validation Loss Force: 2.968948146440009, time: 0.1581883430480957
Test Loss Energy: 8.881979858310205, Test Loss Force: 10.990850247843237, time: 9.825329303741455

Epoch 14, Batch 100/183, Loss: 1.499873161315918

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.931251551616613, Training Loss Force: 4.326837408615154, time: 2.8204407691955566
Validation Loss Energy: 5.144724301479186, Validation Loss Force: 5.216737433071496, time: 0.1602649688720703
Test Loss Energy: 9.160886605858819, Test Loss Force: 12.021381754742071, time: 9.857499122619629

Epoch 15, Batch 100/183, Loss: 0.26712021231651306

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.8887203096613225, Training Loss Force: 3.184239716016129, time: 2.6214187145233154
Validation Loss Energy: 7.580696404724602, Validation Loss Force: 3.117025577842593, time: 0.1586616039276123
Test Loss Energy: 9.816267775085668, Test Loss Force: 11.021746265677196, time: 9.786440372467041

Epoch 16, Batch 100/183, Loss: 0.45888689160346985

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.087674439583144, Training Loss Force: 2.841544322381674, time: 2.6529760360717773
Validation Loss Energy: 9.714118141955513, Validation Loss Force: 2.8280327938931435, time: 0.1683974266052246
Test Loss Energy: 13.764921508943694, Test Loss Force: 11.363771916847194, time: 10.051152229309082

Epoch 17, Batch 100/183, Loss: 0.5900571942329407

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.384420856690548, Training Loss Force: 3.489183544162814, time: 2.6402335166931152
Validation Loss Energy: 11.36909766229796, Validation Loss Force: 5.879504723620694, time: 0.16883516311645508
Test Loss Energy: 17.230878099882677, Test Loss Force: 12.512992891501915, time: 9.79272174835205

Epoch 18, Batch 100/183, Loss: 0.19545702636241913

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 12.46901754026935, Training Loss Force: 5.781047176721767, time: 2.644866466522217
Validation Loss Energy: 4.5196633587631085, Validation Loss Force: 7.144657667079556, time: 0.18824505805969238
Test Loss Energy: 9.676902779977768, Test Loss Force: 12.574738182970478, time: 9.97329568862915

Epoch 19, Batch 100/183, Loss: 0.14733412861824036

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 13.327005635997613, Training Loss Force: 6.027456339655201, time: 2.6245312690734863
Validation Loss Energy: 6.129558760237361, Validation Loss Force: 8.248214415725217, time: 0.16370820999145508
Test Loss Energy: 12.733339292796233, Test Loss Force: 14.323214230012017, time: 9.86165475845337

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–ƒâ–ƒâ–‚â–‚â–‚â–„â–‚â–ƒâ–„â–‚â–„â–â–â–‚â–…â–ˆâ–‚â–„
wandb:   test_error_force â–â–â–â–â–â–â–â–‚â–â–‚â–†â–‚â–„â–â–ƒâ–â–‚â–„â–„â–ˆ
wandb:          test_loss â–â–â–‚â–‚â–‚â–â–‚â–ƒâ–‚â–ƒâ–†â–ƒâ–…â–â–ƒâ–â–ƒâ–†â–„â–ˆ
wandb: train_error_energy â–â–â–â–â–â–â–†â–…â–†â–„â–„â–†â–†â–…â–†â–„â–„â–…â–‡â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–‡â–‡â–†â–ƒâ–ƒâ–†â–†â–„â–…â–ƒâ–‚â–ƒâ–ˆâ–ˆ
wandb:         train_loss â–â–â–â–â–â–â–†â–†â–†â–ƒâ–ƒâ–†â–†â–…â–…â–ƒâ–ƒâ–„â–ˆâ–ˆ
wandb: valid_error_energy â–â–â–‚â–‚â–â–â–„â–ˆâ–„â–ƒâ–ƒâ–‚â–‡â–‚â–ƒâ–„â–…â–†â–ƒâ–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–‚â–‚â–†â–ƒâ–†â–‚â–„â–‚â–‚â–…â–‡â–ˆ
wandb:         valid_loss â–â–â–â–â–â–â–„â–†â–ƒâ–ƒâ–†â–ƒâ–ˆâ–‚â–…â–ƒâ–ƒâ–‡â–†â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 5851
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.73334
wandb:   test_error_force 14.32321
wandb:          test_loss 5.64472
wandb: train_error_energy 13.32701
wandb:  train_error_force 6.02746
wandb:         train_loss 2.90866
wandb: valid_error_energy 6.12956
wandb:  valid_error_force 8.24821
wandb:         valid_loss 3.17007
wandb: 
wandb: ğŸš€ View run al_77_56 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/q26baze1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_201004-q26baze1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.9541898369789124, Uncertainty Bias: -0.7859417200088501
3.8146973e-05 0.00079250336
-11.295358 5.1860623
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 8 steps.
Found uncertainty sample 4 after 9 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 4 steps.
Found uncertainty sample 7 after 6 steps.
Found uncertainty sample 8 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 8 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 4 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 6 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 7 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 13 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 7 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 18 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 5 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 5 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 6 steps.
Found uncertainty sample 56 after 19 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 3 steps.
Found uncertainty sample 61 after 13 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 7 steps.
Found uncertainty sample 64 after 4 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 3 steps.
Found uncertainty sample 67 after 8 steps.
Found uncertainty sample 68 after 7 steps.
Found uncertainty sample 69 after 3 steps.
Found uncertainty sample 70 after 4 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 2 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 6 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 18 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 8 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 3 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 3 steps.
Found uncertainty sample 91 after 5 steps.
Found uncertainty sample 92 after 10 steps.
Found uncertainty sample 93 after 5 steps.
Found uncertainty sample 94 after 6 steps.
Found uncertainty sample 95 after 7 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 5 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_201913-ot5geaz8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_57
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/ot5geaz8
Training model 57. Added 100 samples to the dataset.
Epoch 0, Batch 100/186, Loss: 0.0706787109375

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.101433241260893, Training Loss Force: 2.271894469104246, time: 2.721299409866333
Validation Loss Energy: 1.672827482233358, Validation Loss Force: 2.361136584561185, time: 0.15494465827941895
Test Loss Energy: 10.187508979864026, Test Loss Force: 11.093213814338517, time: 8.971883058547974

Epoch 1, Batch 100/186, Loss: 0.11012764275074005

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7032036680292377, Training Loss Force: 2.1310443765743265, time: 2.6594202518463135
Validation Loss Energy: 2.0795444230679103, Validation Loss Force: 2.319556127592641, time: 0.1572098731994629
Test Loss Energy: 9.53292488690472, Test Loss Force: 11.005903199726287, time: 9.066879510879517

Epoch 2, Batch 100/186, Loss: 0.10161494463682175

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.531836328500207, Training Loss Force: 2.1005283632341127, time: 2.6883976459503174
Validation Loss Energy: 1.874091248327605, Validation Loss Force: 2.263903425939752, time: 0.1552109718322754
Test Loss Energy: 9.533784534368076, Test Loss Force: 10.947043271371955, time: 9.192944765090942

Epoch 3, Batch 100/186, Loss: 0.07020914554595947

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5233130629796912, Training Loss Force: 2.0860739578394862, time: 2.7081284523010254
Validation Loss Energy: 1.6563853698289044, Validation Loss Force: 2.2619222828811463, time: 0.15648198127746582
Test Loss Energy: 9.659443387841748, Test Loss Force: 10.993740899058235, time: 9.136497259140015

Epoch 4, Batch 100/186, Loss: 0.12272047996520996

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6942576726173402, Training Loss Force: 2.1325975968217246, time: 2.7678956985473633
Validation Loss Energy: 1.1455117072840135, Validation Loss Force: 2.4654910364731073, time: 0.15243244171142578
Test Loss Energy: 9.655006088397208, Test Loss Force: 11.035617900616547, time: 11.787190914154053

Epoch 5, Batch 100/186, Loss: 0.1364710032939911

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.394944091450354, Training Loss Force: 2.0730886456552042, time: 2.8562426567077637
Validation Loss Energy: 2.7929890309282173, Validation Loss Force: 2.3020887351057624, time: 0.19468164443969727
Test Loss Energy: 9.616911351946467, Test Loss Force: 10.955424407454288, time: 10.024291753768921

Epoch 6, Batch 100/186, Loss: 0.5888545513153076

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.508617940628877, Training Loss Force: 4.485388280217148, time: 2.802715539932251
Validation Loss Energy: 4.0031715124415514, Validation Loss Force: 3.6238923647737993, time: 0.15276169776916504
Test Loss Energy: 9.65234816957457, Test Loss Force: 11.560936291649318, time: 9.031753063201904

Epoch 7, Batch 100/186, Loss: 0.6326006650924683

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 14.04810435246946, Training Loss Force: 5.949342358502956, time: 2.867323160171509
Validation Loss Energy: 8.192113777973772, Validation Loss Force: 4.941958659829298, time: 0.15468645095825195
Test Loss Energy: 14.61570469313365, Test Loss Force: 11.279539242683633, time: 9.017624616622925

Epoch 8, Batch 100/186, Loss: 0.08772087842226028

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.772488393038408, Training Loss Force: 5.765246642442551, time: 2.6706905364990234
Validation Loss Energy: 8.145739476962547, Validation Loss Force: 4.415289237743296, time: 0.16274809837341309
Test Loss Energy: 13.509049133761572, Test Loss Force: 11.418846917390656, time: 9.034458875656128

Epoch 9, Batch 100/186, Loss: 0.4742423892021179

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.830343452991647, Training Loss Force: 4.509146695000939, time: 2.67819881439209
Validation Loss Energy: 5.893950866912573, Validation Loss Force: 3.5833290604638535, time: 0.15094351768493652
Test Loss Energy: 9.654232141741472, Test Loss Force: 11.251667197758538, time: 9.43869948387146

Epoch 10, Batch 100/186, Loss: 0.6840596795082092

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.402010691694608, Training Loss Force: 4.5859228956051, time: 2.667522430419922
Validation Loss Energy: 3.4465432927858406, Validation Loss Force: 5.0761963131358865, time: 0.15487384796142578
Test Loss Energy: 9.293855405267998, Test Loss Force: 11.754120172176872, time: 9.070481061935425

Epoch 11, Batch 100/186, Loss: 0.4430837333202362

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.001838903268787, Training Loss Force: 4.384912123860022, time: 2.705447196960449
Validation Loss Energy: 3.2487516345176886, Validation Loss Force: 3.1212208315537318, time: 0.17462754249572754
Test Loss Energy: 10.327036633034316, Test Loss Force: 11.26156993587182, time: 10.996991157531738

Epoch 12, Batch 100/186, Loss: 0.37411433458328247

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 5.880521767937583, Training Loss Force: 2.9288382048911927, time: 2.8337979316711426
Validation Loss Energy: 2.8809148419190147, Validation Loss Force: 3.2042263224269627, time: 0.19623804092407227
Test Loss Energy: 9.208293762366658, Test Loss Force: 11.320735091163899, time: 12.163752317428589

Epoch 13, Batch 100/186, Loss: 0.08758024126291275

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.141212347518572, Training Loss Force: 2.978389891660255, time: 2.8502097129821777
Validation Loss Energy: 7.557881352917599, Validation Loss Force: 2.960037094221838, time: 0.21579813957214355
Test Loss Energy: 13.757619243345017, Test Loss Force: 11.682787488737944, time: 10.137819528579712

Epoch 14, Batch 100/186, Loss: 0.3732459545135498

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.044448482378658, Training Loss Force: 2.954727267706465, time: 2.917388677597046
Validation Loss Energy: 12.628119930385042, Validation Loss Force: 4.290842458117725, time: 0.18277215957641602
Test Loss Energy: 11.274684491051792, Test Loss Force: 11.300074882516485, time: 10.098258256912231

Epoch 15, Batch 100/186, Loss: 1.1664332151412964

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 12.744059883698224, Training Loss Force: 5.64528074776935, time: 2.7138824462890625
Validation Loss Energy: 11.571312123588626, Validation Loss Force: 4.913770415973435, time: 0.16830682754516602
Test Loss Energy: 10.905571112631922, Test Loss Force: 11.622647658132818, time: 10.83627438545227

Epoch 16, Batch 100/186, Loss: 1.2790217399597168

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.147696817648608, Training Loss Force: 5.636556721472289, time: 2.6307716369628906
Validation Loss Energy: 13.328161887516451, Validation Loss Force: 4.358331472226124, time: 0.1683664321899414
Test Loss Energy: 11.240119659788828, Test Loss Force: 11.184272503269252, time: 10.202001571655273

Epoch 17, Batch 100/186, Loss: 0.7112610340118408

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.713451432253505, Training Loss Force: 5.026957397505221, time: 2.702172040939331
Validation Loss Energy: 5.1607677247778545, Validation Loss Force: 4.2968338168272044, time: 0.1797008514404297
Test Loss Energy: 9.165616742043815, Test Loss Force: 11.391040997041257, time: 10.023497104644775

Epoch 18, Batch 100/186, Loss: 1.254118800163269

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.416773774616638, Training Loss Force: 5.053469408382021, time: 2.715040445327759
Validation Loss Energy: 2.6202851713280304, Validation Loss Force: 4.022287547284879, time: 0.1651601791381836
Test Loss Energy: 10.473224103519195, Test Loss Force: 11.968357152316276, time: 10.18664813041687

Epoch 19, Batch 100/186, Loss: 0.21417443454265594

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.078810530884009, Training Loss Force: 3.54904273473173, time: 2.6270968914031982
Validation Loss Energy: 6.095431039583019, Validation Loss Force: 3.1860193343718857, time: 0.16402363777160645
Test Loss Energy: 9.424683092781583, Test Loss Force: 11.154262051120918, time: 10.002302646636963

wandb: - 0.039 MB of 0.059 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–â–‚â–‚â–‚â–‚â–ˆâ–‡â–‚â–â–‚â–â–‡â–„â–ƒâ–„â–â–ƒâ–
wandb:   test_error_force â–‚â–â–â–â–‚â–â–…â–ƒâ–„â–ƒâ–‡â–ƒâ–„â–†â–ƒâ–†â–ƒâ–„â–ˆâ–‚
wandb:          test_loss â–‚â–â–â–â–â–â–„â–‡â–‡â–‚â–„â–ƒâ–‚â–ˆâ–„â–…â–„â–ƒâ–†â–‚
wandb: train_error_energy â–â–â–â–â–â–â–…â–ˆâ–†â–…â–…â–†â–ƒâ–„â–„â–‡â–‡â–…â–…â–„
wandb:  train_error_force â–â–â–â–â–â–â–…â–ˆâ–ˆâ–…â–†â–…â–ƒâ–ƒâ–ƒâ–‡â–‡â–†â–†â–„
wandb:         train_loss â–â–â–â–â–â–â–…â–ˆâ–‡â–…â–…â–…â–ƒâ–ƒâ–ƒâ–‡â–‡â–†â–†â–„
wandb: valid_error_energy â–â–‚â–â–â–â–‚â–ƒâ–…â–…â–„â–‚â–‚â–‚â–…â–ˆâ–‡â–ˆâ–ƒâ–‚â–„
wandb:  valid_error_force â–â–â–â–â–‚â–â–„â–ˆâ–†â–„â–ˆâ–ƒâ–ƒâ–ƒâ–†â–ˆâ–†â–†â–…â–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–„â–‡â–†â–„â–†â–ƒâ–ƒâ–„â–‡â–ˆâ–ˆâ–…â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 5941
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.42468
wandb:   test_error_force 11.15426
wandb:          test_loss 4.36296
wandb: train_error_energy 6.07881
wandb:  train_error_force 3.54904
wandb:         train_loss 1.59432
wandb: valid_error_energy 6.09543
wandb:  valid_error_force 3.18602
wandb:         valid_loss 1.47396
wandb: 
wandb: ğŸš€ View run al_77_57 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/ot5geaz8
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_201913-ot5geaz8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.889779269695282, Uncertainty Bias: -0.6681613922119141
5.722046e-05 0.06900787
-9.617594 31.0152
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 4 steps.
Found uncertainty sample 2 after 3 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 4 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 3 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 5 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 7 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 5 steps.
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 3 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 8 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 3 steps.
Found uncertainty sample 77 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 5 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 5 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 4 steps.
Found uncertainty sample 94 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_202820-crrxbof2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_58
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/crrxbof2
Training model 58. Added 100 samples to the dataset.
Epoch 0, Batch 100/189, Loss: 0.13367588818073273

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.0088160437019096, Training Loss Force: 2.255317248594561, time: 2.7213523387908936
Validation Loss Energy: 0.7430176241762846, Validation Loss Force: 2.2998620436464847, time: 0.1777796745300293
Test Loss Energy: 9.635765375736364, Test Loss Force: 11.02162960103441, time: 10.039293050765991

Epoch 1, Batch 100/189, Loss: 0.10802866518497467

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.8232820301449473, Training Loss Force: 2.2126178155479743, time: 2.677267551422119
Validation Loss Energy: 1.5984915032222524, Validation Loss Force: 2.381880386528531, time: 0.1686079502105713
Test Loss Energy: 10.151562233570449, Test Loss Force: 11.106455703627363, time: 9.97640347480774

Epoch 2, Batch 100/189, Loss: 0.16792255640029907

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.5983392855583545, Training Loss Force: 2.1716067500937988, time: 2.972583293914795
Validation Loss Energy: 3.6651958769920965, Validation Loss Force: 2.3270236714050476, time: 0.17050909996032715
Test Loss Energy: 9.572517051211491, Test Loss Force: 10.994791016076887, time: 9.896826267242432

Epoch 3, Batch 100/189, Loss: 0.20868176221847534

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.9472123030619146, Training Loss Force: 2.2158011060286666, time: 2.6792402267456055
Validation Loss Energy: 3.4964357106354367, Validation Loss Force: 2.333567973271152, time: 0.17497897148132324
Test Loss Energy: 9.467718729341687, Test Loss Force: 10.96478146859482, time: 9.965120077133179

Epoch 4, Batch 100/189, Loss: 0.3301200866699219

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 3.250078088463168, Training Loss Force: 2.200829089491793, time: 2.7169575691223145
Validation Loss Energy: 4.823119134636482, Validation Loss Force: 2.3812930966561465, time: 0.16842222213745117
Test Loss Energy: 9.630830355957745, Test Loss Force: 10.932733003862879, time: 10.140798807144165

Epoch 5, Batch 100/189, Loss: 0.06953853368759155

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.332153794751039, Training Loss Force: 2.143973237522054, time: 2.729512929916382
Validation Loss Energy: 2.1729357317464353, Validation Loss Force: 2.2526421565536, time: 0.16989374160766602
Test Loss Energy: 10.382309135281183, Test Loss Force: 11.025840262925836, time: 10.087051153182983

Epoch 6, Batch 100/189, Loss: 0.14661791920661926

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.272006501317605, Training Loss Force: 5.808291517386474, time: 2.72078800201416
Validation Loss Energy: 6.901668143121165, Validation Loss Force: 4.343550471650292, time: 0.17097067832946777
Test Loss Energy: 12.329599836274388, Test Loss Force: 11.6916524720505, time: 10.090784549713135

Epoch 7, Batch 100/189, Loss: 1.0836924314498901

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.122502106248884, Training Loss Force: 5.044822149182751, time: 2.723043203353882
Validation Loss Energy: 2.1814099349339386, Validation Loss Force: 6.031938157302013, time: 0.17761588096618652
Test Loss Energy: 9.22719776663626, Test Loss Force: 12.133309219864113, time: 10.110815525054932

Epoch 8, Batch 100/189, Loss: 0.6491605043411255

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.971195158609222, Training Loss Force: 4.634072714590369, time: 2.7729270458221436
Validation Loss Energy: 6.786537951895786, Validation Loss Force: 3.08316845586784, time: 0.16955947875976562
Test Loss Energy: 13.0028702819909, Test Loss Force: 11.276976383767579, time: 9.970993757247925

Epoch 9, Batch 100/189, Loss: 0.48314133286476135

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.027983672851914, Training Loss Force: 2.928408979347666, time: 2.8744521141052246
Validation Loss Energy: 2.755565862581431, Validation Loss Force: 3.169673916044051, time: 0.17105817794799805
Test Loss Energy: 9.710194074296414, Test Loss Force: 11.063465855304829, time: 10.020322322845459

Epoch 10, Batch 100/189, Loss: 0.7167009115219116

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.953624043059246, Training Loss Force: 4.335911884356719, time: 2.6792211532592773
Validation Loss Energy: 14.480932080118489, Validation Loss Force: 8.175784002153438, time: 0.16809821128845215
Test Loss Energy: 12.828312827075123, Test Loss Force: 13.425172823496057, time: 10.012718200683594

Epoch 11, Batch 100/189, Loss: 0.37513524293899536

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.1853689391604725, Training Loss Force: 4.784724904671624, time: 2.657896041870117
Validation Loss Energy: 1.3345971411146047, Validation Loss Force: 3.3086002578891547, time: 0.1725614070892334
Test Loss Energy: 9.298117115518384, Test Loss Force: 11.282879387126792, time: 11.14965534210205

Epoch 12, Batch 100/189, Loss: 0.31208837032318115

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.228353417804263, Training Loss Force: 3.5891585481293915, time: 2.7526743412017822
Validation Loss Energy: 18.27134700900152, Validation Loss Force: 9.373167071487837, time: 0.168226957321167
Test Loss Energy: 20.07061358745097, Test Loss Force: 16.72165302227824, time: 9.918173551559448

Epoch 13, Batch 100/189, Loss: 0.4974641799926758

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.955454589137782, Training Loss Force: 5.078656619873552, time: 2.7068865299224854
Validation Loss Energy: 16.51280674091781, Validation Loss Force: 6.4117167518640805, time: 0.17031598091125488
Test Loss Energy: 12.143692559600803, Test Loss Force: 12.312849553269391, time: 10.235013246536255

Epoch 14, Batch 100/189, Loss: 0.20589925348758698

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.223982830809415, Training Loss Force: 5.213552638690461, time: 2.666334390640259
Validation Loss Energy: 7.686888619320916, Validation Loss Force: 4.401083391571395, time: 0.1750478744506836
Test Loss Energy: 13.494896660567886, Test Loss Force: 12.394953263246792, time: 10.016838073730469

Epoch 15, Batch 100/189, Loss: 1.1831066608428955

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.172910811083295, Training Loss Force: 4.745460299305004, time: 2.7168333530426025
Validation Loss Energy: 15.976535268586538, Validation Loss Force: 4.86010158507619, time: 0.174668550491333
Test Loss Energy: 12.203795919098276, Test Loss Force: 11.188786065147562, time: 10.087254047393799

Epoch 16, Batch 100/189, Loss: 0.30313029885292053

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.291855061576268, Training Loss Force: 5.20334789316671, time: 2.857515573501587
Validation Loss Energy: 5.827210355138211, Validation Loss Force: 4.776734479509972, time: 0.17037606239318848
Test Loss Energy: 9.220484462094369, Test Loss Force: 11.370162829108379, time: 9.96076512336731

Epoch 17, Batch 100/189, Loss: 0.8791912794113159

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.476827501485447, Training Loss Force: 4.4750821917665276, time: 2.681090831756592
Validation Loss Energy: 9.282730443207557, Validation Loss Force: 4.523483619076743, time: 0.17830681800842285
Test Loss Energy: 14.251742442476273, Test Loss Force: 11.862404578879179, time: 9.952426195144653

Epoch 18, Batch 100/189, Loss: 0.30389881134033203

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.076396036256035, Training Loss Force: 4.737312467674695, time: 2.7122652530670166
Validation Loss Energy: 33.67708996222531, Validation Loss Force: 4.967534549265137, time: 0.176863431930542
Test Loss Energy: 36.05652263951431, Test Loss Force: 12.176877495010958, time: 10.133381128311157

Epoch 19, Batch 100/189, Loss: 0.6276558637619019

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.631996413028086, Training Loss Force: 4.896769463508956, time: 2.6727166175842285
Validation Loss Energy: 28.48662318343647, Validation Loss Force: 6.575780046591113, time: 0.16689085960388184
Test Loss Energy: 21.052000862026176, Test Loss Force: 12.703002568561338, time: 10.116403818130493

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–â–‚â–â–‚â–â–„â–‚â–‚â–‚â–â–‚â–ˆâ–„
wandb:   test_error_force â–â–â–â–â–â–â–‚â–‚â–â–â–„â–â–ˆâ–ƒâ–ƒâ–â–‚â–‚â–ƒâ–ƒ
wandb:          test_loss â–â–â–â–â–â–â–‚â–‚â–‚â–â–„â–â–ˆâ–ƒâ–ƒâ–‚â–â–ƒâ–‡â–…
wandb: train_error_energy â–â–‚â–â–‚â–‚â–â–ˆâ–‡â–†â–„â–†â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–†â–‚â–…â–†â–„â–‡â–‡â–†â–‡â–…â–†â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–†â–ƒâ–†â–…â–„â–‡â–‡â–†â–‡â–†â–†â–†
wandb: valid_error_energy â–â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–„â–â–…â–„â–‚â–„â–‚â–ƒâ–ˆâ–‡
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–…â–‚â–‚â–‡â–‚â–ˆâ–…â–ƒâ–„â–ƒâ–ƒâ–„â–…
wandb:         valid_loss â–â–â–â–â–‚â–â–ƒâ–„â–‚â–‚â–‡â–‚â–ˆâ–†â–ƒâ–…â–ƒâ–„â–‡â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6031
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 21.052
wandb:   test_error_force 12.703
wandb:          test_loss 5.65928
wandb: train_error_energy 8.632
wandb:  train_error_force 4.89677
wandb:         train_loss 2.21613
wandb: valid_error_energy 28.48662
wandb:  valid_error_force 6.57578
wandb:         valid_loss 4.10662
wandb: 
wandb: ğŸš€ View run al_77_58 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/crrxbof2
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_202820-crrxbof2/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.9580771923065186, Uncertainty Bias: -0.6840535402297974
6.1035156e-05 0.0009371042
-9.404626 23.295053
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 12 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 7 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 7 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 4 steps.
Found uncertainty sample 17 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 2 steps.
Found uncertainty sample 22 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 4 steps.
Found uncertainty sample 37 after 3 steps.
Found uncertainty sample 38 after 7 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 12 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 15 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 3 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 8 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 4 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 7 steps.
Found uncertainty sample 70 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 5 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 7 steps.
Found uncertainty sample 94 after 7 steps.
Found uncertainty sample 95 after 7 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_203730-cao5h4s0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_59
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/cao5h4s0
Training model 59. Added 100 samples to the dataset.
Epoch 0, Batch 100/192, Loss: 0.10297203809022903

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.0271295575250305, Training Loss Force: 2.235061274705877, time: 2.7664670944213867
Validation Loss Energy: 1.8815479926864347, Validation Loss Force: 2.3606818384979142, time: 0.1816861629486084
Test Loss Energy: 9.505502997386198, Test Loss Force: 10.872813874521386, time: 9.989314317703247

Epoch 1, Batch 100/192, Loss: 0.04589360952377319

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.418232961950552, Training Loss Force: 2.145344782797966, time: 2.818450927734375
Validation Loss Energy: 1.006188304601256, Validation Loss Force: 2.3192546335690545, time: 0.16980957984924316
Test Loss Energy: 10.036064218563753, Test Loss Force: 11.0574405074909, time: 10.019656419754028

Epoch 2, Batch 100/192, Loss: 0.1453111618757248

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5259580550119363, Training Loss Force: 2.1370595741071416, time: 3.013104200363159
Validation Loss Energy: 2.8699369190210557, Validation Loss Force: 2.3827434521381474, time: 0.1703016757965088
Test Loss Energy: 10.861027220199936, Test Loss Force: 11.133098978598733, time: 10.026736974716187

Epoch 3, Batch 100/192, Loss: 0.13009579479694366

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.25137790840766, Training Loss Force: 2.1065259822312616, time: 2.8050460815429688
Validation Loss Energy: 3.493146497679078, Validation Loss Force: 2.271243825669835, time: 0.17702531814575195
Test Loss Energy: 11.109230571549704, Test Loss Force: 11.086099835904342, time: 10.088799953460693

Epoch 4, Batch 100/192, Loss: 0.11625410616397858

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.282282200051641, Training Loss Force: 2.095301497324123, time: 2.813544511795044
Validation Loss Energy: 4.017908702862923, Validation Loss Force: 2.9294451369356844, time: 0.16964197158813477
Test Loss Energy: 11.119347333183857, Test Loss Force: 11.076079402574845, time: 10.231380462646484

Epoch 5, Batch 100/192, Loss: 0.09707141667604446

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.308929755786034, Training Loss Force: 2.0855945327995955, time: 2.7514712810516357
Validation Loss Energy: 3.4818025623725, Validation Loss Force: 2.273403326714329, time: 0.17624425888061523
Test Loss Energy: 11.513439442610533, Test Loss Force: 11.032771144368475, time: 10.0236074924469

Epoch 6, Batch 100/192, Loss: 1.5044199228286743

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.878358491628168, Training Loss Force: 5.693477498584117, time: 2.7363128662109375
Validation Loss Energy: 18.012911408839937, Validation Loss Force: 6.996395828512618, time: 0.17158913612365723
Test Loss Energy: 24.36366883594475, Test Loss Force: 13.550990539164372, time: 10.256145477294922

Epoch 7, Batch 100/192, Loss: 0.5142078995704651

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.729844287737436, Training Loss Force: 5.043619185031003, time: 2.7142903804779053
Validation Loss Energy: 2.2016755827440484, Validation Loss Force: 5.585530051534828, time: 0.20850896835327148
Test Loss Energy: 11.872237401212258, Test Loss Force: 11.555743763746987, time: 10.95503830909729

Epoch 8, Batch 100/192, Loss: 1.0978543758392334

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.184062701173104, Training Loss Force: 5.050660671148175, time: 2.743253707885742
Validation Loss Energy: 2.471434743985358, Validation Loss Force: 6.597847004290848, time: 0.1683187484741211
Test Loss Energy: 9.24428416113929, Test Loss Force: 12.890905898091304, time: 9.974050045013428

Epoch 9, Batch 100/192, Loss: 0.18651103973388672

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.341239652770788, Training Loss Force: 6.379652569970486, time: 2.9742958545684814
Validation Loss Energy: 4.327873339096673, Validation Loss Force: 3.773488455418898, time: 0.17337870597839355
Test Loss Energy: 10.657125264089979, Test Loss Force: 11.546020149554819, time: 10.119796752929688

Epoch 10, Batch 100/192, Loss: 1.2665107250213623

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.78432191484535, Training Loss Force: 5.271503107827144, time: 2.7368557453155518
Validation Loss Energy: 2.1334192229251387, Validation Loss Force: 6.768442187079099, time: 0.1749727725982666
Test Loss Energy: 11.030353223247708, Test Loss Force: 12.496951684985344, time: 10.044051170349121

Epoch 11, Batch 100/192, Loss: 1.75784170627594

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.816716045115083, Training Loss Force: 5.704869741498008, time: 2.8093886375427246
Validation Loss Energy: 5.043202646441756, Validation Loss Force: 5.403818445639851, time: 0.17804551124572754
Test Loss Energy: 9.380518973653144, Test Loss Force: 11.768514713545743, time: 10.29867672920227

Epoch 12, Batch 100/192, Loss: 0.13682964444160461

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.153486887477873, Training Loss Force: 4.8759683992695555, time: 2.759848117828369
Validation Loss Energy: 12.625161325443099, Validation Loss Force: 4.548192657527485, time: 0.1806023120880127
Test Loss Energy: 16.24157470167671, Test Loss Force: 11.887014625739063, time: 10.029620170593262

Epoch 13, Batch 100/192, Loss: 1.0585095882415771

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 13.087830875436461, Training Loss Force: 5.562355015622437, time: 2.687692165374756
Validation Loss Energy: 3.7704083844578515, Validation Loss Force: 5.972285649780824, time: 0.1683664321899414
Test Loss Energy: 9.557898101312578, Test Loss Force: 12.452092665582171, time: 10.283178806304932

Epoch 14, Batch 100/192, Loss: 0.3119296133518219

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.5175736632582835, Training Loss Force: 3.803288220829008, time: 2.700279712677002
Validation Loss Energy: 6.603416850307086, Validation Loss Force: 3.0562596132571915, time: 0.1712942123413086
Test Loss Energy: 14.158454170884777, Test Loss Force: 11.206015979824503, time: 10.100767850875854

Epoch 15, Batch 100/192, Loss: 0.13933710753917694

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.620348556089452, Training Loss Force: 4.9892099490704345, time: 2.745150327682495
Validation Loss Energy: 14.232519931560844, Validation Loss Force: 4.093902175329029, time: 0.17989301681518555
Test Loss Energy: 12.176063673091, Test Loss Force: 11.115664175664943, time: 10.002484560012817

Epoch 16, Batch 100/192, Loss: 2.1577577590942383

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 13.157404595211423, Training Loss Force: 5.88539776114516, time: 2.9436092376708984
Validation Loss Energy: 3.6271058145532566, Validation Loss Force: 4.747946253640406, time: 0.18158626556396484
Test Loss Energy: 9.089100255602492, Test Loss Force: 11.575184832006883, time: 10.016122102737427

Epoch 17, Batch 100/192, Loss: 0.09875163435935974

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.093100052407491, Training Loss Force: 5.318900284789858, time: 2.703721046447754
Validation Loss Energy: 3.1374431695111547, Validation Loss Force: 5.389527205850486, time: 0.1700284481048584
Test Loss Energy: 8.661935725135354, Test Loss Force: 11.936828991932527, time: 10.035626411437988

Epoch 18, Batch 100/192, Loss: 0.9018347263336182

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.262252330181106, Training Loss Force: 4.335833764542797, time: 2.742652177810669
Validation Loss Energy: 12.524083338678931, Validation Loss Force: 4.663338212923932, time: 0.17937779426574707
Test Loss Energy: 17.719256458365134, Test Loss Force: 11.956040200348632, time: 10.18388319015503

Epoch 19, Batch 100/192, Loss: 0.31044450402259827

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.512990422901368, Training Loss Force: 4.633983256966457, time: 2.810770034790039
Validation Loss Energy: 3.7369632004030655, Validation Loss Force: 5.712611563829589, time: 0.1734311580657959
Test Loss Energy: 10.298817643932113, Test Loss Force: 13.133163605523315, time: 9.95606541633606

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–â–‚â–‚â–â–„â–â–ƒâ–ƒâ–â–â–…â–‚
wandb:   test_error_force â–â–â–‚â–‚â–‚â–â–ˆâ–ƒâ–†â–ƒâ–…â–ƒâ–„â–…â–‚â–‚â–ƒâ–„â–„â–‡
wandb:          test_loss â–â–â–‚â–‚â–‚â–‚â–ˆâ–‚â–ƒâ–‚â–ƒâ–‚â–„â–ƒâ–ƒâ–‚â–‚â–‚â–„â–„
wandb: train_error_energy â–â–â–â–â–‚â–‚â–‡â–‡â–†â–†â–…â–†â–…â–ˆâ–„â–†â–ˆâ–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–‡â–†â–†â–ˆâ–†â–‡â–†â–‡â–„â–†â–‡â–†â–…â–…
wandb:         train_loss â–â–â–â–â–â–â–‡â–‡â–†â–ˆâ–†â–‡â–†â–ˆâ–„â–†â–ˆâ–‡â–…â–†
wandb: valid_error_energy â–â–â–‚â–‚â–‚â–‚â–ˆâ–â–‚â–‚â–â–ƒâ–†â–‚â–ƒâ–†â–‚â–‚â–†â–‚
wandb:  valid_error_force â–â–â–â–â–‚â–â–ˆâ–†â–‡â–ƒâ–ˆâ–†â–„â–†â–‚â–„â–…â–†â–…â–†
wandb:         valid_loss â–â–â–â–â–‚â–â–ˆâ–„â–…â–ƒâ–…â–„â–…â–…â–ƒâ–…â–„â–„â–…â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 6121
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.29882
wandb:   test_error_force 13.13316
wandb:          test_loss 5.0836
wandb: train_error_energy 10.51299
wandb:  train_error_force 4.63398
wandb:         train_loss 2.25408
wandb: valid_error_energy 3.73696
wandb:  valid_error_force 5.71261
wandb:         valid_loss 2.16154
wandb: 
wandb: ğŸš€ View run al_77_59 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/cao5h4s0
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_203730-cao5h4s0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.9809058904647827, Uncertainty Bias: -0.7014217972755432
4.386902e-05 0.03905964
-3.3657825 50.48285
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 3 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 2 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 2 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 4 steps.
Found uncertainty sample 20 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 5 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 3 steps.
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 4 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 2 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 8 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 3 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 4 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_204640-vfu923ty
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_60
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/vfu923ty
Training model 60. Added 100 samples to the dataset.
Epoch 0, Batch 100/195, Loss: 0.09868185222148895

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7486915703656996, Training Loss Force: 2.2507331291800017, time: 2.81558895111084
Validation Loss Energy: 0.8018635753420478, Validation Loss Force: 2.3291813910963746, time: 1.0927495956420898
Test Loss Energy: 9.527294341978786, Test Loss Force: 10.997811267729652, time: 9.12923789024353

Epoch 1, Batch 100/195, Loss: 0.06819340586662292

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.1374799459766287, Training Loss Force: 2.127515015634746, time: 2.7909317016601562
Validation Loss Energy: 1.4255612447679435, Validation Loss Force: 2.3338954639175253, time: 0.17096662521362305
Test Loss Energy: 9.407736002894183, Test Loss Force: 10.999250857566478, time: 9.228391170501709

Epoch 2, Batch 100/195, Loss: 0.0901104137301445

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.2847157943463967, Training Loss Force: 2.094340591837416, time: 3.0922892093658447
Validation Loss Energy: 1.6097892346810503, Validation Loss Force: 2.2853764136927253, time: 0.16288137435913086
Test Loss Energy: 10.560592630762283, Test Loss Force: 11.019207427049535, time: 9.184929132461548

Epoch 3, Batch 100/195, Loss: 0.06744427978992462

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.656741193839467, Training Loss Force: 2.1201810007996866, time: 2.8673031330108643
Validation Loss Energy: 2.246884823116491, Validation Loss Force: 2.495024869977813, time: 0.16044211387634277
Test Loss Energy: 9.47207994355513, Test Loss Force: 11.090686689740066, time: 9.45147705078125

Epoch 4, Batch 100/195, Loss: 0.05399707704782486

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.008727497961166, Training Loss Force: 2.153226281630897, time: 2.9147424697875977
Validation Loss Energy: 0.840902027423859, Validation Loss Force: 2.259250308797299, time: 0.2010188102722168
Test Loss Energy: 9.720166415753617, Test Loss Force: 11.00154817827988, time: 12.200322151184082

Epoch 5, Batch 100/195, Loss: 0.10181204974651337

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7054055550941671, Training Loss Force: 2.135592926407254, time: 2.8740227222442627
Validation Loss Energy: 0.7894743197299122, Validation Loss Force: 2.3107277969621824, time: 0.16283774375915527
Test Loss Energy: 10.1794957818121, Test Loss Force: 11.028785184790468, time: 9.134993314743042

Epoch 6, Batch 100/195, Loss: 0.36479318141937256

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.114132295675168, Training Loss Force: 4.650908973640966, time: 2.9414398670196533
Validation Loss Energy: 8.816532060423174, Validation Loss Force: 3.0732028982418425, time: 0.16395044326782227
Test Loss Energy: 10.190597892888619, Test Loss Force: 10.922793048525435, time: 9.238362073898315

Epoch 7, Batch 100/195, Loss: 0.2723959982395172

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 6.1076020231939125, Training Loss Force: 2.955144675975623, time: 2.9191348552703857
Validation Loss Energy: 5.186404514225105, Validation Loss Force: 3.3278264582924297, time: 0.1723489761352539
Test Loss Energy: 9.30771544742208, Test Loss Force: 11.253398747648923, time: 9.106689214706421

Epoch 8, Batch 100/195, Loss: 0.23024936020374298

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.243029024348164, Training Loss Force: 3.0258807406375996, time: 2.887019395828247
Validation Loss Energy: 1.4885695154864813, Validation Loss Force: 5.131226431856762, time: 0.16500377655029297
Test Loss Energy: 10.823278136212098, Test Loss Force: 12.29372745914394, time: 9.002395153045654

Epoch 9, Batch 100/195, Loss: 2.480931282043457

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 12.26033167709092, Training Loss Force: 5.514929334885837, time: 2.8333287239074707
Validation Loss Energy: 2.2393903363374266, Validation Loss Force: 4.8639906130443915, time: 0.1650388240814209
Test Loss Energy: 11.143545868809156, Test Loss Force: 11.478588545614437, time: 9.255447387695312

Epoch 10, Batch 100/195, Loss: 0.3152032494544983

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 12.353065877558144, Training Loss Force: 6.101293175393866, time: 2.807844638824463
Validation Loss Energy: 8.313249690068542, Validation Loss Force: 8.041947821316956, time: 0.15563535690307617
Test Loss Energy: 10.38071505735775, Test Loss Force: 13.40294941682226, time: 9.090833187103271

Epoch 11, Batch 100/195, Loss: 0.769443690776825

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.019696757540396, Training Loss Force: 5.02508093553105, time: 2.814401865005493
Validation Loss Energy: 2.678385229563811, Validation Loss Force: 4.300469558641913, time: 0.1985945701599121
Test Loss Energy: 11.70016465883457, Test Loss Force: 11.687796653844732, time: 12.14829397201538

Epoch 12, Batch 100/195, Loss: 0.6206673979759216

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.798370895195834, Training Loss Force: 3.374994340149863, time: 3.0732791423797607
Validation Loss Energy: 5.546858544288684, Validation Loss Force: 2.953892093801669, time: 0.20349860191345215
Test Loss Energy: 9.425669137044947, Test Loss Force: 10.930646125311101, time: 11.667453050613403

Epoch 13, Batch 100/195, Loss: 0.5083650350570679

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.039688381878962, Training Loss Force: 2.81497697448152, time: 2.896472454071045
Validation Loss Energy: 8.948306770306656, Validation Loss Force: 2.7538868258865623, time: 0.17003655433654785
Test Loss Energy: 10.376491114592449, Test Loss Force: 10.819109168243894, time: 10.283342123031616

Epoch 14, Batch 100/195, Loss: 1.4262621402740479

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.084564530818302, Training Loss Force: 4.058172175460099, time: 2.7839081287384033
Validation Loss Energy: 5.039728228423272, Validation Loss Force: 6.737478706322964, time: 0.16996002197265625
Test Loss Energy: 11.152205781998962, Test Loss Force: 13.220754428854072, time: 10.158789873123169

Epoch 15, Batch 100/195, Loss: 0.5106021165847778

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.083546124745954, Training Loss Force: 4.502335155617457, time: 2.7165801525115967
Validation Loss Energy: 10.223064695821211, Validation Loss Force: 4.0453938132232325, time: 0.17090272903442383
Test Loss Energy: 10.67919214018322, Test Loss Force: 11.192002662023144, time: 10.084530591964722

Epoch 16, Batch 100/195, Loss: 0.8855604529380798

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.054913638244624, Training Loss Force: 3.972362945464971, time: 2.972195863723755
Validation Loss Energy: 5.7381288912800485, Validation Loss Force: 3.200876710464771, time: 0.1772291660308838
Test Loss Energy: 13.08177812780524, Test Loss Force: 11.160472020801997, time: 10.120312929153442

Epoch 17, Batch 100/195, Loss: 0.4315316379070282

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 5.896647004309906, Training Loss Force: 2.8396021197622745, time: 2.7782671451568604
Validation Loss Energy: 3.9142876674503997, Validation Loss Force: 3.664740017583096, time: 0.1746537685394287
Test Loss Energy: 11.564092145314534, Test Loss Force: 11.47998561147343, time: 10.052003383636475

Epoch 18, Batch 100/195, Loss: 0.44257351756095886

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.018329779266252, Training Loss Force: 2.8130214495081534, time: 2.8130242824554443
Validation Loss Energy: 7.407487494389928, Validation Loss Force: 4.187281710376462, time: 0.17255473136901855
Test Loss Energy: 9.73416808014774, Test Loss Force: 11.409478263464853, time: 10.362324714660645

Epoch 19, Batch 100/195, Loss: 0.8288888931274414

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.633269270088059, Training Loss Force: 4.503098465449697, time: 2.795254707336426
Validation Loss Energy: 12.713616866332057, Validation Loss Force: 5.21844682699326, time: 0.17670297622680664
Test Loss Energy: 17.113825317966164, Test Loss Force: 13.280455971968358, time: 11.022202014923096

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–â–â–‚â–‚â–â–‚â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–‚â–„â–ƒâ–â–ˆ
wandb:   test_error_force â–â–â–‚â–‚â–â–‚â–â–‚â–…â–ƒâ–ˆâ–ƒâ–â–â–ˆâ–‚â–‚â–ƒâ–ƒâ–ˆ
wandb:          test_loss â–â–â–‚â–â–â–â–â–‚â–„â–ƒâ–†â–ƒâ–â–â–†â–‚â–ƒâ–ƒâ–‚â–ˆ
wandb: train_error_energy â–â–â–â–â–‚â–â–…â–„â–„â–ˆâ–ˆâ–‡â–…â–„â–…â–†â–†â–„â–„â–‡
wandb:  train_error_force â–â–â–â–â–â–â–…â–ƒâ–ƒâ–‡â–ˆâ–†â–ƒâ–‚â–„â–…â–„â–‚â–‚â–…
wandb:         train_loss â–â–â–â–â–â–â–…â–ƒâ–ƒâ–‡â–ˆâ–†â–„â–ƒâ–…â–…â–…â–ƒâ–ƒâ–†
wandb: valid_error_energy â–â–â–â–‚â–â–â–†â–„â–â–‚â–…â–‚â–„â–†â–ƒâ–‡â–„â–ƒâ–…â–ˆ
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–‚â–„â–„â–ˆâ–ƒâ–‚â–‚â–†â–ƒâ–‚â–ƒâ–ƒâ–…
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–ƒâ–„â–„â–ˆâ–ƒâ–ƒâ–ƒâ–†â–…â–ƒâ–ƒâ–„â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 6211
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 17.11383
wandb:   test_error_force 13.28046
wandb:          test_loss 5.58895
wandb: train_error_energy 10.63327
wandb:  train_error_force 4.5031
wandb:         train_loss 2.21834
wandb: valid_error_energy 12.71362
wandb:  valid_error_force 5.21845
wandb:         valid_loss 2.59691
wandb: 
wandb: ğŸš€ View run al_77_60 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/vfu923ty
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_204640-vfu923ty/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5999029874801636, Uncertainty Bias: -0.4629514217376709
4.5776367e-05 0.008342266
-6.2310944 23.984093
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 8 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 7 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 2 steps.
Found uncertainty sample 8 after 6 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 3 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 20 steps.
Found uncertainty sample 15 after 3 steps.
Found uncertainty sample 16 after 4 steps.
Found uncertainty sample 17 after 5 steps.
Found uncertainty sample 18 after 8 steps.
Found uncertainty sample 19 after 6 steps.
Found uncertainty sample 20 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 7 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 18 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 8 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 8 steps.
Found uncertainty sample 34 after 5 steps.
Found uncertainty sample 35 after 2 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 5 steps.
Found uncertainty sample 38 after 8 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 4 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 7 steps.
Found uncertainty sample 44 after 7 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 2 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 7 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 3 steps.
Found uncertainty sample 60 after 13 steps.
Found uncertainty sample 61 after 6 steps.
Found uncertainty sample 62 after 3 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 3 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 4 steps.
Found uncertainty sample 73 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 5 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 5 steps.
Found uncertainty sample 89 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 4 steps.
Found uncertainty sample 92 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 10 steps.
Found uncertainty sample 96 after 10 steps.
Found uncertainty sample 97 after 9 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 15 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_205555-2us8agss
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_61
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/2us8agss
Training model 61. Added 100 samples to the dataset.
Epoch 0, Batch 100/197, Loss: 0.059972092509269714

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8745981385315311, Training Loss Force: 2.198635238445296, time: 3.0536627769470215
Validation Loss Energy: 1.941368504078227, Validation Loss Force: 2.4376184261315963, time: 0.19858074188232422
Test Loss Energy: 9.427148187118691, Test Loss Force: 10.934842270573181, time: 11.446434259414673

Epoch 1, Batch 100/197, Loss: 0.08254984021186829

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.1287759836177105, Training Loss Force: 2.121548504271819, time: 3.0137810707092285
Validation Loss Energy: 2.890243510120373, Validation Loss Force: 2.269039623794295, time: 0.1921367645263672
Test Loss Energy: 9.454122420720948, Test Loss Force: 10.86571477178513, time: 11.475608348846436

Epoch 2, Batch 100/197, Loss: 0.2178657054901123

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.053546193135197, Training Loss Force: 2.0843913342826728, time: 2.947789192199707
Validation Loss Energy: 2.470556091535527, Validation Loss Force: 2.234569048322512, time: 0.20137453079223633
Test Loss Energy: 9.436216738994522, Test Loss Force: 10.935827606025859, time: 11.567626237869263

Epoch 3, Batch 100/197, Loss: 0.21818064153194427

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.8786699532845097, Training Loss Force: 2.0777031788233224, time: 2.9789090156555176
Validation Loss Energy: 1.126428710975213, Validation Loss Force: 2.308284615425776, time: 0.20406031608581543
Test Loss Energy: 10.267553282366896, Test Loss Force: 11.026057821173671, time: 11.496894121170044

Epoch 4, Batch 100/197, Loss: 0.08025109767913818

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.1364222290272767, Training Loss Force: 2.065746820005801, time: 3.0054643154144287
Validation Loss Energy: 3.562722812361404, Validation Loss Force: 2.280966785261344, time: 0.19976210594177246
Test Loss Energy: 11.441269770334006, Test Loss Force: 11.094486670451518, time: 11.484856128692627

Epoch 5, Batch 100/197, Loss: 0.11825195699930191

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.2676226842258123, Training Loss Force: 2.0521190709744883, time: 3.0768282413482666
Validation Loss Energy: 1.3023819535882806, Validation Loss Force: 2.25873460705533, time: 0.18553996086120605
Test Loss Energy: 10.440487369622998, Test Loss Force: 10.943770016273904, time: 11.369295358657837

Epoch 6, Batch 100/197, Loss: 0.1190769374370575

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.064055277300541, Training Loss Force: 5.718024848004484, time: 3.2049307823181152
Validation Loss Energy: 14.215667662589592, Validation Loss Force: 5.742146399345451, time: 0.2054004669189453
Test Loss Energy: 19.232536641517704, Test Loss Force: 12.41993820567354, time: 11.350300073623657

Epoch 7, Batch 100/197, Loss: 0.8349102735519409

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.667378057328905, Training Loss Force: 5.364835532767376, time: 2.963620901107788
Validation Loss Energy: 7.935809678211578, Validation Loss Force: 7.141492610727813, time: 0.19285273551940918
Test Loss Energy: 10.074322028104193, Test Loss Force: 13.600594705684081, time: 11.286545276641846

Epoch 8, Batch 100/197, Loss: 0.939456045627594

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.36365352714172, Training Loss Force: 4.998406603991464, time: 3.122832775115967
Validation Loss Energy: 8.710834579557588, Validation Loss Force: 3.557586002166709, time: 0.1900160312652588
Test Loss Energy: 15.322558225346157, Test Loss Force: 11.452689824971541, time: 11.408539533615112

Epoch 9, Batch 100/197, Loss: 0.37614011764526367

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.07195431020955, Training Loss Force: 4.325721050215391, time: 3.056527853012085
Validation Loss Energy: 2.6376725699242343, Validation Loss Force: 3.7907355167858805, time: 0.20258593559265137
Test Loss Energy: 10.345011624321284, Test Loss Force: 11.312153520207636, time: 11.42007327079773

Epoch 10, Batch 100/197, Loss: 0.12934237718582153

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.504421469697402, Training Loss Force: 3.079847652150266, time: 3.0942673683166504
Validation Loss Energy: 7.552884545948406, Validation Loss Force: 2.7321817055489594, time: 0.26523256301879883
Test Loss Energy: 9.836967481275826, Test Loss Force: 10.93843795992099, time: 11.421833515167236

Epoch 11, Batch 100/197, Loss: 0.32880309224128723

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.017860324978467, Training Loss Force: 2.8417459655529584, time: 2.919625997543335
Validation Loss Energy: 11.343146612462757, Validation Loss Force: 2.807935196343042, time: 0.19679498672485352
Test Loss Energy: 17.6916249380867, Test Loss Force: 11.192607198262072, time: 11.311210632324219

Epoch 12, Batch 100/197, Loss: 0.4500974416732788

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.456517807011602, Training Loss Force: 4.81535772293616, time: 3.167459011077881
Validation Loss Energy: 3.2833634935625637, Validation Loss Force: 3.697565068066473, time: 0.19397330284118652
Test Loss Energy: 12.000952114749133, Test Loss Force: 11.610887355713851, time: 10.428349494934082

Epoch 13, Batch 100/197, Loss: 2.1546499729156494

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.931278104140292, Training Loss Force: 4.740674684673321, time: 2.9560892581939697
Validation Loss Energy: 8.147288140872474, Validation Loss Force: 3.361143941592026, time: 0.19762754440307617
Test Loss Energy: 10.052655755954808, Test Loss Force: 10.960296674171193, time: 12.105597257614136

Epoch 14, Batch 100/197, Loss: 0.5248463749885559

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 5.761818514776277, Training Loss Force: 2.876415872579181, time: 3.0990710258483887
Validation Loss Energy: 8.6571325709576, Validation Loss Force: 2.854671557385544, time: 0.20847153663635254
Test Loss Energy: 10.28523355763245, Test Loss Force: 10.857547836426335, time: 9.214197874069214

Epoch 15, Batch 100/197, Loss: 0.6824150085449219

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.977179840182751, Training Loss Force: 2.828196802340805, time: 2.932730197906494
Validation Loss Energy: 8.662918406045389, Validation Loss Force: 3.087540954219138, time: 0.160552978515625
Test Loss Energy: 14.246763191978298, Test Loss Force: 11.360784620228024, time: 9.949742794036865

Epoch 16, Batch 100/197, Loss: 0.447133868932724

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.2731168243394, Training Loss Force: 3.6561670248836284, time: 2.8573238849639893
Validation Loss Energy: 14.861538892568836, Validation Loss Force: 7.216030550654275, time: 0.16508865356445312
Test Loss Energy: 18.434695380717788, Test Loss Force: 13.244834094129661, time: 9.252665996551514

Epoch 17, Batch 100/197, Loss: 0.9883096218109131

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.001495029193178, Training Loss Force: 5.33507358263477, time: 2.868586778640747
Validation Loss Energy: 3.434519771245891, Validation Loss Force: 4.635089403060377, time: 0.1614546775817871
Test Loss Energy: 9.338538289879324, Test Loss Force: 11.889555889245376, time: 8.953077554702759

Epoch 18, Batch 100/197, Loss: 0.5529288053512573

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.861620457195993, Training Loss Force: 4.798930328171043, time: 2.8787126541137695
Validation Loss Energy: 3.723707205140406, Validation Loss Force: 3.3369109141079916, time: 0.15694046020507812
Test Loss Energy: 8.829503887012223, Test Loss Force: 11.012452913560153, time: 8.993619918823242

Epoch 19, Batch 100/197, Loss: 0.4026196002960205

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.57938792010167, Training Loss Force: 4.167612444997675, time: 2.8450253009796143
Validation Loss Energy: 6.505281324971781, Validation Loss Force: 3.260685420391829, time: 0.16214227676391602
Test Loss Energy: 12.088498418240391, Test Loss Force: 11.682268560042067, time: 9.222398281097412

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.056 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–ƒâ–‚â–ˆâ–‚â–…â–‚â–‚â–‡â–ƒâ–‚â–‚â–…â–‡â–â–â–ƒ
wandb:   test_error_force â–â–â–â–â–‚â–â–…â–ˆâ–ƒâ–‚â–â–‚â–ƒâ–â–â–‚â–‡â–„â–â–ƒ
wandb:          test_loss â–â–â–â–‚â–‚â–â–‡â–†â–„â–‚â–â–„â–ƒâ–â–â–ƒâ–ˆâ–ƒâ–â–ƒ
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–ˆâ–†â–…â–„â–„â–ˆâ–ˆâ–„â–„â–†â–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–‡â–…â–ƒâ–ƒâ–†â–†â–ƒâ–‚â–„â–‡â–†â–…
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–†â–…â–ƒâ–ƒâ–‡â–‡â–ƒâ–ƒâ–…â–‡â–†â–…
wandb: valid_error_energy â–â–‚â–‚â–â–‚â–â–ˆâ–„â–…â–‚â–„â–†â–‚â–…â–…â–…â–ˆâ–‚â–‚â–„
wandb:  valid_error_force â–â–â–â–â–â–â–†â–ˆâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ˆâ–„â–ƒâ–‚
wandb:         valid_loss â–â–â–â–â–â–â–‡â–‡â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–„â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6301
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.0885
wandb:   test_error_force 11.68227
wandb:          test_loss 4.71789
wandb: train_error_energy 8.57939
wandb:  train_error_force 4.16761
wandb:         train_loss 1.96863
wandb: valid_error_energy 6.50528
wandb:  valid_error_force 3.26069
wandb:         valid_loss 1.52637
wandb: 
wandb: ğŸš€ View run al_77_61 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/2us8agss
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_205555-2us8agss/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8897773027420044, Uncertainty Bias: -0.7688578367233276
4.5776367e-05 0.0006790161
-11.311722 40.357796
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 8 steps.
Found uncertainty sample 3 after 15 steps.
Found uncertainty sample 4 after 6 steps.
Found uncertainty sample 5 after 11 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 40 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 4 steps.
Found uncertainty sample 12 after 7 steps.
Found uncertainty sample 13 after 5 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 10 steps.
Found uncertainty sample 17 after 23 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 4 steps.
Found uncertainty sample 20 after 9 steps.
Found uncertainty sample 21 after 6 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 14 steps.
Found uncertainty sample 24 after 11 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 7 steps.
Found uncertainty sample 27 after 7 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 10 steps.
Found uncertainty sample 32 after 9 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 12 steps.
Found uncertainty sample 35 after 7 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 2 steps.
Found uncertainty sample 38 after 7 steps.
Found uncertainty sample 39 after 7 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 7 steps.
Found uncertainty sample 42 after 35 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 32 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 8 steps.
Found uncertainty sample 51 after 3 steps.
Found uncertainty sample 52 after 4 steps.
Found uncertainty sample 53 after 5 steps.
Found uncertainty sample 54 after 10 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 14 steps.
Found uncertainty sample 58 after 15 steps.
Found uncertainty sample 59 after 16 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 14 steps.
Found uncertainty sample 63 after 7 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 9 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 3 steps.
Found uncertainty sample 71 after 16 steps.
Found uncertainty sample 72 after 15 steps.
Found uncertainty sample 73 after 11 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 11 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 13 steps.
Found uncertainty sample 81 after 8 steps.
Found uncertainty sample 82 after 3 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 15 steps.
Found uncertainty sample 86 after 5 steps.
Found uncertainty sample 87 after 10 steps.
Found uncertainty sample 88 after 4 steps.
Found uncertainty sample 89 after 11 steps.
Found uncertainty sample 90 after 2 steps.
Found uncertainty sample 91 after 3 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 11 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 28 steps.
Found uncertainty sample 97 after 6 steps.
Found uncertainty sample 98 after 25 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_210531-lzvp901h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_62
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/lzvp901h
Training model 62. Added 100 samples to the dataset.
Epoch 0, Batch 100/200, Loss: 0.06567616015672684
Epoch 0, Batch 200/200, Loss: 0.16135729849338531

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8449786475096903, Training Loss Force: 2.1755775230275516, time: 2.8848137855529785
Validation Loss Energy: 1.6714784412127548, Validation Loss Force: 2.3599008821358587, time: 0.18882489204406738
Test Loss Energy: 10.432064819411139, Test Loss Force: 10.979947685627298, time: 9.89694595336914

Epoch 1, Batch 100/200, Loss: 0.22271102666854858
Epoch 1, Batch 200/200, Loss: 0.21768569946289062

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.837498820153848, Training Loss Force: 2.09833037038595, time: 2.8173818588256836
Validation Loss Energy: 1.7778327682251844, Validation Loss Force: 2.3922691825336515, time: 0.17184710502624512
Test Loss Energy: 10.55043295810235, Test Loss Force: 10.959196142992615, time: 9.856066703796387

Epoch 2, Batch 100/200, Loss: 0.1629147231578827
Epoch 2, Batch 200/200, Loss: 0.14458782970905304

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.2092823960871213, Training Loss Force: 2.071511878007953, time: 3.117995023727417
Validation Loss Energy: 0.931060917695058, Validation Loss Force: 2.2243974104019575, time: 0.18457531929016113
Test Loss Energy: 9.636001460989785, Test Loss Force: 10.974716303707716, time: 9.931482553482056

Epoch 3, Batch 100/200, Loss: 0.09193848073482513
Epoch 3, Batch 200/200, Loss: 0.07949768006801605

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3788412691128809, Training Loss Force: 2.046556577501927, time: 2.7783477306365967
Validation Loss Energy: 0.8812108904841179, Validation Loss Force: 2.253118501524026, time: 0.18595123291015625
Test Loss Energy: 9.482570318368394, Test Loss Force: 10.919509235878767, time: 10.006022453308105

Epoch 4, Batch 100/200, Loss: 0.1307135820388794
Epoch 4, Batch 200/200, Loss: 0.06416334956884384

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.320328058060866, Training Loss Force: 2.152914472230747, time: 2.808608055114746
Validation Loss Energy: 0.9763986242478798, Validation Loss Force: 2.1708838912339967, time: 0.17792487144470215
Test Loss Energy: 10.362647771458501, Test Loss Force: 10.985936865443408, time: 10.02122974395752

Epoch 5, Batch 100/200, Loss: 0.09720225632190704
Epoch 5, Batch 200/200, Loss: 0.08570749312639236

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.471291318849496, Training Loss Force: 2.011167737852597, time: 2.8716065883636475
Validation Loss Energy: 2.100941313640263, Validation Loss Force: 2.2202034370384074, time: 0.17925477027893066
Test Loss Energy: 10.701364032175231, Test Loss Force: 11.004485177319825, time: 9.823609352111816

Epoch 6, Batch 100/200, Loss: 0.14809811115264893
Epoch 6, Batch 200/200, Loss: 1.1600959300994873

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.744915081634035, Training Loss Force: 5.754153778485666, time: 2.8587300777435303
Validation Loss Energy: 2.810036191069443, Validation Loss Force: 8.315582594710426, time: 0.17557787895202637
Test Loss Energy: 10.66704238826222, Test Loss Force: 13.489590522437233, time: 10.025856733322144

Epoch 7, Batch 100/200, Loss: 0.4601513147354126
Epoch 7, Batch 200/200, Loss: 0.4461561441421509

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.002242555709099, Training Loss Force: 5.343901166690621, time: 2.923295021057129
Validation Loss Energy: 11.496634236103947, Validation Loss Force: 4.704850867137543, time: 0.18481111526489258
Test Loss Energy: 16.662524174880822, Test Loss Force: 12.473220153010061, time: 9.873143672943115

Epoch 8, Batch 100/200, Loss: 1.5406076908111572
Epoch 8, Batch 200/200, Loss: 0.4784404933452606

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.604737942579945, Training Loss Force: 4.569906834932598, time: 2.8092610836029053
Validation Loss Energy: 7.303692164703429, Validation Loss Force: 2.9336167320500683, time: 0.18351149559020996
Test Loss Energy: 12.269867599792176, Test Loss Force: 10.921251786324445, time: 9.9440279006958

Epoch 9, Batch 100/200, Loss: 0.33287203311920166
Epoch 9, Batch 200/200, Loss: 0.4374862015247345

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 5.769434003286669, Training Loss Force: 2.8756322769547786, time: 3.1158194541931152
Validation Loss Energy: 5.941403665932099, Validation Loss Force: 2.982536809406684, time: 0.17414093017578125
Test Loss Energy: 12.778030324067199, Test Loss Force: 11.48752570637287, time: 9.837057828903198

Epoch 10, Batch 100/200, Loss: 0.7117984294891357
Epoch 10, Batch 200/200, Loss: 0.06868917495012283

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 5.95329104978292, Training Loss Force: 2.845532939526927, time: 2.867788076400757
Validation Loss Energy: 19.833350582123526, Validation Loss Force: 3.5224043327774055, time: 0.1748793125152588
Test Loss Energy: 22.170599298527197, Test Loss Force: 12.224473460964864, time: 9.888183355331421

Epoch 11, Batch 100/200, Loss: 0.2828735411167145
Epoch 11, Batch 200/200, Loss: 2.3727405071258545

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 12.10091810358776, Training Loss Force: 5.138786840949824, time: 2.88273024559021
Validation Loss Energy: 25.7214139654832, Validation Loss Force: 6.077875136110635, time: 0.17525339126586914
Test Loss Energy: 27.335128525972028, Test Loss Force: 12.947497161269444, time: 10.083060503005981

Epoch 12, Batch 100/200, Loss: 0.4450035095214844
Epoch 12, Batch 200/200, Loss: 0.6868281960487366

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 12.618863896859084, Training Loss Force: 5.283822557265093, time: 2.870473861694336
Validation Loss Energy: 4.2492682915525535, Validation Loss Force: 5.0577486197340615, time: 0.17687034606933594
Test Loss Energy: 9.23941694936174, Test Loss Force: 11.756127561340971, time: 10.72006893157959

Epoch 13, Batch 100/200, Loss: 1.1119762659072876
Epoch 13, Batch 200/200, Loss: 0.3390205204486847

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.81857968580868, Training Loss Force: 4.794871939124234, time: 2.88619065284729
Validation Loss Energy: 9.950376934305373, Validation Loss Force: 4.448667342717871, time: 0.173414945602417
Test Loss Energy: 14.603035443602936, Test Loss Force: 11.824182475676965, time: 10.085690975189209

Epoch 14, Batch 100/200, Loss: 0.553367018699646
Epoch 14, Batch 200/200, Loss: 1.5658271312713623

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.788686852345695, Training Loss Force: 4.395547574967384, time: 2.870413303375244
Validation Loss Energy: 33.695703321471704, Validation Loss Force: 5.797342851918333, time: 0.1869795322418213
Test Loss Energy: 34.446913853256916, Test Loss Force: 13.207869421081602, time: 9.926905155181885

Epoch 15, Batch 100/200, Loss: 0.5249636769294739
Epoch 15, Batch 200/200, Loss: 0.5793428421020508

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.581122025975924, Training Loss Force: 5.235087532442239, time: 2.954908609390259
Validation Loss Energy: 6.4290281646142375, Validation Loss Force: 4.757577028205501, time: 0.17792415618896484
Test Loss Energy: 9.883472195875239, Test Loss Force: 11.934150163194696, time: 9.879726648330688

Epoch 16, Batch 100/200, Loss: 0.2365400195121765
Epoch 16, Batch 200/200, Loss: 0.2898022532463074

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.19566430049414, Training Loss Force: 5.536342158862947, time: 2.998868465423584
Validation Loss Energy: 13.380443044296275, Validation Loss Force: 5.509597089409128, time: 0.17145228385925293
Test Loss Energy: 12.014986780809132, Test Loss Force: 11.763019378921092, time: 9.916515588760376

Epoch 17, Batch 100/200, Loss: 0.9919084310531616
Epoch 17, Batch 200/200, Loss: 2.555499792098999

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.01797246720978, Training Loss Force: 4.796729057695249, time: 2.841118335723877
Validation Loss Energy: 39.3337618038729, Validation Loss Force: 5.6889171173422675, time: 0.1810758113861084
Test Loss Energy: 40.44753182199166, Test Loss Force: 12.854872289846286, time: 9.822038412094116

Epoch 18, Batch 100/200, Loss: 0.20181329548358917
Epoch 18, Batch 200/200, Loss: 0.7305221557617188

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.025051215064298, Training Loss Force: 4.8130993270131155, time: 2.8277204036712646
Validation Loss Energy: 7.008288476262049, Validation Loss Force: 3.3597735243334133, time: 0.17312359809875488
Test Loss Energy: 12.724623206359684, Test Loss Force: 11.187945457644895, time: 10.048966407775879

Epoch 19, Batch 100/200, Loss: 0.6377910375595093
Epoch 19, Batch 200/200, Loss: 0.7740039229393005

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.287657962218612, Training Loss Force: 4.145113509290475, time: 2.9404070377349854
Validation Loss Energy: 4.20966541493628, Validation Loss Force: 4.500160696059401, time: 0.18178200721740723
Test Loss Energy: 11.081137158519349, Test Loss Force: 11.705711783271466, time: 9.904756307601929

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–„â–…â–â–‚â–‡â–â–‚â–ˆâ–‚â–
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–…â–â–ƒâ–…â–‡â–ƒâ–ƒâ–‡â–„â–ƒâ–†â–‚â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–„â–â–‚â–„â–†â–‚â–ƒâ–‡â–‚â–‚â–ˆâ–‚â–‚
wandb: train_error_energy â–â–â–‚â–â–‚â–â–‡â–‡â–†â–„â–„â–ˆâ–ˆâ–†â–†â–‡â–†â–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–†â–ƒâ–ƒâ–‡â–‡â–†â–…â–‡â–ˆâ–†â–†â–…
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–ˆâ–†â–ƒâ–ƒâ–ˆâ–ˆâ–†â–†â–‡â–ˆâ–‡â–†â–†
wandb: valid_error_energy â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–„â–†â–‚â–ƒâ–‡â–‚â–ƒâ–ˆâ–‚â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–„â–‚â–‚â–ƒâ–…â–„â–„â–…â–„â–…â–…â–‚â–„
wandb:         valid_loss â–â–â–â–â–â–â–…â–„â–‚â–‚â–„â–‡â–ƒâ–„â–‡â–ƒâ–…â–ˆâ–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6391
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.08114
wandb:   test_error_force 11.70571
wandb:          test_loss 4.65833
wandb: train_error_energy 9.28766
wandb:  train_error_force 4.14511
wandb:         train_loss 2.0085
wandb: valid_error_energy 4.20967
wandb:  valid_error_force 4.50016
wandb:         valid_loss 1.78748
wandb: 
wandb: ğŸš€ View run al_77_62 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/lzvp901h
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_210531-lzvp901h/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4626460075378418, Uncertainty Bias: -0.3274800181388855
2.670288e-05 1.2339287
-4.1877165 48.050976
(48745, 22, 3)
Found uncertainty sample 0 after 4 steps.
Found uncertainty sample 1 after 14 steps.
Found uncertainty sample 2 after 30 steps.
Found uncertainty sample 3 after 6 steps.
Found uncertainty sample 4 after 6 steps.
Found uncertainty sample 5 after 65 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 4 steps.
Found uncertainty sample 8 after 19 steps.
Found uncertainty sample 9 after 56 steps.
Found uncertainty sample 10 after 7 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 25 steps.
Found uncertainty sample 13 after 2 steps.
Found uncertainty sample 14 after 19 steps.
Found uncertainty sample 15 after 23 steps.
Found uncertainty sample 16 after 7 steps.
Found uncertainty sample 17 after 31 steps.
Found uncertainty sample 18 after 25 steps.
Found uncertainty sample 19 after 7 steps.
Found uncertainty sample 20 after 16 steps.
Found uncertainty sample 21 after 6 steps.
Found uncertainty sample 22 after 11 steps.
Found uncertainty sample 23 after 21 steps.
Found uncertainty sample 24 after 27 steps.
Found uncertainty sample 25 after 11 steps.
Found uncertainty sample 26 after 4 steps.
Found uncertainty sample 27 after 39 steps.
Found uncertainty sample 28 after 32 steps.
Found uncertainty sample 29 after 28 steps.
Found uncertainty sample 30 after 3 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 59 steps.
Found uncertainty sample 33 after 7 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 16 steps.
Found uncertainty sample 38 after 14 steps.
Found uncertainty sample 39 after 49 steps.
Found uncertainty sample 40 after 4 steps.
Found uncertainty sample 41 after 15 steps.
Found uncertainty sample 42 after 15 steps.
Found uncertainty sample 43 after 32 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 6 steps.
Found uncertainty sample 46 after 5 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 8 steps.
Found uncertainty sample 49 after 8 steps.
Found uncertainty sample 50 after 8 steps.
Found uncertainty sample 51 after 13 steps.
Found uncertainty sample 52 after 6 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 55 steps.
Found uncertainty sample 57 after 9 steps.
Found uncertainty sample 58 after 10 steps.
Found uncertainty sample 59 after 7 steps.
Found uncertainty sample 60 after 5 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 6 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 29 steps.
Found uncertainty sample 65 after 17 steps.
Found uncertainty sample 66 after 10 steps.
Found uncertainty sample 67 after 17 steps.
Found uncertainty sample 68 after 10 steps.
Found uncertainty sample 69 after 14 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 13 steps.
Found uncertainty sample 72 after 6 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 56 steps.
Found uncertainty sample 76 after 10 steps.
Found uncertainty sample 77 after 13 steps.
Found uncertainty sample 78 after 17 steps.
Found uncertainty sample 79 after 22 steps.
Found uncertainty sample 80 after 14 steps.
Found uncertainty sample 81 after 41 steps.
Found uncertainty sample 82 after 8 steps.
Found uncertainty sample 83 after 5 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 53 steps.
Found uncertainty sample 86 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 26 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 15 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 21 steps.
Found uncertainty sample 93 after 52 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 6 steps.
Found uncertainty sample 97 after 14 steps.
Found uncertainty sample 98 after 11 steps.
Found uncertainty sample 99 after 12 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_211507-3ginqxb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_63
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3ginqxb6
Training model 63. Added 100 samples to the dataset.
Epoch 0, Batch 100/203, Loss: 0.06729183346033096
Epoch 0, Batch 200/203, Loss: 0.05897773057222366

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.3515555185152461, Training Loss Force: 2.1333915355799142, time: 2.8523123264312744
Validation Loss Energy: 1.4263951685530867, Validation Loss Force: 2.4739402728826243, time: 0.17305564880371094
Test Loss Energy: 9.864459995385237, Test Loss Force: 10.940872309594662, time: 9.957311630249023

Epoch 1, Batch 100/203, Loss: 0.054867491126060486
Epoch 1, Batch 200/203, Loss: 0.030873103067278862

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.1675341322514559, Training Loss Force: 2.070484269862021, time: 2.946227788925171
Validation Loss Energy: 0.7970169587117837, Validation Loss Force: 2.2565626922059505, time: 0.17499113082885742
Test Loss Energy: 9.793205019980817, Test Loss Force: 10.955844443070001, time: 9.895359992980957

Epoch 2, Batch 100/203, Loss: 0.19632230699062347
Epoch 2, Batch 200/203, Loss: 0.15868648886680603

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.141305640719141, Training Loss Force: 2.0746737498060814, time: 3.1042165756225586
Validation Loss Energy: 3.088381864248734, Validation Loss Force: 2.3013111744481827, time: 0.17812037467956543
Test Loss Energy: 9.522111089237317, Test Loss Force: 10.8390794942501, time: 9.903066873550415

Epoch 3, Batch 100/203, Loss: 0.0821412205696106
Epoch 3, Batch 200/203, Loss: 0.09506671130657196

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.24906461063645, Training Loss Force: 2.051817455212556, time: 2.879011392593384
Validation Loss Energy: 1.94029072974465, Validation Loss Force: 2.253295523495039, time: 0.17737936973571777
Test Loss Energy: 10.454844940918807, Test Loss Force: 10.941172831335175, time: 9.988897562026978

Epoch 4, Batch 100/203, Loss: 0.11102496832609177
Epoch 4, Batch 200/203, Loss: 0.09776630252599716

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.270608154892589, Training Loss Force: 2.0549029272715065, time: 2.916724920272827
Validation Loss Energy: 1.8563668121453964, Validation Loss Force: 2.1717687784199144, time: 0.17810702323913574
Test Loss Energy: 10.459938925483092, Test Loss Force: 10.843773952076237, time: 9.993505954742432

Epoch 5, Batch 100/203, Loss: 0.19908472895622253
Epoch 5, Batch 200/203, Loss: 0.07007633149623871

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.9104082350228768, Training Loss Force: 2.0510136601807734, time: 2.905367851257324
Validation Loss Energy: 1.4920014315317365, Validation Loss Force: 2.1852109807558557, time: 0.17960810661315918
Test Loss Energy: 10.309104170388412, Test Loss Force: 10.919767574057095, time: 9.802930355072021

Epoch 6, Batch 100/203, Loss: 0.5372022390365601
Epoch 6, Batch 200/203, Loss: 0.2949279546737671

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.110723113536787, Training Loss Force: 4.8988409164666695, time: 2.9431111812591553
Validation Loss Energy: 4.370334452537362, Validation Loss Force: 3.450510811627982, time: 0.17863845825195312
Test Loss Energy: 10.974571179383307, Test Loss Force: 11.413158399211904, time: 10.881580591201782

Epoch 7, Batch 100/203, Loss: 0.18348954617977142
Epoch 7, Batch 200/203, Loss: 0.3159230947494507

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 5.907892633510312, Training Loss Force: 2.9136140388139102, time: 2.9203951358795166
Validation Loss Energy: 2.1378717986680402, Validation Loss Force: 2.963657137861759, time: 0.1788320541381836
Test Loss Energy: 9.21218909210485, Test Loss Force: 11.201679098335743, time: 9.923454284667969

Epoch 8, Batch 100/203, Loss: 1.3754487037658691
Epoch 8, Batch 200/203, Loss: 0.6732735633850098

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.933361310742056, Training Loss Force: 3.9038388294779045, time: 2.863304853439331
Validation Loss Energy: 9.613241151207069, Validation Loss Force: 3.51902282346006, time: 0.17231154441833496
Test Loss Energy: 10.435640344960268, Test Loss Force: 11.219576493505679, time: 9.836046934127808

Epoch 9, Batch 100/203, Loss: 1.1232924461364746
Epoch 9, Batch 200/203, Loss: 0.704750120639801

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 12.346576449111126, Training Loss Force: 5.241335638411891, time: 3.1243138313293457
Validation Loss Energy: 2.8953414645491033, Validation Loss Force: 4.815042479375388, time: 0.1737987995147705
Test Loss Energy: 9.302147179908582, Test Loss Force: 11.344988774125738, time: 9.875207901000977

Epoch 10, Batch 100/203, Loss: 0.15788960456848145
Epoch 10, Batch 200/203, Loss: 1.7908909320831299

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.92533688976863, Training Loss Force: 4.914618320885512, time: 2.9021637439727783
Validation Loss Energy: 12.604910679966697, Validation Loss Force: 6.35524540709619, time: 0.17495989799499512
Test Loss Energy: 16.46962881760765, Test Loss Force: 13.008935775536289, time: 9.88559079170227

Epoch 11, Batch 100/203, Loss: 1.6557104587554932
Epoch 11, Batch 200/203, Loss: 0.44263458251953125

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.992662435107825, Training Loss Force: 4.616873368934134, time: 2.8881518840789795
Validation Loss Energy: 10.083527962179833, Validation Loss Force: 3.6913895063060256, time: 0.1761641502380371
Test Loss Energy: 15.795177787074273, Test Loss Force: 11.82498386543058, time: 10.013012409210205

Epoch 12, Batch 100/203, Loss: 0.649118185043335
Epoch 12, Batch 200/203, Loss: 0.10533738881349564

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.050549572627716, Training Loss Force: 4.988518759595629, time: 2.9794249534606934
Validation Loss Energy: 7.580856244059855, Validation Loss Force: 6.576015519212931, time: 0.1838819980621338
Test Loss Energy: 13.337269036931984, Test Loss Force: 13.017821999853236, time: 9.962498903274536

Epoch 13, Batch 100/203, Loss: 0.09903095662593842
Epoch 13, Batch 200/203, Loss: 0.8083866834640503

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.916180427358222, Training Loss Force: 4.341438669792066, time: 2.9406778812408447
Validation Loss Energy: 8.82945600032197, Validation Loss Force: 3.8418037947278343, time: 0.17723822593688965
Test Loss Energy: 10.352756574834983, Test Loss Force: 11.07303957935338, time: 10.038960695266724

Epoch 14, Batch 100/203, Loss: 0.19207531213760376
Epoch 14, Batch 200/203, Loss: 0.20965540409088135

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.512941279405675, Training Loss Force: 4.255500705243215, time: 3.0016541481018066
Validation Loss Energy: 6.51237812465622, Validation Loss Force: 3.9525549745293977, time: 0.1797804832458496
Test Loss Energy: 9.51144571745988, Test Loss Force: 11.295525124660992, time: 9.864277601242065

Epoch 15, Batch 100/203, Loss: 0.7258716821670532
Epoch 15, Batch 200/203, Loss: 0.6989896893501282

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.229215495885766, Training Loss Force: 4.237889995544839, time: 2.8920114040374756
Validation Loss Energy: 11.836217821123435, Validation Loss Force: 3.743931185986584, time: 0.17362761497497559
Test Loss Energy: 16.55885099670797, Test Loss Force: 11.647740008694397, time: 9.944271326065063

Epoch 16, Batch 100/203, Loss: 0.9072731733322144
Epoch 16, Batch 200/203, Loss: 0.34651461243629456

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.302671143479419, Training Loss Force: 4.272659924180576, time: 3.235301971435547
Validation Loss Energy: 6.529344734233905, Validation Loss Force: 5.385419294111838, time: 0.18149781227111816
Test Loss Energy: 12.457231174649186, Test Loss Force: 11.947048964796142, time: 10.132272243499756

Epoch 17, Batch 100/203, Loss: 1.2126675844192505
Epoch 17, Batch 200/203, Loss: 0.32358318567276

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.922960408884574, Training Loss Force: 4.42603582823959, time: 2.9748992919921875
Validation Loss Energy: 7.028731173876814, Validation Loss Force: 3.4403978470273797, time: 0.17720746994018555
Test Loss Energy: 13.747291582552881, Test Loss Force: 11.447702786882495, time: 9.928917407989502

Epoch 18, Batch 100/203, Loss: 1.1187477111816406
Epoch 18, Batch 200/203, Loss: 0.6569712162017822

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.274348812386577, Training Loss Force: 4.487317220343875, time: 2.905059337615967
Validation Loss Energy: 12.717828653227283, Validation Loss Force: 3.9452537089017126, time: 0.1862945556640625
Test Loss Energy: 16.919730363894807, Test Loss Force: 12.165322663752368, time: 10.113590478897095

Epoch 19, Batch 100/203, Loss: 0.7380287647247314
Epoch 19, Batch 200/203, Loss: 0.35277438163757324

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.561240178922585, Training Loss Force: 4.39182691725925, time: 2.900555372238159
Validation Loss Energy: 17.846300556414345, Validation Loss Force: 8.61463571911899, time: 0.1932680606842041
Test Loss Energy: 24.551711973718987, Test Loss Force: 17.908484363871924, time: 9.892568826675415

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.039 MB of 0.057 MB uploadedwandb: - 0.039 MB of 0.057 MB uploadedwandb: \ 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–‚â–‚â–‚â–â–‚â–â–„â–„â–ƒâ–‚â–â–„â–‚â–ƒâ–…â–ˆ
wandb:   test_error_force â–â–â–â–â–â–â–‚â–â–â–‚â–ƒâ–‚â–ƒâ–â–â–‚â–‚â–‚â–‚â–ˆ
wandb:          test_loss â–â–â–â–â–â–â–‚â–â–â–â–ƒâ–ƒâ–ƒâ–â–â–ƒâ–‚â–‚â–ƒâ–ˆ
wandb: train_error_energy â–â–â–‚â–‚â–‚â–â–†â–„â–†â–ˆâ–†â–†â–‡â–†â–†â–†â–…â–†â–†â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–‡â–ƒâ–…â–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–ƒâ–…â–ˆâ–‡â–†â–‡â–†â–†â–†â–†â–†â–†â–‡
wandb: valid_error_energy â–â–â–‚â–â–â–â–‚â–‚â–…â–‚â–†â–…â–„â–„â–ƒâ–†â–ƒâ–„â–†â–ˆ
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–‚â–‚â–„â–†â–ƒâ–†â–ƒâ–ƒâ–ƒâ–„â–‚â–ƒâ–ˆ
wandb:         valid_loss â–â–â–â–â–â–â–‚â–‚â–ƒâ–ƒâ–†â–ƒâ–…â–ƒâ–ƒâ–„â–„â–ƒâ–„â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6481
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 24.55171
wandb:   test_error_force 17.90848
wandb:          test_loss 7.63525
wandb: train_error_energy 11.56124
wandb:  train_error_force 4.39183
wandb:         train_loss 2.2432
wandb: valid_error_energy 17.8463
wandb:  valid_error_force 8.61464
wandb:         valid_loss 4.07677
wandb: 
wandb: ğŸš€ View run al_77_63 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3ginqxb6
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_211507-3ginqxb6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7828873991966248, Uncertainty Bias: -0.6299005746841431
2.2888184e-05 0.002872467
-8.915766 41.56257
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 5 steps.
Found uncertainty sample 4 after 5 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 8 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 6 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 5 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 8 steps.
Found uncertainty sample 16 after 4 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 3 steps.
Found uncertainty sample 21 after 9 steps.
Found uncertainty sample 22 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 12 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 4 steps.
Found uncertainty sample 27 after 5 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 6 steps.
Found uncertainty sample 34 after 3 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 7 steps.
Found uncertainty sample 37 after 7 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 18 steps.
Found uncertainty sample 41 after 11 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 7 steps.
Found uncertainty sample 45 after 3 steps.
Found uncertainty sample 46 after 15 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 2 steps.
Found uncertainty sample 50 after 12 steps.
Found uncertainty sample 51 after 3 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 5 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 6 steps.
Found uncertainty sample 61 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 13 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 5 steps.
Found uncertainty sample 68 after 4 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 2 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 4 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 6 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 8 steps.
Found uncertainty sample 80 after 6 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 14 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 19 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 7 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 5 steps.
Found uncertainty sample 93 after 15 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 3 steps.
Found uncertainty sample 97 after 8 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_212427-di93gf7u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_64
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/di93gf7u
Training model 64. Added 100 samples to the dataset.
Epoch 0, Batch 100/206, Loss: 0.07607696950435638
Epoch 0, Batch 200/206, Loss: 0.05678043141961098

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4462495079429571, Training Loss Force: 2.1432837284826287, time: 3.019145965576172
Validation Loss Energy: 1.0916356549128394, Validation Loss Force: 2.237161752190408, time: 0.17855596542358398
Test Loss Energy: 9.44516490030825, Test Loss Force: 10.951420895485386, time: 9.851017951965332

Epoch 1, Batch 100/206, Loss: 0.320636510848999
Epoch 1, Batch 200/206, Loss: 0.27251917123794556

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5126964249401587, Training Loss Force: 2.135164649885105, time: 2.9531619548797607
Validation Loss Energy: 1.923372383704343, Validation Loss Force: 2.6998965720771713, time: 0.17956876754760742
Test Loss Energy: 10.330664368086227, Test Loss Force: 11.168044119436486, time: 9.872466325759888

Epoch 2, Batch 100/206, Loss: 0.11651504039764404
Epoch 2, Batch 200/206, Loss: 0.14931334555149078

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.554900336601233, Training Loss Force: 2.1575503276610677, time: 3.172520399093628
Validation Loss Energy: 5.001215150274455, Validation Loss Force: 2.4365915316978644, time: 0.17644190788269043
Test Loss Energy: 12.07135996812111, Test Loss Force: 11.125731912781164, time: 9.831693649291992

Epoch 3, Batch 100/206, Loss: 0.08362560719251633
Epoch 3, Batch 200/206, Loss: 0.17695222795009613

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.03352358392427, Training Loss Force: 2.080656485477435, time: 2.970767021179199
Validation Loss Energy: 5.238113188630748, Validation Loss Force: 2.2930172497679506, time: 0.1724395751953125
Test Loss Energy: 12.392295111262744, Test Loss Force: 11.101209492642145, time: 10.978839635848999

Epoch 4, Batch 100/206, Loss: 0.2974099814891815
Epoch 4, Batch 200/206, Loss: 0.06835867464542389

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.3923305503124075, Training Loss Force: 2.114070599079478, time: 2.948774814605713
Validation Loss Energy: 2.6411109712083647, Validation Loss Force: 2.2143810285434014, time: 0.18022418022155762
Test Loss Energy: 10.773587807225184, Test Loss Force: 11.07047734064668, time: 9.980215787887573

Epoch 5, Batch 100/206, Loss: 0.1136457622051239
Epoch 5, Batch 200/206, Loss: 0.06490222364664078

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.9663808430370697, Training Loss Force: 2.0448386157488336, time: 2.986379384994507
Validation Loss Energy: 1.9487846948188545, Validation Loss Force: 2.181844779942292, time: 0.1816418170928955
Test Loss Energy: 10.586740731522164, Test Loss Force: 10.962468264926214, time: 9.889038324356079

Epoch 6, Batch 100/206, Loss: 0.9787259697914124
Epoch 6, Batch 200/206, Loss: 1.4638758897781372

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.184473153042171, Training Loss Force: 5.712154812122627, time: 2.902470111846924
Validation Loss Energy: 2.870311893862798, Validation Loss Force: 5.613695137799514, time: 0.18363213539123535
Test Loss Energy: 10.22744385879994, Test Loss Force: 12.107889314828224, time: 9.950291633605957

Epoch 7, Batch 100/206, Loss: 0.7153069972991943
Epoch 7, Batch 200/206, Loss: 0.13202184438705444

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.303615878186248, Training Loss Force: 4.64875060590593, time: 3.0184242725372314
Validation Loss Energy: 4.212144724541614, Validation Loss Force: 3.5941827896456706, time: 0.18076157569885254
Test Loss Energy: 11.694611164427972, Test Loss Force: 11.472476680927958, time: 9.874696254730225

Epoch 8, Batch 100/206, Loss: 0.3522573709487915
Epoch 8, Batch 200/206, Loss: 0.37801334261894226

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 5.756288874526971, Training Loss Force: 2.88614769061345, time: 2.9432687759399414
Validation Loss Energy: 7.283184559032627, Validation Loss Force: 3.642316009907865, time: 0.18362069129943848
Test Loss Energy: 9.814481068532286, Test Loss Force: 11.343679959860184, time: 9.896363735198975

Epoch 9, Batch 100/206, Loss: 0.568012535572052
Epoch 9, Batch 200/206, Loss: 3.011361598968506

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.357486501042482, Training Loss Force: 3.4747192887577603, time: 3.054107189178467
Validation Loss Energy: 23.736385775619038, Validation Loss Force: 5.840986004520179, time: 0.17331671714782715
Test Loss Energy: 25.771518298200167, Test Loss Force: 12.321473680846088, time: 9.80106806755066

Epoch 10, Batch 100/206, Loss: 0.41284096240997314
Epoch 10, Batch 200/206, Loss: 0.37660396099090576

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.070048991205138, Training Loss Force: 3.1812825049802047, time: 2.9317498207092285
Validation Loss Energy: 4.635902368112159, Validation Loss Force: 2.9389032872453997, time: 0.17803263664245605
Test Loss Energy: 9.346739010326331, Test Loss Force: 11.087185440089495, time: 9.837737798690796

Epoch 11, Batch 100/206, Loss: 0.879758358001709
Epoch 11, Batch 200/206, Loss: 0.6051170229911804

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.033670061592291, Training Loss Force: 4.4881441588722994, time: 2.920029401779175
Validation Loss Energy: 2.0626845555120528, Validation Loss Force: 3.8152725433213925, time: 0.17528748512268066
Test Loss Energy: 10.605452937216437, Test Loss Force: 11.159036422688496, time: 10.088808059692383

Epoch 12, Batch 100/206, Loss: 0.8629959225654602
Epoch 12, Batch 200/206, Loss: 0.13384269177913666

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.394296960613415, Training Loss Force: 4.1086589017252, time: 2.8882157802581787
Validation Loss Energy: 16.322843239143896, Validation Loss Force: 4.123260768215888, time: 0.17453527450561523
Test Loss Energy: 20.231533067802157, Test Loss Force: 11.841739944828216, time: 9.785731792449951

Epoch 13, Batch 100/206, Loss: 0.9385322332382202
Epoch 13, Batch 200/206, Loss: 2.369187831878662

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 13.096383096596199, Training Loss Force: 5.56758126781098, time: 3.0268380641937256
Validation Loss Energy: 4.536144141375906, Validation Loss Force: 7.4993529905796255, time: 0.1844496726989746
Test Loss Energy: 12.387398185395181, Test Loss Force: 12.722077610714384, time: 10.163487911224365

Epoch 14, Batch 100/206, Loss: 2.1301164627075195
Epoch 14, Batch 200/206, Loss: 0.5346219539642334

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.385716589434539, Training Loss Force: 5.218307533390545, time: 2.901726245880127
Validation Loss Energy: 16.842980995837827, Validation Loss Force: 5.406626971577438, time: 0.17268633842468262
Test Loss Energy: 13.117152668020907, Test Loss Force: 11.821168332853127, time: 9.972686529159546

Epoch 15, Batch 100/206, Loss: 1.0204799175262451
Epoch 15, Batch 200/206, Loss: 0.15902628004550934

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.377087752523682, Training Loss Force: 4.330351403484258, time: 2.9324119091033936
Validation Loss Energy: 2.466876817902789, Validation Loss Force: 3.973247117254889, time: 0.2056102752685547
Test Loss Energy: 8.668357869367325, Test Loss Force: 11.511636680562502, time: 9.87058973312378

Epoch 16, Batch 100/206, Loss: 0.14092937111854553
Epoch 16, Batch 200/206, Loss: 0.5553831458091736

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.287281925396371, Training Loss Force: 4.705470773716133, time: 3.1036295890808105
Validation Loss Energy: 4.958659847785039, Validation Loss Force: 2.927585626898041, time: 0.1829836368560791
Test Loss Energy: 9.252010859777865, Test Loss Force: 11.052261232483076, time: 10.00403356552124

Epoch 17, Batch 100/206, Loss: 0.5685391426086426
Epoch 17, Batch 200/206, Loss: 0.38489460945129395

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.675448756912072, Training Loss Force: 5.0132898441026175, time: 2.9909274578094482
Validation Loss Energy: 1.7239910625697696, Validation Loss Force: 4.056714869275066, time: 0.18458938598632812
Test Loss Energy: 10.035061694451977, Test Loss Force: 11.454293289973313, time: 9.815475225448608

Epoch 18, Batch 100/206, Loss: 0.5015552043914795
Epoch 18, Batch 200/206, Loss: 1.1002354621887207

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.76112293672001, Training Loss Force: 4.979972472678054, time: 2.881747007369995
Validation Loss Energy: 4.235418771681423, Validation Loss Force: 6.7298836844245145, time: 0.17359066009521484
Test Loss Energy: 9.904138779155584, Test Loss Force: 12.95091387125621, time: 10.046144723892212

Epoch 19, Batch 100/206, Loss: 0.11085641384124756
Epoch 19, Batch 200/206, Loss: 0.5637205839157104

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.26243507426373, Training Loss Force: 4.751402579250325, time: 3.036773920059204
Validation Loss Energy: 5.24118969688546, Validation Loss Force: 3.4393157863127435, time: 0.1787574291229248
Test Loss Energy: 9.828735144141712, Test Loss Force: 11.212438284937845, time: 9.840191841125488

wandb: - 0.039 MB of 0.057 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ˆâ–â–‚â–†â–ƒâ–ƒâ–â–â–‚â–‚â–
wandb:   test_error_force â–â–‚â–‚â–‚â–â–â–…â–ƒâ–‚â–†â–â–‚â–„â–‡â–„â–ƒâ–â–ƒâ–ˆâ–‚
wandb:          test_loss â–â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–ˆâ–â–‚â–†â–…â–ƒâ–‚â–â–‚â–„â–‚
wandb: train_error_energy â–â–‚â–‚â–â–‚â–â–‡â–†â–„â–…â–„â–‡â–…â–ˆâ–†â–†â–†â–†â–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–ƒâ–„â–ƒâ–†â–…â–ˆâ–‡â–…â–†â–‡â–‡â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–ƒâ–„â–ƒâ–†â–…â–ˆâ–‡â–†â–†â–†â–‡â–‡
wandb: valid_error_energy â–â–â–‚â–‚â–â–â–‚â–‚â–ƒâ–ˆâ–‚â–â–†â–‚â–†â–â–‚â–â–‚â–‚
wandb:  valid_error_force â–â–‚â–â–â–â–â–†â–ƒâ–ƒâ–†â–‚â–ƒâ–„â–ˆâ–…â–ƒâ–‚â–ƒâ–‡â–ƒ
wandb:         valid_loss â–â–‚â–‚â–‚â–â–â–„â–ƒâ–ƒâ–ˆâ–‚â–ƒâ–…â–†â–†â–ƒâ–‚â–ƒâ–…â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6571
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.82874
wandb:   test_error_force 11.21244
wandb:          test_loss 4.40946
wandb: train_error_energy 11.26244
wandb:  train_error_force 4.7514
wandb:         train_loss 2.34352
wandb: valid_error_energy 5.24119
wandb:  valid_error_force 3.43932
wandb:         valid_loss 1.50155
wandb: 
wandb: ğŸš€ View run al_77_64 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/di93gf7u
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_212427-di93gf7u/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4587065577507019, Uncertainty Bias: -0.317429780960083
0.0 0.0017223358
-0.6355587 23.861399
(48745, 22, 3)
Found uncertainty sample 0 after 11 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 8 steps.
Found uncertainty sample 3 after 11 steps.
Found uncertainty sample 4 after 9 steps.
Found uncertainty sample 5 after 14 steps.
Found uncertainty sample 6 after 20 steps.
Found uncertainty sample 7 after 10 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 6 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 10 steps.
Found uncertainty sample 12 after 14 steps.
Found uncertainty sample 13 after 30 steps.
Found uncertainty sample 14 after 12 steps.
Found uncertainty sample 15 after 11 steps.
Found uncertainty sample 16 after 25 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 18 steps.
Found uncertainty sample 20 after 18 steps.
Found uncertainty sample 21 after 17 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 8 steps.
Found uncertainty sample 27 after 16 steps.
Found uncertainty sample 28 after 57 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 15 steps.
Found uncertainty sample 31 after 19 steps.
Found uncertainty sample 32 after 24 steps.
Found uncertainty sample 33 after 5 steps.
Found uncertainty sample 34 after 30 steps.
Found uncertainty sample 35 after 9 steps.
Found uncertainty sample 36 after 17 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 4 steps.
Found uncertainty sample 39 after 14 steps.
Found uncertainty sample 40 after 12 steps.
Found uncertainty sample 41 after 8 steps.
Found uncertainty sample 42 after 9 steps.
Found uncertainty sample 43 after 36 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 9 steps.
Found uncertainty sample 47 after 12 steps.
Found uncertainty sample 48 after 19 steps.
Found uncertainty sample 49 after 28 steps.
Found uncertainty sample 50 after 25 steps.
Found uncertainty sample 51 after 11 steps.
Found uncertainty sample 52 after 16 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 48 steps.
Found uncertainty sample 57 after 23 steps.
Found uncertainty sample 58 after 46 steps.
Found uncertainty sample 59 after 5 steps.
Found uncertainty sample 60 after 29 steps.
Found uncertainty sample 61 after 27 steps.
Found uncertainty sample 62 after 53 steps.
Found uncertainty sample 63 after 26 steps.
Found uncertainty sample 64 after 9 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 6 steps.
Found uncertainty sample 68 after 9 steps.
Found uncertainty sample 69 after 15 steps.
Found uncertainty sample 70 after 27 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 78 steps.
Found uncertainty sample 73 after 19 steps.
Found uncertainty sample 74 after 21 steps.
Found uncertainty sample 75 after 4 steps.
Found uncertainty sample 76 after 12 steps.
Found uncertainty sample 77 after 26 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 8 steps.
Found uncertainty sample 81 after 50 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 7 steps.
Found uncertainty sample 84 after 14 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 40 steps.
Found uncertainty sample 87 after 28 steps.
Found uncertainty sample 88 after 14 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 25 steps.
Found uncertainty sample 91 after 16 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 5 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 10 steps.
Found uncertainty sample 97 after 23 steps.
Found uncertainty sample 98 after 8 steps.
Found uncertainty sample 99 after 25 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_213406-xspyv24d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_65
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/xspyv24d
Training model 65. Added 100 samples to the dataset.
Epoch 0, Batch 100/209, Loss: 0.18247678875923157
Epoch 0, Batch 200/209, Loss: 0.3652404546737671

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7565724422914035, Training Loss Force: 2.152287998198024, time: 3.0080127716064453
Validation Loss Energy: 3.6860378203855104, Validation Loss Force: 2.4476377026890503, time: 0.18322205543518066
Test Loss Energy: 9.38017427715202, Test Loss Force: 10.902843655750559, time: 10.08541989326477

Epoch 1, Batch 100/209, Loss: 0.05193533003330231
Epoch 1, Batch 200/209, Loss: 0.3285463750362396

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6636459209625936, Training Loss Force: 2.0801739248576627, time: 3.0697460174560547
Validation Loss Energy: 3.5907211194357913, Validation Loss Force: 2.548721924189304, time: 0.19369173049926758
Test Loss Energy: 9.449888256159463, Test Loss Force: 10.952838232397307, time: 10.186808109283447

Epoch 2, Batch 100/209, Loss: 0.23066331446170807
Epoch 2, Batch 200/209, Loss: 0.1978270411491394

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.8024124179497516, Training Loss Force: 2.1152209745008634, time: 3.1635286808013916
Validation Loss Energy: 1.9105014947295658, Validation Loss Force: 2.3086698887155177, time: 0.18189311027526855
Test Loss Energy: 9.422867525508954, Test Loss Force: 10.888411015807398, time: 10.188247442245483

Epoch 3, Batch 100/209, Loss: 0.08858145773410797
Epoch 3, Batch 200/209, Loss: 0.06749007105827332

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.2168087375040226, Training Loss Force: 2.041797227831818, time: 2.9657857418060303
Validation Loss Energy: 3.0711609108391116, Validation Loss Force: 2.4084532213979752, time: 0.17971158027648926
Test Loss Energy: 9.36236704549908, Test Loss Force: 10.878610136587403, time: 10.128459215164185

Epoch 4, Batch 100/209, Loss: 0.09589958190917969
Epoch 4, Batch 200/209, Loss: 0.04829993098974228

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.245834609683086, Training Loss Force: 2.0341842192737687, time: 3.0078816413879395
Validation Loss Energy: 2.4890459250732193, Validation Loss Force: 2.34510685541499, time: 0.18114948272705078
Test Loss Energy: 9.446778297986063, Test Loss Force: 10.871336638443404, time: 10.240917921066284

Epoch 5, Batch 100/209, Loss: 0.15326660871505737
Epoch 5, Batch 200/209, Loss: 0.19171199202537537

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.238058787363259, Training Loss Force: 2.029850816670374, time: 2.9555139541625977
Validation Loss Energy: 1.5546343655711694, Validation Loss Force: 2.228617658724727, time: 0.18737506866455078
Test Loss Energy: 9.613455466179701, Test Loss Force: 10.86120275343535, time: 10.097314596176147

Epoch 6, Batch 100/209, Loss: 0.27123844623565674
Epoch 6, Batch 200/209, Loss: 0.39214247465133667

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.374488226446676, Training Loss Force: 5.095760995944284, time: 3.0201432704925537
Validation Loss Energy: 6.221705932896658, Validation Loss Force: 3.3273346378163615, time: 0.18791460990905762
Test Loss Energy: 9.800048671076993, Test Loss Force: 11.115359046654984, time: 10.210184335708618

Epoch 7, Batch 100/209, Loss: 0.5900896787643433
Epoch 7, Batch 200/209, Loss: 0.7965801954269409

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.473527626893942, Training Loss Force: 4.288964509157074, time: 3.0387091636657715
Validation Loss Energy: 16.956858005109922, Validation Loss Force: 6.834606097100083, time: 0.17930006980895996
Test Loss Energy: 19.261855830709287, Test Loss Force: 13.184470573131632, time: 10.057467937469482

Epoch 8, Batch 100/209, Loss: 0.12444321811199188
Epoch 8, Batch 200/209, Loss: 1.2961264848709106

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.227117694578427, Training Loss Force: 5.228392621933505, time: 3.070512294769287
Validation Loss Energy: 4.8597454257772785, Validation Loss Force: 5.085866500139812, time: 0.1856691837310791
Test Loss Energy: 9.721848759260174, Test Loss Force: 12.342131613838152, time: 10.233553647994995

Epoch 9, Batch 100/209, Loss: 0.9352734088897705
Epoch 9, Batch 200/209, Loss: 0.6191927790641785

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.49135902909888, Training Loss Force: 4.350243220057599, time: 3.0197315216064453
Validation Loss Energy: 7.600498557429583, Validation Loss Force: 5.059707440210788, time: 0.1852128505706787
Test Loss Energy: 13.169083971935038, Test Loss Force: 12.508606645079567, time: 10.082592010498047

Epoch 10, Batch 100/209, Loss: 0.28524816036224365
Epoch 10, Batch 200/209, Loss: 0.12882570922374725

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.131643895699387, Training Loss Force: 5.093692020295285, time: 2.966501474380493
Validation Loss Energy: 9.295208154137912, Validation Loss Force: 3.7550276245374627, time: 0.17856669425964355
Test Loss Energy: 10.19361874609782, Test Loss Force: 10.961801022257855, time: 10.118345737457275

Epoch 11, Batch 100/209, Loss: 1.4794028997421265
Epoch 11, Batch 200/209, Loss: 1.061621904373169

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.005340931210283, Training Loss Force: 4.8204360795664165, time: 3.2242703437805176
Validation Loss Energy: 12.889105760274937, Validation Loss Force: 6.258856609876115, time: 0.18262672424316406
Test Loss Energy: 11.120536712849086, Test Loss Force: 12.275215361897622, time: 10.111942291259766

Epoch 12, Batch 100/209, Loss: 0.4338095784187317
Epoch 12, Batch 200/209, Loss: 0.7666908502578735

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.623009723773714, Training Loss Force: 5.229450755303576, time: 3.0295231342315674
Validation Loss Energy: 10.545066547747385, Validation Loss Force: 5.8125377944364445, time: 0.18011260032653809
Test Loss Energy: 10.393165526874842, Test Loss Force: 11.844712379739152, time: 10.048325777053833

Epoch 13, Batch 100/209, Loss: 0.5271358489990234
Epoch 13, Batch 200/209, Loss: 0.7600598335266113

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.356891089976195, Training Loss Force: 5.020787203538124, time: 2.997267961502075
Validation Loss Energy: 8.907044173543923, Validation Loss Force: 4.114220289556647, time: 0.19120454788208008
Test Loss Energy: 15.125504798069056, Test Loss Force: 12.406793712568607, time: 10.370960712432861

Epoch 14, Batch 100/209, Loss: 0.5307432413101196
Epoch 14, Batch 200/209, Loss: 0.4602014422416687

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.801440724259487, Training Loss Force: 4.96483495381952, time: 3.039370059967041
Validation Loss Energy: 17.448812973342065, Validation Loss Force: 5.5808999806489785, time: 0.19743847846984863
Test Loss Energy: 22.243593695351183, Test Loss Force: 12.120998684498453, time: 10.031492471694946

Epoch 15, Batch 100/209, Loss: 1.0700924396514893
Epoch 15, Batch 200/209, Loss: 0.5112316608428955

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.610791224873076, Training Loss Force: 4.7008027532513035, time: 3.0078887939453125
Validation Loss Energy: 2.8776551326244917, Validation Loss Force: 4.807446900804073, time: 0.18278837203979492
Test Loss Energy: 9.583812457759585, Test Loss Force: 11.824384565996905, time: 10.207666397094727

Epoch 16, Batch 100/209, Loss: 0.8545910120010376
Epoch 16, Batch 200/209, Loss: 0.21836575865745544

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.280025888524793, Training Loss Force: 5.225468264358528, time: 2.9962868690490723
Validation Loss Energy: 3.1035442414042547, Validation Loss Force: 6.212885228684447, time: 0.18511748313903809
Test Loss Energy: 9.622830574013314, Test Loss Force: 13.139326844994729, time: 10.140188217163086

Epoch 17, Batch 100/209, Loss: 0.08745379745960236
Epoch 17, Batch 200/209, Loss: 0.9442691206932068

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.182131758172416, Training Loss Force: 4.762307627818322, time: 2.9615025520324707
Validation Loss Energy: 6.199555696936345, Validation Loss Force: 6.947257090054701, time: 0.17861342430114746
Test Loss Energy: 13.577786595480832, Test Loss Force: 13.48402961349782, time: 10.06197214126587

Epoch 18, Batch 100/209, Loss: 0.05876687914133072
Epoch 18, Batch 200/209, Loss: 0.8026099801063538

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.014949545241592, Training Loss Force: 5.154082812939461, time: 3.284630060195923
Validation Loss Energy: 7.7982885456791395, Validation Loss Force: 5.1678723178824, time: 0.18602228164672852
Test Loss Energy: 13.099374698124839, Test Loss Force: 12.449741787693046, time: 10.97891616821289

Epoch 19, Batch 100/209, Loss: 0.6849617958068848
Epoch 19, Batch 200/209, Loss: 0.6510116457939148

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.151683414494864, Training Loss Force: 3.9225770937344526, time: 3.1212236881256104
Validation Loss Energy: 13.264052991043236, Validation Loss Force: 3.8904220997244323, time: 0.17956280708312988
Test Loss Energy: 17.752041340092358, Test Loss Force: 11.726678911608463, time: 10.045297622680664

wandb: - 0.039 MB of 0.040 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–†â–â–ƒâ–â–‚â–‚â–„â–ˆâ–â–â–ƒâ–ƒâ–†
wandb:   test_error_force â–â–â–â–â–â–â–‚â–‡â–…â–…â–â–…â–„â–…â–„â–„â–‡â–ˆâ–…â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–‚â–ˆâ–„â–…â–â–„â–ƒâ–…â–‡â–ƒâ–…â–‡â–…â–…
wandb: train_error_energy â–â–â–‚â–â–â–â–‡â–…â–‡â–†â–…â–‡â–‡â–†â–ˆâ–†â–ˆâ–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–ˆâ–†â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–…
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–‡â–†â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–‡â–…
wandb: valid_error_energy â–‚â–‚â–â–‚â–â–â–ƒâ–ˆâ–‚â–„â–„â–†â–…â–„â–ˆâ–‚â–‚â–ƒâ–„â–†
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–ˆâ–…â–…â–ƒâ–‡â–†â–„â–†â–…â–‡â–ˆâ–…â–ƒ
wandb:         valid_loss â–‚â–‚â–â–â–â–â–ƒâ–ˆâ–„â–…â–„â–‡â–†â–„â–‡â–„â–…â–†â–…â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 6661
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 17.75204
wandb:   test_error_force 11.72668
wandb:          test_loss 5.11176
wandb: train_error_energy 9.15168
wandb:  train_error_force 3.92258
wandb:         train_loss 1.92494
wandb: valid_error_energy 13.26405
wandb:  valid_error_force 3.89042
wandb:         valid_loss 2.18939
wandb: 
wandb: ğŸš€ View run al_77_65 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/xspyv24d
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_213406-xspyv24d/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8558273911476135, Uncertainty Bias: -0.691830575466156
5.531311e-05 0.00086545944
-9.964598 18.781609
(48745, 22, 3)
Found uncertainty sample 0 after 10 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 2 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 4 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 6 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 8 steps.
Found uncertainty sample 16 after 7 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 8 steps.
Found uncertainty sample 20 after 6 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 3 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 10 steps.
Found uncertainty sample 27 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 12 steps.
Found uncertainty sample 37 after 6 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 5 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 4 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 27 steps.
Found uncertainty sample 51 after 5 steps.
Found uncertainty sample 52 after 4 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 9 steps.
Found uncertainty sample 55 after 5 steps.
Found uncertainty sample 56 after 3 steps.
Found uncertainty sample 57 after 3 steps.
Found uncertainty sample 58 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 4 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 7 steps.
Found uncertainty sample 70 after 9 steps.
Found uncertainty sample 71 after 3 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 10 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 8 steps.
Found uncertainty sample 88 after 4 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 8 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 4 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 3 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_214327-1mfejgul
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_66
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/1mfejgul
Training model 66. Added 100 samples to the dataset.
Epoch 0, Batch 100/211, Loss: 0.17665892839431763
Epoch 0, Batch 200/211, Loss: 0.15315479040145874

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.645536538972558, Training Loss Force: 2.1743679088254004, time: 2.993807554244995
Validation Loss Energy: 1.1180781169973424, Validation Loss Force: 2.2433010745473565, time: 0.19599533081054688
Test Loss Energy: 9.68128585074172, Test Loss Force: 10.912114255977274, time: 10.017755031585693

Epoch 1, Batch 100/211, Loss: 0.10756652802228928
Epoch 1, Batch 200/211, Loss: 0.09722252190113068

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.0091583133351323, Training Loss Force: 2.0753423801903415, time: 3.0442521572113037
Validation Loss Energy: 1.9909141147330407, Validation Loss Force: 2.2496606723569132, time: 0.17797327041625977
Test Loss Energy: 9.291252060204828, Test Loss Force: 10.854892099994489, time: 10.133636236190796

Epoch 2, Batch 100/211, Loss: 0.09419219940900803
Epoch 2, Batch 200/211, Loss: 0.0755906030535698

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.9422876208411584, Training Loss Force: 2.051567675194865, time: 3.194159746170044
Validation Loss Energy: 1.0969842791325979, Validation Loss Force: 2.3370354328788263, time: 0.18375301361083984
Test Loss Energy: 9.43580997492623, Test Loss Force: 10.894048238476095, time: 10.132313013076782

Epoch 3, Batch 100/211, Loss: 0.08512841165065765
Epoch 3, Batch 200/211, Loss: 0.14887258410453796

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.640812164009844, Training Loss Force: 2.043180774274038, time: 3.0360517501831055
Validation Loss Energy: 3.0948726141852427, Validation Loss Force: 2.291809881991444, time: 0.17805767059326172
Test Loss Energy: 11.069906570843052, Test Loss Force: 10.9983178668502, time: 10.09479308128357

Epoch 4, Batch 100/211, Loss: 0.12671233713626862
Epoch 4, Batch 200/211, Loss: 0.1261935830116272

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.180808092023529, Training Loss Force: 2.041457631148108, time: 3.067417860031128
Validation Loss Energy: 1.9274787924262569, Validation Loss Force: 2.210388682091489, time: 0.18699145317077637
Test Loss Energy: 9.512051223650271, Test Loss Force: 10.871186002183284, time: 10.213221788406372

Epoch 5, Batch 100/211, Loss: 0.06895730644464493
Epoch 5, Batch 200/211, Loss: 0.08815355598926544

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.0987301503199145, Training Loss Force: 2.0323589471962427, time: 3.105221748352051
Validation Loss Energy: 2.5480568671177046, Validation Loss Force: 2.3095626249888284, time: 0.18847942352294922
Test Loss Energy: 10.317223606104893, Test Loss Force: 10.970050342869028, time: 10.056164979934692

Epoch 6, Batch 100/211, Loss: 0.303959459066391
Epoch 6, Batch 200/211, Loss: 0.38101547956466675

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.23794533136317, Training Loss Force: 4.652436963138759, time: 3.0910189151763916
Validation Loss Energy: 2.3090073994734572, Validation Loss Force: 3.6399819362529637, time: 0.18604230880737305
Test Loss Energy: 11.0883187817416, Test Loss Force: 11.437452535678263, time: 10.234471559524536

Epoch 7, Batch 100/211, Loss: 0.2978058457374573
Epoch 7, Batch 200/211, Loss: 0.5587149858474731

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.546123886424768, Training Loss Force: 5.052356819333726, time: 2.944841146469116
Validation Loss Energy: 5.827064840851348, Validation Loss Force: 3.3854904746731975, time: 0.17876768112182617
Test Loss Energy: 13.390179688713802, Test Loss Force: 11.658171911289426, time: 10.190033912658691

Epoch 8, Batch 100/211, Loss: 0.47599780559539795
Epoch 8, Batch 200/211, Loss: 1.0234739780426025

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 11.30490081010503, Training Loss Force: 5.468961134277628, time: 3.1159255504608154
Validation Loss Energy: 16.468539224030785, Validation Loss Force: 3.7732054429344233, time: 0.18228507041931152
Test Loss Energy: 21.202090069524775, Test Loss Force: 12.005772456420178, time: 10.23400616645813

Epoch 9, Batch 100/211, Loss: 0.7145195007324219
Epoch 9, Batch 200/211, Loss: 0.5811526775360107

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 11.191421083746125, Training Loss Force: 5.044549080861457, time: 2.952629327774048
Validation Loss Energy: 6.178458240697979, Validation Loss Force: 4.129274839178252, time: 0.1855146884918213
Test Loss Energy: 12.966189212491496, Test Loss Force: 12.12144761347601, time: 10.121194839477539

Epoch 10, Batch 100/211, Loss: 0.09773129969835281
Epoch 10, Batch 200/211, Loss: 0.3821125626564026

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.156862074631972, Training Loss Force: 4.487520375972538, time: 2.997737407684326
Validation Loss Energy: 1.6217013659807489, Validation Loss Force: 5.207209049202003, time: 0.18740177154541016
Test Loss Energy: 8.85026077370641, Test Loss Force: 11.910761970721131, time: 9.97676396369934

Epoch 11, Batch 100/211, Loss: 1.2845172882080078
Epoch 11, Batch 200/211, Loss: 0.6277999877929688

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.151135081691718, Training Loss Force: 4.484483242693364, time: 3.233027935028076
Validation Loss Energy: 7.391186999562284, Validation Loss Force: 4.013085410825695, time: 0.1812286376953125
Test Loss Energy: 13.409983817267424, Test Loss Force: 11.6854989460834, time: 10.106765747070312

Epoch 12, Batch 100/211, Loss: 0.8371484279632568
Epoch 12, Batch 200/211, Loss: 0.12206404656171799

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.082975179672184, Training Loss Force: 5.2467699197060185, time: 2.95491361618042
Validation Loss Energy: 6.449840381676669, Validation Loss Force: 4.9714534148988925, time: 0.19568777084350586
Test Loss Energy: 10.623024549938776, Test Loss Force: 11.947618237703523, time: 10.084155797958374

Epoch 13, Batch 100/211, Loss: 0.05534431338310242
Epoch 13, Batch 200/211, Loss: 0.7610520124435425

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.758988270364184, Training Loss Force: 4.59678058955557, time: 3.002457857131958
Validation Loss Energy: 6.875455753931691, Validation Loss Force: 4.402121029791418, time: 0.18099260330200195
Test Loss Energy: 12.636891567527366, Test Loss Force: 11.968659296860194, time: 10.221763372421265

Epoch 14, Batch 100/211, Loss: 0.380644291639328
Epoch 14, Batch 200/211, Loss: 0.34799760580062866

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.323276002337822, Training Loss Force: 5.116717529283688, time: 2.997690439224243
Validation Loss Energy: 11.207632174273419, Validation Loss Force: 4.577309097718148, time: 0.1798868179321289
Test Loss Energy: 15.747558337103223, Test Loss Force: 11.82226265727499, time: 10.079917430877686

Epoch 15, Batch 100/211, Loss: 0.3210185766220093
Epoch 15, Batch 200/211, Loss: 0.22405162453651428

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.868451890038237, Training Loss Force: 5.4377705077759595, time: 3.0069727897644043
Validation Loss Energy: 19.47002044603657, Validation Loss Force: 4.507493970376815, time: 0.1846146583557129
Test Loss Energy: 21.178370764533796, Test Loss Force: 12.315189416613542, time: 10.28347897529602

Epoch 16, Batch 100/211, Loss: 1.4412729740142822
Epoch 16, Batch 200/211, Loss: 0.06682531535625458

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.635148077636284, Training Loss Force: 4.842425567429137, time: 3.049513101577759
Validation Loss Energy: 19.530196337385263, Validation Loss Force: 4.373771350804757, time: 0.19779157638549805
Test Loss Energy: 15.690116342554017, Test Loss Force: 11.665101379773928, time: 10.106072902679443

Epoch 17, Batch 100/211, Loss: 0.7827264666557312
Epoch 17, Batch 200/211, Loss: 0.5670245885848999

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.676821252781476, Training Loss Force: 4.139362277030178, time: 2.9764328002929688
Validation Loss Energy: 4.523572352731395, Validation Loss Force: 3.927994957404569, time: 0.1883068084716797
Test Loss Energy: 11.399061245894197, Test Loss Force: 11.538832212321598, time: 10.083003759384155

Epoch 18, Batch 100/211, Loss: 0.39989423751831055
Epoch 18, Batch 200/211, Loss: 0.4809266924858093

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.18403658606953, Training Loss Force: 3.9872404331141302, time: 3.2060139179229736
Validation Loss Energy: 5.885856286785234, Validation Loss Force: 3.364728914841464, time: 0.18500828742980957
Test Loss Energy: 9.435050502930787, Test Loss Force: 11.356409197158296, time: 10.951585531234741

Epoch 19, Batch 100/211, Loss: 0.689368486404419
Epoch 19, Batch 200/211, Loss: 0.679555356502533

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.948510239736974, Training Loss Force: 4.908467911704788, time: 3.0296239852905273
Validation Loss Energy: 9.009030205085185, Validation Loss Force: 6.115885649997092, time: 0.1811676025390625
Test Loss Energy: 15.043268419249735, Test Loss Force: 13.235895772227405, time: 10.106740713119507

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.060 MB uploadedwandb: | 0.039 MB of 0.060 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–â–‚â–‚â–„â–ˆâ–ƒâ–â–„â–‚â–ƒâ–…â–ˆâ–…â–‚â–â–…
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–„â–…â–„â–ƒâ–„â–„â–„â–…â–ƒâ–ƒâ–‚â–ˆ
wandb:          test_loss â–â–â–â–‚â–â–‚â–ƒâ–„â–‡â–…â–ƒâ–„â–ƒâ–„â–…â–ˆâ–…â–ƒâ–‚â–‡
wandb: train_error_energy â–â–â–â–â–â–â–‡â–†â–ˆâ–ˆâ–†â–‡â–ˆâ–…â–†â–ˆâ–…â–‡â–†â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–†â–‡â–ˆâ–‡â–†â–†â–ˆâ–†â–‡â–ˆâ–‡â–…â–…â–‡
wandb:         train_loss â–â–â–â–â–â–â–†â–‡â–ˆâ–‡â–†â–†â–ˆâ–†â–‡â–ˆâ–†â–†â–†â–‡
wandb: valid_error_energy â–â–â–â–‚â–â–‚â–â–ƒâ–‡â–ƒâ–â–ƒâ–ƒâ–ƒâ–…â–ˆâ–ˆâ–‚â–ƒâ–„
wandb:  valid_error_force â–â–â–â–â–â–â–„â–ƒâ–„â–„â–†â–„â–†â–…â–…â–…â–…â–„â–ƒâ–ˆ
wandb:         valid_loss â–â–â–â–‚â–â–â–ƒâ–ƒâ–†â–„â–…â–…â–…â–…â–†â–ˆâ–ˆâ–„â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 6751
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 15.04327
wandb:   test_error_force 13.2359
wandb:          test_loss 5.43548
wandb: train_error_energy 10.94851
wandb:  train_error_force 4.90847
wandb:         train_loss 2.37507
wandb: valid_error_energy 9.00903
wandb:  valid_error_force 6.11589
wandb:         valid_loss 2.64928
wandb: 
wandb: ğŸš€ View run al_77_66 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/1mfejgul
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_214327-1mfejgul/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8909960389137268, Uncertainty Bias: -0.8458718657493591
1.1444092e-05 0.00866127
-12.239039 72.534676
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 31 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 15 steps.
Found uncertainty sample 7 after 10 steps.
Found uncertainty sample 8 after 17 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 13 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 12 steps.
Found uncertainty sample 13 after 16 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 10 steps.
Found uncertainty sample 18 after 14 steps.
Found uncertainty sample 19 after 10 steps.
Found uncertainty sample 20 after 35 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 3 steps.
Found uncertainty sample 26 after 19 steps.
Found uncertainty sample 27 after 33 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 14 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 5 steps.
Found uncertainty sample 33 after 21 steps.
Found uncertainty sample 34 after 4 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 7 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 19 steps.
Found uncertainty sample 41 after 6 steps.
Found uncertainty sample 42 after 14 steps.
Found uncertainty sample 43 after 29 steps.
Found uncertainty sample 44 after 10 steps.
Found uncertainty sample 45 after 3 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 7 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 9 steps.
Found uncertainty sample 51 after 10 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 48 steps.
Found uncertainty sample 54 after 16 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 9 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 4 steps.
Found uncertainty sample 59 after 12 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 28 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 11 steps.
Found uncertainty sample 64 after 6 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 27 steps.
Found uncertainty sample 67 after 30 steps.
Found uncertainty sample 68 after 5 steps.
Found uncertainty sample 69 after 20 steps.
Found uncertainty sample 70 after 11 steps.
Found uncertainty sample 71 after 8 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 5 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 2 steps.
Found uncertainty sample 78 after 9 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 13 steps.
Found uncertainty sample 82 after 3 steps.
Found uncertainty sample 83 after 12 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 9 steps.
Found uncertainty sample 86 after 8 steps.
Found uncertainty sample 87 after 20 steps.
Found uncertainty sample 88 after 10 steps.
Found uncertainty sample 89 after 3 steps.
Found uncertainty sample 90 after 5 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 8 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 25 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 19 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 24 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_215301-5b90ngu0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_67
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/5b90ngu0
Training model 67. Added 100 samples to the dataset.
Epoch 0, Batch 100/214, Loss: 0.23248855769634247
Epoch 0, Batch 200/214, Loss: 0.2146582305431366

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.7003814009761715, Training Loss Force: 2.199462704055388, time: 3.0842912197113037
Validation Loss Energy: 4.459777197084792, Validation Loss Force: 2.3509667836610846, time: 0.20240283012390137
Test Loss Energy: 11.27996811045353, Test Loss Force: 11.008673488358546, time: 10.228548049926758

Epoch 1, Batch 100/214, Loss: 0.08226920664310455
Epoch 1, Batch 200/214, Loss: 0.044842757284641266

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.491031676836239, Training Loss Force: 2.0929346090069294, time: 3.094184637069702
Validation Loss Energy: 1.478957278127784, Validation Loss Force: 2.3144415843125388, time: 0.18803048133850098
Test Loss Energy: 9.796599686887944, Test Loss Force: 11.058331766978135, time: 10.511799335479736

Epoch 2, Batch 100/214, Loss: 0.05447880178689957
Epoch 2, Batch 200/214, Loss: 0.09073947370052338

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5461273623860627, Training Loss Force: 2.060276891785442, time: 3.113227367401123
Validation Loss Energy: 1.1347751855738837, Validation Loss Force: 2.179767889150937, time: 0.18465209007263184
Test Loss Energy: 9.456298003199299, Test Loss Force: 10.887473175333804, time: 10.278974533081055

Epoch 3, Batch 100/214, Loss: 0.08140992373228073
Epoch 3, Batch 200/214, Loss: 0.09634875506162643

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.689510186568967, Training Loss Force: 2.0542861162818338, time: 3.0746257305145264
Validation Loss Energy: 3.375097503621591, Validation Loss Force: 2.417516300975528, time: 0.19701576232910156
Test Loss Energy: 10.77872182560345, Test Loss Force: 11.079361984129271, time: 10.221461296081543

Epoch 4, Batch 100/214, Loss: 0.2680516541004181
Epoch 4, Batch 200/214, Loss: 0.21965469419956207

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.528675274865872, Training Loss Force: 2.127731012910317, time: 3.051652193069458
Validation Loss Energy: 1.829727761038682, Validation Loss Force: 2.237439086611661, time: 0.25747060775756836
Test Loss Energy: 9.294957713136519, Test Loss Force: 10.922313062448147, time: 10.426567316055298

Epoch 5, Batch 100/214, Loss: 0.19547301530838013
Epoch 5, Batch 200/214, Loss: 0.12592732906341553

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.3315030981849063, Training Loss Force: 2.0239591384008637, time: 3.1063976287841797
Validation Loss Energy: 4.563366669145879, Validation Loss Force: 2.36688907494437, time: 0.18465757369995117
Test Loss Energy: 9.652148769937295, Test Loss Force: 10.858060609661004, time: 10.322955846786499

Epoch 6, Batch 100/214, Loss: 0.29045116901397705
Epoch 6, Batch 200/214, Loss: 0.27714893221855164

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 5.981813825895148, Training Loss Force: 3.113033160635999, time: 3.080549955368042
Validation Loss Energy: 5.348849783603253, Validation Loss Force: 3.2387607482763414, time: 0.19022178649902344
Test Loss Energy: 9.272882037531685, Test Loss Force: 11.172310133846489, time: 10.46558666229248

Epoch 7, Batch 100/214, Loss: 0.17668867111206055
Epoch 7, Batch 200/214, Loss: 0.4208548963069916

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.083923614661792, Training Loss Force: 4.752666032098293, time: 3.061738967895508
Validation Loss Energy: 5.333825405650922, Validation Loss Force: 3.4628773549872327, time: 0.18981409072875977
Test Loss Energy: 11.208381220456443, Test Loss Force: 11.319493436710696, time: 10.25119686126709

Epoch 8, Batch 100/214, Loss: 0.8044577836990356
Epoch 8, Batch 200/214, Loss: 0.834915816783905

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.136430929103595, Training Loss Force: 4.1177088044049865, time: 3.045496940612793
Validation Loss Energy: 3.87111167471671, Validation Loss Force: 5.063854764388266, time: 0.18549752235412598
Test Loss Energy: 13.240977347941483, Test Loss Force: 11.89424467192281, time: 10.36456036567688

Epoch 9, Batch 100/214, Loss: 0.27799198031425476
Epoch 9, Batch 200/214, Loss: 0.33207666873931885

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.95122109010131, Training Loss Force: 3.8621917464277473, time: 3.0698699951171875
Validation Loss Energy: 3.6647719706055364, Validation Loss Force: 6.231139422598461, time: 0.1864314079284668
Test Loss Energy: 12.653309483606561, Test Loss Force: 12.987575271259125, time: 10.309200048446655

Epoch 10, Batch 100/214, Loss: 0.4621237516403198
Epoch 10, Batch 200/214, Loss: 0.6874992847442627

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.503126689561556, Training Loss Force: 5.000733194353704, time: 3.0940256118774414
Validation Loss Energy: 4.325343594929787, Validation Loss Force: 3.7560097624728637, time: 0.18889379501342773
Test Loss Energy: 9.473514907469596, Test Loss Force: 11.075290057939643, time: 10.48262643814087

Epoch 11, Batch 100/214, Loss: 1.6726841926574707
Epoch 11, Batch 200/214, Loss: 0.5156819224357605

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.209434684337419, Training Loss Force: 4.151150655185899, time: 3.0785627365112305
Validation Loss Energy: 1.3380035615426513, Validation Loss Force: 3.273849898635512, time: 0.18712496757507324
Test Loss Energy: 9.22185966207639, Test Loss Force: 10.99413340763796, time: 10.33853793144226

Epoch 12, Batch 100/214, Loss: 0.2499551624059677
Epoch 12, Batch 200/214, Loss: 0.41853970289230347

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 5.7251826563728905, Training Loss Force: 2.7647379147741846, time: 3.0882160663604736
Validation Loss Energy: 9.417506065170707, Validation Loss Force: 3.2765091805780413, time: 0.1889019012451172
Test Loss Energy: 15.278160492475234, Test Loss Force: 11.361215465206675, time: 11.314974308013916

Epoch 13, Batch 100/214, Loss: 0.2185175120830536
Epoch 13, Batch 200/214, Loss: 0.3454151749610901

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 5.924984357345412, Training Loss Force: 2.7123912682932185, time: 3.3037631511688232
Validation Loss Energy: 9.781335425482832, Validation Loss Force: 2.701954702992982, time: 0.1826004981994629
Test Loss Energy: 10.40836824299434, Test Loss Force: 11.050665555072374, time: 10.323681354522705

Epoch 14, Batch 100/214, Loss: 1.14993417263031
Epoch 14, Batch 200/214, Loss: 0.8235762119293213

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.999180101931605, Training Loss Force: 3.717031637731757, time: 3.0894083976745605
Validation Loss Energy: 24.613255536267793, Validation Loss Force: 5.370331789456636, time: 0.19437456130981445
Test Loss Energy: 26.52946454133871, Test Loss Force: 12.27064042727315, time: 10.315338134765625

Epoch 15, Batch 100/214, Loss: 1.5254385471343994
Epoch 15, Batch 200/214, Loss: 0.8012884259223938

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 11.068425407172025, Training Loss Force: 4.895181138291121, time: 3.0688467025756836
Validation Loss Energy: 18.66083052695428, Validation Loss Force: 6.2500474844594915, time: 0.18880772590637207
Test Loss Energy: 13.036343908531657, Test Loss Force: 12.086919766275797, time: 10.363902568817139

Epoch 16, Batch 100/214, Loss: 0.22415947914123535
Epoch 16, Batch 200/214, Loss: 1.445469617843628

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.389272525937008, Training Loss Force: 5.268712643832162, time: 3.076481342315674
Validation Loss Energy: 5.629337867970088, Validation Loss Force: 5.84730613422362, time: 0.1874401569366455
Test Loss Energy: 9.91575980960152, Test Loss Force: 12.005205752951527, time: 10.391557455062866

Epoch 17, Batch 100/214, Loss: 0.43916869163513184
Epoch 17, Batch 200/214, Loss: 0.256650447845459

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.589001680675563, Training Loss Force: 4.453899075000608, time: 2.9881794452667236
Validation Loss Energy: 4.344536116624742, Validation Loss Force: 3.169367467631968, time: 0.18782949447631836
Test Loss Energy: 9.325392057390278, Test Loss Force: 10.949845325914627, time: 10.426397323608398

Epoch 18, Batch 100/214, Loss: 0.39427125453948975
Epoch 18, Batch 200/214, Loss: 0.24892094731330872

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 5.597872590847994, Training Loss Force: 2.730572513089701, time: 3.0991413593292236
Validation Loss Energy: 7.826406537419462, Validation Loss Force: 3.0923549498195233, time: 0.2050180435180664
Test Loss Energy: 14.672495985484874, Test Loss Force: 11.328151074245147, time: 10.163052558898926

Epoch 19, Batch 100/214, Loss: 1.9582780599594116
Epoch 19, Batch 200/214, Loss: 0.5403569936752319

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.340680423895057, Training Loss Force: 4.026416810289296, time: 3.0303118228912354
Validation Loss Energy: 7.5219071483706506, Validation Loss Force: 4.220536901982485, time: 0.1869492530822754
Test Loss Energy: 10.249695687849535, Test Loss Force: 11.187843482170901, time: 10.460707664489746

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–â–‚â–â–â–â–‚â–ƒâ–‚â–â–â–ƒâ–â–ˆâ–ƒâ–â–â–ƒâ–
wandb:   test_error_force â–â–‚â–â–‚â–â–â–‚â–ƒâ–„â–ˆâ–‚â–â–ƒâ–‚â–†â–…â–…â–â–ƒâ–‚
wandb:          test_loss â–‚â–â–â–‚â–â–â–â–‚â–„â–…â–â–â–ƒâ–‚â–ˆâ–„â–ƒâ–â–ƒâ–‚
wandb: train_error_energy â–‚â–â–â–â–‚â–â–„â–ˆâ–†â–†â–†â–†â–„â–„â–‡â–ˆâ–ˆâ–…â–„â–†
wandb:  train_error_force â–â–â–â–â–â–â–ƒâ–‡â–†â–…â–‡â–†â–ƒâ–‚â–…â–‡â–ˆâ–†â–ƒâ–…
wandb:         train_loss â–‚â–â–â–â–â–â–„â–‡â–†â–†â–‡â–†â–ƒâ–ƒâ–…â–ˆâ–ˆâ–†â–ƒâ–†
wandb: valid_error_energy â–‚â–â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–„â–ˆâ–†â–‚â–‚â–ƒâ–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–†â–ˆâ–„â–ƒâ–ƒâ–‚â–†â–ˆâ–‡â–ƒâ–ƒâ–…
wandb:         valid_loss â–‚â–â–â–‚â–â–‚â–ƒâ–ƒâ–„â–…â–ƒâ–‚â–ƒâ–ƒâ–ˆâ–ˆâ–…â–‚â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 6841
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.2497
wandb:   test_error_force 11.18784
wandb:          test_loss 4.4294
wandb: train_error_energy 8.34068
wandb:  train_error_force 4.02642
wandb:         train_loss 1.90542
wandb: valid_error_energy 7.52191
wandb:  valid_error_force 4.22054
wandb:         valid_loss 1.91558
wandb: 
wandb: ğŸš€ View run al_77_67 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/5b90ngu0
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_215301-5b90ngu0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8741015195846558, Uncertainty Bias: -0.7767733931541443
0.0001487732 0.16985703
-9.170058 61.72898
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 10 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 7 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 5 steps.
Found uncertainty sample 6 after 12 steps.
Found uncertainty sample 7 after 5 steps.
Found uncertainty sample 8 after 5 steps.
Found uncertainty sample 9 after 4 steps.
Found uncertainty sample 10 after 37 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 20 steps.
Found uncertainty sample 16 after 13 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 2 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 28 after 6 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 6 steps.
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 6 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 17 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 7 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 3 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 7 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 2 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 5 steps.
Found uncertainty sample 53 after 3 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 11 steps.
Found uncertainty sample 56 after 9 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 3 steps.
Found uncertainty sample 62 after 4 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 10 steps.
Found uncertainty sample 66 after 4 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 3 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 3 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 5 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 4 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 7 steps.
Found uncertainty sample 80 after 6 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 6 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 3 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 4 steps.
Found uncertainty sample 87 after 8 steps.
Found uncertainty sample 88 after 6 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 11 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_220233-xmw0301j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_68
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/xmw0301j
Training model 68. Added 100 samples to the dataset.
Epoch 0, Batch 100/217, Loss: 0.06937358528375626
Epoch 0, Batch 200/217, Loss: 0.26529639959335327

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.823369513353221, Training Loss Force: 2.123508676506889, time: 3.1000313758850098
Validation Loss Energy: 1.196177816914779, Validation Loss Force: 2.2915848191521975, time: 0.1972343921661377
Test Loss Energy: 9.839332824498788, Test Loss Force: 10.873757166308733, time: 10.203068733215332

Epoch 1, Batch 100/217, Loss: 0.21835678815841675
Epoch 1, Batch 200/217, Loss: 0.03537070006132126

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.241827304173311, Training Loss Force: 2.0817207996374747, time: 3.091130495071411
Validation Loss Energy: 4.528856736902553, Validation Loss Force: 2.2085898493961116, time: 0.1870589256286621
Test Loss Energy: 11.593029751870036, Test Loss Force: 11.017398462908018, time: 10.20037031173706

Epoch 2, Batch 100/217, Loss: 0.12964969873428345
Epoch 2, Batch 200/217, Loss: 0.07789023220539093

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.4869935764018962, Training Loss Force: 2.0131235257884774, time: 3.349998950958252
Validation Loss Energy: 1.2405666606526944, Validation Loss Force: 2.1316898351696474, time: 0.2017683982849121
Test Loss Energy: 10.18445024725226, Test Loss Force: 10.910946634934765, time: 10.24372410774231

Epoch 3, Batch 100/217, Loss: 0.06551335752010345
Epoch 3, Batch 200/217, Loss: 0.07328826189041138

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.2337887584582758, Training Loss Force: 2.0047735959700757, time: 3.104159116744995
Validation Loss Energy: 0.974705517646353, Validation Loss Force: 2.2177071489106974, time: 0.19398999214172363
Test Loss Energy: 10.07791885994291, Test Loss Force: 10.99346981627727, time: 10.20887017250061

Epoch 4, Batch 100/217, Loss: 0.04128386825323105
Epoch 4, Batch 200/217, Loss: 0.09646829217672348

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.2470466015157027, Training Loss Force: 1.97943596457263, time: 3.0929031372070312
Validation Loss Energy: 1.2773151693209623, Validation Loss Force: 2.145059280362157, time: 0.18701791763305664
Test Loss Energy: 10.30735330074365, Test Loss Force: 10.92962118972466, time: 10.436246156692505

Epoch 5, Batch 100/217, Loss: 0.09504140913486481
Epoch 5, Batch 200/217, Loss: 0.145912766456604

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.470724282190466, Training Loss Force: 1.9518073036723593, time: 3.140718936920166
Validation Loss Energy: 2.0489484299034864, Validation Loss Force: 2.2072640216339705, time: 0.1864032745361328
Test Loss Energy: 10.70566254462247, Test Loss Force: 10.908376468740151, time: 10.243088960647583

Epoch 6, Batch 100/217, Loss: 0.5578826665878296
Epoch 6, Batch 200/217, Loss: 2.608611822128296

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.845089599173132, Training Loss Force: 4.776802510064679, time: 3.1253554821014404
Validation Loss Energy: 7.520186917883786, Validation Loss Force: 4.52523624792434, time: 0.1950070858001709
Test Loss Energy: 10.475250919691923, Test Loss Force: 11.461055213332283, time: 10.343096733093262

Epoch 7, Batch 100/217, Loss: 0.07261025160551071
Epoch 7, Batch 200/217, Loss: 0.35663464665412903

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 6.463033255623615, Training Loss Force: 3.7891781864799197, time: 3.1372885704040527
Validation Loss Energy: 6.776591209812619, Validation Loss Force: 2.8998329238916125, time: 0.19043636322021484
Test Loss Energy: 13.32200234598499, Test Loss Force: 11.381495504176145, time: 11.236198425292969

Epoch 8, Batch 100/217, Loss: 0.19969934225082397
Epoch 8, Batch 200/217, Loss: 0.3724730610847473

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 5.754435317110749, Training Loss Force: 2.7678806853878526, time: 3.0880188941955566
Validation Loss Energy: 6.787813846101952, Validation Loss Force: 2.801225117297794, time: 0.1946268081665039
Test Loss Energy: 13.856680432882214, Test Loss Force: 11.329502557048613, time: 10.547726154327393

Epoch 9, Batch 100/217, Loss: 0.24685746431350708
Epoch 9, Batch 200/217, Loss: 1.135171890258789

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.226577786215367, Training Loss Force: 4.392112282047049, time: 3.112860679626465
Validation Loss Energy: 13.745461012509363, Validation Loss Force: 4.234701777031535, time: 0.1972339153289795
Test Loss Energy: 18.57456712883277, Test Loss Force: 11.62425202780086, time: 10.267590761184692

Epoch 10, Batch 100/217, Loss: 0.39165204763412476
Epoch 10, Batch 200/217, Loss: 0.19978147745132446

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.888310548104762, Training Loss Force: 4.016451253008537, time: 3.1592981815338135
Validation Loss Energy: 7.63364386181209, Validation Loss Force: 4.9128343148839555, time: 0.1892554759979248
Test Loss Energy: 10.180024097586315, Test Loss Force: 11.53443071714308, time: 10.360554456710815

Epoch 11, Batch 100/217, Loss: 0.6305936574935913
Epoch 11, Batch 200/217, Loss: 0.12155404686927795

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.59294679097627, Training Loss Force: 4.492235091030068, time: 3.0913808345794678
Validation Loss Energy: 33.23216405362799, Validation Loss Force: 9.012635886565194, time: 0.18830394744873047
Test Loss Energy: 28.413131574570276, Test Loss Force: 16.164402896441505, time: 10.143382549285889

Epoch 12, Batch 100/217, Loss: 0.18648989498615265
Epoch 12, Batch 200/217, Loss: 1.173750877380371

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.320957747569265, Training Loss Force: 4.58586590205193, time: 3.10479998588562
Validation Loss Energy: 2.6830239845463524, Validation Loss Force: 4.365389638813744, time: 0.18875765800476074
Test Loss Energy: 9.397069064272076, Test Loss Force: 11.218082558407561, time: 10.197048664093018

Epoch 13, Batch 100/217, Loss: 1.2544314861297607
Epoch 13, Batch 200/217, Loss: 0.7945336103439331

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.973990817915654, Training Loss Force: 4.467601145953071, time: 3.274435520172119
Validation Loss Energy: 8.806923218768953, Validation Loss Force: 4.374886255218417, time: 0.1961071491241455
Test Loss Energy: 10.292373687450297, Test Loss Force: 11.108038090837733, time: 10.219721555709839

Epoch 14, Batch 100/217, Loss: 1.3285105228424072
Epoch 14, Batch 200/217, Loss: 0.5240644216537476

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 12.157556670536986, Training Loss Force: 5.0001351987599545, time: 3.1521542072296143
Validation Loss Energy: 4.021295667574344, Validation Loss Force: 3.2578138234186578, time: 0.1920475959777832
Test Loss Energy: 9.426598961429287, Test Loss Force: 10.964221881212382, time: 10.125039100646973

Epoch 15, Batch 100/217, Loss: 0.47339773178100586
Epoch 15, Batch 200/217, Loss: 0.49572792649269104

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.521107962698864, Training Loss Force: 2.7916612401887675, time: 3.0100035667419434
Validation Loss Energy: 3.727619502034357, Validation Loss Force: 3.0581685001779437, time: 0.18375062942504883
Test Loss Energy: 9.065953361871463, Test Loss Force: 11.034176342897423, time: 10.344980478286743

Epoch 16, Batch 100/217, Loss: 0.2854458689689636
Epoch 16, Batch 200/217, Loss: 1.144545555114746

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 12.707341720926719, Training Loss Force: 5.196069674242183, time: 3.1693217754364014
Validation Loss Energy: 6.199266227046639, Validation Loss Force: 4.6415502596640446, time: 0.18548583984375
Test Loss Energy: 13.493613638617436, Test Loss Force: 11.789242371125653, time: 10.279828071594238

Epoch 17, Batch 100/217, Loss: 0.29915863275527954
Epoch 17, Batch 200/217, Loss: 0.20104753971099854

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.604087183457401, Training Loss Force: 4.06493695365464, time: 3.131828784942627
Validation Loss Energy: 8.3435200017734, Validation Loss Force: 2.7744127460946184, time: 0.187347412109375
Test Loss Energy: 9.968599263215378, Test Loss Force: 10.927232101343769, time: 10.371599912643433

Epoch 18, Batch 100/217, Loss: 1.631719946861267
Epoch 18, Batch 200/217, Loss: 1.0099821090698242

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.349952926679743, Training Loss Force: 4.241292356882958, time: 3.108018159866333
Validation Loss Energy: 15.168892900068442, Validation Loss Force: 4.803141258863841, time: 0.19359922409057617
Test Loss Energy: 12.813901302229192, Test Loss Force: 11.261822230506418, time: 10.162120819091797

Epoch 19, Batch 100/217, Loss: 0.6616392731666565
Epoch 19, Batch 200/217, Loss: 0.7115407586097717

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.49097220580132, Training Loss Force: 5.679607687190748, time: 3.159501075744629
Validation Loss Energy: 5.27714864859591, Validation Loss Force: 4.560479318330634, time: 0.1906280517578125
Test Loss Energy: 9.598359883064568, Test Loss Force: 11.207917807492134, time: 10.434258699417114

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–â–â–‚â–‚â–ƒâ–ƒâ–„â–â–ˆâ–â–â–â–â–ƒâ–â–‚â–
wandb:   test_error_force â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ˆâ–â–â–â–â–‚â–â–‚â–
wandb:          test_loss â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–‚â–ˆâ–â–â–â–â–‚â–â–‚â–
wandb: train_error_energy â–â–‚â–â–â–â–â–‡â–„â–„â–†â–…â–‡â–†â–†â–ˆâ–„â–ˆâ–„â–‡â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–†â–„â–ƒâ–†â–…â–†â–†â–†â–‡â–ƒâ–‡â–…â–…â–ˆ
wandb:         train_loss â–â–â–â–â–â–â–‡â–„â–ƒâ–†â–…â–†â–†â–†â–‡â–ƒâ–‡â–…â–†â–ˆ
wandb: valid_error_energy â–â–‚â–â–â–â–â–‚â–‚â–‚â–„â–‚â–ˆâ–â–ƒâ–‚â–‚â–‚â–ƒâ–„â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–‚â–‚â–ƒâ–„â–ˆâ–ƒâ–ƒâ–‚â–‚â–„â–‚â–„â–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–ˆâ–‚â–ƒâ–‚â–‚â–ƒâ–‚â–„â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 6931
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.59836
wandb:   test_error_force 11.20792
wandb:          test_loss 4.39253
wandb: train_error_energy 12.49097
wandb:  train_error_force 5.67961
wandb:         train_loss 2.73632
wandb: valid_error_energy 5.27715
wandb:  valid_error_force 4.56048
wandb:         valid_loss 1.8791
wandb: 
wandb: ğŸš€ View run al_77_68 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/xmw0301j
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_220233-xmw0301j/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7423743605613708, Uncertainty Bias: -0.6324093341827393
6.2942505e-05 0.011268616
-0.6347104 87.17522
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 7 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 9 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 5 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 7 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 3 steps.
Found uncertainty sample 22 after 12 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 29 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 9 steps.
Found uncertainty sample 27 after 25 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 20 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 10 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 8 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 15 steps.
Found uncertainty sample 40 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 7 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 5 steps.
Found uncertainty sample 48 after 11 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 3 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 4 steps.
Found uncertainty sample 56 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 4 steps.
Found uncertainty sample 60 after 4 steps.
Found uncertainty sample 61 after 9 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 5 steps.
Found uncertainty sample 65 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 7 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 8 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 2 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 6 steps.
Found uncertainty sample 75 after 11 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 16 steps.
Found uncertainty sample 78 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 4 steps.
Found uncertainty sample 81 after 4 steps.
Found uncertainty sample 82 after 5 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 6 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 9 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 2 steps.
Found uncertainty sample 89 after 5 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 8 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 8 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 3 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_221205-vz1kfe8n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_69
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/vz1kfe8n
Training model 69. Added 100 samples to the dataset.
Epoch 0, Batch 100/220, Loss: 0.058435551822185516
Epoch 0, Batch 200/220, Loss: 0.24888893961906433

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4990705822871109, Training Loss Force: 2.088456974254286, time: 3.2150557041168213
Validation Loss Energy: 0.7806261993791095, Validation Loss Force: 2.180711505324093, time: 0.19074130058288574
Test Loss Energy: 9.605328233545691, Test Loss Force: 10.854688382207911, time: 10.179543733596802

Epoch 1, Batch 100/220, Loss: 0.17692650854587555
Epoch 1, Batch 200/220, Loss: 0.15138506889343262

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.314668479617643, Training Loss Force: 2.059584936224259, time: 3.1952295303344727
Validation Loss Energy: 2.1092826390149337, Validation Loss Force: 2.1910374529627754, time: 0.19800710678100586
Test Loss Energy: 9.521821665979894, Test Loss Force: 10.85338800372215, time: 10.105815410614014

Epoch 2, Batch 100/220, Loss: 0.19807852804660797
Epoch 2, Batch 200/220, Loss: 0.22911906242370605

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.0878496471951653, Training Loss Force: 2.0190589386115145, time: 3.2720329761505127
Validation Loss Energy: 2.3493295530983644, Validation Loss Force: 2.177062626029774, time: 0.2019946575164795
Test Loss Energy: 9.461439207612203, Test Loss Force: 10.794744257963341, time: 11.079856634140015

Epoch 3, Batch 100/220, Loss: 0.21417434513568878
Epoch 3, Batch 200/220, Loss: 0.19737356901168823

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.1863131348305807, Training Loss Force: 1.996689229313584, time: 3.1776771545410156
Validation Loss Energy: 2.732537141173147, Validation Loss Force: 2.156238774504467, time: 0.22859716415405273
Test Loss Energy: 11.103702286554682, Test Loss Force: 10.918319949478104, time: 10.130974054336548

Epoch 4, Batch 100/220, Loss: 0.18303519487380981
Epoch 4, Batch 200/220, Loss: 0.08011888712644577

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.875604088314417, Training Loss Force: 1.9753109058055385, time: 3.3901021480560303
Validation Loss Energy: 1.1947541817302896, Validation Loss Force: 2.120635982092927, time: 0.1861429214477539
Test Loss Energy: 9.74202294761303, Test Loss Force: 10.79728779246178, time: 10.202930450439453

Epoch 5, Batch 100/220, Loss: 0.07245190441608429
Epoch 5, Batch 200/220, Loss: 0.06765461713075638

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5020517178041577, Training Loss Force: 1.9497563116371104, time: 3.2486138343811035
Validation Loss Energy: 1.2553256726503665, Validation Loss Force: 2.1086672928301624, time: 0.18970751762390137
Test Loss Energy: 9.58241164732836, Test Loss Force: 10.851944207653498, time: 10.121341705322266

Epoch 6, Batch 100/220, Loss: 0.19284206628799438
Epoch 6, Batch 200/220, Loss: 0.3591326177120209

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.96065809726618, Training Loss Force: 5.396230782956673, time: 3.102386236190796
Validation Loss Energy: 7.983416564280086, Validation Loss Force: 4.550401090116797, time: 0.18662738800048828
Test Loss Energy: 10.466900043297297, Test Loss Force: 11.551477586123998, time: 10.207352876663208

Epoch 7, Batch 100/220, Loss: 0.49374645948410034
Epoch 7, Batch 200/220, Loss: 0.6771633625030518

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.071560566928445, Training Loss Force: 5.609699029251168, time: 3.2023589611053467
Validation Loss Energy: 11.399507736003414, Validation Loss Force: 3.915841786754403, time: 0.19328570365905762
Test Loss Energy: 10.869041257632936, Test Loss Force: 11.27681928427319, time: 10.102571725845337

Epoch 8, Batch 100/220, Loss: 0.33755427598953247
Epoch 8, Batch 200/220, Loss: 1.1128902435302734

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.680152982758212, Training Loss Force: 4.270296665700528, time: 3.099842071533203
Validation Loss Energy: 1.8051204823876295, Validation Loss Force: 6.309048128055979, time: 0.19476604461669922
Test Loss Energy: 9.516847237259865, Test Loss Force: 12.722990512243637, time: 10.208884954452515

Epoch 9, Batch 100/220, Loss: 0.8433184623718262
Epoch 9, Batch 200/220, Loss: 2.3196842670440674

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.42624835223448, Training Loss Force: 5.162659979325539, time: 3.1532835960388184
Validation Loss Energy: 8.55987625474499, Validation Loss Force: 7.875758926922898, time: 0.18880128860473633
Test Loss Energy: 13.560583070012205, Test Loss Force: 13.664129100605313, time: 10.13422155380249

Epoch 10, Batch 100/220, Loss: 0.695307731628418
Epoch 10, Batch 200/220, Loss: 0.2959970235824585

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.952616448631225, Training Loss Force: 4.4738945616230295, time: 3.1284008026123047
Validation Loss Energy: 3.6095036515658054, Validation Loss Force: 3.616626670542176, time: 0.19091224670410156
Test Loss Energy: 10.031601117477654, Test Loss Force: 11.209538927475826, time: 10.155702352523804

Epoch 11, Batch 100/220, Loss: 0.4466670751571655
Epoch 11, Batch 200/220, Loss: 1.0677721500396729

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.671006728503496, Training Loss Force: 4.256596532535135, time: 3.170220136642456
Validation Loss Energy: 2.3968850582614483, Validation Loss Force: 3.7751920970543917, time: 0.19192910194396973
Test Loss Energy: 9.403717764450667, Test Loss Force: 11.444342296774751, time: 10.04825234413147

Epoch 12, Batch 100/220, Loss: 0.07811141014099121
Epoch 12, Batch 200/220, Loss: 0.33055204153060913

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.808193078734131, Training Loss Force: 4.2557140390425365, time: 3.2175378799438477
Validation Loss Energy: 2.962165978258767, Validation Loss Force: 3.7266159321936607, time: 0.18800020217895508
Test Loss Energy: 10.295286904026488, Test Loss Force: 11.082534919199338, time: 10.07654881477356

Epoch 13, Batch 100/220, Loss: 0.9861919283866882
Epoch 13, Batch 200/220, Loss: 0.5194056034088135

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.92345835937468, Training Loss Force: 4.9945335749041515, time: 3.3385767936706543
Validation Loss Energy: 10.963958202471972, Validation Loss Force: 3.887983649907377, time: 0.18661093711853027
Test Loss Energy: 10.489697982999866, Test Loss Force: 11.262965369600947, time: 10.046741247177124

Epoch 14, Batch 100/220, Loss: 0.513770580291748
Epoch 14, Batch 200/220, Loss: 1.1287423372268677

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.915370293838057, Training Loss Force: 4.809610389650545, time: 3.1842970848083496
Validation Loss Energy: 1.5470869392026607, Validation Loss Force: 5.36200184804205, time: 0.20148658752441406
Test Loss Energy: 9.750971136906127, Test Loss Force: 12.28484442447507, time: 10.094887971878052

Epoch 15, Batch 100/220, Loss: 0.7793882489204407
Epoch 15, Batch 200/220, Loss: 1.2726562023162842

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.587712025328273, Training Loss Force: 4.366975588672156, time: 3.0919530391693115
Validation Loss Energy: 11.60421950598871, Validation Loss Force: 5.722065316423672, time: 0.19033265113830566
Test Loss Energy: 14.688302440431283, Test Loss Force: 12.808267632940375, time: 10.224737405776978

Epoch 16, Batch 100/220, Loss: 0.19682563841342926
Epoch 16, Batch 200/220, Loss: 1.0610029697418213

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.061699303642037, Training Loss Force: 4.916366915338137, time: 3.168447732925415
Validation Loss Energy: 1.7466573064421267, Validation Loss Force: 4.465109343787752, time: 0.18938970565795898
Test Loss Energy: 10.324984290350493, Test Loss Force: 11.415664250163367, time: 10.154345512390137

Epoch 17, Batch 100/220, Loss: 0.08135516941547394
Epoch 17, Batch 200/220, Loss: 0.2172887921333313

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.157552981358485, Training Loss Force: 4.937220925374994, time: 3.1346962451934814
Validation Loss Energy: 1.5226376721442867, Validation Loss Force: 7.302339338307345, time: 0.19239568710327148
Test Loss Energy: 8.5935199282332, Test Loss Force: 13.144888616631366, time: 10.280327081680298

Epoch 18, Batch 100/220, Loss: 0.8325034379959106
Epoch 18, Batch 200/220, Loss: 0.7757537961006165

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.325996156585235, Training Loss Force: 4.6057154085535466, time: 3.101367712020874
Validation Loss Energy: 13.592194499330077, Validation Loss Force: 4.438831155865657, time: 0.1840071678161621
Test Loss Energy: 19.215823510451674, Test Loss Force: 12.660335513557131, time: 11.122504234313965

Epoch 19, Batch 100/220, Loss: 0.08616030961275101
Epoch 19, Batch 200/220, Loss: 0.4437490999698639

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.064514493113982, Training Loss Force: 3.8604007270640563, time: 3.147791624069214
Validation Loss Energy: 5.340434225902058, Validation Loss Force: 2.950198235339873, time: 0.1857283115386963
Test Loss Energy: 12.516636845261461, Test Loss Force: 11.404969504711579, time: 10.188652515411377

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–„â–‚â–‚â–‚â–‚â–‚â–…â–‚â–â–ˆâ–„
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–‚â–†â–ˆâ–‚â–ƒâ–‚â–‚â–…â–†â–ƒâ–‡â–†â–‚
wandb:          test_loss â–â–â–â–‚â–â–â–ƒâ–‚â–…â–ˆâ–‚â–‚â–‚â–‚â–„â–‡â–‚â–…â–ˆâ–ƒ
wandb: train_error_energy â–â–‚â–â–‚â–â–â–‡â–‡â–†â–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–…â–‡â–†â–…â–…â–‡â–†â–†â–‡â–‡â–†â–…
wandb:         train_loss â–â–â–â–â–â–â–‡â–ˆâ–†â–ˆâ–†â–†â–†â–‡â–‡â–†â–‡â–‡â–†â–…
wandb: valid_error_energy â–â–‚â–‚â–‚â–â–â–…â–‡â–‚â–…â–ƒâ–‚â–‚â–‡â–â–‡â–‚â–â–ˆâ–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–„â–ƒâ–†â–ˆâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–„â–‡â–„â–‚
wandb:         valid_loss â–â–â–â–â–â–â–…â–…â–…â–ˆâ–ƒâ–ƒâ–ƒâ–…â–„â–‡â–ƒâ–†â–†â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7021
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.51664
wandb:   test_error_force 11.40497
wandb:          test_loss 4.65376
wandb: train_error_energy 8.06451
wandb:  train_error_force 3.8604
wandb:         train_loss 1.83138
wandb: valid_error_energy 5.34043
wandb:  valid_error_force 2.9502
wandb:         valid_loss 1.34453
wandb: 
wandb: ğŸš€ View run al_77_69 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/vz1kfe8n
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_221205-vz1kfe8n/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6823278069496155, Uncertainty Bias: -0.6382156014442444
7.6293945e-05 0.003911972
-3.4502575 17.109238
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 25 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 12 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 5 steps.
Found uncertainty sample 7 after 18 steps.
Found uncertainty sample 8 after 13 steps.
Found uncertainty sample 9 after 19 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 4 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 12 steps.
Found uncertainty sample 16 after 5 steps.
Found uncertainty sample 17 after 25 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 31 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 51 steps.
Found uncertainty sample 22 after 16 steps.
Found uncertainty sample 23 after 9 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 5 steps.
Found uncertainty sample 27 after 10 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 46 steps.
Found uncertainty sample 31 after 31 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 30 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 6 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 4 steps.
Found uncertainty sample 38 after 10 steps.
Found uncertainty sample 39 after 8 steps.
Found uncertainty sample 40 after 3 steps.
Found uncertainty sample 41 after 7 steps.
Found uncertainty sample 42 after 25 steps.
Found uncertainty sample 43 after 42 steps.
Found uncertainty sample 44 after 31 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 7 steps.
Found uncertainty sample 47 after 24 steps.
Found uncertainty sample 48 after 33 steps.
Found uncertainty sample 49 after 6 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 12 steps.
Found uncertainty sample 52 after 4 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 3 steps.
Found uncertainty sample 55 after 15 steps.
Found uncertainty sample 56 after 34 steps.
Found uncertainty sample 57 after 5 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 4 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 26 steps.
Found uncertainty sample 63 after 51 steps.
Found uncertainty sample 64 after 7 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 17 steps.
Found uncertainty sample 67 after 3 steps.
Found uncertainty sample 68 after 39 steps.
Found uncertainty sample 69 after 14 steps.
Found uncertainty sample 70 after 26 steps.
Found uncertainty sample 71 after 16 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 17 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 7 steps.
Found uncertainty sample 76 after 23 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 14 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 11 steps.
Found uncertainty sample 82 after 3 steps.
Found uncertainty sample 83 after 9 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 4 steps.
Found uncertainty sample 86 after 12 steps.
Found uncertainty sample 87 after 30 steps.
Found uncertainty sample 88 after 35 steps.
Found uncertainty sample 89 after 10 steps.
Found uncertainty sample 90 after 3 steps.
Found uncertainty sample 91 after 3 steps.
Found uncertainty sample 92 after 12 steps.
Found uncertainty sample 93 after 6 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 18 steps.
Found uncertainty sample 97 after 12 steps.
Found uncertainty sample 98 after 40 steps.
Found uncertainty sample 99 after 32 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_222149-1eo6lo4c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_70
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/1eo6lo4c
Training model 70. Added 100 samples to the dataset.
Epoch 0, Batch 100/223, Loss: 0.06504668295383453
Epoch 0, Batch 200/223, Loss: 0.04800098016858101

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.5515024852163721, Training Loss Force: 2.123190649477524, time: 3.247196912765503
Validation Loss Energy: 0.9200517932830262, Validation Loss Force: 2.3432256908029054, time: 0.19206571578979492
Test Loss Energy: 9.624349545983568, Test Loss Force: 10.948497597009856, time: 10.135242223739624

Epoch 1, Batch 100/223, Loss: 0.24025985598564148
Epoch 1, Batch 200/223, Loss: 0.14308908581733704

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.436789470838994, Training Loss Force: 2.0956849499597174, time: 3.2548046112060547
Validation Loss Energy: 0.9357133111627869, Validation Loss Force: 2.1602433120921187, time: 0.20481276512145996
Test Loss Energy: 9.471927129567442, Test Loss Force: 10.907318722680182, time: 10.05307126045227

Epoch 2, Batch 100/223, Loss: 0.1330440640449524
Epoch 2, Batch 200/223, Loss: 0.15387985110282898

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.2799769480160017, Training Loss Force: 2.041296433910281, time: 3.3626632690429688
Validation Loss Energy: 1.3598264460248068, Validation Loss Force: 2.427589463466296, time: 0.18792223930358887
Test Loss Energy: 10.019314649032669, Test Loss Force: 11.017489862180483, time: 10.133320808410645

Epoch 3, Batch 100/223, Loss: 0.23985324800014496
Epoch 3, Batch 200/223, Loss: 0.08244568854570389

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.844113267261791, Training Loss Force: 2.033046354831841, time: 3.174633741378784
Validation Loss Energy: 0.9131357870282604, Validation Loss Force: 2.1248169293114403, time: 0.1940453052520752
Test Loss Energy: 9.571353678369862, Test Loss Force: 10.928356582862321, time: 10.167770385742188

Epoch 4, Batch 100/223, Loss: 0.05459899827837944
Epoch 4, Batch 200/223, Loss: 0.11733650416135788

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4115744896834466, Training Loss Force: 1.9803505816119442, time: 3.16070294380188
Validation Loss Energy: 0.8237554455089762, Validation Loss Force: 2.2415084765880646, time: 0.21678447723388672
Test Loss Energy: 9.887441293220121, Test Loss Force: 10.954246611247877, time: 10.303904294967651

Epoch 5, Batch 100/223, Loss: 0.07986188679933548
Epoch 5, Batch 200/223, Loss: 0.09958119690418243

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.3484612692799274, Training Loss Force: 1.957271438872449, time: 3.2126877307891846
Validation Loss Energy: 1.8614699436317692, Validation Loss Force: 2.1189104075465597, time: 0.195540189743042
Test Loss Energy: 9.676349968265887, Test Loss Force: 10.93976971726922, time: 10.115407943725586

Epoch 6, Batch 100/223, Loss: 0.680442214012146
Epoch 6, Batch 200/223, Loss: 0.15955573320388794

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.717770833177171, Training Loss Force: 5.263957295035052, time: 3.21472430229187
Validation Loss Energy: 5.562777495534781, Validation Loss Force: 5.317433616473448, time: 0.19484186172485352
Test Loss Energy: 12.651964986318925, Test Loss Force: 12.160853299950816, time: 10.339437007904053

Epoch 7, Batch 100/223, Loss: 0.47518354654312134
Epoch 7, Batch 200/223, Loss: 0.3215547502040863

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.07573368745523, Training Loss Force: 3.6377086722437433, time: 3.201225757598877
Validation Loss Energy: 7.364514464582751, Validation Loss Force: 3.095809383282161, time: 0.20452022552490234
Test Loss Energy: 9.726877328059452, Test Loss Force: 10.830709111571966, time: 10.240490436553955

Epoch 8, Batch 100/223, Loss: 0.40735283493995667
Epoch 8, Batch 200/223, Loss: 0.5518224239349365

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.433009929501143, Training Loss Force: 3.738883088727327, time: 3.2510793209075928
Validation Loss Energy: 5.302102177768881, Validation Loss Force: 5.839075057909827, time: 0.1893932819366455
Test Loss Energy: 9.637487585180692, Test Loss Force: 12.323819349909328, time: 10.261169672012329

Epoch 9, Batch 100/223, Loss: 0.0890512466430664
Epoch 9, Batch 200/223, Loss: 1.1863389015197754

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.409243459704664, Training Loss Force: 4.935268299734548, time: 3.2530758380889893
Validation Loss Energy: 9.74686356966346, Validation Loss Force: 5.515499380460236, time: 0.19260001182556152
Test Loss Energy: 15.897387956678342, Test Loss Force: 12.248780477267472, time: 10.126527070999146

Epoch 10, Batch 100/223, Loss: 0.7577661871910095
Epoch 10, Batch 200/223, Loss: 0.13012105226516724

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.868846639292618, Training Loss Force: 4.19272534093058, time: 3.302821636199951
Validation Loss Energy: 13.824056095609201, Validation Loss Force: 3.6558182215786337, time: 0.19237971305847168
Test Loss Energy: 12.38562601878729, Test Loss Force: 11.272938872201967, time: 10.223252773284912

Epoch 11, Batch 100/223, Loss: 0.36341625452041626
Epoch 11, Batch 200/223, Loss: 0.7024731040000916

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.217582099165071, Training Loss Force: 4.663905167012344, time: 3.179246187210083
Validation Loss Energy: 1.6797920721124011, Validation Loss Force: 4.715981763738753, time: 0.19286012649536133
Test Loss Energy: 9.27058723029828, Test Loss Force: 11.656625514874206, time: 10.162136316299438

Epoch 12, Batch 100/223, Loss: 0.2333313673734665
Epoch 12, Batch 200/223, Loss: 0.21296155452728271

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.271756153985836, Training Loss Force: 4.16816486723415, time: 3.2101502418518066
Validation Loss Energy: 5.536591381304526, Validation Loss Force: 3.0213851716219815, time: 0.1932981014251709
Test Loss Energy: 9.162310107558401, Test Loss Force: 11.143871725868406, time: 10.022796630859375

Epoch 13, Batch 100/223, Loss: 0.5508612990379333
Epoch 13, Batch 200/223, Loss: 0.06450631469488144

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.09879627446878, Training Loss Force: 4.543813954145963, time: 3.410247325897217
Validation Loss Energy: 5.755244466516361, Validation Loss Force: 3.9595531874494756, time: 0.19459128379821777
Test Loss Energy: 11.956026252633125, Test Loss Force: 11.908816942907576, time: 10.174022674560547

Epoch 14, Batch 100/223, Loss: 0.5240533947944641
Epoch 14, Batch 200/223, Loss: 0.18324227631092072

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.03734657053326, Training Loss Force: 4.175365708827393, time: 3.187303066253662
Validation Loss Energy: 11.87448217783625, Validation Loss Force: 5.137034199855423, time: 0.1892242431640625
Test Loss Energy: 11.366510570139432, Test Loss Force: 11.493386931870758, time: 10.10481071472168

Epoch 15, Batch 100/223, Loss: 0.46807557344436646
Epoch 15, Batch 200/223, Loss: 0.4435933828353882

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.026343872522881, Training Loss Force: 4.246256676126506, time: 3.2596356868743896
Validation Loss Energy: 7.148020045406113, Validation Loss Force: 2.7952559698356216, time: 0.19131088256835938
Test Loss Energy: 10.200972493735685, Test Loss Force: 11.081582669350645, time: 11.419183254241943

Epoch 16, Batch 100/223, Loss: 0.3127172589302063
Epoch 16, Batch 200/223, Loss: 0.4565269947052002

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 5.639952210595626, Training Loss Force: 2.696928768802618, time: 3.155426263809204
Validation Loss Energy: 5.198946353869488, Validation Loss Force: 2.776190369553847, time: 0.19213151931762695
Test Loss Energy: 11.526753486029113, Test Loss Force: 11.27992243413344, time: 10.097276449203491

Epoch 17, Batch 100/223, Loss: 0.31626471877098083
Epoch 17, Batch 200/223, Loss: 0.34638890624046326

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 5.725399045485552, Training Loss Force: 2.9275168683133304, time: 3.1180553436279297
Validation Loss Energy: 4.807920963415753, Validation Loss Force: 2.9659355442301782, time: 0.18931913375854492
Test Loss Energy: 12.402849638106176, Test Loss Force: 11.46112439232244, time: 10.259958982467651

Epoch 18, Batch 100/223, Loss: 0.5886348485946655
Epoch 18, Batch 200/223, Loss: 1.0162750482559204

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.051803719555553, Training Loss Force: 4.240450954587103, time: 3.182185173034668
Validation Loss Energy: 10.311705942777873, Validation Loss Force: 4.143835619968762, time: 0.1949937343597412
Test Loss Energy: 10.477774080546416, Test Loss Force: 11.162928105782248, time: 10.122346639633179

Epoch 19, Batch 100/223, Loss: 0.505172610282898
Epoch 19, Batch 200/223, Loss: 0.5888360142707825

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.00692550329159, Training Loss Force: 4.992018207598062, time: 3.2949202060699463
Validation Loss Energy: 1.982531372897227, Validation Loss Force: 3.813309659132753, time: 0.1906752586364746
Test Loss Energy: 8.927615941737159, Test Loss Force: 11.55830771852166, time: 10.293451309204102

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.055 MB uploadedwandb: | 0.058 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–‚â–‚â–‚â–‚â–…â–‚â–‚â–ˆâ–„â–â–â–„â–ƒâ–‚â–„â–„â–ƒâ–
wandb:   test_error_force â–‚â–â–‚â–â–‚â–‚â–‡â–â–ˆâ–ˆâ–ƒâ–…â–‚â–†â–„â–‚â–ƒâ–„â–ƒâ–„
wandb:          test_loss â–â–â–‚â–â–â–â–†â–â–…â–ˆâ–„â–ƒâ–‚â–…â–„â–‚â–ƒâ–„â–‚â–‚
wandb: train_error_energy â–â–‚â–‚â–â–â–â–ˆâ–…â–…â–†â–†â–‡â–…â–†â–‡â–†â–„â–„â–‡â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–…â–…â–‡â–†â–‡â–†â–†â–†â–†â–ƒâ–ƒâ–†â–‡
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–…â–…â–‡â–†â–‡â–…â–†â–†â–†â–ƒâ–ƒâ–†â–‡
wandb: valid_error_energy â–â–â–â–â–â–‚â–„â–…â–ƒâ–†â–ˆâ–â–„â–„â–‡â–„â–ƒâ–ƒâ–†â–‚
wandb:  valid_error_force â–â–â–‚â–â–â–â–‡â–ƒâ–ˆâ–‡â–„â–†â–ƒâ–„â–‡â–‚â–‚â–ƒâ–…â–„
wandb:         valid_loss â–â–â–‚â–â–â–â–‡â–„â–‡â–ˆâ–‡â–…â–ƒâ–…â–ˆâ–„â–ƒâ–ƒâ–†â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 7111
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 8.92762
wandb:   test_error_force 11.55831
wandb:          test_loss 4.46489
wandb: train_error_energy 11.00693
wandb:  train_error_force 4.99202
wandb:         train_loss 2.40694
wandb: valid_error_energy 1.98253
wandb:  valid_error_force 3.81331
wandb:         valid_loss 1.40862
wandb: 
wandb: ğŸš€ View run al_77_70 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/1eo6lo4c
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_222149-1eo6lo4c/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5515758991241455, Uncertainty Bias: -0.45625633001327515
9.918213e-05 0.119766235
-6.6912 31.190771
(48745, 22, 3)
Found uncertainty sample 0 after 18 steps.
Found uncertainty sample 1 after 11 steps.
Found uncertainty sample 2 after 16 steps.
Found uncertainty sample 3 after 7 steps.
Found uncertainty sample 4 after 26 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 19 steps.
Found uncertainty sample 7 after 6 steps.
Found uncertainty sample 8 after 4 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 12 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 21 steps.
Found uncertainty sample 14 after 11 steps.
Found uncertainty sample 15 after 37 steps.
Found uncertainty sample 16 after 14 steps.
Found uncertainty sample 17 after 3 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 9 steps.
Found uncertainty sample 20 after 4 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 9 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 9 steps.
Found uncertainty sample 27 after 9 steps.
Found uncertainty sample 28 after 2 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 17 steps.
Found uncertainty sample 32 after 2 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 9 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 9 steps.
Found uncertainty sample 38 after 8 steps.
Found uncertainty sample 39 after 8 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 4 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 12 steps.
Found uncertainty sample 45 after 6 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 5 steps.
Found uncertainty sample 49 after 17 steps.
Found uncertainty sample 50 after 4 steps.
Found uncertainty sample 51 after 17 steps.
Found uncertainty sample 52 after 22 steps.
Found uncertainty sample 53 after 16 steps.
Found uncertainty sample 54 after 5 steps.
Found uncertainty sample 55 after 24 steps.
Found uncertainty sample 56 after 4 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 17 steps.
Found uncertainty sample 59 after 55 steps.
Found uncertainty sample 60 after 7 steps.
Found uncertainty sample 61 after 9 steps.
Found uncertainty sample 62 after 6 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 14 steps.
Found uncertainty sample 65 after 23 steps.
Found uncertainty sample 66 after 6 steps.
Found uncertainty sample 67 after 12 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 4 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 12 steps.
Found uncertainty sample 74 after 26 steps.
Found uncertainty sample 75 after 27 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 7 steps.
Found uncertainty sample 78 after 6 steps.
Found uncertainty sample 79 after 17 steps.
Found uncertainty sample 80 after 13 steps.
Found uncertainty sample 81 after 15 steps.
Found uncertainty sample 82 after 3 steps.
Found uncertainty sample 83 after 5 steps.
Found uncertainty sample 84 after 8 steps.
Found uncertainty sample 85 after 3 steps.
Found uncertainty sample 86 after 12 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 3 steps.
Found uncertainty sample 89 after 23 steps.
Found uncertainty sample 90 after 11 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 6 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 6 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 8 steps.
Found uncertainty sample 98 after 8 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_223129-4mluhgn9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_71
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/4mluhgn9
Training model 71. Added 100 samples to the dataset.
Epoch 0, Batch 100/226, Loss: 0.07543440908193588
Epoch 0, Batch 200/226, Loss: 0.0635550320148468

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.6980096411392722, Training Loss Force: 2.1111098245864723, time: 3.1882593631744385
Validation Loss Energy: 5.613108404599995, Validation Loss Force: 2.307776039096477, time: 0.20013427734375
Test Loss Energy: 12.739709794107645, Test Loss Force: 11.101276097530338, time: 10.007118225097656

Epoch 1, Batch 100/226, Loss: 0.13637441396713257
Epoch 1, Batch 200/226, Loss: 0.3400171995162964

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.701170556518162, Training Loss Force: 2.040272409895151, time: 3.2467846870422363
Validation Loss Energy: 2.8604979739251766, Validation Loss Force: 2.2549199505818684, time: 0.19556879997253418
Test Loss Energy: 10.898069354131405, Test Loss Force: 11.07427362808141, time: 10.018800735473633

Epoch 2, Batch 100/226, Loss: 0.11932650208473206
Epoch 2, Batch 200/226, Loss: 0.13042213022708893

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.123331013024941, Training Loss Force: 1.9961142058240628, time: 3.4276819229125977
Validation Loss Energy: 1.3417039887442523, Validation Loss Force: 2.184268366593857, time: 0.1870725154876709
Test Loss Energy: 10.361305335317725, Test Loss Force: 10.846265766710859, time: 10.123663902282715

Epoch 3, Batch 100/226, Loss: 0.09969043731689453
Epoch 3, Batch 200/226, Loss: 0.0836278647184372

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.71907831615895, Training Loss Force: 1.9919129351196754, time: 3.1552398204803467
Validation Loss Energy: 1.7263105235619698, Validation Loss Force: 2.1728258445229196, time: 0.19696545600891113
Test Loss Energy: 10.48966167976843, Test Loss Force: 10.934749295129588, time: 10.121268510818481

Epoch 4, Batch 100/226, Loss: 0.05552724003791809
Epoch 4, Batch 200/226, Loss: 0.08700934797525406

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.413638730164581, Training Loss Force: 1.9646962361579632, time: 3.300158977508545
Validation Loss Energy: 2.3835484011006765, Validation Loss Force: 2.109534366484405, time: 0.18898510932922363
Test Loss Energy: 10.671679908072193, Test Loss Force: 10.97875593704215, time: 10.310839653015137

Epoch 5, Batch 100/226, Loss: 0.09871150553226471
Epoch 5, Batch 200/226, Loss: 0.078640416264534

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2964982519624415, Training Loss Force: 1.93861888693814, time: 3.2620182037353516
Validation Loss Energy: 1.113158492720857, Validation Loss Force: 2.106078764008752, time: 0.19709181785583496
Test Loss Energy: 9.634718005359113, Test Loss Force: 10.949624200297228, time: 10.11549186706543

Epoch 6, Batch 100/226, Loss: 0.5782342553138733
Epoch 6, Batch 200/226, Loss: 0.05077657848596573

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.001934201492208, Training Loss Force: 4.075185246849817, time: 3.40051531791687
Validation Loss Energy: 3.6329739258904645, Validation Loss Force: 4.966021512418558, time: 0.1960439682006836
Test Loss Energy: 9.51352430945517, Test Loss Force: 11.709260224791286, time: 10.370661735534668

Epoch 7, Batch 100/226, Loss: 0.22543632984161377
Epoch 7, Batch 200/226, Loss: 0.49970921874046326

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 12.163765486069643, Training Loss Force: 5.243716273435955, time: 3.2616238594055176
Validation Loss Energy: 13.995088489265482, Validation Loss Force: 6.713781921809611, time: 0.19443225860595703
Test Loss Energy: 17.957476388427526, Test Loss Force: 13.15970926047582, time: 10.081932306289673

Epoch 8, Batch 100/226, Loss: 1.2756537199020386
Epoch 8, Batch 200/226, Loss: 0.1409052312374115

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.616268172721991, Training Loss Force: 5.523781229280348, time: 3.18851637840271
Validation Loss Energy: 6.804102762134008, Validation Loss Force: 3.7537314117717306, time: 0.19328641891479492
Test Loss Energy: 9.977641864226346, Test Loss Force: 11.530990299304213, time: 10.360169172286987

Epoch 9, Batch 100/226, Loss: 0.44157707691192627
Epoch 9, Batch 200/226, Loss: 0.23642554879188538

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.236669949283101, Training Loss Force: 3.6640530123290653, time: 3.3002724647521973
Validation Loss Energy: 6.874543589775398, Validation Loss Force: 3.0497140867107846, time: 0.189208984375
Test Loss Energy: 12.78512851747396, Test Loss Force: 11.279218813621991, time: 10.133009910583496

Epoch 10, Batch 100/226, Loss: 0.624534010887146
Epoch 10, Batch 200/226, Loss: 1.2141809463500977

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.857363548125305, Training Loss Force: 4.940021010656146, time: 3.303697347640991
Validation Loss Energy: 3.404455150164581, Validation Loss Force: 5.282583223105997, time: 0.19879889488220215
Test Loss Energy: 10.040536613463626, Test Loss Force: 12.121058499726407, time: 10.265570878982544

Epoch 11, Batch 100/226, Loss: 0.4389406442642212
Epoch 11, Batch 200/226, Loss: 0.5730218291282654

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.928817236050993, Training Loss Force: 4.7247527181462985, time: 3.211388111114502
Validation Loss Energy: 3.1345796442945235, Validation Loss Force: 4.426946301536335, time: 0.18739533424377441
Test Loss Energy: 9.79371112013595, Test Loss Force: 11.324393710776095, time: 10.105756998062134

Epoch 12, Batch 100/226, Loss: 1.0439138412475586
Epoch 12, Batch 200/226, Loss: 0.6099339723587036

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.621054480497868, Training Loss Force: 4.678986558100978, time: 3.2589499950408936
Validation Loss Energy: 10.409826883284122, Validation Loss Force: 6.032834734618092, time: 0.19034028053283691
Test Loss Energy: 10.346615541656194, Test Loss Force: 12.20464579243535, time: 11.061852931976318

Epoch 13, Batch 100/226, Loss: 0.9909587502479553
Epoch 13, Batch 200/226, Loss: 0.2367187738418579

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.877155424716187, Training Loss Force: 3.8351418713338106, time: 3.3800547122955322
Validation Loss Energy: 2.1254778113365536, Validation Loss Force: 3.479555162682651, time: 0.19736099243164062
Test Loss Energy: 9.609690478332562, Test Loss Force: 11.459498436571119, time: 10.230893850326538

Epoch 14, Batch 100/226, Loss: 0.2469356507062912
Epoch 14, Batch 200/226, Loss: 0.478030264377594

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 5.538129342809155, Training Loss Force: 2.708445687914407, time: 3.2729809284210205
Validation Loss Energy: 7.132632828080492, Validation Loss Force: 3.2322159454046586, time: 0.19281983375549316
Test Loss Energy: 9.794737715831625, Test Loss Force: 11.096923943969264, time: 10.040278434753418

Epoch 15, Batch 100/226, Loss: 0.8377377390861511
Epoch 15, Batch 200/226, Loss: 0.39167192578315735

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.618323003422043, Training Loss Force: 3.946160644274123, time: 3.2123863697052
Validation Loss Energy: 2.118306139151798, Validation Loss Force: 2.973647356095669, time: 0.18982768058776855
Test Loss Energy: 10.651134834528715, Test Loss Force: 11.366785015107922, time: 10.183179140090942

Epoch 16, Batch 100/226, Loss: 0.23802849650382996
Epoch 16, Batch 200/226, Loss: 0.3817451000213623

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 5.650933309959658, Training Loss Force: 2.7115499825197973, time: 3.2873692512512207
Validation Loss Energy: 4.008561219118733, Validation Loss Force: 3.2813361466561775, time: 0.1918351650238037
Test Loss Energy: 9.510080622693339, Test Loss Force: 11.483507514561635, time: 10.163697242736816

Epoch 17, Batch 100/226, Loss: 0.30970555543899536
Epoch 17, Batch 200/226, Loss: 0.6548520922660828

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.726131317920417, Training Loss Force: 2.745000016525339, time: 3.3009555339813232
Validation Loss Energy: 49.18344240633024, Validation Loss Force: 7.094079446816263, time: 0.20644307136535645
Test Loss Energy: 33.914062175637845, Test Loss Force: 13.342205390393865, time: 10.31215763092041

Epoch 18, Batch 100/226, Loss: 0.8078561425209045
Epoch 18, Batch 200/226, Loss: 1.8848414421081543

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.674299220353653, Training Loss Force: 5.363639022937484, time: 3.2657182216644287
Validation Loss Energy: 12.123808160568782, Validation Loss Force: 4.551746457096493, time: 0.19086170196533203
Test Loss Energy: 16.34987154054537, Test Loss Force: 11.940240525736003, time: 10.050623893737793

Epoch 19, Batch 100/226, Loss: 0.9307177066802979
Epoch 19, Batch 200/226, Loss: 1.615351676940918

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 12.056337005806727, Training Loss Force: 4.5178258709667345, time: 3.201256275177002
Validation Loss Energy: 8.219489724785289, Validation Loss Force: 7.228550791391504, time: 0.19443607330322266
Test Loss Energy: 10.712698954918983, Test Loss Force: 13.475648178968877, time: 10.188240766525269

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.050 MB uploadedwandb: | 0.039 MB of 0.050 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–â–â–â–â–â–ƒâ–â–‚â–â–â–â–â–â–â–â–ˆâ–ƒâ–
wandb:   test_error_force â–‚â–‚â–â–â–â–â–ƒâ–‡â–ƒâ–‚â–„â–‚â–…â–ƒâ–‚â–‚â–ƒâ–ˆâ–„â–ˆ
wandb:          test_loss â–‚â–â–â–â–â–â–‚â–…â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–ˆâ–ƒâ–„
wandb: train_error_energy â–â–â–‚â–â–â–â–†â–ˆâ–†â–„â–‡â–†â–†â–…â–„â–†â–„â–„â–‡â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–…â–‡â–ˆâ–„â–‡â–†â–†â–…â–ƒâ–…â–ƒâ–ƒâ–ˆâ–†
wandb:         train_loss â–â–â–â–â–â–â–†â–ˆâ–ˆâ–„â–‡â–‡â–‡â–…â–ƒâ–…â–ƒâ–ƒâ–ˆâ–‡
wandb: valid_error_energy â–‚â–â–â–â–â–â–â–ƒâ–‚â–‚â–â–â–‚â–â–‚â–â–â–ˆâ–ƒâ–‚
wandb:  valid_error_force â–â–â–â–â–â–â–…â–‡â–ƒâ–‚â–…â–„â–†â–ƒâ–ƒâ–‚â–ƒâ–ˆâ–„â–ˆ
wandb:         valid_loss â–‚â–â–â–â–â–â–ƒâ–„â–‚â–‚â–ƒâ–‚â–„â–‚â–‚â–‚â–‚â–ˆâ–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 7201
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.7127
wandb:   test_error_force 13.47565
wandb:          test_loss 5.2259
wandb: train_error_energy 12.05634
wandb:  train_error_force 4.51783
wandb:         train_loss 2.3185
wandb: valid_error_energy 8.21949
wandb:  valid_error_force 7.22855
wandb:         valid_loss 2.96875
wandb: 
wandb: ğŸš€ View run al_77_71 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/4mluhgn9
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_223129-4mluhgn9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4242216646671295, Uncertainty Bias: -0.3173382580280304
3.0517578e-05 0.12852097
-4.027839 33.609356
(48745, 22, 3)
Found uncertainty sample 0 after 4 steps.
Found uncertainty sample 1 after 10 steps.
Found uncertainty sample 2 after 12 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 20 steps.
Found uncertainty sample 5 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 11 steps.
Found uncertainty sample 8 after 23 steps.
Found uncertainty sample 9 after 9 steps.
Found uncertainty sample 10 after 8 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 20 steps.
Found uncertainty sample 13 after 13 steps.
Found uncertainty sample 14 after 19 steps.
Found uncertainty sample 15 after 34 steps.
Found uncertainty sample 16 after 10 steps.
Found uncertainty sample 17 after 66 steps.
Found uncertainty sample 18 after 15 steps.
Found uncertainty sample 19 after 35 steps.
Found uncertainty sample 20 after 6 steps.
Found uncertainty sample 21 after 10 steps.
Found uncertainty sample 22 after 20 steps.
Found uncertainty sample 23 after 7 steps.
Found uncertainty sample 24 after 78 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 13 steps.
Found uncertainty sample 29 after 52 steps.
Found uncertainty sample 30 after 10 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 31 steps.
Found uncertainty sample 33 after 27 steps.
Found uncertainty sample 34 after 5 steps.
Found uncertainty sample 35 after 11 steps.
Found uncertainty sample 36 after 45 steps.
Found uncertainty sample 37 after 71 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 10 steps.
Found uncertainty sample 40 after 3 steps.
Found uncertainty sample 41 after 5 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 8 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 12 steps.
Found uncertainty sample 46 after 4 steps.
Found uncertainty sample 47 after 7 steps.
Found uncertainty sample 48 after 13 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 16 steps.
Found uncertainty sample 51 after 31 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 18 steps.
Found uncertainty sample 54 after 39 steps.
Found uncertainty sample 55 after 29 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 4 steps.
Found uncertainty sample 58 after 71 steps.
Found uncertainty sample 59 after 14 steps.
Found uncertainty sample 60 after 20 steps.
Found uncertainty sample 61 after 9 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 17 steps.
Found uncertainty sample 64 after 13 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 10 steps.
Found uncertainty sample 67 after 15 steps.
Found uncertainty sample 68 after 18 steps.
Found uncertainty sample 69 after 14 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 28 steps.
Found uncertainty sample 72 after 3 steps.
Found uncertainty sample 73 after 10 steps.
Found uncertainty sample 74 after 19 steps.
Found uncertainty sample 75 after 11 steps.
Found uncertainty sample 76 after 11 steps.
Found uncertainty sample 77 after 3 steps.
Found uncertainty sample 78 after 6 steps.
Found uncertainty sample 79 after 8 steps.
Found uncertainty sample 80 after 20 steps.
Found uncertainty sample 81 after 15 steps.
Found uncertainty sample 82 after 6 steps.
Found uncertainty sample 83 after 7 steps.
Found uncertainty sample 84 after 12 steps.
Found uncertainty sample 85 after 50 steps.
Found uncertainty sample 86 after 9 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 27 steps.
Found uncertainty sample 89 after 26 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 52 steps.
Found uncertainty sample 92 after 49 steps.
Found uncertainty sample 93 after 15 steps.
Found uncertainty sample 94 after 7 steps.
Found uncertainty sample 95 after 7 steps.
Found uncertainty sample 96 after 16 steps.
Found uncertainty sample 97 after 22 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 18 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_224126-bmucfhtq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_72
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/bmucfhtq
Training model 72. Added 100 samples to the dataset.
Epoch 0, Batch 100/228, Loss: 0.24476298689842224
Epoch 0, Batch 200/228, Loss: 0.05516120046377182

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8181240567812842, Training Loss Force: 2.0902358283955844, time: 3.3177764415740967
Validation Loss Energy: 1.2001372361667373, Validation Loss Force: 2.22169537598341, time: 0.19802331924438477
Test Loss Energy: 9.615546504331018, Test Loss Force: 10.93316930481851, time: 9.947628498077393

Epoch 1, Batch 100/228, Loss: 0.13470062613487244
Epoch 1, Batch 200/228, Loss: 0.2324412763118744

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5930564168417005, Training Loss Force: 2.0077377096803506, time: 3.274311065673828
Validation Loss Energy: 1.5589497492865365, Validation Loss Force: 2.1738319750587944, time: 0.1888439655303955
Test Loss Energy: 9.601858726489391, Test Loss Force: 10.9384301863196, time: 9.966167449951172

Epoch 2, Batch 100/228, Loss: 0.19367440044879913
Epoch 2, Batch 200/228, Loss: 0.2173466831445694

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.030945654114856, Training Loss Force: 1.9883054729303762, time: 3.4801976680755615
Validation Loss Energy: 1.9202364784549555, Validation Loss Force: 2.1696773027995007, time: 0.19185185432434082
Test Loss Energy: 10.657733106557691, Test Loss Force: 11.024285710524515, time: 10.102930784225464

Epoch 3, Batch 100/228, Loss: 0.2006933093070984
Epoch 3, Batch 200/228, Loss: 0.15362752974033356

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7783490456840898, Training Loss Force: 1.974491960515719, time: 3.2595138549804688
Validation Loss Energy: 0.9885442937311921, Validation Loss Force: 2.1300803825828813, time: 0.19060039520263672
Test Loss Energy: 9.579414394717013, Test Loss Force: 10.96931526888346, time: 9.972934246063232

Epoch 4, Batch 100/228, Loss: 0.09239815175533295
Epoch 4, Batch 200/228, Loss: 0.13801781833171844

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.8334794678214783, Training Loss Force: 2.0054650131152716, time: 3.2747883796691895
Validation Loss Energy: 0.8392144736341587, Validation Loss Force: 2.0935340296654723, time: 0.20259571075439453
Test Loss Energy: 9.93190340661939, Test Loss Force: 10.976965285397723, time: 10.143043518066406

Epoch 5, Batch 100/228, Loss: 0.05574560910463333
Epoch 5, Batch 200/228, Loss: 0.04865572601556778

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.1444000045091665, Training Loss Force: 2.0106319144081666, time: 3.234250068664551
Validation Loss Energy: 2.242612525880861, Validation Loss Force: 2.1372751010242403, time: 0.19358158111572266
Test Loss Energy: 9.807410332213092, Test Loss Force: 10.88992811782315, time: 10.030089139938354

Epoch 6, Batch 100/228, Loss: 0.49160513281822205
Epoch 6, Batch 200/228, Loss: 1.1058834791183472

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 13.000684871327083, Training Loss Force: 5.7050952339886045, time: 3.3006479740142822
Validation Loss Energy: 3.8567561948787454, Validation Loss Force: 4.462428518605354, time: 0.19159722328186035
Test Loss Energy: 9.458281339432409, Test Loss Force: 11.350593349533897, time: 10.204110383987427

Epoch 7, Batch 100/228, Loss: 0.05718132108449936
Epoch 7, Batch 200/228, Loss: 0.4281466007232666

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 11.40731310455679, Training Loss Force: 5.376293181586845, time: 3.259774684906006
Validation Loss Energy: 37.67237564686383, Validation Loss Force: 8.437913015438525, time: 0.19234967231750488
Test Loss Energy: 40.375656049671626, Test Loss Force: 16.29300763240482, time: 10.049166202545166

Epoch 8, Batch 100/228, Loss: 0.537463366985321
Epoch 8, Batch 200/228, Loss: 0.08832846581935883

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.062477951316229, Training Loss Force: 5.136215933715376, time: 3.316157817840576
Validation Loss Energy: 13.485598548460672, Validation Loss Force: 4.368412929142578, time: 0.19165802001953125
Test Loss Energy: 19.624133905211437, Test Loss Force: 11.81834339584974, time: 10.133176565170288

Epoch 9, Batch 100/228, Loss: 0.28827255964279175
Epoch 9, Batch 200/228, Loss: 0.6783574223518372

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.457559472274074, Training Loss Force: 4.3984590149707286, time: 3.279841184616089
Validation Loss Energy: 8.445932444729117, Validation Loss Force: 3.047868382160189, time: 0.1920018196105957
Test Loss Energy: 15.271708673790696, Test Loss Force: 11.136608095115255, time: 9.89573049545288

Epoch 10, Batch 100/228, Loss: 0.5493409633636475
Epoch 10, Batch 200/228, Loss: 1.2701388597488403

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.616807244297364, Training Loss Force: 4.079776128700609, time: 3.4005093574523926
Validation Loss Energy: 4.577841610980634, Validation Loss Force: 3.610312719907534, time: 0.19296765327453613
Test Loss Energy: 9.197273887319218, Test Loss Force: 11.562027155955867, time: 10.029144048690796

Epoch 11, Batch 100/228, Loss: 0.79585862159729
Epoch 11, Batch 200/228, Loss: 0.5622128248214722

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.87627287546425, Training Loss Force: 4.815497074062706, time: 3.595039129257202
Validation Loss Energy: 2.4864386920435506, Validation Loss Force: 3.2727128943091186, time: 0.21575927734375
Test Loss Energy: 11.238927127781347, Test Loss Force: 11.182481609136959, time: 10.03344988822937

Epoch 12, Batch 100/228, Loss: 0.24983707070350647
Epoch 12, Batch 200/228, Loss: 0.5438376665115356

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 5.526463948580844, Training Loss Force: 2.761690758321609, time: 3.2132182121276855
Validation Loss Energy: 7.400819332811402, Validation Loss Force: 2.646636060726271, time: 0.19465351104736328
Test Loss Energy: 13.687652713225429, Test Loss Force: 11.232563430119857, time: 11.013493537902832

Epoch 13, Batch 100/228, Loss: 0.5645616054534912
Epoch 13, Batch 200/228, Loss: 0.5804026126861572

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.99273598878203, Training Loss Force: 3.311697206780814, time: 3.4522454738616943
Validation Loss Energy: 5.948986218358378, Validation Loss Force: 3.7864265904757457, time: 0.19486784934997559
Test Loss Energy: 9.74985963315162, Test Loss Force: 11.20804320169808, time: 10.007411241531372

Epoch 14, Batch 100/228, Loss: 1.5164263248443604
Epoch 14, Batch 200/228, Loss: 0.7947647571563721

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.33755213522188, Training Loss Force: 4.789183569642123, time: 3.271803379058838
Validation Loss Energy: 4.376746433263498, Validation Loss Force: 4.336439421140515, time: 0.19022202491760254
Test Loss Energy: 8.948342703977259, Test Loss Force: 11.87745641158746, time: 9.944008350372314

Epoch 15, Batch 100/228, Loss: 0.5090144276618958
Epoch 15, Batch 200/228, Loss: 0.07242029160261154

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.48930502664447, Training Loss Force: 4.321804058882167, time: 3.319674253463745
Validation Loss Energy: 16.200829453941306, Validation Loss Force: 6.138235731515875, time: 0.19456052780151367
Test Loss Energy: 19.736931015758067, Test Loss Force: 12.48329813523861, time: 10.220575332641602

Epoch 16, Batch 100/228, Loss: 0.42414993047714233
Epoch 16, Batch 200/228, Loss: 1.4716116189956665

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.870243833419991, Training Loss Force: 4.898722456577255, time: 3.2229864597320557
Validation Loss Energy: 10.790516305987609, Validation Loss Force: 6.823057473181111, time: 0.19696736335754395
Test Loss Energy: 17.830090564353547, Test Loss Force: 13.671827418837138, time: 9.990619897842407

Epoch 17, Batch 100/228, Loss: 0.21370774507522583
Epoch 17, Batch 200/228, Loss: 0.14497053623199463

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.972548103118642, Training Loss Force: 4.899953993722285, time: 3.2850027084350586
Validation Loss Energy: 13.454604784700692, Validation Loss Force: 4.14508791124546, time: 0.19884824752807617
Test Loss Energy: 11.902414060139632, Test Loss Force: 11.303286181964847, time: 10.167098760604858

Epoch 18, Batch 100/228, Loss: 0.47003817558288574
Epoch 18, Batch 200/228, Loss: 2.1122004985809326

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.44012787532728, Training Loss Force: 3.8423080319386456, time: 3.3242270946502686
Validation Loss Energy: 12.02107908208756, Validation Loss Force: 5.294283373378773, time: 0.19008445739746094
Test Loss Energy: 11.504135449006768, Test Loss Force: 11.790220026241508, time: 9.970609903335571

Epoch 19, Batch 100/228, Loss: 0.21418136358261108
Epoch 19, Batch 200/228, Loss: 0.6087387800216675

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.734232709078968, Training Loss Force: 4.639082952706972, time: 3.355382204055786
Validation Loss Energy: 8.082534171883506, Validation Loss Force: 3.390868992225797, time: 0.1945962905883789
Test Loss Energy: 12.482459421006585, Test Loss Force: 11.708040879221732, time: 10.196766138076782

wandb: - 0.039 MB of 0.060 MB uploadedwandb: \ 0.060 MB of 0.060 MB uploadedwandb: | 0.060 MB of 0.060 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–â–ˆâ–ƒâ–‚â–â–‚â–‚â–â–â–ƒâ–ƒâ–‚â–‚â–‚
wandb:   test_error_force â–â–â–â–â–â–â–‚â–ˆâ–‚â–â–‚â–â–â–â–‚â–ƒâ–…â–‚â–‚â–‚
wandb:          test_loss â–â–â–â–â–â–â–â–ˆâ–ƒâ–‚â–â–â–‚â–â–â–ƒâ–„â–‚â–‚â–‚
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–‡â–†â–†â–…â–†â–ƒâ–…â–†â–†â–†â–†â–„â–…
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–‡â–†â–…â–†â–‚â–„â–†â–…â–†â–†â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–‡â–†â–…â–†â–ƒâ–„â–†â–†â–†â–†â–„â–†
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–ˆâ–ƒâ–‚â–‚â–â–‚â–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–‚
wandb:  valid_error_force â–â–â–â–â–â–â–„â–ˆâ–„â–‚â–ƒâ–‚â–‚â–ƒâ–ƒâ–…â–†â–ƒâ–…â–‚
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–ˆâ–ƒâ–‚â–‚â–‚â–‚â–‚â–ƒâ–…â–„â–ƒâ–„â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 7291
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.48246
wandb:   test_error_force 11.70804
wandb:          test_loss 4.75288
wandb: train_error_energy 8.73423
wandb:  train_error_force 4.63908
wandb:         train_loss 2.13675
wandb: valid_error_energy 8.08253
wandb:  valid_error_force 3.39087
wandb:         valid_loss 1.67548
wandb: 
wandb: ğŸš€ View run al_77_72 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/bmucfhtq
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_224126-bmucfhtq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.3283383846282959, Uncertainty Bias: -0.20707888901233673
4.5776367e-05 0.0025815964
-2.9059904 14.80736
(48745, 22, 3)
Found uncertainty sample 0 after 36 steps.
Found uncertainty sample 1 after 24 steps.
Found uncertainty sample 2 after 40 steps.
Found uncertainty sample 3 after 92 steps.
Found uncertainty sample 4 after 24 steps.
Found uncertainty sample 5 after 12 steps.
Found uncertainty sample 6 after 35 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 21 steps.
Found uncertainty sample 9 after 31 steps.
Found uncertainty sample 10 after 29 steps.
Found uncertainty sample 11 after 37 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 28 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 28 steps.
Found uncertainty sample 16 after 67 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 22 steps.
Found uncertainty sample 19 after 13 steps.
Found uncertainty sample 20 after 40 steps.
Found uncertainty sample 21 after 4 steps.
Found uncertainty sample 22 after 33 steps.
Found uncertainty sample 23 after 15 steps.
Found uncertainty sample 24 after 10 steps.
Found uncertainty sample 25 after 11 steps.
Found uncertainty sample 26 after 9 steps.
Found uncertainty sample 27 after 18 steps.
Found uncertainty sample 28 after 50 steps.
Found uncertainty sample 29 after 63 steps.
Found uncertainty sample 30 after 80 steps.
Found uncertainty sample 31 after 58 steps.
Found uncertainty sample 32 after 13 steps.
Found uncertainty sample 33 after 8 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 88 steps.
Found uncertainty sample 36 after 11 steps.
Found uncertainty sample 37 after 23 steps.
Found uncertainty sample 38 after 83 steps.
Found uncertainty sample 39 after 15 steps.
Found uncertainty sample 40 after 9 steps.
Found uncertainty sample 41 after 14 steps.
Found uncertainty sample 42 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 12 steps.
Found uncertainty sample 45 after 34 steps.
Found uncertainty sample 46 after 54 steps.
Found uncertainty sample 47 after 21 steps.
Found uncertainty sample 48 after 19 steps.
Found uncertainty sample 49 after 6 steps.
Found uncertainty sample 50 after 20 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 9 steps.
Found uncertainty sample 53 after 33 steps.
Found uncertainty sample 54 after 24 steps.
Found uncertainty sample 55 after 10 steps.
Found uncertainty sample 56 after 9 steps.
Found uncertainty sample 57 after 210 steps.
Found uncertainty sample 58 after 16 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 24 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 16 steps.
Found uncertainty sample 63 after 67 steps.
Found uncertainty sample 64 after 3 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 44 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 18 steps.
Found uncertainty sample 69 after 13 steps.
Found uncertainty sample 70 after 37 steps.
Found uncertainty sample 71 after 3 steps.
Found uncertainty sample 72 after 28 steps.
Found uncertainty sample 73 after 14 steps.
Found uncertainty sample 74 after 94 steps.
Found uncertainty sample 75 after 23 steps.
Found uncertainty sample 76 after 36 steps.
Found uncertainty sample 77 after 60 steps.
Found uncertainty sample 78 after 36 steps.
Found uncertainty sample 79 after 3 steps.
Found uncertainty sample 80 after 8 steps.
Found uncertainty sample 81 after 6 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 49 steps.
Found uncertainty sample 85 after 67 steps.
Found uncertainty sample 86 after 16 steps.
Found uncertainty sample 87 after 44 steps.
Found uncertainty sample 88 after 29 steps.
Found uncertainty sample 89 after 46 steps.
Found uncertainty sample 90 after 18 steps.
Found uncertainty sample 91 after 25 steps.
Found uncertainty sample 92 after 73 steps.
Found uncertainty sample 93 after 18 steps.
Found uncertainty sample 94 after 8 steps.
Found uncertainty sample 95 after 76 steps.
Found uncertainty sample 96 after 53 steps.
Found uncertainty sample 97 after 29 steps.
Found uncertainty sample 98 after 36 steps.
Found uncertainty sample 99 after 174 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_225148-ycsyripf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_73
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/ycsyripf
Training model 73. Added 100 samples to the dataset.
Epoch 0, Batch 100/231, Loss: 0.09124403446912766
Epoch 0, Batch 200/231, Loss: 0.1061876192688942

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4915260824273773, Training Loss Force: 2.0522743403737844, time: 3.3549230098724365
Validation Loss Energy: 1.3510689520163968, Validation Loss Force: 2.1556767019128587, time: 0.19993829727172852
Test Loss Energy: 9.996177021136049, Test Loss Force: 10.980904109065216, time: 9.876991510391235

Epoch 1, Batch 100/231, Loss: 0.0843500941991806
Epoch 1, Batch 200/231, Loss: 0.06818336248397827

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.332916889137807, Training Loss Force: 2.005602615075486, time: 3.3018877506256104
Validation Loss Energy: 1.6728633319658526, Validation Loss Force: 2.1171612857380104, time: 0.20040273666381836
Test Loss Energy: 10.259982265601348, Test Loss Force: 11.061685262243596, time: 9.964240074157715

Epoch 2, Batch 100/231, Loss: 0.048653874546289444
Epoch 2, Batch 200/231, Loss: 0.08837445825338364

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.2981574231016704, Training Loss Force: 1.9710457989277457, time: 3.5088820457458496
Validation Loss Energy: 0.8608791656049541, Validation Loss Force: 2.1751839664455814, time: 0.1928861141204834
Test Loss Energy: 9.878105571011774, Test Loss Force: 11.000733661538916, time: 9.998648881912231

Epoch 3, Batch 100/231, Loss: 0.05774923413991928
Epoch 3, Batch 200/231, Loss: 0.1380479633808136

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5842786879157567, Training Loss Force: 1.9736621474153297, time: 3.2978880405426025
Validation Loss Energy: 1.2395543788816634, Validation Loss Force: 2.162452524023441, time: 0.1948838233947754
Test Loss Energy: 10.16502237592813, Test Loss Force: 11.029808450941221, time: 9.969415187835693

Epoch 4, Batch 100/231, Loss: 0.1733686923980713
Epoch 4, Batch 200/231, Loss: 0.050324201583862305

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.3151293910774031, Training Loss Force: 1.941833265433386, time: 3.3522727489471436
Validation Loss Energy: 1.4334725960218282, Validation Loss Force: 2.148665295845885, time: 0.2141435146331787
Test Loss Energy: 9.347795091613833, Test Loss Force: 10.959818166337245, time: 10.186905860900879

Epoch 5, Batch 100/231, Loss: 0.17225603759288788
Epoch 5, Batch 200/231, Loss: 0.1689840853214264

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.115429342227511, Training Loss Force: 1.965008899558246, time: 3.3253984451293945
Validation Loss Energy: 2.603353019800676, Validation Loss Force: 2.137160370746801, time: 0.193556547164917
Test Loss Energy: 9.5001107900529, Test Loss Force: 10.98305998474125, time: 9.97164511680603

Epoch 6, Batch 100/231, Loss: 2.1780896186828613
Epoch 6, Batch 200/231, Loss: 0.8642592430114746

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.464418791809255, Training Loss Force: 5.400242328549627, time: 3.3516016006469727
Validation Loss Energy: 6.265043694764864, Validation Loss Force: 4.938110602983947, time: 0.19546008110046387
Test Loss Energy: 11.758618401360902, Test Loss Force: 11.665403216544787, time: 10.170953512191772

Epoch 7, Batch 100/231, Loss: 1.1256911754608154
Epoch 7, Batch 200/231, Loss: 0.37075623869895935

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.429593494168822, Training Loss Force: 4.739851249381358, time: 3.3050944805145264
Validation Loss Energy: 12.95435316409128, Validation Loss Force: 3.7549161229626393, time: 0.19333267211914062
Test Loss Energy: 11.523847488130155, Test Loss Force: 11.046191532705862, time: 10.012887954711914

Epoch 8, Batch 100/231, Loss: 0.6328641772270203
Epoch 8, Batch 200/231, Loss: 0.40713781118392944

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.904323913270208, Training Loss Force: 5.156839110862399, time: 3.3525426387786865
Validation Loss Energy: 1.2998001605085343, Validation Loss Force: 4.003323150129928, time: 0.20078587532043457
Test Loss Energy: 9.316276085147013, Test Loss Force: 11.550864399596778, time: 10.149380207061768

Epoch 9, Batch 100/231, Loss: 0.5042624473571777
Epoch 9, Batch 200/231, Loss: 0.34087634086608887

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.717780544030116, Training Loss Force: 4.24427234369105, time: 3.311476945877075
Validation Loss Energy: 9.65479700323499, Validation Loss Force: 2.8611370343739897, time: 0.19283580780029297
Test Loss Energy: 14.125611852175973, Test Loss Force: 11.27744049155289, time: 9.998565196990967

Epoch 10, Batch 100/231, Loss: 0.5162128806114197
Epoch 10, Batch 200/231, Loss: 0.9573280811309814

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.793948930098882, Training Loss Force: 4.707718297739648, time: 3.342313289642334
Validation Loss Energy: 2.2295122041848483, Validation Loss Force: 5.445482960210983, time: 0.19376134872436523
Test Loss Energy: 9.082967970639782, Test Loss Force: 12.09259923779187, time: 10.106111764907837

Epoch 11, Batch 100/231, Loss: 0.9665158987045288
Epoch 11, Batch 200/231, Loss: 0.7279148101806641

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.576174131764265, Training Loss Force: 4.044431903719411, time: 3.451489210128784
Validation Loss Energy: 21.381104665921033, Validation Loss Force: 7.119786822382773, time: 0.201798677444458
Test Loss Energy: 22.717191709607178, Test Loss Force: 14.095833661433948, time: 9.976956367492676

Epoch 12, Batch 100/231, Loss: 0.24647563695907593
Epoch 12, Batch 200/231, Loss: 0.411234587430954

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.438706875076438, Training Loss Force: 4.384352757307992, time: 3.350428581237793
Validation Loss Energy: 2.791924115737818, Validation Loss Force: 3.31058346063736, time: 0.1948072910308838
Test Loss Energy: 8.922234156718858, Test Loss Force: 11.059006069773972, time: 9.905768394470215

Epoch 13, Batch 100/231, Loss: 1.5447888374328613
Epoch 13, Batch 200/231, Loss: 0.5241163372993469

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.464537283813128, Training Loss Force: 4.283945209991371, time: 3.3943920135498047
Validation Loss Energy: 4.623306531199889, Validation Loss Force: 3.0844534238417034, time: 0.27890753746032715
Test Loss Energy: 9.331734047724556, Test Loss Force: 10.880631692638737, time: 10.048134565353394

Epoch 14, Batch 100/231, Loss: 0.42314469814300537
Epoch 14, Batch 200/231, Loss: 0.3716571033000946

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 5.592544115172517, Training Loss Force: 2.6854057691624953, time: 3.3189127445220947
Validation Loss Energy: 7.644837439777881, Validation Loss Force: 2.7467334682390594, time: 0.20748353004455566
Test Loss Energy: 12.428618343283995, Test Loss Force: 11.150548498680681, time: 10.92026138305664

Epoch 15, Batch 100/231, Loss: 0.4453355669975281
Epoch 15, Batch 200/231, Loss: 0.2090977132320404

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.7270204760691215, Training Loss Force: 2.649054476826758, time: 3.398552656173706
Validation Loss Energy: 7.751931501526725, Validation Loss Force: 2.775832317567195, time: 0.19275784492492676
Test Loss Energy: 14.600328313733243, Test Loss Force: 11.253456664943005, time: 10.162190675735474

Epoch 16, Batch 100/231, Loss: 0.21043643355369568
Epoch 16, Batch 200/231, Loss: 0.33419620990753174

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.63346312889701, Training Loss Force: 2.926755006641023, time: 3.2953708171844482
Validation Loss Energy: 4.493692825377638, Validation Loss Force: 5.018459316942541, time: 0.18977665901184082
Test Loss Energy: 9.569952686072737, Test Loss Force: 12.233304883024154, time: 9.93648910522461

Epoch 17, Batch 100/231, Loss: 0.14610710740089417
Epoch 17, Batch 200/231, Loss: 0.10055273771286011

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.932374579403765, Training Loss Force: 4.542076718322667, time: 3.321580410003662
Validation Loss Energy: 5.302290110528045, Validation Loss Force: 2.9863496288596925, time: 0.20020151138305664
Test Loss Energy: 10.068786687644925, Test Loss Force: 10.8475773089649, time: 10.105254173278809

Epoch 18, Batch 100/231, Loss: 0.2894945442676544
Epoch 18, Batch 200/231, Loss: 0.12809094786643982

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 5.530295745689106, Training Loss Force: 2.6903828255712314, time: 3.3762600421905518
Validation Loss Energy: 7.271258288242161, Validation Loss Force: 2.8697348467711263, time: 0.20447325706481934
Test Loss Energy: 9.593772940627511, Test Loss Force: 11.019663653979206, time: 9.944545030593872

Epoch 19, Batch 100/231, Loss: 0.40270066261291504
Epoch 19, Batch 200/231, Loss: 0.1795683205127716

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.063527910069933, Training Loss Force: 3.990656952177384, time: 3.3151633739471436
Validation Loss Energy: 18.505590497134634, Validation Loss Force: 6.063723287206622, time: 0.19415807723999023
Test Loss Energy: 23.094280140872407, Test Loss Force: 12.574540732562584, time: 10.202252388000488

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–â–‚â–â–â–‚â–‚â–â–„â–â–ˆâ–â–â–ƒâ–„â–â–‚â–â–ˆ
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–â–ƒâ–‚â–„â–ˆâ–â–â–‚â–‚â–„â–â–â–…
wandb:          test_loss â–â–â–â–â–â–â–ƒâ–‚â–‚â–ƒâ–‚â–ˆâ–â–â–‚â–ƒâ–ƒâ–â–â–†
wandb: train_error_energy â–â–â–â–â–â–‚â–ˆâ–†â–†â–†â–†â–†â–‡â–‡â–„â–„â–…â–†â–„â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–ˆâ–†â–‡â–…â–†â–†â–ƒâ–‚â–ƒâ–†â–ƒâ–…
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–‡â–†â–†â–†â–†â–†â–ƒâ–ƒâ–„â–†â–ƒâ–†
wandb: valid_error_energy â–â–â–â–â–â–‚â–ƒâ–…â–â–„â–â–ˆâ–‚â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‡
wandb:  valid_error_force â–â–â–â–â–â–â–…â–ƒâ–„â–‚â–†â–ˆâ–ƒâ–‚â–‚â–‚â–…â–‚â–‚â–‡
wandb:         valid_loss â–â–â–â–â–â–â–„â–„â–‚â–ƒâ–„â–ˆâ–‚â–‚â–‚â–ƒâ–„â–‚â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 7381
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 23.09428
wandb:   test_error_force 12.57454
wandb:          test_loss 5.75297
wandb: train_error_energy 9.06353
wandb:  train_error_force 3.99066
wandb:         train_loss 1.94182
wandb: valid_error_energy 18.50559
wandb:  valid_error_force 6.06372
wandb:         valid_loss 3.26735
wandb: 
wandb: ğŸš€ View run al_77_73 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/ycsyripf
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_225148-ycsyripf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.48251646757125854, Uncertainty Bias: -0.37415048480033875
9.1552734e-05 0.0038557053
-1.0655547 22.857674
(48745, 22, 3)
Found uncertainty sample 0 after 11 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 8 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 11 steps.
Found uncertainty sample 5 after 5 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 7 steps.
Found uncertainty sample 8 after 4 steps.
Found uncertainty sample 9 after 7 steps.
Found uncertainty sample 10 after 4 steps.
Found uncertainty sample 11 after 7 steps.
Found uncertainty sample 12 after 23 steps.
Found uncertainty sample 13 after 5 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 8 steps.
Found uncertainty sample 16 after 20 steps.
Found uncertainty sample 17 after 31 steps.
Found uncertainty sample 18 after 28 steps.
Found uncertainty sample 19 after 35 steps.
Found uncertainty sample 20 after 7 steps.
Found uncertainty sample 21 after 8 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 9 steps.
Found uncertainty sample 24 after 11 steps.
Found uncertainty sample 25 after 19 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 2 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 25 steps.
Found uncertainty sample 32 after 25 steps.
Found uncertainty sample 33 after 4 steps.
Found uncertainty sample 34 after 8 steps.
Found uncertainty sample 35 after 22 steps.
Found uncertainty sample 36 after 35 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 8 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 12 steps.
Found uncertainty sample 42 after 13 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 55 steps.
Found uncertainty sample 46 after 6 steps.
Found uncertainty sample 47 after 7 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 4 steps.
Found uncertainty sample 50 after 5 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 9 steps.
Found uncertainty sample 53 after 4 steps.
Found uncertainty sample 54 after 15 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 10 steps.
Found uncertainty sample 59 after 20 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 16 steps.
Found uncertainty sample 63 after 5 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 7 steps.
Found uncertainty sample 67 after 21 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 9 steps.
Found uncertainty sample 70 after 28 steps.
Found uncertainty sample 71 after 7 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 27 steps.
Found uncertainty sample 75 after 4 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 2 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 10 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 3 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 4 steps.
Found uncertainty sample 86 after 3 steps.
Found uncertainty sample 87 after 7 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 6 steps.
Found uncertainty sample 91 after 7 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 20 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 24 steps.
Found uncertainty sample 96 after 8 steps.
Found uncertainty sample 97 after 5 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_230125-va2gmmc5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_74
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/va2gmmc5
Training model 74. Added 100 samples to the dataset.
Epoch 0, Batch 100/234, Loss: 0.0493583045899868
Epoch 0, Batch 200/234, Loss: 0.08145761489868164

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.6381111625738227, Training Loss Force: 2.0549176582497504, time: 3.402710437774658
Validation Loss Energy: 1.1141688122200397, Validation Loss Force: 2.152274094693936, time: 0.19323229789733887
Test Loss Energy: 10.033622067309325, Test Loss Force: 11.038266819408785, time: 9.154211282730103

Epoch 1, Batch 100/234, Loss: 0.09790688753128052
Epoch 1, Batch 200/234, Loss: 0.10564327239990234

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.409712438228072, Training Loss Force: 1.9596092935951293, time: 3.5672647953033447
Validation Loss Energy: 1.0321937089027677, Validation Loss Force: 2.1190748487108273, time: 0.18063926696777344
Test Loss Energy: 9.851852813421694, Test Loss Force: 10.953712569029449, time: 9.290934324264526

Epoch 2, Batch 100/234, Loss: 0.06847164034843445
Epoch 2, Batch 200/234, Loss: 0.09605342149734497

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3764458437493592, Training Loss Force: 1.9760330167634932, time: 3.758572816848755
Validation Loss Energy: 0.9821749402296792, Validation Loss Force: 2.095448984320451, time: 0.23770737648010254
Test Loss Energy: 10.290961932422517, Test Loss Force: 10.984616062018185, time: 11.98528265953064

Epoch 3, Batch 100/234, Loss: 0.08670583367347717
Epoch 3, Batch 200/234, Loss: 0.05714927986264229

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.6268697411921396, Training Loss Force: 1.9720013834809877, time: 3.5215959548950195
Validation Loss Energy: 1.4464875519340208, Validation Loss Force: 2.074490306627285, time: 0.18697142601013184
Test Loss Energy: 9.952152118021903, Test Loss Force: 11.039747897986793, time: 9.047268152236938

Epoch 4, Batch 100/234, Loss: 0.08547249436378479
Epoch 4, Batch 200/234, Loss: 0.09544551372528076

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.40741081348646, Training Loss Force: 1.9118611781934511, time: 3.573833703994751
Validation Loss Energy: 1.098576892365203, Validation Loss Force: 2.0784427396347893, time: 0.2157459259033203
Test Loss Energy: 10.306262851464409, Test Loss Force: 10.940480038839576, time: 9.21717643737793

Epoch 5, Batch 100/234, Loss: 0.21675534546375275
Epoch 5, Batch 200/234, Loss: 0.05125153809785843

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6694794387088576, Training Loss Force: 1.9741435810393728, time: 3.4689390659332275
Validation Loss Energy: 0.7530787416579396, Validation Loss Force: 2.094400769315996, time: 0.1824190616607666
Test Loss Energy: 10.143612217597232, Test Loss Force: 11.041330742193525, time: 9.055564165115356

Epoch 6, Batch 100/234, Loss: 1.6156482696533203
Epoch 6, Batch 200/234, Loss: 0.7195741534233093

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.230470922446413, Training Loss Force: 4.672436361940392, time: 3.3952221870422363
Validation Loss Energy: 7.759286263416721, Validation Loss Force: 5.039208184632957, time: 0.1776444911956787
Test Loss Energy: 13.586607315431444, Test Loss Force: 12.030801844074773, time: 9.225842952728271

Epoch 7, Batch 100/234, Loss: 0.2374274581670761
Epoch 7, Batch 200/234, Loss: 0.9356334209442139

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.88308093090278, Training Loss Force: 4.609271050714061, time: 3.4769911766052246
Validation Loss Energy: 7.047993168345297, Validation Loss Force: 3.530678718240082, time: 0.1798388957977295
Test Loss Energy: 9.67724267801709, Test Loss Force: 10.904891297900368, time: 9.045666456222534

Epoch 8, Batch 100/234, Loss: 0.8035084009170532
Epoch 8, Batch 200/234, Loss: 0.1742493063211441

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.405573505880024, Training Loss Force: 4.228291118046611, time: 3.5020062923431396
Validation Loss Energy: 12.807952654026678, Validation Loss Force: 4.507635699288197, time: 0.1844468116760254
Test Loss Energy: 11.180264169522404, Test Loss Force: 11.41596488848458, time: 9.168818712234497

Epoch 9, Batch 100/234, Loss: 0.7928192615509033
Epoch 9, Batch 200/234, Loss: 0.8794854879379272

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.31802464377859, Training Loss Force: 4.172931414732973, time: 3.571871042251587
Validation Loss Energy: 2.2807004172955283, Validation Loss Force: 4.240987504351931, time: 0.21686720848083496
Test Loss Energy: 10.686821293414578, Test Loss Force: 11.60022842385454, time: 11.958501100540161

Epoch 10, Batch 100/234, Loss: 0.7973201274871826
Epoch 10, Batch 200/234, Loss: 0.4281238317489624

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.924062100033442, Training Loss Force: 4.746534331423939, time: 3.738467216491699
Validation Loss Energy: 9.236173887500392, Validation Loss Force: 3.78658095586116, time: 0.23568987846374512
Test Loss Energy: 10.107732990184575, Test Loss Force: 11.307223704591673, time: 11.822579383850098

Epoch 11, Batch 100/234, Loss: 0.19728443026542664
Epoch 11, Batch 200/234, Loss: 0.27563321590423584

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.704049857048345, Training Loss Force: 3.2864855866638605, time: 3.431814432144165
Validation Loss Energy: 5.558982545280733, Validation Loss Force: 2.5550923214524386, time: 0.1988205909729004
Test Loss Energy: 9.384944605700761, Test Loss Force: 10.932833557052273, time: 9.945314884185791

Epoch 12, Batch 100/234, Loss: 0.3290891945362091
Epoch 12, Batch 200/234, Loss: 1.8216288089752197

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.7664313625605885, Training Loss Force: 3.490527456523632, time: 3.4080774784088135
Validation Loss Energy: 13.34380328167215, Validation Loss Force: 6.08861101764697, time: 0.20204925537109375
Test Loss Energy: 17.5114048785512, Test Loss Force: 14.197268387486032, time: 10.95556378364563

Epoch 13, Batch 100/234, Loss: 0.3057657480239868
Epoch 13, Batch 200/234, Loss: 0.5843380689620972

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 5.565461498986185, Training Loss Force: 3.0099596409197185, time: 3.5634326934814453
Validation Loss Energy: 2.4345401921910046, Validation Loss Force: 4.164751712187769, time: 0.19879603385925293
Test Loss Energy: 10.842141938791503, Test Loss Force: 11.502449817924342, time: 10.087649822235107

Epoch 14, Batch 100/234, Loss: 1.3437838554382324
Epoch 14, Batch 200/234, Loss: 0.7668849229812622

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.514481478455302, Training Loss Force: 4.533037332037374, time: 3.300354480743408
Validation Loss Energy: 9.734525694956746, Validation Loss Force: 3.3804893256556596, time: 0.19340753555297852
Test Loss Energy: 10.556870843061068, Test Loss Force: 11.104106723672109, time: 9.911017656326294

Epoch 15, Batch 100/234, Loss: 0.703778862953186
Epoch 15, Batch 200/234, Loss: 0.5332107543945312

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.41417252154697, Training Loss Force: 4.784615471395912, time: 3.381113290786743
Validation Loss Energy: 10.3660073767187, Validation Loss Force: 3.407261616412568, time: 0.20769762992858887
Test Loss Energy: 15.142721711665807, Test Loss Force: 11.49537381215729, time: 10.085824012756348

Epoch 16, Batch 100/234, Loss: 0.24220812320709229
Epoch 16, Batch 200/234, Loss: 0.8877959847450256

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.629013902169854, Training Loss Force: 4.167500185017962, time: 3.377700090408325
Validation Loss Energy: 12.196753904767288, Validation Loss Force: 3.6482940641358743, time: 0.19740653038024902
Test Loss Energy: 16.155034205209873, Test Loss Force: 12.220569159175636, time: 9.893730163574219

Epoch 17, Batch 100/234, Loss: 0.748927891254425
Epoch 17, Batch 200/234, Loss: 0.17750336229801178

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.775619387033798, Training Loss Force: 3.9460370627633967, time: 3.354588270187378
Validation Loss Energy: 4.776747839658728, Validation Loss Force: 3.5808302673082517, time: 0.20523667335510254
Test Loss Energy: 9.358238176630055, Test Loss Force: 10.997262974685885, time: 10.067065954208374

Epoch 18, Batch 100/234, Loss: 1.0594053268432617
Epoch 18, Batch 200/234, Loss: 0.5173851251602173

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.071002746833086, Training Loss Force: 4.298028091372458, time: 3.3069043159484863
Validation Loss Energy: 12.794742584275365, Validation Loss Force: 6.5935221014694925, time: 0.19475197792053223
Test Loss Energy: 11.447947900145687, Test Loss Force: 12.204134129048986, time: 10.007146835327148

Epoch 19, Batch 100/234, Loss: 0.8702104091644287
Epoch 19, Batch 200/234, Loss: 0.09176167845726013

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.65032010341377, Training Loss Force: 4.696676550521254, time: 3.430420160293579
Validation Loss Energy: 6.584346017300382, Validation Loss Force: 7.389054163148015, time: 0.19624543190002441
Test Loss Energy: 9.803316787297582, Test Loss Force: 13.15311423405366, time: 10.005979061126709

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‚â–‚â–‚â–‚â–…â–â–ƒâ–‚â–‚â–â–ˆâ–‚â–‚â–†â–‡â–â–ƒâ–
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–â–‚â–‚â–‚â–â–ˆâ–‚â–â–‚â–„â–â–„â–†
wandb:          test_loss â–â–â–â–â–â–â–„â–â–‚â–‚â–‚â–â–ˆâ–‚â–‚â–ƒâ–…â–â–ƒâ–„
wandb: train_error_energy â–â–â–â–â–â–â–‡â–‡â–‡â–‡â–‡â–…â–†â–„â–ˆâ–‡â–†â–‡â–ˆâ–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–‡â–‡â–ˆâ–„â–…â–„â–‡â–ˆâ–†â–†â–‡â–ˆ
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–‡â–‡â–ˆâ–…â–…â–„â–ˆâ–ˆâ–‡â–†â–‡â–ˆ
wandb: valid_error_energy â–â–â–â–â–â–â–…â–„â–ˆâ–‚â–†â–„â–ˆâ–‚â–†â–†â–‡â–ƒâ–ˆâ–„
wandb:  valid_error_force â–â–â–â–â–â–â–…â–ƒâ–„â–„â–ƒâ–‚â–†â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‡â–ˆ
wandb:         valid_loss â–â–â–â–â–â–â–…â–„â–†â–ƒâ–„â–‚â–ˆâ–ƒâ–„â–„â–…â–ƒâ–ˆâ–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7471
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.80332
wandb:   test_error_force 13.15311
wandb:          test_loss 5.05712
wandb: train_error_energy 10.65032
wandb:  train_error_force 4.69668
wandb:         train_loss 2.28425
wandb: valid_error_energy 6.58435
wandb:  valid_error_force 7.38905
wandb:         valid_loss 2.91303
wandb: 
wandb: ğŸš€ View run al_77_74 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/va2gmmc5
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_230125-va2gmmc5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5403518080711365, Uncertainty Bias: -0.4553011357784271
1.5258789e-05 0.0034103394
-2.371858 46.477367
(48745, 22, 3)
Found uncertainty sample 0 after 4 steps.
Found uncertainty sample 1 after 12 steps.
Found uncertainty sample 2 after 2 steps.
Found uncertainty sample 3 after 15 steps.
Found uncertainty sample 4 after 17 steps.
Found uncertainty sample 5 after 4 steps.
Found uncertainty sample 6 after 5 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 21 steps.
Found uncertainty sample 9 after 6 steps.
Found uncertainty sample 10 after 9 steps.
Found uncertainty sample 11 after 51 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 19 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 27 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 3 steps.
Found uncertainty sample 18 after 6 steps.
Found uncertainty sample 19 after 32 steps.
Found uncertainty sample 20 after 6 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 5 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 5 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 9 steps.
Found uncertainty sample 29 after 19 steps.
Found uncertainty sample 30 after 2 steps.
Found uncertainty sample 31 after 2 steps.
Found uncertainty sample 32 after 4 steps.
Found uncertainty sample 33 after 10 steps.
Found uncertainty sample 34 after 5 steps.
Found uncertainty sample 35 after 8 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 4 steps.
Found uncertainty sample 38 after 12 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 9 steps.
Found uncertainty sample 41 after 5 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 14 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 6 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 4 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 9 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 5 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 14 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 25 steps.
Found uncertainty sample 60 after 5 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 11 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 5 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 10 steps.
Found uncertainty sample 67 after 27 steps.
Found uncertainty sample 68 after 6 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 4 steps.
Found uncertainty sample 71 after 4 steps.
Found uncertainty sample 72 after 4 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 9 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 15 steps.
Found uncertainty sample 77 after 4 steps.
Found uncertainty sample 78 after 45 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 37 steps.
Found uncertainty sample 81 after 14 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 10 steps.
Found uncertainty sample 84 after 7 steps.
Found uncertainty sample 85 after 6 steps.
Found uncertainty sample 86 after 16 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 21 steps.
Found uncertainty sample 89 after 7 steps.
Found uncertainty sample 90 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 43 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 4 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 5 steps.
Found uncertainty sample 98 after 9 steps.
Found uncertainty sample 99 after 5 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_231104-an3ts7n4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_75
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/an3ts7n4
Training model 75. Added 100 samples to the dataset.
Epoch 0, Batch 100/237, Loss: 0.12079818546772003
Epoch 0, Batch 200/237, Loss: 0.08556093275547028

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.5399503430455819, Training Loss Force: 2.041837223196471, time: 3.372619867324829
Validation Loss Energy: 1.1427471028354605, Validation Loss Force: 2.1446796851356362, time: 0.21241211891174316
Test Loss Energy: 9.839960336462996, Test Loss Force: 11.038432485253354, time: 9.89012336730957

Epoch 1, Batch 100/237, Loss: 0.06877639889717102
Epoch 1, Batch 200/237, Loss: 0.04459554702043533

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.3308587832339922, Training Loss Force: 1.944233434130472, time: 3.4055581092834473
Validation Loss Energy: 1.1239272457898601, Validation Loss Force: 2.1845685246025677, time: 0.1979515552520752
Test Loss Energy: 9.733474084539269, Test Loss Force: 10.994189758087368, time: 9.910776138305664

Epoch 2, Batch 100/237, Loss: 0.13902893662452698
Epoch 2, Batch 200/237, Loss: 0.04652678966522217

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3748288835500067, Training Loss Force: 1.9329151511081077, time: 3.5801312923431396
Validation Loss Energy: 2.0305554017769865, Validation Loss Force: 2.103684235680731, time: 0.19896912574768066
Test Loss Energy: 10.4371732926254, Test Loss Force: 11.047315062925861, time: 9.975996017456055

Epoch 3, Batch 100/237, Loss: 0.11748184263706207
Epoch 3, Batch 200/237, Loss: 0.08536291122436523

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.547528024349411, Training Loss Force: 1.93466662480079, time: 3.4045112133026123
Validation Loss Energy: 1.1818044678778539, Validation Loss Force: 2.127022578536428, time: 0.19804930686950684
Test Loss Energy: 9.515189389690976, Test Loss Force: 10.96620867289758, time: 9.930288314819336

Epoch 4, Batch 100/237, Loss: 0.12140844762325287
Epoch 4, Batch 200/237, Loss: 0.04653118550777435

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5769204274791087, Training Loss Force: 1.9288090484388278, time: 3.632836103439331
Validation Loss Energy: 2.94531693799013, Validation Loss Force: 2.1024350061047894, time: 0.2516500949859619
Test Loss Energy: 9.420537543545334, Test Loss Force: 10.923887368284069, time: 9.95629096031189

Epoch 5, Batch 100/237, Loss: 0.1778099536895752
Epoch 5, Batch 200/237, Loss: 0.24339833855628967

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.1048319704050806, Training Loss Force: 1.9314634671274895, time: 3.4231042861938477
Validation Loss Energy: 0.8577850108204282, Validation Loss Force: 2.057700051013173, time: 0.23376011848449707
Test Loss Energy: 9.80426827237685, Test Loss Force: 11.030073728003094, time: 10.017031908035278

Epoch 6, Batch 100/237, Loss: 0.23821419477462769
Epoch 6, Batch 200/237, Loss: 0.3408651649951935

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 13.894393245395793, Training Loss Force: 6.289441830715954, time: 3.3850860595703125
Validation Loss Energy: 27.45020786236241, Validation Loss Force: 6.808328911189143, time: 0.2001030445098877
Test Loss Energy: 30.167188489167867, Test Loss Force: 13.03652788221009, time: 10.25358247756958

Epoch 7, Batch 100/237, Loss: 0.2518889904022217
Epoch 7, Batch 200/237, Loss: 1.2133991718292236

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.772150873332919, Training Loss Force: 5.83637364334769, time: 3.393735408782959
Validation Loss Energy: 13.546969059777268, Validation Loss Force: 5.473434952103018, time: 0.20318102836608887
Test Loss Energy: 11.708438246885224, Test Loss Force: 11.631915133996648, time: 10.027658700942993

Epoch 8, Batch 100/237, Loss: 0.8721084594726562
Epoch 8, Batch 200/237, Loss: 0.3607979118824005

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.599144154052066, Training Loss Force: 4.912102460122057, time: 3.336573600769043
Validation Loss Energy: 5.144286979276583, Validation Loss Force: 6.448976091386751, time: 0.20348000526428223
Test Loss Energy: 9.874864632085059, Test Loss Force: 12.567751198229516, time: 10.159712314605713

Epoch 9, Batch 100/237, Loss: 0.7781708240509033
Epoch 9, Batch 200/237, Loss: 0.11824502050876617

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 7.2194816128402755, Training Loss Force: 4.006304109362964, time: 3.369131088256836
Validation Loss Energy: 10.933629106112583, Validation Loss Force: 3.001272859352486, time: 0.20175766944885254
Test Loss Energy: 10.781691439261335, Test Loss Force: 11.021463692701975, time: 10.03611421585083

Epoch 10, Batch 100/237, Loss: 0.687896728515625
Epoch 10, Batch 200/237, Loss: 0.3855783939361572

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.421959492693576, Training Loss Force: 4.398498692753249, time: 3.4386744499206543
Validation Loss Energy: 5.979463278365233, Validation Loss Force: 4.325437004636504, time: 0.21613740921020508
Test Loss Energy: 9.581868572436935, Test Loss Force: 12.001661088646536, time: 10.237210750579834

Epoch 11, Batch 100/237, Loss: 0.6754109263420105
Epoch 11, Batch 200/237, Loss: 1.2442197799682617

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.883660788407612, Training Loss Force: 5.487337106685373, time: 3.3828136920928955
Validation Loss Energy: 12.484578046815859, Validation Loss Force: 5.999282391036828, time: 0.20673441886901855
Test Loss Energy: 16.301085462799414, Test Loss Force: 13.083280680549047, time: 10.083396196365356

Epoch 12, Batch 100/237, Loss: 1.2621188163757324
Epoch 12, Batch 200/237, Loss: 0.842843234539032

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.832686074678493, Training Loss Force: 4.638237186581261, time: 3.4330191612243652
Validation Loss Energy: 8.790644990855903, Validation Loss Force: 3.9353274523799704, time: 0.2001020908355713
Test Loss Energy: 15.270046330869835, Test Loss Force: 11.742872503095727, time: 11.112014770507812

Epoch 13, Batch 100/237, Loss: 0.7724997401237488
Epoch 13, Batch 200/237, Loss: 0.40841493010520935

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.423858947649958, Training Loss Force: 4.263035232644874, time: 3.6495416164398193
Validation Loss Energy: 2.15380383172646, Validation Loss Force: 4.025642837310292, time: 0.19564485549926758
Test Loss Energy: 8.850494098328744, Test Loss Force: 12.1844112199407, time: 9.99413275718689

Epoch 14, Batch 100/237, Loss: 0.8187094926834106
Epoch 14, Batch 200/237, Loss: 0.3580337464809418

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 11.449466771622882, Training Loss Force: 4.97775593260033, time: 3.3459696769714355
Validation Loss Energy: 21.304005919481828, Validation Loss Force: 6.0197074977890646, time: 0.2006516456604004
Test Loss Energy: 17.63536963718328, Test Loss Force: 12.531276131498503, time: 10.028686761856079

Epoch 15, Batch 100/237, Loss: 0.6127477884292603
Epoch 15, Batch 200/237, Loss: 0.868739128112793

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.836737242426688, Training Loss Force: 4.840888453331787, time: 3.4322702884674072
Validation Loss Energy: 10.697592463347313, Validation Loss Force: 4.281245529262811, time: 0.19684481620788574
Test Loss Energy: 10.370684565445053, Test Loss Force: 11.347824795622524, time: 10.231172561645508

Epoch 16, Batch 100/237, Loss: 0.792760968208313
Epoch 16, Batch 200/237, Loss: 1.1118156909942627

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.579968222171486, Training Loss Force: 4.500756516884718, time: 3.3638226985931396
Validation Loss Energy: 21.170892522693027, Validation Loss Force: 4.896344551282272, time: 0.1991894245147705
Test Loss Energy: 23.45287522573185, Test Loss Force: 12.024694913083719, time: 10.00222396850586

Epoch 17, Batch 100/237, Loss: 0.3626343011856079
Epoch 17, Batch 200/237, Loss: 0.6200303435325623

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.018327135229045, Training Loss Force: 5.294325952041322, time: 3.3961172103881836
Validation Loss Energy: 23.931574431571292, Validation Loss Force: 5.900681821190253, time: 0.20888543128967285
Test Loss Energy: 16.89016467646126, Test Loss Force: 12.111851445310194, time: 10.190330743789673

Epoch 18, Batch 100/237, Loss: 0.8582639694213867
Epoch 18, Batch 200/237, Loss: 0.5953450798988342

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.753083006406094, Training Loss Force: 4.712026589410993, time: 3.3689732551574707
Validation Loss Energy: 13.927410651721335, Validation Loss Force: 3.579312109071316, time: 0.20054244995117188
Test Loss Energy: 18.31805469760261, Test Loss Force: 11.425482168819158, time: 9.970650434494019

Epoch 19, Batch 100/237, Loss: 0.07930585741996765
Epoch 19, Batch 200/237, Loss: 1.8109374046325684

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.585652095929913, Training Loss Force: 4.998776909973095, time: 3.478060245513916
Validation Loss Energy: 13.202189044899265, Validation Loss Force: 5.867696615923666, time: 0.20534896850585938
Test Loss Energy: 12.05151145962483, Test Loss Force: 12.119584635162406, time: 10.145211935043335

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–â–â–â–ˆâ–‚â–â–‚â–â–ƒâ–ƒâ–â–„â–â–†â–„â–„â–‚
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–ƒâ–†â–â–„â–ˆâ–„â–…â–†â–‚â–…â–…â–ƒâ–…
wandb:          test_loss â–â–â–â–â–â–â–ˆâ–‚â–ƒâ–â–‚â–…â–ƒâ–‚â–…â–‚â–…â–„â–„â–ƒ
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–†â–…â–„â–…â–‡â–†â–…â–‡â–†â–†â–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–†â–„â–…â–‡â–…â–…â–†â–†â–…â–†â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–†â–„â–…â–‡â–…â–…â–†â–†â–…â–†â–†â–†
wandb: valid_error_energy â–â–â–â–â–‚â–â–ˆâ–„â–‚â–„â–‚â–„â–ƒâ–â–†â–„â–†â–‡â–„â–„
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–†â–‡â–‚â–„â–‡â–„â–„â–‡â–„â–…â–‡â–ƒâ–‡
wandb:         valid_loss â–â–â–â–â–â–â–ˆâ–…â–…â–ƒâ–ƒâ–…â–ƒâ–ƒâ–‡â–„â–†â–‡â–„â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 7561
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.05151
wandb:   test_error_force 12.11958
wandb:          test_loss 4.86175
wandb: train_error_energy 9.58565
wandb:  train_error_force 4.99878
wandb:         train_loss 2.31408
wandb: valid_error_energy 13.20219
wandb:  valid_error_force 5.8677
wandb:         valid_loss 2.84685
wandb: 
wandb: ğŸš€ View run al_77_75 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/an3ts7n4
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_231104-an3ts7n4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.746495246887207, Uncertainty Bias: -0.5776660442352295
0.0 0.0008530617
0.5943244 41.413868
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 2 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 3 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 3 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 5 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 20 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 4 steps.
Found uncertainty sample 41 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 4 steps.
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 5 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 7 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 6 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 6 steps.
Found uncertainty sample 92 after 8 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 2 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 3 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_232034-kuuztrl7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_76
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/kuuztrl7
Training model 76. Added 100 samples to the dataset.
Epoch 0, Batch 100/240, Loss: 0.04185879975557327
Epoch 0, Batch 200/240, Loss: 0.11614580452442169

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.3940710278539248, Training Loss Force: 2.058214183401557, time: 3.4915754795074463
Validation Loss Energy: 1.203149560040135, Validation Loss Force: 2.1398128056130927, time: 0.23398327827453613
Test Loss Energy: 9.85510714980299, Test Loss Force: 10.94976774378728, time: 10.0063955783844

Epoch 1, Batch 100/240, Loss: 0.044606782495975494
Epoch 1, Batch 200/240, Loss: 0.03085605800151825

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.3448729927184204, Training Loss Force: 1.9858560556568499, time: 3.524519681930542
Validation Loss Energy: 1.277397696073172, Validation Loss Force: 2.1356349757626227, time: 0.20382046699523926
Test Loss Energy: 9.368998529039205, Test Loss Force: 11.027900649678282, time: 10.179038286209106

Epoch 2, Batch 100/240, Loss: 0.14647118747234344
Epoch 2, Batch 200/240, Loss: 0.42076849937438965

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5177080475877056, Training Loss Force: 1.9914034366757336, time: 3.490811586380005
Validation Loss Energy: 1.2357592616257567, Validation Loss Force: 2.118265001027088, time: 0.20539212226867676
Test Loss Energy: 9.411071526078217, Test Loss Force: 10.966019196609889, time: 10.198861837387085

Epoch 3, Batch 100/240, Loss: 0.04028524458408356
Epoch 3, Batch 200/240, Loss: 0.08090892434120178

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3563808470765077, Training Loss Force: 1.954944341765991, time: 3.5063629150390625
Validation Loss Energy: 1.2585450231621906, Validation Loss Force: 2.286534682922555, time: 0.20858097076416016
Test Loss Energy: 9.908415374430344, Test Loss Force: 11.183574441325806, time: 10.171822547912598

Epoch 4, Batch 100/240, Loss: 0.07519327104091644
Epoch 4, Batch 200/240, Loss: 0.2299147993326187

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.7764343807065321, Training Loss Force: 1.9460292893893927, time: 3.6394519805908203
Validation Loss Energy: 2.290646268702112, Validation Loss Force: 2.1237235124703617, time: 0.20982146263122559
Test Loss Energy: 10.868520580226491, Test Loss Force: 11.052553099272071, time: 10.126912355422974

Epoch 5, Batch 100/240, Loss: 0.13576377928256989
Epoch 5, Batch 200/240, Loss: 0.1315212845802307

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.7872590199231024, Training Loss Force: 2.022975698124093, time: 3.4083609580993652
Validation Loss Energy: 2.365179922247515, Validation Loss Force: 2.2527666286061736, time: 0.21434617042541504
Test Loss Energy: 9.537067409705875, Test Loss Force: 10.967067359735927, time: 10.124614953994751

Epoch 6, Batch 100/240, Loss: 0.3433758020401001
Epoch 6, Batch 200/240, Loss: 2.009138584136963

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.886825677916079, Training Loss Force: 5.445836520775404, time: 3.5232913494110107
Validation Loss Energy: 3.3870694215869714, Validation Loss Force: 5.742809730731169, time: 0.199493408203125
Test Loss Energy: 9.768020939057573, Test Loss Force: 12.6883594771301, time: 10.23759412765503

Epoch 7, Batch 100/240, Loss: 1.0327361822128296
Epoch 7, Batch 200/240, Loss: 1.1939905881881714

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.57211653240531, Training Loss Force: 5.054665590056739, time: 3.4610283374786377
Validation Loss Energy: 2.3559021896132877, Validation Loss Force: 4.771713401603874, time: 0.2048187255859375
Test Loss Energy: 10.342220616966188, Test Loss Force: 11.660066263802275, time: 9.949205160140991

Epoch 8, Batch 100/240, Loss: 1.0283188819885254
Epoch 8, Batch 200/240, Loss: 0.3394804298877716

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.612405047680676, Training Loss Force: 4.755960828743614, time: 3.472330093383789
Validation Loss Energy: 17.403663241298872, Validation Loss Force: 5.047535454017791, time: 0.19864916801452637
Test Loss Energy: 13.80062231658269, Test Loss Force: 12.206212567157749, time: 10.169374704360962

Epoch 9, Batch 100/240, Loss: 1.6597859859466553
Epoch 9, Batch 200/240, Loss: 0.5613764524459839

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.65018787312567, Training Loss Force: 5.102209983101654, time: 3.45196270942688
Validation Loss Energy: 9.622342084539431, Validation Loss Force: 4.2672914105603805, time: 0.2041454315185547
Test Loss Energy: 10.192319691968864, Test Loss Force: 11.573600514220043, time: 10.059531450271606

Epoch 10, Batch 100/240, Loss: 1.2752246856689453
Epoch 10, Batch 200/240, Loss: 0.6552861332893372

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 10.127308026153312, Training Loss Force: 4.728797108822866, time: 3.4466490745544434
Validation Loss Energy: 6.3536743649370955, Validation Loss Force: 3.491412740041598, time: 0.19799423217773438
Test Loss Energy: 12.772546694145298, Test Loss Force: 11.595939708849723, time: 10.180668830871582

Epoch 11, Batch 100/240, Loss: 0.5055856704711914
Epoch 11, Batch 200/240, Loss: 0.3628973960876465

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.739153457564563, Training Loss Force: 3.874955097260461, time: 3.4077346324920654
Validation Loss Energy: 7.062754908040008, Validation Loss Force: 3.0715434007645293, time: 0.20268964767456055
Test Loss Energy: 13.495288819405497, Test Loss Force: 11.648655502978519, time: 10.048714399337769

Epoch 12, Batch 100/240, Loss: 0.20258547365665436
Epoch 12, Batch 200/240, Loss: 0.2914481461048126

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 5.489136446284831, Training Loss Force: 2.666240499929853, time: 3.3941128253936768
Validation Loss Energy: 9.338572524969058, Validation Loss Force: 2.6838697932971587, time: 0.20484018325805664
Test Loss Energy: 14.378947669049024, Test Loss Force: 11.712675372596612, time: 10.065747261047363

Epoch 13, Batch 100/240, Loss: 0.6004649996757507
Epoch 13, Batch 200/240, Loss: 1.4230964183807373

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.00923272535986, Training Loss Force: 4.127386951813537, time: 3.7070600986480713
Validation Loss Energy: 3.8227516659293888, Validation Loss Force: 5.229624706784262, time: 0.22077274322509766
Test Loss Energy: 10.045883848624289, Test Loss Force: 11.973886584465431, time: 10.046432495117188

Epoch 14, Batch 100/240, Loss: 0.4260130822658539
Epoch 14, Batch 200/240, Loss: 0.8591803908348083

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.7656050958404395, Training Loss Force: 3.5851198585142154, time: 3.4539971351623535
Validation Loss Energy: 6.240708740479281, Validation Loss Force: 4.120864848205706, time: 0.19966888427734375
Test Loss Energy: 9.822119846666624, Test Loss Force: 11.185426302503414, time: 11.088126182556152

Epoch 15, Batch 100/240, Loss: 0.07663793861865997
Epoch 15, Batch 200/240, Loss: 0.07666530460119247

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.235079576600432, Training Loss Force: 4.092925679273113, time: 3.6774237155914307
Validation Loss Energy: 5.0704025732934195, Validation Loss Force: 3.096929186417032, time: 0.22746634483337402
Test Loss Energy: 11.445200022988091, Test Loss Force: 11.446820634679908, time: 10.231792449951172

Epoch 16, Batch 100/240, Loss: 2.266828775405884
Epoch 16, Batch 200/240, Loss: 0.5939338207244873

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.857937479436798, Training Loss Force: 5.393720321937826, time: 3.4436094760894775
Validation Loss Energy: 7.986785103119681, Validation Loss Force: 5.506139482911161, time: 0.20914006233215332
Test Loss Energy: 9.834398672080612, Test Loss Force: 12.488045073034762, time: 10.134296178817749

Epoch 17, Batch 100/240, Loss: 0.29407671093940735
Epoch 17, Batch 200/240, Loss: 0.6450442671775818

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.72997863810568, Training Loss Force: 4.236644379772134, time: 3.3535120487213135
Validation Loss Energy: 25.47164910186404, Validation Loss Force: 3.702441800698724, time: 0.21339035034179688
Test Loss Energy: 18.74143245828375, Test Loss Force: 10.977795372657603, time: 10.125501871109009

Epoch 18, Batch 100/240, Loss: 0.7703628540039062
Epoch 18, Batch 200/240, Loss: 0.620820164680481

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.070535350981395, Training Loss Force: 4.527388485315737, time: 3.4071385860443115
Validation Loss Energy: 4.3246706887707935, Validation Loss Force: 3.7874303829855878, time: 0.20071768760681152
Test Loss Energy: 9.34360660337244, Test Loss Force: 11.078564250346558, time: 10.073338747024536

Epoch 19, Batch 100/240, Loss: 0.7100931406021118
Epoch 19, Batch 200/240, Loss: 0.2575165033340454

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.188251467751662, Training Loss Force: 3.6710431891754984, time: 3.4642372131347656
Validation Loss Energy: 8.148225355307684, Validation Loss Force: 3.7812258742361697, time: 0.20990276336669922
Test Loss Energy: 9.702772224874108, Test Loss Force: 11.402796636232036, time: 10.35811161994934

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.039 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–‚â–â–â–‚â–„â–‚â–„â–„â–…â–‚â–â–ƒâ–â–ˆâ–â–
wandb:   test_error_force â–â–â–â–‚â–â–â–ˆâ–„â–†â–„â–„â–„â–„â–…â–‚â–ƒâ–‡â–â–‚â–ƒ
wandb:          test_loss â–â–â–â–‚â–‚â–â–‡â–„â–ˆâ–„â–…â–†â–‡â–…â–‚â–„â–†â–‡â–â–ƒ
wandb: train_error_energy â–â–â–â–â–â–‚â–‡â–†â–†â–‡â–‡â–…â–„â–‡â–…â–†â–ˆâ–…â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–‡â–‡â–‡â–…â–‚â–…â–„â–…â–ˆâ–†â–†â–„
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–†â–‡â–‡â–…â–ƒâ–†â–…â–…â–ˆâ–…â–†â–…
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–â–†â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–ˆâ–‚â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–†â–‡â–…â–„â–ƒâ–‚â–‡â–…â–ƒâ–ˆâ–„â–„â–„
wandb:         valid_loss â–â–â–â–â–â–â–…â–„â–ˆâ–…â–„â–ƒâ–ƒâ–…â–„â–ƒâ–†â–ˆâ–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 7651
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.70277
wandb:   test_error_force 11.4028
wandb:          test_loss 4.46473
wandb: train_error_energy 8.18825
wandb:  train_error_force 3.67104
wandb:         train_loss 1.77631
wandb: valid_error_energy 8.14823
wandb:  valid_error_force 3.78123
wandb:         valid_loss 1.81049
wandb: 
wandb: ğŸš€ View run al_77_76 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/kuuztrl7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_232034-kuuztrl7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.8678555488586426, Uncertainty Bias: -0.8213531970977783
1.7166138e-05 0.29812193
-10.787801 72.65372
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 6 steps.
Found uncertainty sample 4 after 6 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 13 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 7 steps.
Found uncertainty sample 11 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 2 steps.
Found uncertainty sample 16 after 2 steps.
Found uncertainty sample 17 after 5 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 9 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 23 steps.
Found uncertainty sample 25 after 7 steps.
Found uncertainty sample 26 after 16 steps.
Found uncertainty sample 27 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 4 steps.
Found uncertainty sample 30 after 2 steps.
Found uncertainty sample 31 after 13 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 8 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 7 steps.
Found uncertainty sample 37 after 13 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 4 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 4 steps.
Found uncertainty sample 53 after 7 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 4 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 16 steps.
Found uncertainty sample 60 after 5 steps.
Found uncertainty sample 61 after 4 steps.
Found uncertainty sample 62 after 6 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 9 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 14 steps.
Found uncertainty sample 70 after 4 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 4 steps.
Found uncertainty sample 73 after 7 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 7 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 6 steps.
Found uncertainty sample 86 after 5 steps.
Found uncertainty sample 87 after 3 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 8 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 3 steps.
Found uncertainty sample 98 after 9 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_233010-hledwpj3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_77
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/hledwpj3
Training model 77. Added 100 samples to the dataset.
Epoch 0, Batch 100/242, Loss: 0.1001277044415474
Epoch 0, Batch 200/242, Loss: 0.16470324993133545

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8991289793320418, Training Loss Force: 2.0397657912564373, time: 3.5005033016204834
Validation Loss Energy: 0.7900805364797007, Validation Loss Force: 2.1775570072010204, time: 0.2097034454345703
Test Loss Energy: 9.528931419529332, Test Loss Force: 10.997891535876436, time: 10.055208444595337

Epoch 1, Batch 100/242, Loss: 0.060249775648117065
Epoch 1, Batch 200/242, Loss: 0.06305639445781708

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.896983289525348, Training Loss Force: 1.9777904907569126, time: 3.5086863040924072
Validation Loss Energy: 2.2785980865542506, Validation Loss Force: 2.190195874925741, time: 0.20533990859985352
Test Loss Energy: 9.294669479002282, Test Loss Force: 10.987279697958584, time: 10.352492570877075

Epoch 2, Batch 100/242, Loss: 0.20192094147205353
Epoch 2, Batch 200/242, Loss: 0.19302409887313843

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.0891455950582234, Training Loss Force: 1.9599684686713503, time: 3.4451653957366943
Validation Loss Energy: 2.1062609630374074, Validation Loss Force: 2.0961696530112466, time: 0.21645164489746094
Test Loss Energy: 10.915061686658673, Test Loss Force: 10.993150347840634, time: 10.107212781906128

Epoch 3, Batch 100/242, Loss: 0.1433326005935669
Epoch 3, Batch 200/242, Loss: 0.0514746829867363

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.0869933445627784, Training Loss Force: 1.9918028885076866, time: 3.492159366607666
Validation Loss Energy: 2.253929239884349, Validation Loss Force: 2.0900875009575155, time: 0.20540833473205566
Test Loss Energy: 9.5348540364263, Test Loss Force: 11.012645057214367, time: 10.161950588226318

Epoch 4, Batch 100/242, Loss: 0.20069250464439392
Epoch 4, Batch 200/242, Loss: 0.2950240671634674

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.360276606645728, Training Loss Force: 2.0042717764031646, time: 3.79060697555542
Validation Loss Energy: 2.2269027722973638, Validation Loss Force: 2.2845429630795175, time: 0.21095895767211914
Test Loss Energy: 11.122971934546428, Test Loss Force: 11.12479649718623, time: 10.058047533035278

Epoch 5, Batch 100/242, Loss: 0.06708205491304398
Epoch 5, Batch 200/242, Loss: 0.12370507419109344

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.3248535482601906, Training Loss Force: 2.0006611377742978, time: 3.5037784576416016
Validation Loss Energy: 2.032881515009921, Validation Loss Force: 2.0703650266952534, time: 0.20689892768859863
Test Loss Energy: 9.616955652614243, Test Loss Force: 10.999975625817752, time: 10.049083948135376

Epoch 6, Batch 100/242, Loss: 0.3292490839958191
Epoch 6, Batch 200/242, Loss: 0.2110951840877533

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.496383982711716, Training Loss Force: 5.125056449246587, time: 3.5423743724823
Validation Loss Energy: 11.829774666716869, Validation Loss Force: 4.784295767032814, time: 0.20774197578430176
Test Loss Energy: 17.357216814720186, Test Loss Force: 12.43887747793792, time: 10.298126935958862

Epoch 7, Batch 100/242, Loss: 1.1679376363754272
Epoch 7, Batch 200/242, Loss: 0.7923117876052856

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.978487616608938, Training Loss Force: 4.2939624715069735, time: 3.4853036403656006
Validation Loss Energy: 1.605978725420431, Validation Loss Force: 4.831317478585824, time: 0.2046492099761963
Test Loss Energy: 8.832706484876786, Test Loss Force: 12.360001323422344, time: 10.062243700027466

Epoch 8, Batch 100/242, Loss: 0.24809414148330688
Epoch 8, Batch 200/242, Loss: 0.15187612175941467

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.231678226887965, Training Loss Force: 5.018551001423438, time: 3.508401393890381
Validation Loss Energy: 11.916599601450198, Validation Loss Force: 3.66881302593312, time: 0.19966721534729004
Test Loss Energy: 11.410642410402362, Test Loss Force: 11.733315426771801, time: 10.262268781661987

Epoch 9, Batch 100/242, Loss: 0.13731282949447632
Epoch 9, Batch 200/242, Loss: 0.6743384599685669

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.370038950917412, Training Loss Force: 5.0906288806641244, time: 3.5226848125457764
Validation Loss Energy: 18.688938498583035, Validation Loss Force: 5.5497772914535055, time: 0.2098081111907959
Test Loss Energy: 14.680624290661363, Test Loss Force: 12.384011912298794, time: 10.025442361831665

Epoch 10, Batch 100/242, Loss: 0.4236217141151428
Epoch 10, Batch 200/242, Loss: 0.6540002226829529

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 10.064227462971333, Training Loss Force: 4.455225213140009, time: 3.472083330154419
Validation Loss Energy: 1.6535507637676004, Validation Loss Force: 3.76214563345805, time: 0.21091938018798828
Test Loss Energy: 9.077040023672996, Test Loss Force: 11.22273374147612, time: 10.18626618385315

Epoch 11, Batch 100/242, Loss: 0.5255837440490723
Epoch 11, Batch 200/242, Loss: 0.49689698219299316

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.258617863421481, Training Loss Force: 4.25461405373769, time: 3.5457043647766113
Validation Loss Energy: 5.426550516078473, Validation Loss Force: 3.3738491647222335, time: 0.21708202362060547
Test Loss Energy: 12.509626777540976, Test Loss Force: 11.609366779593103, time: 10.022587299346924

Epoch 12, Batch 100/242, Loss: 0.9284425377845764
Epoch 12, Batch 200/242, Loss: 0.1568385362625122

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.153473539826257, Training Loss Force: 4.260669455589995, time: 3.424516439437866
Validation Loss Energy: 31.384157994740246, Validation Loss Force: 5.624774375158871, time: 0.2102367877960205
Test Loss Energy: 24.26308176353312, Test Loss Force: 11.747262983729271, time: 10.171651124954224

Epoch 13, Batch 100/242, Loss: 0.569659948348999
Epoch 13, Batch 200/242, Loss: 1.5054676532745361

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.099875775373285, Training Loss Force: 4.491567888197325, time: 3.5209648609161377
Validation Loss Energy: 1.5075170176199848, Validation Loss Force: 6.417376890218993, time: 0.20689821243286133
Test Loss Energy: 9.180249066081172, Test Loss Force: 12.443230186423149, time: 10.080527067184448

Epoch 14, Batch 100/242, Loss: 0.1278989613056183
Epoch 14, Batch 200/242, Loss: 0.1335112750530243

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.764098158382147, Training Loss Force: 4.390014993230795, time: 3.563610076904297
Validation Loss Energy: 6.636368209863491, Validation Loss Force: 2.920680601015534, time: 0.21245670318603516
Test Loss Energy: 13.074751995263654, Test Loss Force: 11.458148861027983, time: 10.112178802490234

Epoch 15, Batch 100/242, Loss: 0.17859208583831787
Epoch 15, Batch 200/242, Loss: 0.08626791089773178

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.369463753356416, Training Loss Force: 2.675132634640512, time: 3.697138786315918
Validation Loss Energy: 8.20939190168661, Validation Loss Force: 2.827081013184637, time: 0.20236992835998535
Test Loss Energy: 9.964316708334993, Test Loss Force: 11.199052446458092, time: 11.069447994232178

Epoch 16, Batch 100/242, Loss: 0.2668033838272095
Epoch 16, Batch 200/242, Loss: 0.18061266839504242

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 5.563361731987492, Training Loss Force: 2.63237196587, time: 3.4413726329803467
Validation Loss Energy: 6.473974488392626, Validation Loss Force: 3.2294295750470914, time: 0.20226573944091797
Test Loss Energy: 12.096873258964669, Test Loss Force: 11.787345481816674, time: 10.152543306350708

Epoch 17, Batch 100/242, Loss: 2.2769694328308105
Epoch 17, Batch 200/242, Loss: 0.2816474437713623

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.575067568471681, Training Loss Force: 5.240842025697586, time: 3.4162003993988037
Validation Loss Energy: 3.183187038502971, Validation Loss Force: 3.6751660550028724, time: 0.20286250114440918
Test Loss Energy: 10.16754523833602, Test Loss Force: 11.566365245220727, time: 10.273658752441406

Epoch 18, Batch 100/242, Loss: 0.08664025366306305
Epoch 18, Batch 200/242, Loss: 0.33694612979888916

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.816135670446187, Training Loss Force: 4.453569530942092, time: 3.523595094680786
Validation Loss Energy: 13.043879520903008, Validation Loss Force: 3.5012375707673695, time: 0.2078249454498291
Test Loss Energy: 11.499613549276464, Test Loss Force: 12.033736772641406, time: 10.0673189163208

Epoch 19, Batch 100/242, Loss: 1.0352901220321655
Epoch 19, Batch 200/242, Loss: 0.49892979860305786

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.049309688792734, Training Loss Force: 4.503501953328019, time: 3.582486867904663
Validation Loss Energy: 3.90286607627918, Validation Loss Force: 3.6373679118143443, time: 0.20342803001403809
Test Loss Energy: 9.557894048967283, Test Loss Force: 11.471136744277485, time: 10.183382034301758

wandb: - 0.039 MB of 0.060 MB uploadedwandb: \ 0.039 MB of 0.060 MB uploadedwandb: | 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–â–‚â–â–…â–â–‚â–„â–â–ƒâ–ˆâ–â–ƒâ–‚â–‚â–‚â–‚â–
wandb:   test_error_force â–â–â–â–â–‚â–â–ˆâ–ˆâ–…â–ˆâ–‚â–„â–…â–ˆâ–ƒâ–‚â–…â–„â–†â–ƒ
wandb:          test_loss â–â–â–‚â–â–‚â–â–‡â–ƒâ–ƒâ–†â–â–ƒâ–ˆâ–„â–ƒâ–‚â–„â–‚â–„â–‚
wandb: train_error_energy â–â–â–â–â–â–â–†â–‡â–†â–†â–‡â–†â–‡â–†â–…â–„â–„â–ˆâ–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–ˆâ–ˆâ–†â–†â–†â–†â–†â–ƒâ–‚â–ˆâ–†â–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–†â–‡â–‡â–‡â–†â–†â–†â–†â–ƒâ–ƒâ–ˆâ–†â–‡
wandb: valid_error_energy â–â–â–â–â–â–â–„â–â–„â–…â–â–‚â–ˆâ–â–‚â–ƒâ–‚â–‚â–„â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–…â–…â–„â–‡â–„â–ƒâ–‡â–ˆâ–‚â–‚â–ƒâ–„â–ƒâ–„
wandb:         valid_loss â–â–â–â–â–â–â–…â–ƒâ–„â–†â–‚â–ƒâ–ˆâ–„â–‚â–ƒâ–ƒâ–‚â–„â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7741
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.55789
wandb:   test_error_force 11.47114
wandb:          test_loss 4.4779
wandb: train_error_energy 10.04931
wandb:  train_error_force 4.5035
wandb:         train_loss 2.17939
wandb: valid_error_energy 3.90287
wandb:  valid_error_force 3.63737
wandb:         valid_loss 1.47826
wandb: 
wandb: ğŸš€ View run al_77_77 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/hledwpj3
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_233010-hledwpj3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.46431031823158264, Uncertainty Bias: -0.36185699701309204
0.0 0.04369688
-5.349456 26.092299
(48745, 22, 3)
Found uncertainty sample 0 after 73 steps.
Found uncertainty sample 1 after 14 steps.
Found uncertainty sample 2 after 67 steps.
Found uncertainty sample 3 after 28 steps.
Found uncertainty sample 4 after 13 steps.
Found uncertainty sample 5 after 15 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 6 steps.
Found uncertainty sample 8 after 8 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 73 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 2 steps.
Found uncertainty sample 13 after 30 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 73 steps.
Found uncertainty sample 17 after 5 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 22 steps.
Found uncertainty sample 20 after 21 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 26 steps.
Found uncertainty sample 24 after 3 steps.
Found uncertainty sample 25 after 21 steps.
Found uncertainty sample 26 after 80 steps.
Found uncertainty sample 27 after 21 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 60 steps.
Found uncertainty sample 30 after 20 steps.
Found uncertainty sample 31 after 10 steps.
Found uncertainty sample 32 after 10 steps.
Found uncertainty sample 33 after 20 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 8 steps.
Found uncertainty sample 37 after 14 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 15 steps.
Found uncertainty sample 40 after 14 steps.
Found uncertainty sample 41 after 111 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 11 steps.
Found uncertainty sample 44 after 8 steps.
Found uncertainty sample 45 after 48 steps.
Found uncertainty sample 46 after 10 steps.
Found uncertainty sample 47 after 46 steps.
Found uncertainty sample 48 after 20 steps.
Found uncertainty sample 49 after 8 steps.
Found uncertainty sample 50 after 6 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 18 steps.
Found uncertainty sample 53 after 26 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 19 steps.
Found uncertainty sample 56 after 74 steps.
Found uncertainty sample 57 after 9 steps.
Found uncertainty sample 58 after 42 steps.
Found uncertainty sample 59 after 13 steps.
Found uncertainty sample 60 after 26 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 11 steps.
Found uncertainty sample 63 after 31 steps.
Found uncertainty sample 64 after 34 steps.
Found uncertainty sample 65 after 38 steps.
Found uncertainty sample 66 after 46 steps.
Found uncertainty sample 67 after 8 steps.
Found uncertainty sample 68 after 11 steps.
Found uncertainty sample 69 after 5 steps.
Found uncertainty sample 70 after 9 steps.
Found uncertainty sample 71 after 7 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 60 steps.
Found uncertainty sample 74 after 25 steps.
Found uncertainty sample 75 after 4 steps.
Found uncertainty sample 76 after 9 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 55 steps.
Found uncertainty sample 79 after 7 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 21 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 45 steps.
Found uncertainty sample 85 after 36 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 61 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 9 steps.
Found uncertainty sample 94 after 17 steps.
Found uncertainty sample 95 after 21 steps.
Found uncertainty sample 96 after 42 steps.
Found uncertainty sample 97 after 7 steps.
Found uncertainty sample 98 after 13 steps.
Found uncertainty sample 99 after 32 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_234021-pg0gm7ml
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_78
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/pg0gm7ml
Training model 78. Added 100 samples to the dataset.
Epoch 0, Batch 100/245, Loss: 0.027536209672689438
Epoch 0, Batch 200/245, Loss: 0.08140671253204346

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.640980993459849, Training Loss Force: 2.036783035853276, time: 3.544102191925049
Validation Loss Energy: 0.775877278836621, Validation Loss Force: 2.141023068088497, time: 0.20937156677246094
Test Loss Energy: 9.646649313018925, Test Loss Force: 11.072692024293378, time: 10.04645848274231

Epoch 1, Batch 100/245, Loss: 0.07271076738834381
Epoch 1, Batch 200/245, Loss: 0.03873952105641365

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.245207724350737, Training Loss Force: 1.9711410836684096, time: 3.571409225463867
Validation Loss Energy: 2.874604899329, Validation Loss Force: 2.150091157721833, time: 0.20641064643859863
Test Loss Energy: 10.81549640681769, Test Loss Force: 11.172509263016531, time: 10.228989362716675

Epoch 2, Batch 100/245, Loss: 0.06101240962743759
Epoch 2, Batch 200/245, Loss: 0.05303024500608444

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3695779791800406, Training Loss Force: 1.948415084826642, time: 3.4715428352355957
Validation Loss Energy: 0.8271519014927424, Validation Loss Force: 2.1060616184845156, time: 0.21204471588134766
Test Loss Energy: 9.57122762198551, Test Loss Force: 11.09287293235805, time: 10.043233871459961

Epoch 3, Batch 100/245, Loss: 0.2619000971317291
Epoch 3, Batch 200/245, Loss: 0.07041852921247482

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.4947584931765778, Training Loss Force: 1.956309577186012, time: 3.4743869304656982
Validation Loss Energy: 0.8407320334477648, Validation Loss Force: 2.1261145337593343, time: 0.2236635684967041
Test Loss Energy: 9.73912234973231, Test Loss Force: 11.238687894848898, time: 10.126183271408081

Epoch 4, Batch 100/245, Loss: 0.055030353367328644
Epoch 4, Batch 200/245, Loss: 0.16840040683746338

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6375981781197981, Training Loss Force: 1.9196336480938925, time: 3.7021713256835938
Validation Loss Energy: 1.7098061194639425, Validation Loss Force: 2.102798680487376, time: 0.2054426670074463
Test Loss Energy: 10.473198311204095, Test Loss Force: 11.155195955829923, time: 9.983257293701172

Epoch 5, Batch 100/245, Loss: 0.07331740856170654
Epoch 5, Batch 200/245, Loss: 0.0905674546957016

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.0763122194340644, Training Loss Force: 1.9172480794724343, time: 3.552279233932495
Validation Loss Energy: 2.929226308832524, Validation Loss Force: 2.1183301874195584, time: 0.2066969871520996
Test Loss Energy: 9.640554217301005, Test Loss Force: 11.043638988850564, time: 10.097354888916016

Epoch 6, Batch 100/245, Loss: 0.12520277500152588
Epoch 6, Batch 200/245, Loss: 0.5897220373153687

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.723214381222176, Training Loss Force: 5.7363340289118705, time: 3.582268238067627
Validation Loss Energy: 2.9846189329229684, Validation Loss Force: 5.499593455359176, time: 0.20745253562927246
Test Loss Energy: 11.262012630113293, Test Loss Force: 12.510574998360516, time: 10.215025663375854

Epoch 7, Batch 100/245, Loss: 0.5391127467155457
Epoch 7, Batch 200/245, Loss: 1.182813048362732

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.567743704339206, Training Loss Force: 4.482145665621907, time: 3.524993419647217
Validation Loss Energy: 11.40986830869429, Validation Loss Force: 5.0130733956408395, time: 0.21335554122924805
Test Loss Energy: 11.50475109305524, Test Loss Force: 12.743540622073557, time: 10.090260744094849

Epoch 8, Batch 100/245, Loss: 0.5599088668823242
Epoch 8, Batch 200/245, Loss: 0.36908620595932007

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 5.342196133236638, Training Loss Force: 2.9220782716143274, time: 3.481519937515259
Validation Loss Energy: 8.512350642914747, Validation Loss Force: 2.790638646758376, time: 0.21959519386291504
Test Loss Energy: 14.63689619735809, Test Loss Force: 11.456170803569178, time: 10.290339469909668

Epoch 9, Batch 100/245, Loss: 0.3340614438056946
Epoch 9, Batch 200/245, Loss: 0.3111025393009186

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 5.521124750618505, Training Loss Force: 2.732439166067299, time: 3.4499857425689697
Validation Loss Energy: 6.23498402807205, Validation Loss Force: 2.8371204969670836, time: 0.20922040939331055
Test Loss Energy: 12.45527564058252, Test Loss Force: 11.610777434099031, time: 10.139493703842163

Epoch 10, Batch 100/245, Loss: 0.3278316259384155
Epoch 10, Batch 200/245, Loss: 0.5067710876464844

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 5.689963854777889, Training Loss Force: 2.6749024721313854, time: 3.6459808349609375
Validation Loss Energy: 3.4972537970775757, Validation Loss Force: 2.78049885052569, time: 0.21176362037658691
Test Loss Energy: 12.721243042092219, Test Loss Force: 11.17346218437008, time: 10.192219972610474

Epoch 11, Batch 100/245, Loss: 0.5419794917106628
Epoch 11, Batch 200/245, Loss: 0.7273703217506409

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.059029824129015, Training Loss Force: 4.170459310880642, time: 3.5352532863616943
Validation Loss Energy: 1.977755370219865, Validation Loss Force: 3.9075220881205297, time: 0.20818686485290527
Test Loss Energy: 10.984394092356487, Test Loss Force: 11.437258055746721, time: 10.016485691070557

Epoch 12, Batch 100/245, Loss: 0.7322897911071777
Epoch 12, Batch 200/245, Loss: 0.09665372222661972

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.38551479953147, Training Loss Force: 4.691051034523046, time: 3.4093470573425293
Validation Loss Energy: 7.053201065664829, Validation Loss Force: 3.431130412841151, time: 0.2126636505126953
Test Loss Energy: 10.006703377418004, Test Loss Force: 11.209697467907725, time: 10.188081741333008

Epoch 13, Batch 100/245, Loss: 0.8980275988578796
Epoch 13, Batch 200/245, Loss: 0.28512606024742126

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.050174170367965, Training Loss Force: 3.2977066591151183, time: 3.593508243560791
Validation Loss Energy: 3.7238365202469033, Validation Loss Force: 2.679667024502345, time: 0.21030831336975098
Test Loss Energy: 9.423535882688851, Test Loss Force: 10.802559268817319, time: 10.014055728912354

Epoch 14, Batch 100/245, Loss: 0.46593114733695984
Epoch 14, Batch 200/245, Loss: 0.6341184973716736

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.525970094610521, Training Loss Force: 4.947268753652701, time: 3.5536367893218994
Validation Loss Energy: 1.3035651617624544, Validation Loss Force: 3.500540115335037, time: 0.21583938598632812
Test Loss Energy: 10.415270147137031, Test Loss Force: 11.298709377385864, time: 9.984144926071167

Epoch 15, Batch 100/245, Loss: 0.15855690836906433
Epoch 15, Batch 200/245, Loss: 0.7259536981582642

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.25425925684895, Training Loss Force: 4.777912310678698, time: 3.735764503479004
Validation Loss Energy: 4.689805001612628, Validation Loss Force: 2.8928491287987335, time: 0.20554900169372559
Test Loss Energy: 9.211514220395175, Test Loss Force: 10.9737593020295, time: 10.03005051612854

Epoch 16, Batch 100/245, Loss: 0.5319836735725403
Epoch 16, Batch 200/245, Loss: 0.3184879422187805

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 5.859905567015385, Training Loss Force: 2.655631253707139, time: 3.4827466011047363
Validation Loss Energy: 43.38159829762697, Validation Loss Force: 4.519772726425284, time: 0.2083580493927002
Test Loss Energy: 34.387441708326826, Test Loss Force: 11.734562103521764, time: 10.001010179519653

Epoch 17, Batch 100/245, Loss: 0.05818326026201248
Epoch 17, Batch 200/245, Loss: 0.20975330471992493

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.116908361555463, Training Loss Force: 4.817015129423785, time: 3.5947256088256836
Validation Loss Energy: 2.551760467354635, Validation Loss Force: 3.458388289338042, time: 0.2088007926940918
Test Loss Energy: 9.479846288959749, Test Loss Force: 10.786564368156645, time: 11.28780460357666

Epoch 18, Batch 100/245, Loss: 0.977088987827301
Epoch 18, Batch 200/245, Loss: 0.38217592239379883

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.783418500517087, Training Loss Force: 5.2423472001788705, time: 3.5844807624816895
Validation Loss Energy: 23.464176752766946, Validation Loss Force: 4.451560482930911, time: 0.21249890327453613
Test Loss Energy: 24.86855021908404, Test Loss Force: 12.252185342654643, time: 10.180340766906738

Epoch 19, Batch 100/245, Loss: 0.5466926097869873
Epoch 19, Batch 200/245, Loss: 0.26060551404953003

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.885997664145657, Training Loss Force: 4.298679335971653, time: 3.4954705238342285
Validation Loss Energy: 3.7369605283733143, Validation Loss Force: 4.389823235530972, time: 0.2038116455078125
Test Loss Energy: 10.655671376795528, Test Loss Force: 11.864959594245416, time: 10.295684099197388

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–â–‚â–‚â–ƒâ–‚â–‚â–â–â–â–â–â–ˆâ–â–…â–
wandb:   test_error_force â–‚â–‚â–‚â–ƒâ–‚â–‚â–‡â–ˆâ–ƒâ–„â–‚â–ƒâ–ƒâ–â–ƒâ–‚â–„â–â–†â–…
wandb:          test_loss â–â–‚â–â–‚â–‚â–â–ƒâ–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–‚â–â–ˆâ–â–†â–ƒ
wandb: train_error_energy â–â–â–â–â–â–‚â–‡â–‡â–„â–„â–„â–‡â–†â–…â–‡â–†â–„â–‡â–ˆâ–…
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–ƒâ–‚â–‚â–…â–†â–„â–‡â–†â–‚â–†â–‡â–…
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–ƒâ–ƒâ–ƒâ–†â–†â–„â–‡â–†â–ƒâ–‡â–ˆâ–†
wandb: valid_error_energy â–â–â–â–â–â–â–â–ƒâ–‚â–‚â–â–â–‚â–â–â–‚â–ˆâ–â–…â–
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–‡â–‚â–ƒâ–‚â–…â–„â–‚â–„â–ƒâ–†â–„â–†â–†
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–„â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–ˆâ–‚â–…â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 7831
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.65567
wandb:   test_error_force 11.86496
wandb:          test_loss 4.68314
wandb: train_error_energy 7.886
wandb:  train_error_force 4.29868
wandb:         train_loss 1.96609
wandb: valid_error_energy 3.73696
wandb:  valid_error_force 4.38982
wandb:         valid_loss 1.71893
wandb: 
wandb: ğŸš€ View run al_77_78 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/pg0gm7ml
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_234021-pg0gm7ml/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.3365930914878845, Uncertainty Bias: -0.2433588057756424
1.5258789e-05 0.08669281
-3.5542185 13.943557
(48745, 22, 3)
Found uncertainty sample 0 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 35 steps.
Found uncertainty sample 3 after 60 steps.
Found uncertainty sample 4 after 77 steps.
Found uncertainty sample 5 after 14 steps.
Found uncertainty sample 6 after 8 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 11 steps.
Found uncertainty sample 9 after 5 steps.
Found uncertainty sample 10 after 40 steps.
Found uncertainty sample 11 after 26 steps.
Found uncertainty sample 12 after 40 steps.
Found uncertainty sample 13 after 6 steps.
Found uncertainty sample 14 after 17 steps.
Found uncertainty sample 15 after 197 steps.
Found uncertainty sample 16 after 10 steps.
Found uncertainty sample 17 after 77 steps.
Found uncertainty sample 18 after 14 steps.
Found uncertainty sample 19 after 5 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 22 steps.
Found uncertainty sample 22 after 130 steps.
Found uncertainty sample 23 after 63 steps.
Found uncertainty sample 24 after 81 steps.
Found uncertainty sample 25 after 37 steps.
Found uncertainty sample 26 after 20 steps.
Found uncertainty sample 27 after 60 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 48 steps.
Found uncertainty sample 30 after 19 steps.
Found uncertainty sample 31 after 38 steps.
Found uncertainty sample 32 after 30 steps.
Found uncertainty sample 33 after 8 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 68 steps.
Found uncertainty sample 36 after 64 steps.
Found uncertainty sample 37 after 79 steps.
Found uncertainty sample 38 after 13 steps.
Found uncertainty sample 39 after 37 steps.
Found uncertainty sample 40 after 54 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 20 steps.
Found uncertainty sample 43 after 81 steps.
Found uncertainty sample 44 after 29 steps.
Found uncertainty sample 45 after 85 steps.
Found uncertainty sample 46 after 9 steps.
Found uncertainty sample 47 after 9 steps.
Found uncertainty sample 48 after 24 steps.
Found uncertainty sample 49 after 47 steps.
Found uncertainty sample 50 after 65 steps.
Found uncertainty sample 51 after 16 steps.
Found uncertainty sample 52 after 116 steps.
Found uncertainty sample 53 after 47 steps.
Found uncertainty sample 54 after 49 steps.
Found uncertainty sample 55 after 4 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 38 steps.
Found uncertainty sample 58 after 52 steps.
Found uncertainty sample 59 after 3 steps.
Found uncertainty sample 60 after 67 steps.
Found uncertainty sample 61 after 61 steps.
Found uncertainty sample 62 after 5 steps.
Found uncertainty sample 63 after 63 steps.
Found uncertainty sample 64 after 73 steps.
Found uncertainty sample 65 after 25 steps.
Found uncertainty sample 66 after 249 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 41 steps.
Found uncertainty sample 69 after 22 steps.
Found uncertainty sample 70 after 73 steps.
Found uncertainty sample 71 after 155 steps.
Found uncertainty sample 72 after 68 steps.
Found uncertainty sample 73 after 17 steps.
Found uncertainty sample 74 after 27 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 11 steps.
Found uncertainty sample 77 after 20 steps.
Found uncertainty sample 78 after 4 steps.
Found uncertainty sample 79 after 19 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 126 steps.
Found uncertainty sample 82 after 13 steps.
Found uncertainty sample 83 after 6 steps.
Found uncertainty sample 84 after 13 steps.
Found uncertainty sample 85 after 22 steps.
Found uncertainty sample 86 after 8 steps.
Found uncertainty sample 87 after 35 steps.
Found uncertainty sample 88 after 144 steps.
Found uncertainty sample 89 after 188 steps.
Found uncertainty sample 90 after 35 steps.
Found uncertainty sample 91 after 167 steps.
Found uncertainty sample 92 after 10 steps.
Found uncertainty sample 93 after 37 steps.
Found uncertainty sample 94 after 8 steps.
Found uncertainty sample 95 after 15 steps.
Found uncertainty sample 96 after 143 steps.
Found uncertainty sample 97 after 4 steps.
Found uncertainty sample 98 after 21 steps.
Found uncertainty sample 99 after 101 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241207_235121-3kmpjmgr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_79
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3kmpjmgr
Training model 79. Added 100 samples to the dataset.
Epoch 0, Batch 100/248, Loss: 0.21145756542682648
Epoch 0, Batch 200/248, Loss: 0.045071158558130264

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.529834856781116, Training Loss Force: 1.9982665695731547, time: 3.7257468700408936
Validation Loss Energy: 1.779813521612785, Validation Loss Force: 2.112349628163657, time: 0.22063040733337402
Test Loss Energy: 10.298414829618093, Test Loss Force: 11.090265460337937, time: 10.161477088928223

Epoch 1, Batch 100/248, Loss: 0.10111826658248901
Epoch 1, Batch 200/248, Loss: 0.32235652208328247

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5933797201865938, Training Loss Force: 1.9667813583977114, time: 3.5995426177978516
Validation Loss Energy: 0.7512861019481831, Validation Loss Force: 2.1076001509483033, time: 0.21314382553100586
Test Loss Energy: 9.748363100635402, Test Loss Force: 11.06149924607122, time: 10.273776531219482

Epoch 2, Batch 100/248, Loss: 0.09476182609796524
Epoch 2, Batch 200/248, Loss: 0.09251325577497482

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5218152173914132, Training Loss Force: 1.9188663530006673, time: 3.5445172786712646
Validation Loss Energy: 1.0096407479584015, Validation Loss Force: 2.0683852753188097, time: 0.21091485023498535
Test Loss Energy: 9.912401511211021, Test Loss Force: 11.026259230662294, time: 10.187334775924683

Epoch 3, Batch 100/248, Loss: 0.1124214306473732
Epoch 3, Batch 200/248, Loss: 0.18579551577568054

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7479681962919766, Training Loss Force: 1.9228749269155185, time: 3.4961347579956055
Validation Loss Energy: 1.8896085601017343, Validation Loss Force: 2.1133202986380426, time: 0.2118363380432129
Test Loss Energy: 9.601726314125022, Test Loss Force: 11.105016421563594, time: 10.24291467666626

Epoch 4, Batch 100/248, Loss: 0.10866653919219971
Epoch 4, Batch 200/248, Loss: 0.06420241296291351

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.0397117599737746, Training Loss Force: 1.9117705997714296, time: 3.8215298652648926
Validation Loss Energy: 2.198894890449702, Validation Loss Force: 2.0412710204210422, time: 0.2334740161895752
Test Loss Energy: 9.691830309675499, Test Loss Force: 11.069889680630556, time: 10.193960905075073

Epoch 5, Batch 100/248, Loss: 0.07255467027425766
Epoch 5, Batch 200/248, Loss: 0.0782470628619194

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.9377671803591043, Training Loss Force: 1.899242711516257, time: 3.560626268386841
Validation Loss Energy: 1.318259321371375, Validation Loss Force: 2.0342223729394115, time: 0.21917343139648438
Test Loss Energy: 10.463809600733859, Test Loss Force: 11.079454413975165, time: 10.131497859954834

Epoch 6, Batch 100/248, Loss: 0.4597455859184265
Epoch 6, Batch 200/248, Loss: 0.47927558422088623

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.726700149018459, Training Loss Force: 4.865702495234003, time: 3.5450127124786377
Validation Loss Energy: 6.152525189952072, Validation Loss Force: 4.172824916077101, time: 0.22017121315002441
Test Loss Energy: 9.547193875517886, Test Loss Force: 11.679413783041513, time: 10.38414478302002

Epoch 7, Batch 100/248, Loss: 0.43689003586769104
Epoch 7, Batch 200/248, Loss: 0.32314544916152954

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.677467935729444, Training Loss Force: 4.720853397427135, time: 3.5900516510009766
Validation Loss Energy: 7.242196624661396, Validation Loss Force: 5.742542861759775, time: 0.22222185134887695
Test Loss Energy: 16.340225391899843, Test Loss Force: 12.571727155183066, time: 10.157475709915161

Epoch 8, Batch 100/248, Loss: 1.0881462097167969
Epoch 8, Batch 200/248, Loss: 1.274234414100647

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.578892710659686, Training Loss Force: 4.940634642151828, time: 3.726680278778076
Validation Loss Energy: 4.024808607355073, Validation Loss Force: 7.105871782115453, time: 0.22071480751037598
Test Loss Energy: 9.781308672542723, Test Loss Force: 13.657375072091657, time: 10.382699012756348

Epoch 9, Batch 100/248, Loss: 0.578833281993866
Epoch 9, Batch 200/248, Loss: 0.6488268375396729

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.3561543586522, Training Loss Force: 4.07068399659622, time: 3.555375576019287
Validation Loss Energy: 9.386972113197068, Validation Loss Force: 7.148443116783376, time: 0.20844078063964844
Test Loss Energy: 10.604671417478126, Test Loss Force: 14.0259584279958, time: 10.166415691375732

Epoch 10, Batch 100/248, Loss: 0.26842600107192993
Epoch 10, Batch 200/248, Loss: 0.29617053270339966

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.882092536219972, Training Loss Force: 3.422855011899426, time: 3.6519360542297363
Validation Loss Energy: 11.451962129378769, Validation Loss Force: 4.062653232727815, time: 0.20859813690185547
Test Loss Energy: 10.717509126089352, Test Loss Force: 11.628903392735323, time: 10.297361135482788

Epoch 11, Batch 100/248, Loss: 0.09548062831163406
Epoch 11, Batch 200/248, Loss: 0.7648056745529175

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.32612443080126, Training Loss Force: 4.606482451424224, time: 3.5772860050201416
Validation Loss Energy: 3.065437089108465, Validation Loss Force: 3.070642592734678, time: 0.21296072006225586
Test Loss Energy: 10.790152545043847, Test Loss Force: 11.301733354957952, time: 10.235635995864868

Epoch 12, Batch 100/248, Loss: 0.4111446738243103
Epoch 12, Batch 200/248, Loss: 1.6429431438446045

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.933398347595669, Training Loss Force: 2.987931250496796, time: 3.559563159942627
Validation Loss Energy: 33.41721458336615, Validation Loss Force: 4.777483761186019, time: 0.2123584747314453
Test Loss Energy: 25.93970355438947, Test Loss Force: 11.497844367134608, time: 10.295617580413818

Epoch 13, Batch 100/248, Loss: 0.8332915306091309
Epoch 13, Batch 200/248, Loss: 0.7831679582595825

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.931324994506342, Training Loss Force: 4.60793009088136, time: 3.61367130279541
Validation Loss Energy: 10.502345022740533, Validation Loss Force: 7.195207422803692, time: 0.2181682586669922
Test Loss Energy: 16.277241565677734, Test Loss Force: 13.095571849528048, time: 10.18713927268982

Epoch 14, Batch 100/248, Loss: 0.0887366309762001
Epoch 14, Batch 200/248, Loss: 0.259334921836853

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.026746036377231, Training Loss Force: 4.763222179490318, time: 3.590158224105835
Validation Loss Energy: 12.209645112301631, Validation Loss Force: 3.81554681604189, time: 0.2088918685913086
Test Loss Energy: 11.332435825506009, Test Loss Force: 10.940527268014653, time: 10.279690742492676

Epoch 15, Batch 100/248, Loss: 1.0051738023757935
Epoch 15, Batch 200/248, Loss: 0.8403370380401611

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.705724657500985, Training Loss Force: 5.209188133468869, time: 3.5361132621765137
Validation Loss Energy: 4.500599910949744, Validation Loss Force: 2.827818604730523, time: 0.21303987503051758
Test Loss Energy: 8.912313507146107, Test Loss Force: 10.898801798056276, time: 10.065083980560303

Epoch 16, Batch 100/248, Loss: 0.300178200006485
Epoch 16, Batch 200/248, Loss: 0.5607558488845825

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 5.378095203998836, Training Loss Force: 2.64765321248701, time: 3.565126657485962
Validation Loss Energy: 7.426496202704918, Validation Loss Force: 2.898356794455971, time: 0.21042251586914062
Test Loss Energy: 9.914917537326389, Test Loss Force: 10.911579682150833, time: 10.184917688369751

Epoch 17, Batch 100/248, Loss: 0.5593104958534241
Epoch 17, Batch 200/248, Loss: 0.33391231298446655

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.701762050204124, Training Loss Force: 4.255855295614547, time: 3.772460460662842
Validation Loss Energy: 20.81372497788877, Validation Loss Force: 3.6921466370694658, time: 0.2111644744873047
Test Loss Energy: 26.192189931605668, Test Loss Force: 11.492250060154914, time: 10.11998963356018

Epoch 18, Batch 100/248, Loss: 0.25021886825561523
Epoch 18, Batch 200/248, Loss: 0.15148279070854187

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.213450627981109, Training Loss Force: 4.589607667729166, time: 3.556417465209961
Validation Loss Energy: 8.431357413113265, Validation Loss Force: 3.6551743737431837, time: 0.21326398849487305
Test Loss Energy: 9.850728868174386, Test Loss Force: 10.67244512982799, time: 10.194108963012695

Epoch 19, Batch 100/248, Loss: 1.1375324726104736
Epoch 19, Batch 200/248, Loss: 0.6554583311080933

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 11.38417202483461, Training Loss Force: 4.579943599637458, time: 3.5370845794677734
Validation Loss Energy: 13.79932222954869, Validation Loss Force: 4.634175680220062, time: 0.21321368217468262
Test Loss Energy: 12.962673982482823, Test Loss Force: 11.684606847215088, time: 10.386995315551758

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–â–â–â–‚â–â–„â–â–‚â–‚â–‚â–ˆâ–„â–‚â–â–â–ˆâ–â–ƒ
wandb:   test_error_force â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–…â–‡â–ˆâ–ƒâ–‚â–ƒâ–†â–‚â–â–â–ƒâ–â–ƒ
wandb:          test_loss â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–†â–†â–‡â–ƒâ–‚â–ˆâ–‡â–‚â–â–â–ˆâ–â–„
wandb: train_error_energy â–â–â–â–â–â–â–†â–‡â–‡â–†â–…â–‡â–…â–‡â–‡â–ˆâ–„â–‡â–†â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–‡â–‡â–‡â–†â–„â–‡â–ƒâ–‡â–‡â–ˆâ–ƒâ–†â–‡â–‡
wandb:         train_loss â–â–â–â–â–â–â–‡â–‡â–‡â–†â–…â–‡â–„â–‡â–‡â–ˆâ–ƒâ–†â–‡â–‡
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–â–ˆâ–ƒâ–ƒâ–‚â–‚â–…â–ƒâ–„
wandb:  valid_error_force â–â–â–â–â–â–â–„â–†â–ˆâ–ˆâ–„â–‚â–…â–ˆâ–ƒâ–‚â–‚â–ƒâ–ƒâ–…
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–…â–…â–†â–„â–‚â–ˆâ–†â–„â–‚â–ƒâ–…â–ƒâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 7921
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.96267
wandb:   test_error_force 11.68461
wandb:          test_loss 4.77718
wandb: train_error_energy 11.38417
wandb:  train_error_force 4.57994
wandb:         train_loss 2.2943
wandb: valid_error_energy 13.79932
wandb:  valid_error_force 4.63418
wandb:         valid_loss 2.47407
wandb: 
wandb: ğŸš€ View run al_77_79 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/3kmpjmgr
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241207_235121-3kmpjmgr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7088326215744019, Uncertainty Bias: -0.6804536581039429
6.866455e-05 0.0387187
-8.411034 56.96156
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 3 steps.
Found uncertainty sample 3 after 11 steps.
Found uncertainty sample 4 after 4 steps.
Found uncertainty sample 5 after 16 steps.
Found uncertainty sample 6 after 13 steps.
Found uncertainty sample 7 after 15 steps.
Found uncertainty sample 8 after 6 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 27 steps.
Found uncertainty sample 11 after 24 steps.
Found uncertainty sample 12 after 13 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 8 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 7 steps.
Found uncertainty sample 17 after 5 steps.
Found uncertainty sample 18 after 10 steps.
Found uncertainty sample 19 after 17 steps.
Found uncertainty sample 20 after 7 steps.
Found uncertainty sample 21 after 4 steps.
Found uncertainty sample 22 after 14 steps.
Found uncertainty sample 23 after 15 steps.
Found uncertainty sample 24 after 6 steps.
Found uncertainty sample 25 after 13 steps.
Found uncertainty sample 26 after 4 steps.
Found uncertainty sample 27 after 10 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 9 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 2 steps.
Found uncertainty sample 33 after 2 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 15 steps.
Found uncertainty sample 36 after 16 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 16 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 16 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 2 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 24 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 11 steps.
Found uncertainty sample 50 after 12 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 3 steps.
Found uncertainty sample 53 after 4 steps.
Found uncertainty sample 54 after 9 steps.
Found uncertainty sample 55 after 17 steps.
Found uncertainty sample 56 after 22 steps.
Found uncertainty sample 57 after 3 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 8 steps.
Found uncertainty sample 60 after 8 steps.
Found uncertainty sample 61 after 29 steps.
Found uncertainty sample 62 after 3 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 10 steps.
Found uncertainty sample 65 after 14 steps.
Found uncertainty sample 66 after 3 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 21 steps.
Found uncertainty sample 69 after 8 steps.
Found uncertainty sample 70 after 13 steps.
Found uncertainty sample 71 after 4 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 8 steps.
Found uncertainty sample 76 after 11 steps.
Found uncertainty sample 77 after 4 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 3 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 24 steps.
Found uncertainty sample 84 after 7 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 2 steps.
Found uncertainty sample 89 after 4 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 7 steps.
Found uncertainty sample 92 after 8 steps.
Found uncertainty sample 93 after 8 steps.
Found uncertainty sample 94 after 25 steps.
Found uncertainty sample 95 after 10 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 5 steps.
Found uncertainty sample 98 after 10 steps.
Found uncertainty sample 99 after 5 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_000110-la2lorf7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_80
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/la2lorf7
Training model 80. Added 100 samples to the dataset.
Epoch 0, Batch 100/251, Loss: 0.07420879602432251
Epoch 0, Batch 200/251, Loss: 0.18743495643138885

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7480283074310101, Training Loss Force: 2.055475928233226, time: 3.603314161300659
Validation Loss Energy: 1.269193590559679, Validation Loss Force: 2.1129022929838497, time: 0.2178201675415039
Test Loss Energy: 9.967438860850104, Test Loss Force: 11.096933051253913, time: 9.975250959396362

Epoch 1, Batch 100/251, Loss: 0.05615899711847305
Epoch 1, Batch 200/251, Loss: 0.07939483225345612

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.488751845588309, Training Loss Force: 1.9313732851033787, time: 3.638998031616211
Validation Loss Energy: 1.8915026951915688, Validation Loss Force: 2.093970071520485, time: 0.2133007049560547
Test Loss Energy: 9.434969479341039, Test Loss Force: 11.077337135838837, time: 10.089207887649536

Epoch 2, Batch 100/251, Loss: 0.11584459245204926
Epoch 2, Batch 200/251, Loss: 0.09645874053239822

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3874947905813408, Training Loss Force: 1.9431469920437605, time: 3.576439142227173
Validation Loss Energy: 2.0941499325231083, Validation Loss Force: 2.08236026925367, time: 0.2120354175567627
Test Loss Energy: 9.524087188140284, Test Loss Force: 11.20995414597682, time: 11.188248634338379

Epoch 3, Batch 100/251, Loss: 0.17875052988529205
Epoch 3, Batch 200/251, Loss: 0.105745330452919

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.640698634890149, Training Loss Force: 1.9363523601566794, time: 3.8331236839294434
Validation Loss Energy: 1.076059641985115, Validation Loss Force: 2.0794783181652132, time: 0.23895025253295898
Test Loss Energy: 9.608849045438506, Test Loss Force: 11.143175953569997, time: 12.897629261016846

Epoch 4, Batch 100/251, Loss: 0.03559058532118797
Epoch 4, Batch 200/251, Loss: 0.04964183270931244

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.1523350090434337, Training Loss Force: 1.8871028908164507, time: 3.738131284713745
Validation Loss Energy: 1.2024504067555648, Validation Loss Force: 2.058327254329965, time: 0.24125218391418457
Test Loss Energy: 9.761376860138114, Test Loss Force: 11.153416258108505, time: 11.420365810394287

Epoch 5, Batch 100/251, Loss: 0.05641380697488785
Epoch 5, Batch 200/251, Loss: 0.14266376197338104

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5479876754860287, Training Loss Force: 1.8885192134042472, time: 3.888904094696045
Validation Loss Energy: 1.8715140199653504, Validation Loss Force: 2.028992709710865, time: 0.2724006175994873
Test Loss Energy: 9.791338241136236, Test Loss Force: 11.058878427892038, time: 11.675416231155396

Epoch 6, Batch 100/251, Loss: 0.27682480216026306
Epoch 6, Batch 200/251, Loss: 0.08427786827087402

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.97533548115246, Training Loss Force: 4.810165756723328, time: 3.7761218547821045
Validation Loss Energy: 3.7120748576225653, Validation Loss Force: 7.38291873750112, time: 0.24260759353637695
Test Loss Energy: 11.516150699437173, Test Loss Force: 13.51987819417483, time: 11.524552583694458

Epoch 7, Batch 100/251, Loss: 1.137682557106018
Epoch 7, Batch 200/251, Loss: 0.781808614730835

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.08235533633464, Training Loss Force: 5.545104705398397, time: 3.8582706451416016
Validation Loss Energy: 2.342197237685151, Validation Loss Force: 5.010947350635521, time: 0.24257731437683105
Test Loss Energy: 9.652121322176185, Test Loss Force: 12.325424596264273, time: 11.689307689666748

Epoch 8, Batch 100/251, Loss: 0.15022987127304077
Epoch 8, Batch 200/251, Loss: 0.07786041498184204

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.929164222232477, Training Loss Force: 4.592836936375755, time: 3.7076847553253174
Validation Loss Energy: 1.6454520642610606, Validation Loss Force: 3.7267185826699336, time: 0.2520480155944824
Test Loss Energy: 8.896594874776143, Test Loss Force: 11.230538176128572, time: 11.48924207687378

Epoch 9, Batch 100/251, Loss: 0.9407047629356384
Epoch 9, Batch 200/251, Loss: 0.6168313026428223

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.917088792544956, Training Loss Force: 4.868737934316749, time: 3.877769947052002
Validation Loss Energy: 15.276527157165308, Validation Loss Force: 5.239280420293701, time: 0.23732352256774902
Test Loss Energy: 12.088944726483707, Test Loss Force: 12.38884410789589, time: 11.535401344299316

Epoch 10, Batch 100/251, Loss: 0.08508656919002533
Epoch 10, Batch 200/251, Loss: 0.3336433172225952

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.19042786456162, Training Loss Force: 4.712565213527932, time: 3.875981569290161
Validation Loss Energy: 11.899339847941897, Validation Loss Force: 4.325630726793461, time: 0.23726415634155273
Test Loss Energy: 18.681607885220206, Test Loss Force: 12.914472902049697, time: 11.427557468414307

Epoch 11, Batch 100/251, Loss: 0.48448994755744934
Epoch 11, Batch 200/251, Loss: 1.1167094707489014

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.612969568480667, Training Loss Force: 4.043166006643969, time: 3.8361053466796875
Validation Loss Energy: 1.4091219992287438, Validation Loss Force: 4.184060355842325, time: 0.2412269115447998
Test Loss Energy: 10.001031539606474, Test Loss Force: 11.388827670393962, time: 11.682976245880127

Epoch 12, Batch 100/251, Loss: 0.2964605391025543
Epoch 12, Batch 200/251, Loss: 0.49753719568252563

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.615760689331312, Training Loss Force: 4.6683673202262055, time: 3.813697576522827
Validation Loss Energy: 6.258148855995783, Validation Loss Force: 2.7883725985275554, time: 0.23086810111999512
Test Loss Energy: 9.30769986291348, Test Loss Force: 11.315123469617951, time: 11.469971179962158

Epoch 13, Batch 100/251, Loss: 0.2427257001399994
Epoch 13, Batch 200/251, Loss: 0.2558368444442749

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 5.192374855983444, Training Loss Force: 2.7395261621157125, time: 3.8376429080963135
Validation Loss Energy: 2.0563719408922867, Validation Loss Force: 3.1321955257490743, time: 0.23714303970336914
Test Loss Energy: 9.301620477611321, Test Loss Force: 11.22714606933372, time: 11.661778688430786

Epoch 14, Batch 100/251, Loss: 0.6309738159179688
Epoch 14, Batch 200/251, Loss: 0.3224761486053467

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 5.386998327077035, Training Loss Force: 2.626721385509912, time: 3.7468490600585938
Validation Loss Energy: 3.724538373394868, Validation Loss Force: 2.882505868633955, time: 0.2387237548828125
Test Loss Energy: 11.226258265298572, Test Loss Force: 11.512706005418057, time: 11.345855712890625

Epoch 15, Batch 100/251, Loss: 0.831723690032959
Epoch 15, Batch 200/251, Loss: 0.455832302570343

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.350309790684305, Training Loss Force: 4.188690585967663, time: 3.8980634212493896
Validation Loss Energy: 5.725067341849071, Validation Loss Force: 2.7466864850492687, time: 0.2463223934173584
Test Loss Energy: 11.154953253455703, Test Loss Force: 11.055137940645793, time: 11.574025630950928

Epoch 16, Batch 100/251, Loss: 0.5465954542160034
Epoch 16, Batch 200/251, Loss: 0.20877619087696075

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 5.45998959913173, Training Loss Force: 2.596241831342484, time: 3.7948555946350098
Validation Loss Energy: 7.974130370218314, Validation Loss Force: 2.7541344339768346, time: 0.24102401733398438
Test Loss Energy: 14.574367166454605, Test Loss Force: 11.45524668697614, time: 10.374531507492065

Epoch 17, Batch 100/251, Loss: 0.12161316722631454
Epoch 17, Batch 200/251, Loss: 1.0418531894683838

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 11.27985480352547, Training Loss Force: 4.998636830522891, time: 3.8075287342071533
Validation Loss Energy: 5.648120789771108, Validation Loss Force: 3.406322008284039, time: 0.2547285556793213
Test Loss Energy: 11.52786373071201, Test Loss Force: 11.237611094838561, time: 11.978765726089478

Epoch 18, Batch 100/251, Loss: 0.5061034560203552
Epoch 18, Batch 200/251, Loss: 0.4415876865386963

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.123296009534015, Training Loss Force: 3.0178112626338507, time: 3.6328158378601074
Validation Loss Energy: 4.559984770157529, Validation Loss Force: 2.893320797717095, time: 0.19695425033569336
Test Loss Energy: 9.244283610878366, Test Loss Force: 11.060805115229133, time: 9.009765625

Epoch 19, Batch 100/251, Loss: 0.47099676728248596
Epoch 19, Batch 200/251, Loss: 1.4119213819503784

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.418837331693272, Training Loss Force: 3.5131655060318785, time: 3.690404176712036
Validation Loss Energy: 2.594459836781466, Validation Loss Force: 7.119557473162467, time: 0.19798851013183594
Test Loss Energy: 9.125464048382511, Test Loss Force: 13.212966553324915, time: 9.174664497375488

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.060 MB uploadedwandb: | 0.039 MB of 0.060 MB uploadedwandb: / 0.039 MB of 0.060 MB uploadedwandb: - 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–â–‚â–‚â–‚â–ƒâ–‚â–â–ƒâ–ˆâ–‚â–â–â–ƒâ–ƒâ–…â–ƒâ–â–
wandb:   test_error_force â–â–â–â–â–â–â–ˆâ–…â–â–…â–†â–‚â–‚â–â–‚â–â–‚â–‚â–â–‡
wandb:          test_loss â–â–â–â–â–â–â–†â–„â–â–…â–ˆâ–‚â–â–â–ƒâ–‚â–„â–‚â–â–…
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–‡â–†â–‡â–‡â–‡â–†â–„â–„â–‡â–„â–ˆâ–„â–‡
wandb:  train_error_force â–â–â–â–â–â–â–‡â–ˆâ–†â–‡â–†â–…â–†â–ƒâ–‚â–…â–‚â–‡â–ƒâ–„
wandb:         train_loss â–â–â–â–â–â–â–‡â–ˆâ–†â–‡â–‡â–†â–†â–ƒâ–ƒâ–†â–ƒâ–ˆâ–„â–…
wandb: valid_error_energy â–â–â–‚â–â–â–â–‚â–‚â–â–ˆâ–†â–â–„â–â–‚â–ƒâ–„â–ƒâ–ƒâ–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–…â–ƒâ–…â–„â–„â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–ˆ
wandb:         valid_loss â–â–â–â–â–â–â–ˆâ–…â–ƒâ–ˆâ–†â–„â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 8011
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.12546
wandb:   test_error_force 13.21297
wandb:          test_loss 5.03178
wandb: train_error_energy 9.41884
wandb:  train_error_force 3.51317
wandb:         train_loss 1.80583
wandb: valid_error_energy 2.59446
wandb:  valid_error_force 7.11956
wandb:         valid_loss 2.55585
wandb: 
wandb: ğŸš€ View run al_77_80 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/la2lorf7
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_000110-la2lorf7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.713082492351532, Uncertainty Bias: -0.670417308807373
4.005432e-05 0.00024700165
-7.1478124 70.660706
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 11 steps.
Found uncertainty sample 4 after 14 steps.
Found uncertainty sample 5 after 3 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 11 steps.
Found uncertainty sample 8 after 5 steps.
Found uncertainty sample 9 after 14 steps.
Found uncertainty sample 10 after 6 steps.
Found uncertainty sample 11 after 8 steps.
Found uncertainty sample 12 after 2 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 12 steps.
Found uncertainty sample 15 after 15 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 18 steps.
Found uncertainty sample 18 after 12 steps.
Found uncertainty sample 19 after 13 steps.
Found uncertainty sample 20 after 31 steps.
Found uncertainty sample 21 after 12 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 8 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 8 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 7 steps.
Found uncertainty sample 34 after 11 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 8 steps.
Found uncertainty sample 37 after 2 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 11 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 11 steps.
Found uncertainty sample 46 after 5 steps.
Found uncertainty sample 47 after 17 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 12 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 12 steps.
Found uncertainty sample 53 after 19 steps.
Found uncertainty sample 54 after 11 steps.
Found uncertainty sample 55 after 11 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 32 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 6 steps.
Found uncertainty sample 60 after 21 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 3 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 11 steps.
Found uncertainty sample 65 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 8 steps.
Found uncertainty sample 68 after 24 steps.
Found uncertainty sample 69 after 11 steps.
Found uncertainty sample 70 after 10 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 9 steps.
Found uncertainty sample 73 after 8 steps.
Found uncertainty sample 74 after 6 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 8 steps.
Found uncertainty sample 77 after 3 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 6 steps.
Found uncertainty sample 81 after 9 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 22 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 3 steps.
Found uncertainty sample 92 after 4 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 11 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 15 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 10 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_001115-oujrrhkd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_81
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/oujrrhkd
Training model 81. Added 100 samples to the dataset.
Epoch 0, Batch 100/254, Loss: 0.10167908668518066
Epoch 0, Batch 200/254, Loss: 0.2151605188846588

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.278338485012855, Training Loss Force: 2.0410376714842866, time: 3.7506566047668457
Validation Loss Energy: 4.078739631171649, Validation Loss Force: 2.2384367557199907, time: 0.21109533309936523
Test Loss Energy: 11.650092966383294, Test Loss Force: 11.175999521438264, time: 9.841034173965454

Epoch 1, Batch 100/254, Loss: 0.15945307910442352
Epoch 1, Batch 200/254, Loss: 0.20849984884262085

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5519218525163723, Training Loss Force: 1.9904635945619662, time: 3.6826212406158447
Validation Loss Energy: 1.559785593593063, Validation Loss Force: 2.1312309697271763, time: 0.22154760360717773
Test Loss Energy: 10.245899997308014, Test Loss Force: 11.145440808888754, time: 10.080228567123413

Epoch 2, Batch 100/254, Loss: 0.09244245290756226
Epoch 2, Batch 200/254, Loss: 0.05936622992157936

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.0614330521247046, Training Loss Force: 1.9114143356597983, time: 3.702692985534668
Validation Loss Energy: 3.1827700905272205, Validation Loss Force: 2.0597795024997057, time: 0.21161580085754395
Test Loss Energy: 9.501366704657901, Test Loss Force: 11.015591154212506, time: 11.16764760017395

Epoch 3, Batch 100/254, Loss: 0.11219848692417145
Epoch 3, Batch 200/254, Loss: 0.07549925893545151

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.7736079785257346, Training Loss Force: 1.9515679002501063, time: 3.677044153213501
Validation Loss Energy: 0.882814651090841, Validation Loss Force: 2.4684603852013414, time: 0.21323442459106445
Test Loss Energy: 10.241103680588589, Test Loss Force: 11.193333668591634, time: 9.932962656021118

Epoch 4, Batch 100/254, Loss: 0.04576374590396881
Epoch 4, Batch 200/254, Loss: 0.07557453215122223

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.2362661889394273, Training Loss Force: 1.9744591151326039, time: 3.7834346294403076
Validation Loss Energy: 1.5116176926113907, Validation Loss Force: 2.04211348913466, time: 0.21680212020874023
Test Loss Energy: 9.827309590621727, Test Loss Force: 11.058210369158743, time: 9.988018035888672

Epoch 5, Batch 100/254, Loss: 0.11998175829648972
Epoch 5, Batch 200/254, Loss: 0.20132328569889069

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.39188035902984, Training Loss Force: 1.9555521477693232, time: 3.65533447265625
Validation Loss Energy: 3.4513766606019223, Validation Loss Force: 2.1090933145736086, time: 0.2137141227722168
Test Loss Energy: 9.590516058123622, Test Loss Force: 11.072388653320447, time: 9.9322350025177

Epoch 6, Batch 100/254, Loss: 0.4614284038543701
Epoch 6, Batch 200/254, Loss: 0.39701569080352783

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.040776551182421, Training Loss Force: 3.390333678065983, time: 3.7282767295837402
Validation Loss Energy: 4.599273182272466, Validation Loss Force: 3.11111070598474, time: 0.30089449882507324
Test Loss Energy: 13.04317828446697, Test Loss Force: 11.399772961769777, time: 9.955055475234985

Epoch 7, Batch 100/254, Loss: 0.25372645258903503
Epoch 7, Batch 200/254, Loss: 0.4499668776988983

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.627039490122296, Training Loss Force: 3.3215611902339544, time: 3.7055065631866455
Validation Loss Energy: 10.632199211240387, Validation Loss Force: 4.355322878060921, time: 0.20966434478759766
Test Loss Energy: 16.086173401105068, Test Loss Force: 12.13835195555062, time: 10.009108066558838

Epoch 8, Batch 100/254, Loss: 0.9128225445747375
Epoch 8, Batch 200/254, Loss: 0.18024387955665588

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.754084541184424, Training Loss Force: 4.679563178201948, time: 3.7075541019439697
Validation Loss Energy: 6.267557963426107, Validation Loss Force: 3.8126819548103827, time: 0.21801304817199707
Test Loss Energy: 12.779256342716293, Test Loss Force: 11.50157653518177, time: 10.112109661102295

Epoch 9, Batch 100/254, Loss: 0.20308977365493774
Epoch 9, Batch 200/254, Loss: 0.1557551920413971

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.813580199186027, Training Loss Force: 2.837986479630176, time: 3.647664785385132
Validation Loss Energy: 28.64927675980063, Validation Loss Force: 5.031137875442524, time: 0.21208739280700684
Test Loss Energy: 21.703450752688738, Test Loss Force: 11.623088081382164, time: 9.917454957962036

Epoch 10, Batch 100/254, Loss: 0.7944256067276001
Epoch 10, Batch 200/254, Loss: 0.48641759157180786

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.256980785743655, Training Loss Force: 3.9877241314906704, time: 3.6903815269470215
Validation Loss Energy: 8.983762823751029, Validation Loss Force: 2.7759390317553807, time: 0.20907068252563477
Test Loss Energy: 14.192644013316315, Test Loss Force: 11.630793137978722, time: 10.074973344802856

Epoch 11, Batch 100/254, Loss: 0.38938817381858826
Epoch 11, Batch 200/254, Loss: 0.7047923803329468

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.582559168157174, Training Loss Force: 3.189003846518297, time: 3.6809768676757812
Validation Loss Energy: 8.92064168760915, Validation Loss Force: 5.437610158552988, time: 0.2141873836517334
Test Loss Energy: 14.698641432867916, Test Loss Force: 12.122401416179688, time: 9.925431728363037

Epoch 12, Batch 100/254, Loss: 0.45487380027770996
Epoch 12, Batch 200/254, Loss: 0.3603660464286804

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.57050438607515, Training Loss Force: 4.4293427676995805, time: 3.6732101440429688
Validation Loss Energy: 3.204013005385164, Validation Loss Force: 7.856515859331818, time: 0.20796823501586914
Test Loss Energy: 9.510846049631176, Test Loss Force: 13.878849023308673, time: 10.207617998123169

Epoch 13, Batch 100/254, Loss: 0.9480389952659607
Epoch 13, Batch 200/254, Loss: 0.8145415782928467

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.902054777300396, Training Loss Force: 4.7864545032000265, time: 3.592003583908081
Validation Loss Energy: 7.405764703207884, Validation Loss Force: 3.3351444241060597, time: 0.21010804176330566
Test Loss Energy: 9.348209722548077, Test Loss Force: 11.156524486224248, time: 9.982702732086182

Epoch 14, Batch 100/254, Loss: 0.0956878513097763
Epoch 14, Batch 200/254, Loss: 0.41012296080589294

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.007457086141812, Training Loss Force: 4.620344941264081, time: 3.6627719402313232
Validation Loss Energy: 13.530927974504632, Validation Loss Force: 5.058681492120926, time: 0.21309542655944824
Test Loss Energy: 11.431721950518034, Test Loss Force: 11.762769938182142, time: 9.99925971031189

Epoch 15, Batch 100/254, Loss: 1.056380271911621
Epoch 15, Batch 200/254, Loss: 2.1535444259643555

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.290737117858864, Training Loss Force: 4.596966183520073, time: 3.8852291107177734
Validation Loss Energy: 11.628496232292633, Validation Loss Force: 3.8353370930590867, time: 0.20971226692199707
Test Loss Energy: 10.884386901696367, Test Loss Force: 11.066534870334742, time: 10.083540439605713

Epoch 16, Batch 100/254, Loss: 0.6564552783966064
Epoch 16, Batch 200/254, Loss: 0.3369915187358856

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.856524925611351, Training Loss Force: 3.874266565140738, time: 3.7784101963043213
Validation Loss Energy: 6.371355965142357, Validation Loss Force: 4.542234921859041, time: 0.20832419395446777
Test Loss Energy: 11.909238904436526, Test Loss Force: 12.042319024428986, time: 9.927348375320435

Epoch 17, Batch 100/254, Loss: 0.5333733558654785
Epoch 17, Batch 200/254, Loss: 0.22570374608039856

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.433306204017796, Training Loss Force: 4.647849817017693, time: 3.802459716796875
Validation Loss Energy: 10.87276004571262, Validation Loss Force: 5.0497408805736335, time: 0.21038341522216797
Test Loss Energy: 11.251747158132256, Test Loss Force: 12.076115267085404, time: 9.986968040466309

Epoch 18, Batch 100/254, Loss: 0.1711074709892273
Epoch 18, Batch 200/254, Loss: 0.7236774563789368

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.769322510752586, Training Loss Force: 4.521958755342574, time: 3.695856809616089
Validation Loss Energy: 15.836234557105481, Validation Loss Force: 4.546224818950348, time: 0.2146291732788086
Test Loss Energy: 13.10351048722449, Test Loss Force: 11.815117612414634, time: 9.839193344116211

Epoch 19, Batch 100/254, Loss: 0.5065760016441345
Epoch 19, Batch 200/254, Loss: 0.5071585774421692

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.727276040830121, Training Loss Force: 4.553106400138744, time: 3.682593584060669
Validation Loss Energy: 3.7069513519093396, Validation Loss Force: 5.759179475663196, time: 0.21524548530578613
Test Loss Energy: 9.63285031252181, Test Loss Force: 12.654706600761065, time: 10.255415201187134

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.050 MB uploadedwandb: / 0.039 MB of 0.050 MB uploadedwandb: - 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–â–‚â–â–â–ƒâ–…â–ƒâ–ˆâ–„â–„â–â–â–‚â–‚â–‚â–‚â–ƒâ–
wandb:   test_error_force â–â–â–â–â–â–â–‚â–„â–‚â–‚â–ƒâ–„â–ˆâ–â–ƒâ–â–„â–„â–ƒâ–…
wandb:          test_loss â–‚â–‚â–â–‚â–â–â–„â–‡â–„â–ˆâ–…â–†â–ˆâ–â–„â–‚â–„â–„â–„â–…
wandb: train_error_energy â–â–‚â–â–â–â–‚â–…â–†â–‡â–…â–†â–†â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡
wandb:  train_error_force â–â–â–â–â–â–â–…â–„â–ˆâ–ƒâ–†â–„â–‡â–ˆâ–ˆâ–ˆâ–†â–ˆâ–‡â–‡
wandb:         train_loss â–â–â–â–â–â–â–…â–…â–‡â–„â–†â–…â–‡â–ˆâ–‡â–ˆâ–†â–ˆâ–ˆâ–‡
wandb: valid_error_energy â–‚â–â–‚â–â–â–‚â–‚â–ƒâ–‚â–ˆâ–ƒâ–ƒâ–‚â–ƒâ–„â–„â–‚â–„â–…â–‚
wandb:  valid_error_force â–â–â–â–‚â–â–â–‚â–„â–ƒâ–…â–‚â–…â–ˆâ–ƒâ–…â–ƒâ–„â–…â–„â–…
wandb:         valid_loss â–‚â–â–â–â–â–â–‚â–„â–ƒâ–ˆâ–ƒâ–…â–†â–ƒâ–†â–„â–„â–…â–…â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 8101
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.63285
wandb:   test_error_force 12.65471
wandb:          test_loss 4.87894
wandb: train_error_energy 8.72728
wandb:  train_error_force 4.55311
wandb:         train_loss 2.10752
wandb: valid_error_energy 3.70695
wandb:  valid_error_force 5.75918
wandb:         valid_loss 2.17511
wandb: 
wandb: ğŸš€ View run al_77_81 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/oujrrhkd
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_001115-oujrrhkd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5713859796524048, Uncertainty Bias: -0.4026460647583008
7.6293945e-06 0.20900774
-5.552353 47.63117
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 9 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 4 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 4 steps.
Found uncertainty sample 6 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 5 steps.
Found uncertainty sample 10 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 6 steps.
Found uncertainty sample 13 after 6 steps.
Found uncertainty sample 14 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 11 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 9 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 9 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 2 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 4 steps.
Found uncertainty sample 34 after 4 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 6 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 2 steps.
Found uncertainty sample 42 after 7 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 11 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 2 steps.
Found uncertainty sample 52 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 7 steps.
Found uncertainty sample 55 after 5 steps.
Found uncertainty sample 56 after 7 steps.
Found uncertainty sample 57 after 15 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 5 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 9 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 4 steps.
Found uncertainty sample 69 after 2 steps.
Found uncertainty sample 70 after 2 steps.
Found uncertainty sample 71 after 14 steps.
Found uncertainty sample 72 after 3 steps.
Found uncertainty sample 73 after 6 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 4 steps.
Found uncertainty sample 78 after 3 steps.
Found uncertainty sample 79 after 3 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 5 steps.
Found uncertainty sample 83 after 9 steps.
Found uncertainty sample 84 after 2 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 87 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 2 steps.
Found uncertainty sample 91 after 3 steps.
Found uncertainty sample 92 after 8 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 8 steps.
Found uncertainty sample 96 after 5 steps.
Found uncertainty sample 97 after 2 steps.
Found uncertainty sample 98 after 11 steps.
Found uncertainty sample 99 after 22 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_002056-q652cjxh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_82
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/q652cjxh
Training model 82. Added 100 samples to the dataset.
Epoch 0, Batch 100/256, Loss: 0.04068084806203842
Epoch 0, Batch 200/256, Loss: 0.04282642528414726

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.3151841052065711, Training Loss Force: 1.9715215387272922, time: 3.663569927215576
Validation Loss Energy: 1.8920511014643457, Validation Loss Force: 2.048245741746225, time: 0.22615718841552734
Test Loss Energy: 10.449707835483478, Test Loss Force: 11.324814352611469, time: 9.895753860473633

Epoch 1, Batch 100/256, Loss: 0.16375213861465454
Epoch 1, Batch 200/256, Loss: 0.1548846960067749

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.639756862691527, Training Loss Force: 1.944046375850136, time: 3.625662088394165
Validation Loss Energy: 4.348497957403661, Validation Loss Force: 2.207127859451236, time: 0.21168208122253418
Test Loss Energy: 9.533909658106786, Test Loss Force: 11.177600514929097, time: 10.09477949142456

Epoch 2, Batch 100/256, Loss: 0.271077424287796
Epoch 2, Batch 200/256, Loss: 0.24258485436439514

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.8481263863704394, Training Loss Force: 1.9802878473025962, time: 3.7173709869384766
Validation Loss Energy: 3.6226996374936373, Validation Loss Force: 2.1099656766200674, time: 0.21907949447631836
Test Loss Energy: 11.522948142605246, Test Loss Force: 11.171079960261084, time: 10.038017988204956

Epoch 3, Batch 100/256, Loss: 0.23862850666046143
Epoch 3, Batch 200/256, Loss: 0.10087186843156815

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.627472979415104, Training Loss Force: 1.980432049059221, time: 3.6800789833068848
Validation Loss Energy: 2.5588814266414053, Validation Loss Force: 2.0511642662419, time: 0.23527050018310547
Test Loss Energy: 10.933597864815347, Test Loss Force: 11.260421387432597, time: 11.011221170425415

Epoch 4, Batch 100/256, Loss: 0.17612290382385254
Epoch 4, Batch 200/256, Loss: 0.16884899139404297

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.0400644602550897, Training Loss Force: 1.8806811212610062, time: 3.815444231033325
Validation Loss Energy: 0.8937897767076542, Validation Loss Force: 2.039154494521712, time: 0.22560334205627441
Test Loss Energy: 10.050187448653478, Test Loss Force: 11.201813814334834, time: 9.947479248046875

Epoch 5, Batch 100/256, Loss: 0.07227186858654022
Epoch 5, Batch 200/256, Loss: 0.2036341428756714

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.712125102391575, Training Loss Force: 1.9134413515382818, time: 3.6584603786468506
Validation Loss Energy: 1.1895414970241203, Validation Loss Force: 2.009383629711167, time: 0.21100926399230957
Test Loss Energy: 9.926029762944841, Test Loss Force: 11.153975066731835, time: 9.96864366531372

Epoch 6, Batch 100/256, Loss: 0.0957777127623558
Epoch 6, Batch 200/256, Loss: 0.09999781101942062

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 12.406143439021028, Training Loss Force: 5.637393812391204, time: 3.7351343631744385
Validation Loss Energy: 6.650176368940765, Validation Loss Force: 4.296440583115496, time: 0.30863523483276367
Test Loss Energy: 13.35497453093557, Test Loss Force: 11.391399291515771, time: 10.181031942367554

Epoch 7, Batch 100/256, Loss: 0.35384994745254517
Epoch 7, Batch 200/256, Loss: 0.8449618816375732

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.435784768280074, Training Loss Force: 4.894666277403189, time: 3.7509944438934326
Validation Loss Energy: 6.820922966748247, Validation Loss Force: 6.018812367822435, time: 0.21709990501403809
Test Loss Energy: 12.73088456947861, Test Loss Force: 12.659455240635507, time: 10.021254062652588

Epoch 8, Batch 100/256, Loss: 1.6965099573135376
Epoch 8, Batch 200/256, Loss: 0.2589617371559143

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.2645450879147635, Training Loss Force: 3.8704630281872046, time: 3.678199052810669
Validation Loss Energy: 6.008188154862783, Validation Loss Force: 2.749210161481928, time: 0.2203383445739746
Test Loss Energy: 9.362344699309606, Test Loss Force: 11.428960685232315, time: 10.199902296066284

Epoch 9, Batch 100/256, Loss: 0.43600642681121826
Epoch 9, Batch 200/256, Loss: 1.141270637512207

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 7.659321528878547, Training Loss Force: 3.661996077875046, time: 3.6174137592315674
Validation Loss Energy: 13.952284799343783, Validation Loss Force: 4.995204642018426, time: 0.21161437034606934
Test Loss Energy: 19.198417740776577, Test Loss Force: 12.671077231706443, time: 9.863283157348633

Epoch 10, Batch 100/256, Loss: 0.13694804906845093
Epoch 10, Batch 200/256, Loss: 0.7610249519348145

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 10.081171527454341, Training Loss Force: 5.041020128713904, time: 3.7653183937072754
Validation Loss Energy: 2.8911652490499407, Validation Loss Force: 4.294473635214942, time: 0.21502470970153809
Test Loss Energy: 11.391725544949418, Test Loss Force: 11.432783347885668, time: 10.077518224716187

Epoch 11, Batch 100/256, Loss: 0.2887008488178253
Epoch 11, Batch 200/256, Loss: 0.434026837348938

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.737357415452067, Training Loss Force: 4.727845775018296, time: 3.731921911239624
Validation Loss Energy: 1.6859723099249133, Validation Loss Force: 6.506457463400571, time: 0.21262860298156738
Test Loss Energy: 9.740470796774225, Test Loss Force: 13.037512103151235, time: 9.929351568222046

Epoch 12, Batch 100/256, Loss: 0.879264235496521
Epoch 12, Batch 200/256, Loss: 0.46369755268096924

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.627756782783939, Training Loss Force: 5.023928674808666, time: 3.6901166439056396
Validation Loss Energy: 2.2259578757766194, Validation Loss Force: 5.177747360500579, time: 0.2160358428955078
Test Loss Energy: 10.36318629503922, Test Loss Force: 12.127320040048003, time: 10.122783422470093

Epoch 13, Batch 100/256, Loss: 0.06077968329191208
Epoch 13, Batch 200/256, Loss: 0.7078164219856262

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.15220658178422, Training Loss Force: 4.914718250683144, time: 3.700328826904297
Validation Loss Energy: 5.116735680512646, Validation Loss Force: 5.23217593852373, time: 0.2155778408050537
Test Loss Energy: 9.653258142596519, Test Loss Force: 11.539148253285513, time: 9.83017611503601

Epoch 14, Batch 100/256, Loss: 0.49796098470687866
Epoch 14, Batch 200/256, Loss: 0.89044189453125

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.199938170837562, Training Loss Force: 4.301983114759899, time: 3.6860899925231934
Validation Loss Energy: 16.462484849953007, Validation Loss Force: 3.9003714025027776, time: 0.22635698318481445
Test Loss Energy: 19.684911977868826, Test Loss Force: 11.895730562952311, time: 9.956140279769897

Epoch 15, Batch 100/256, Loss: 0.48760756850242615
Epoch 15, Batch 200/256, Loss: 0.12494625896215439

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.003192734926092, Training Loss Force: 4.680044385711217, time: 3.824414014816284
Validation Loss Energy: 11.26593096472194, Validation Loss Force: 4.246840474187216, time: 0.2137010097503662
Test Loss Energy: 11.161409483554104, Test Loss Force: 11.637056468979942, time: 9.867869138717651

Epoch 16, Batch 100/256, Loss: 0.3805103003978729
Epoch 16, Batch 200/256, Loss: 0.8113260865211487

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.349861139387542, Training Loss Force: 4.768822855138463, time: 3.596421003341675
Validation Loss Energy: 10.73261208526567, Validation Loss Force: 3.6538195989920723, time: 0.21358013153076172
Test Loss Energy: 11.151827081237899, Test Loss Force: 11.17907033647426, time: 9.9437894821167

Epoch 17, Batch 100/256, Loss: 0.6368147134780884
Epoch 17, Batch 200/256, Loss: 1.1229157447814941

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.521268335631333, Training Loss Force: 4.810119998671266, time: 3.904062509536743
Validation Loss Energy: 3.915003493087271, Validation Loss Force: 3.344465855560175, time: 0.21615362167358398
Test Loss Energy: 10.784863819407244, Test Loss Force: 11.654813570929875, time: 9.91985034942627

Epoch 18, Batch 100/256, Loss: 0.05671615153551102
Epoch 18, Batch 200/256, Loss: 0.18835821747779846

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 5.470403148593457, Training Loss Force: 2.8430678715618307, time: 3.710527181625366
Validation Loss Energy: 5.1623480077065045, Validation Loss Force: 3.269770878218548, time: 0.21251678466796875
Test Loss Energy: 11.533582466609415, Test Loss Force: 11.93571175240735, time: 9.885274410247803

Epoch 19, Batch 100/256, Loss: 0.4554669260978699
Epoch 19, Batch 200/256, Loss: 0.6563462018966675

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.125091295302052, Training Loss Force: 3.6280648236009254, time: 3.700150728225708
Validation Loss Energy: 22.27739473880102, Validation Loss Force: 4.126169161265425, time: 0.23916244506835938
Test Loss Energy: 15.623823032614077, Test Loss Force: 11.547790735328357, time: 10.135381937026978

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‚â–‚â–â–â–„â–ƒâ–â–ˆâ–‚â–â–‚â–â–ˆâ–‚â–‚â–‚â–‚â–…
wandb:   test_error_force â–‚â–â–â–â–â–â–‚â–‡â–‚â–‡â–‚â–ˆâ–…â–‚â–„â–ƒâ–â–ƒâ–„â–‚
wandb:          test_loss â–‚â–â–‚â–‚â–â–â–ƒâ–…â–â–ˆâ–‚â–…â–ƒâ–‚â–‡â–ƒâ–‚â–‚â–ƒâ–„
wandb: train_error_energy â–â–â–‚â–‚â–â–â–ˆâ–…â–…â–…â–‡â–†â–†â–…â–…â–†â–†â–†â–„â–…
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–…â–„â–‡â–†â–‡â–‡â–†â–†â–†â–†â–ƒâ–„
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–…â–…â–‡â–†â–‡â–†â–…â–†â–†â–†â–ƒâ–…
wandb: valid_error_energy â–â–‚â–‚â–‚â–â–â–ƒâ–ƒâ–ƒâ–…â–‚â–â–â–‚â–†â–„â–„â–‚â–‚â–ˆ
wandb:  valid_error_force â–â–â–â–â–â–â–…â–‡â–‚â–†â–…â–ˆâ–†â–†â–„â–„â–„â–ƒâ–ƒâ–„
wandb:         valid_loss â–â–‚â–‚â–â–â–â–…â–‡â–ƒâ–‡â–„â–†â–…â–…â–†â–†â–…â–ƒâ–ƒâ–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 8191
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 15.62382
wandb:   test_error_force 11.54779
wandb:          test_loss 4.90948
wandb: train_error_energy 8.12509
wandb:  train_error_force 3.62806
wandb:         train_loss 1.7577
wandb: valid_error_energy 22.27739
wandb:  valid_error_force 4.12617
wandb:         valid_loss 2.87145
wandb: 
wandb: ğŸš€ View run al_77_82 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/q652cjxh
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_002056-q652cjxh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7351059913635254, Uncertainty Bias: -0.576706051826477
9.536743e-06 0.0004286766
-7.55048 24.918068
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 3 steps.
Found uncertainty sample 3 after 10 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 2 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 2 steps.
Found uncertainty sample 8 after 12 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 3 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 6 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 6 steps.
Found uncertainty sample 22 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 3 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 28 after 9 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 36 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 5 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 23 steps.
Found uncertainty sample 47 after 2 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 4 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 10 steps.
Found uncertainty sample 53 after 5 steps.
Found uncertainty sample 54 after 5 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 3 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 5 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 2 steps.
Found uncertainty sample 73 after 9 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 6 steps.
Found uncertainty sample 80 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 4 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 7 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 4 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 4 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_003030-7uvh7qoa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_83
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/7uvh7qoa
Training model 83. Added 100 samples to the dataset.
Epoch 0, Batch 100/259, Loss: 0.04315369576215744
Epoch 0, Batch 200/259, Loss: 0.065304234623909

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.2791703631020528, Training Loss Force: 1.965063439534957, time: 3.758065938949585
Validation Loss Energy: 1.0265773862690462, Validation Loss Force: 2.073059412428638, time: 0.21753549575805664
Test Loss Energy: 9.516183953672238, Test Loss Force: 11.234566421859778, time: 9.93804931640625

Epoch 1, Batch 100/259, Loss: 0.11172240227460861
Epoch 1, Batch 200/259, Loss: 0.08321848511695862

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.4533690951412945, Training Loss Force: 1.9219803935453001, time: 3.796029806137085
Validation Loss Energy: 1.4559186256755914, Validation Loss Force: 1.967588029813827, time: 0.22022414207458496
Test Loss Energy: 10.14072183684947, Test Loss Force: 11.272921688206099, time: 9.900814771652222

Epoch 2, Batch 100/259, Loss: 0.10034775733947754
Epoch 2, Batch 200/259, Loss: 0.09391990303993225

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.8772039746001212, Training Loss Force: 1.9239893885163337, time: 3.6513357162475586
Validation Loss Energy: 1.3432168418887864, Validation Loss Force: 2.3639784329364604, time: 0.21838831901550293
Test Loss Energy: 10.274135676103898, Test Loss Force: 11.434985552733737, time: 9.746726036071777

Epoch 3, Batch 100/259, Loss: 0.04551580548286438
Epoch 3, Batch 200/259, Loss: 0.06794445961713791

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.2934067925372565, Training Loss Force: 1.9108925284013343, time: 3.6965091228485107
Validation Loss Energy: 3.240871173041073, Validation Loss Force: 1.9745810211424328, time: 0.21680831909179688
Test Loss Energy: 11.315898250991532, Test Loss Force: 11.28869851300061, time: 9.788645029067993

Epoch 4, Batch 100/259, Loss: 0.05811360478401184
Epoch 4, Batch 200/259, Loss: 0.09737572073936462

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.659214170802407, Training Loss Force: 1.91216766604824, time: 3.928830623626709
Validation Loss Energy: 1.0671875067669236, Validation Loss Force: 2.0225834789486807, time: 0.22334814071655273
Test Loss Energy: 10.294423087637137, Test Loss Force: 11.293324654533807, time: 9.794929265975952

Epoch 5, Batch 100/259, Loss: 0.09151634573936462
Epoch 5, Batch 200/259, Loss: 0.2989560663700104

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2653753488120631, Training Loss Force: 1.8690495139894525, time: 3.6950716972351074
Validation Loss Energy: 0.801921897958095, Validation Loss Force: 2.0126365809971114, time: 0.2187345027923584
Test Loss Energy: 10.105726673562955, Test Loss Force: 11.24854451829864, time: 10.922616958618164

Epoch 6, Batch 100/259, Loss: 0.8006147146224976
Epoch 6, Batch 200/259, Loss: 0.5927743911743164

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.481764598748894, Training Loss Force: 4.877586706841552, time: 3.673210859298706
Validation Loss Energy: 4.804994422830824, Validation Loss Force: 5.593327546487671, time: 0.21480798721313477
Test Loss Energy: 10.857256920935894, Test Loss Force: 12.620192594022111, time: 10.010670900344849

Epoch 7, Batch 100/259, Loss: 0.4268808960914612
Epoch 7, Batch 200/259, Loss: 0.72518390417099

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.241740463977008, Training Loss Force: 4.666600102269757, time: 3.7136237621307373
Validation Loss Energy: 2.411675989881189, Validation Loss Force: 3.684174135301381, time: 0.2203996181488037
Test Loss Energy: 9.167304145152954, Test Loss Force: 11.39560300053397, time: 9.78167200088501

Epoch 8, Batch 100/259, Loss: 0.29021161794662476
Epoch 8, Batch 200/259, Loss: 0.4326738715171814

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.874450518859623, Training Loss Force: 4.191546789993762, time: 3.7089574337005615
Validation Loss Energy: 20.75406656963521, Validation Loss Force: 6.69015772743783, time: 0.2143704891204834
Test Loss Energy: 23.988416830571833, Test Loss Force: 13.407549416298243, time: 10.04691743850708

Epoch 9, Batch 100/259, Loss: 0.5213155746459961
Epoch 9, Batch 200/259, Loss: 0.9636833667755127

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.315801776942775, Training Loss Force: 4.70860325448525, time: 3.63820219039917
Validation Loss Energy: 9.057771679519853, Validation Loss Force: 4.966517285271965, time: 0.23479485511779785
Test Loss Energy: 12.67761682167192, Test Loss Force: 12.42524072047589, time: 9.736942529678345

Epoch 10, Batch 100/259, Loss: 0.857825756072998
Epoch 10, Batch 200/259, Loss: 0.5830100774765015

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.381238531851963, Training Loss Force: 4.198551362955054, time: 3.7042458057403564
Validation Loss Energy: 9.574363029260422, Validation Loss Force: 3.5596555894100783, time: 0.22243642807006836
Test Loss Energy: 10.169649977051751, Test Loss Force: 11.112439427466493, time: 9.988720893859863

Epoch 11, Batch 100/259, Loss: 0.8887217044830322
Epoch 11, Batch 200/259, Loss: 0.19459183514118195

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.193552932080086, Training Loss Force: 4.186297859221214, time: 3.7919349670410156
Validation Loss Energy: 4.653398863456411, Validation Loss Force: 3.7776359911320645, time: 0.23510146141052246
Test Loss Energy: 11.256578374451433, Test Loss Force: 11.430821210687895, time: 9.893291711807251

Epoch 12, Batch 100/259, Loss: 0.37604227662086487
Epoch 12, Batch 200/259, Loss: 0.29657405614852905

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 5.754268281971571, Training Loss Force: 3.1106339666148135, time: 3.6646957397460938
Validation Loss Energy: 5.973021638108859, Validation Loss Force: 2.7322438970414176, time: 0.22371816635131836
Test Loss Energy: 9.440138761050857, Test Loss Force: 11.244160374992209, time: 9.987450122833252

Epoch 13, Batch 100/259, Loss: 0.09316741675138474
Epoch 13, Batch 200/259, Loss: 0.4548293650150299

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 5.365633971698069, Training Loss Force: 2.5406654591736073, time: 3.651153564453125
Validation Loss Energy: 3.374231020040585, Validation Loss Force: 3.0373351852139345, time: 0.2194499969482422
Test Loss Energy: 10.645521095877164, Test Loss Force: 11.331538881543613, time: 9.964493751525879

Epoch 14, Batch 100/259, Loss: 0.3899722099304199
Epoch 14, Batch 200/259, Loss: 0.33190566301345825

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 5.513449612533294, Training Loss Force: 2.5285049907356223, time: 3.805044174194336
Validation Loss Energy: 5.744436974582695, Validation Loss Force: 2.5788447621188175, time: 0.25437378883361816
Test Loss Energy: 13.664080971427191, Test Loss Force: 11.244289112732762, time: 9.93949842453003

Epoch 15, Batch 100/259, Loss: 0.23104999959468842
Epoch 15, Batch 200/259, Loss: 0.08354705572128296

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.2027578430914, Training Loss Force: 4.060420491803496, time: 3.8744258880615234
Validation Loss Energy: 6.894482231244528, Validation Loss Force: 3.609194442056313, time: 0.2282404899597168
Test Loss Energy: 12.359725847880425, Test Loss Force: 11.541043911502257, time: 9.781319856643677

Epoch 16, Batch 100/259, Loss: 0.9535750150680542
Epoch 16, Batch 200/259, Loss: 0.3583085536956787

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.06736706406392, Training Loss Force: 5.076302136862421, time: 3.67692494392395
Validation Loss Energy: 27.760423925462288, Validation Loss Force: 4.4216097237173715, time: 0.21869897842407227
Test Loss Energy: 27.850503077198272, Test Loss Force: 12.502358227082093, time: 9.820918560028076

Epoch 17, Batch 100/259, Loss: 0.2996775507926941
Epoch 17, Batch 200/259, Loss: 0.15567994117736816

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.856917964130717, Training Loss Force: 4.561000877818648, time: 3.754730701446533
Validation Loss Energy: 1.5351387410680501, Validation Loss Force: 7.88270313144163, time: 0.22635936737060547
Test Loss Energy: 10.051848327320691, Test Loss Force: 13.369087623167221, time: 10.029749870300293

Epoch 18, Batch 100/259, Loss: 1.7284570932388306
Epoch 18, Batch 200/259, Loss: 0.18834903836250305

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.757623046743488, Training Loss Force: 5.058578528274369, time: 3.678313970565796
Validation Loss Energy: 7.189994203691573, Validation Loss Force: 3.589444579489102, time: 0.21518397331237793
Test Loss Energy: 13.811063401074016, Test Loss Force: 11.413095566875311, time: 10.054253816604614

Epoch 19, Batch 100/259, Loss: 0.23695559799671173
Epoch 19, Batch 200/259, Loss: 0.5795739889144897

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.01585425226039, Training Loss Force: 3.830229853799821, time: 3.7428619861602783
Validation Loss Energy: 16.37793381253789, Validation Loss Force: 4.055142802837901, time: 0.218658447265625
Test Loss Energy: 13.407768574033346, Test Loss Force: 11.6403253406435, time: 10.109546422958374

wandb: - 0.039 MB of 0.060 MB uploadedwandb: \ 0.039 MB of 0.060 MB uploadedwandb: | 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–â–â–‚â–â–‡â–‚â–â–‚â–â–‚â–ƒâ–‚â–ˆâ–â–ƒâ–ƒ
wandb:   test_error_force â–â–â–‚â–‚â–‚â–â–†â–‚â–ˆâ–…â–â–‚â–â–‚â–â–‚â–…â–ˆâ–‚â–ƒ
wandb:          test_loss â–â–â–â–‚â–â–â–ƒâ–â–ˆâ–„â–â–‚â–â–â–‚â–‚â–ˆâ–„â–‚â–ƒ
wandb: train_error_energy â–â–â–â–â–â–â–‡â–‡â–†â–‡â–‡â–†â–„â–„â–„â–†â–ˆâ–†â–ˆâ–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–†â–‡â–†â–†â–„â–‚â–‚â–†â–ˆâ–‡â–ˆâ–…
wandb:         train_loss â–â–â–â–â–â–â–‡â–‡â–†â–‡â–†â–†â–„â–ƒâ–ƒâ–†â–ˆâ–†â–ˆâ–†
wandb: valid_error_energy â–â–â–â–‚â–â–â–‚â–â–†â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–ƒâ–ˆâ–â–ƒâ–…
wandb:  valid_error_force â–â–â–â–â–â–â–…â–ƒâ–‡â–…â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–„â–ˆâ–ƒâ–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–…â–ƒâ–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–ƒâ–‡â–†â–ƒâ–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 8281
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 13.40777
wandb:   test_error_force 11.64033
wandb:          test_loss 4.79215
wandb: train_error_energy 9.01585
wandb:  train_error_force 3.83023
wandb:         train_loss 1.88495
wandb: valid_error_energy 16.37793
wandb:  valid_error_force 4.05514
wandb:         valid_loss 2.45289
wandb: 
wandb: ğŸš€ View run al_77_83 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/7uvh7qoa
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_003030-7uvh7qoa/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.2903737723827362, Uncertainty Bias: -0.17271870374679565
1.5258789e-05 0.011299133
-2.231239 18.91871
(48745, 22, 3)
Found uncertainty sample 0 after 30 steps.
Found uncertainty sample 1 after 51 steps.
Found uncertainty sample 2 after 9 steps.
Found uncertainty sample 3 after 25 steps.
Found uncertainty sample 4 after 8 steps.
Found uncertainty sample 5 after 11 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 166 steps.
Found uncertainty sample 8 after 83 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 5 steps.
Found uncertainty sample 11 after 7 steps.
Found uncertainty sample 12 after 19 steps.
Found uncertainty sample 13 after 63 steps.
Found uncertainty sample 14 after 32 steps.
Found uncertainty sample 15 after 63 steps.
Found uncertainty sample 16 after 17 steps.
Found uncertainty sample 17 after 6 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 61 steps.
Found uncertainty sample 21 after 45 steps.
Found uncertainty sample 22 after 21 steps.
Found uncertainty sample 23 after 45 steps.
Found uncertainty sample 24 after 10 steps.
Found uncertainty sample 25 after 27 steps.
Found uncertainty sample 26 after 47 steps.
Found uncertainty sample 27 after 45 steps.
Found uncertainty sample 28 after 26 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 15 steps.
Found uncertainty sample 31 after 12 steps.
Found uncertainty sample 32 after 36 steps.
Found uncertainty sample 33 after 46 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 21 steps.
Found uncertainty sample 36 after 34 steps.
Found uncertainty sample 37 after 20 steps.
Found uncertainty sample 38 after 5 steps.
Found uncertainty sample 39 after 15 steps.
Found uncertainty sample 40 after 13 steps.
Found uncertainty sample 41 after 12 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 52 steps.
Found uncertainty sample 44 after 7 steps.
Found uncertainty sample 45 after 18 steps.
Found uncertainty sample 46 after 22 steps.
Found uncertainty sample 47 after 15 steps.
Found uncertainty sample 48 after 17 steps.
Found uncertainty sample 49 after 41 steps.
Found uncertainty sample 50 after 26 steps.
Found uncertainty sample 51 after 91 steps.
Found uncertainty sample 52 after 44 steps.
Found uncertainty sample 53 after 102 steps.
Found uncertainty sample 54 after 7 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 5 steps.
Found uncertainty sample 58 after 15 steps.
Found uncertainty sample 59 after 82 steps.
Found uncertainty sample 60 after 72 steps.
Found uncertainty sample 61 after 38 steps.
Found uncertainty sample 62 after 21 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 20 steps.
Found uncertainty sample 65 after 23 steps.
Found uncertainty sample 66 after 13 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 6 steps.
Found uncertainty sample 69 after 26 steps.
Found uncertainty sample 70 after 5 steps.
Found uncertainty sample 71 after 103 steps.
Found uncertainty sample 72 after 73 steps.
Found uncertainty sample 73 after 42 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 44 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 16 steps.
Found uncertainty sample 78 after 33 steps.
Found uncertainty sample 79 after 46 steps.
Found uncertainty sample 80 after 18 steps.
Found uncertainty sample 81 after 38 steps.
Found uncertainty sample 82 after 25 steps.
Found uncertainty sample 83 after 21 steps.
Found uncertainty sample 84 after 7 steps.
Found uncertainty sample 85 after 22 steps.
Found uncertainty sample 86 after 27 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 34 steps.
Found uncertainty sample 89 after 21 steps.
Found uncertainty sample 90 after 10 steps.
Found uncertainty sample 91 after 21 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 26 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 42 steps.
Found uncertainty sample 96 after 12 steps.
Found uncertainty sample 97 after 278 steps.
Found uncertainty sample 98 after 5 steps.
Found uncertainty sample 99 after 9 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_004057-t2fm989m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_84
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/t2fm989m
Training model 84. Added 100 samples to the dataset.
Epoch 0, Batch 100/262, Loss: 0.05507904291152954
Epoch 0, Batch 200/262, Loss: 0.046616293489933014

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4145453744156604, Training Loss Force: 1.9474720242960857, time: 3.7994368076324463
Validation Loss Energy: 1.2989915208049116, Validation Loss Force: 2.01220469192167, time: 0.2273116111755371
Test Loss Energy: 9.981476203603084, Test Loss Force: 11.300324220664413, time: 9.944971323013306

Epoch 1, Batch 100/262, Loss: 0.036250337958335876
Epoch 1, Batch 200/262, Loss: 0.16074520349502563

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.4558471466063134, Training Loss Force: 1.9149647885498207, time: 3.8399980068206787
Validation Loss Energy: 1.8269545615610965, Validation Loss Force: 2.0416159460615417, time: 0.24657344818115234
Test Loss Energy: 10.63815914562092, Test Loss Force: 11.223239421940717, time: 10.110918283462524

Epoch 2, Batch 100/262, Loss: 0.11789283156394958
Epoch 2, Batch 200/262, Loss: 0.08344130218029022

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.329551461242469, Training Loss Force: 1.8558986470790249, time: 3.77268648147583
Validation Loss Energy: 1.5315392275233837, Validation Loss Force: 1.9618716229657394, time: 0.22320866584777832
Test Loss Energy: 9.687339154608626, Test Loss Force: 11.246814558569067, time: 9.894970893859863

Epoch 3, Batch 100/262, Loss: 0.05392353981733322
Epoch 3, Batch 200/262, Loss: 0.06907040625810623

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3496520821069795, Training Loss Force: 1.8415958981844158, time: 3.7917070388793945
Validation Loss Energy: 1.4372278443671567, Validation Loss Force: 1.964114413404268, time: 0.23441386222839355
Test Loss Energy: 9.856096323092467, Test Loss Force: 11.165094175275094, time: 9.903051137924194

Epoch 4, Batch 100/262, Loss: 0.06466826796531677
Epoch 4, Batch 200/262, Loss: 0.041495975106954575

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.2347897950095585, Training Loss Force: 1.8430470742277156, time: 3.927037477493286
Validation Loss Energy: 1.0120247663027346, Validation Loss Force: 1.9699753993288063, time: 0.2245957851409912
Test Loss Energy: 9.861867170564862, Test Loss Force: 11.231105975455844, time: 9.95570158958435

Epoch 5, Batch 100/262, Loss: 0.060109831392765045
Epoch 5, Batch 200/262, Loss: 0.06814856827259064

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2876333809065883, Training Loss Force: 1.8321239632097677, time: 3.719318151473999
Validation Loss Energy: 0.7070201298283552, Validation Loss Force: 2.0306320333635575, time: 0.22058534622192383
Test Loss Energy: 10.083825710465378, Test Loss Force: 11.39603757143194, time: 9.971739530563354

Epoch 6, Batch 100/262, Loss: 1.1137855052947998
Epoch 6, Batch 200/262, Loss: 1.4607243537902832

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.252979070709099, Training Loss Force: 5.234477013376098, time: 3.734124183654785
Validation Loss Energy: 7.316316815003182, Validation Loss Force: 7.854418271443349, time: 0.22476410865783691
Test Loss Energy: 12.652658319748992, Test Loss Force: 13.31671474884961, time: 10.235429525375366

Epoch 7, Batch 100/262, Loss: 1.0000340938568115
Epoch 7, Batch 200/262, Loss: 1.929564118385315

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.72033890991838, Training Loss Force: 5.105889151108193, time: 3.766446828842163
Validation Loss Energy: 10.189150403543241, Validation Loss Force: 3.9930214956950665, time: 0.22113871574401855
Test Loss Energy: 11.210129217552645, Test Loss Force: 11.57945722104982, time: 9.93702745437622

Epoch 8, Batch 100/262, Loss: 0.5139974355697632
Epoch 8, Batch 200/262, Loss: 0.1713821291923523

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.73224444358228, Training Loss Force: 4.266001618230003, time: 3.74330472946167
Validation Loss Energy: 35.25019029676845, Validation Loss Force: 4.66838779248071, time: 0.21990203857421875
Test Loss Energy: 25.88099598302647, Test Loss Force: 12.936555250734063, time: 10.18671464920044

Epoch 9, Batch 100/262, Loss: 0.5087186694145203
Epoch 9, Batch 200/262, Loss: 0.57382732629776

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.41652053028127, Training Loss Force: 4.6177340327011915, time: 3.7076926231384277
Validation Loss Energy: 3.3931055701951536, Validation Loss Force: 6.84176602348739, time: 0.22207307815551758
Test Loss Energy: 11.130017279588147, Test Loss Force: 13.046368035137876, time: 9.904854536056519

Epoch 10, Batch 100/262, Loss: 0.16921411454677582
Epoch 10, Batch 200/262, Loss: 0.9756290912628174

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.874256400569935, Training Loss Force: 4.6246187391742515, time: 3.7773311138153076
Validation Loss Energy: 8.481121295198374, Validation Loss Force: 5.000698067983782, time: 0.23002052307128906
Test Loss Energy: 14.916045029403195, Test Loss Force: 12.135121718732595, time: 10.104620933532715

Epoch 11, Batch 100/262, Loss: 0.16512420773506165
Epoch 11, Batch 200/262, Loss: 1.1655843257904053

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.155773387667491, Training Loss Force: 4.734476290751096, time: 3.778071165084839
Validation Loss Energy: 7.492394401634066, Validation Loss Force: 4.085856448812305, time: 0.23227405548095703
Test Loss Energy: 9.59287935418941, Test Loss Force: 11.088446703417228, time: 11.062803268432617

Epoch 12, Batch 100/262, Loss: 0.7830809950828552
Epoch 12, Batch 200/262, Loss: 0.27754074335098267

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.32173056846886, Training Loss Force: 4.153333809505455, time: 3.7306172847747803
Validation Loss Energy: 27.312304228771097, Validation Loss Force: 4.653966936981502, time: 0.21829438209533691
Test Loss Energy: 28.865570778494643, Test Loss Force: 12.3384509977786, time: 10.097562789916992

Epoch 13, Batch 100/262, Loss: 0.3447517156600952
Epoch 13, Batch 200/262, Loss: 0.41398343443870544

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.998611645354266, Training Loss Force: 3.9529756292665215, time: 3.716275215148926
Validation Loss Energy: 1.387126640792881, Validation Loss Force: 3.140594260796295, time: 0.22031807899475098
Test Loss Energy: 9.513375995482681, Test Loss Force: 11.346290360387599, time: 9.905067443847656

Epoch 14, Batch 100/262, Loss: 0.3023189604282379
Epoch 14, Batch 200/262, Loss: 0.08132512867450714

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 5.259686634801236, Training Loss Force: 2.6860654262350496, time: 3.726489305496216
Validation Loss Energy: 1.2992268598252426, Validation Loss Force: 2.773647443373522, time: 0.22011685371398926
Test Loss Energy: 10.603091231311463, Test Loss Force: 11.248618696269412, time: 10.165106058120728

Epoch 15, Batch 100/262, Loss: 1.3217096328735352
Epoch 15, Batch 200/262, Loss: 0.6737117767333984

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.633096946733483, Training Loss Force: 5.211601513411738, time: 3.7310023307800293
Validation Loss Energy: 2.1384597008805764, Validation Loss Force: 3.7295510789410797, time: 0.22880220413208008
Test Loss Energy: 9.187326023146769, Test Loss Force: 11.470045012610495, time: 9.950575113296509

Epoch 16, Batch 100/262, Loss: 1.9241762161254883
Epoch 16, Batch 200/262, Loss: 0.10145893692970276

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.838911070471251, Training Loss Force: 4.583335074680761, time: 3.775045871734619
Validation Loss Energy: 7.433227735922375, Validation Loss Force: 3.8629192983004783, time: 0.225449800491333
Test Loss Energy: 13.384190788691388, Test Loss Force: 11.489014978242805, time: 9.983377456665039

Epoch 17, Batch 100/262, Loss: 0.08189660310745239
Epoch 17, Batch 200/262, Loss: 0.15426401793956757

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.10560650202416, Training Loss Force: 4.188413016667402, time: 3.914227247238159
Validation Loss Energy: 16.722669115451392, Validation Loss Force: 3.7569635657594453, time: 0.2212543487548828
Test Loss Energy: 13.022769563715679, Test Loss Force: 11.439707523606177, time: 9.920678853988647

Epoch 18, Batch 100/262, Loss: 0.985496997833252
Epoch 18, Batch 200/262, Loss: 0.46459659934043884

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.620975210839541, Training Loss Force: 3.938779796407129, time: 3.844012975692749
Validation Loss Energy: 1.3045232458633664, Validation Loss Force: 3.644467995868684, time: 0.21569037437438965
Test Loss Energy: 9.98396293065575, Test Loss Force: 11.66909403574761, time: 9.931130409240723

Epoch 19, Batch 100/262, Loss: 0.11375987529754639
Epoch 19, Batch 200/262, Loss: 0.4374362528324127

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.289660535916424, Training Loss Force: 4.985872081781368, time: 3.7871925830841064
Validation Loss Energy: 5.801205404525084, Validation Loss Force: 5.6605938469163135, time: 0.31448912620544434
Test Loss Energy: 11.135726390582574, Test Loss Force: 12.914462419112734, time: 10.035496711730957

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–â–â–â–‚â–‚â–‡â–‚â–ƒâ–â–ˆâ–â–‚â–â–‚â–‚â–â–‚
wandb:   test_error_force â–‚â–â–â–â–â–‚â–ˆâ–ƒâ–‡â–‡â–„â–â–…â–‚â–‚â–‚â–‚â–‚â–ƒâ–‡
wandb:          test_loss â–â–â–â–â–â–‚â–…â–‚â–ˆâ–„â–„â–â–ˆâ–â–â–â–ƒâ–‚â–‚â–„
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–†â–†â–‡â–†â–‡â–‡â–†â–„â–ˆâ–‡â–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–†â–‡â–‡â–‡â–†â–…â–ƒâ–ˆâ–‡â–†â–…â–‡
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–†â–‡â–†â–‡â–†â–†â–ƒâ–ˆâ–‡â–†â–†â–‡
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–ƒâ–ˆâ–‚â–ƒâ–‚â–†â–â–â–â–‚â–„â–â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–ƒâ–„â–‡â–…â–„â–„â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–…
wandb:         valid_loss â–â–â–â–â–â–â–†â–„â–ˆâ–…â–„â–„â–‡â–‚â–‚â–‚â–ƒâ–…â–‚â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 8371
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.13573
wandb:   test_error_force 12.91446
wandb:          test_loss 5.06643
wandb: train_error_energy 8.28966
wandb:  train_error_force 4.98587
wandb:         train_loss 2.22304
wandb: valid_error_energy 5.80121
wandb:  valid_error_force 5.66059
wandb:         valid_loss 2.28227
wandb: 
wandb: ğŸš€ View run al_77_84 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/t2fm989m
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_004057-t2fm989m/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.23415900766849518, Uncertainty Bias: -0.09786692261695862
4.5776367e-05 0.0070381165
-0.9297797 23.775827
(48745, 22, 3)
Found uncertainty sample 0 after 31 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 22 steps.
Found uncertainty sample 3 after 24 steps.
Found uncertainty sample 4 after 32 steps.
Found uncertainty sample 5 after 7 steps.
Found uncertainty sample 6 after 29 steps.
Found uncertainty sample 7 after 20 steps.
Found uncertainty sample 8 after 213 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 4 steps.
Found uncertainty sample 11 after 16 steps.
Found uncertainty sample 12 after 12 steps.
Found uncertainty sample 13 after 15 steps.
Found uncertainty sample 14 after 76 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 28 steps.
Found uncertainty sample 17 after 124 steps.
Found uncertainty sample 18 after 17 steps.
Found uncertainty sample 19 after 9 steps.
Found uncertainty sample 20 after 49 steps.
Found uncertainty sample 21 after 36 steps.
Found uncertainty sample 22 after 14 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 24 steps.
Found uncertainty sample 25 after 109 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 132 steps.
Found uncertainty sample 28 after 26 steps.
Found uncertainty sample 29 after 26 steps.
Found uncertainty sample 30 after 104 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 18 steps.
Found uncertainty sample 33 after 11 steps.
Found uncertainty sample 34 after 16 steps.
Found uncertainty sample 35 after 17 steps.
Found uncertainty sample 36 after 7 steps.
Found uncertainty sample 37 after 2 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 5 steps.
Found uncertainty sample 40 after 3 steps.
Found uncertainty sample 41 after 4 steps.
Found uncertainty sample 42 after 80 steps.
Found uncertainty sample 43 after 27 steps.
Found uncertainty sample 44 after 14 steps.
Found uncertainty sample 45 after 49 steps.
Found uncertainty sample 46 after 9 steps.
Found uncertainty sample 47 after 96 steps.
Found uncertainty sample 48 after 50 steps.
Found uncertainty sample 49 after 28 steps.
Found uncertainty sample 50 after 15 steps.
Found uncertainty sample 51 after 54 steps.
Found uncertainty sample 52 after 3 steps.
Found uncertainty sample 53 after 7 steps.
Found uncertainty sample 54 after 159 steps.
Found uncertainty sample 55 after 60 steps.
Found uncertainty sample 56 after 40 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 53 steps.
Found uncertainty sample 59 after 20 steps.
Found uncertainty sample 60 after 3 steps.
Found uncertainty sample 61 after 34 steps.
Found uncertainty sample 62 after 23 steps.
Found uncertainty sample 63 after 18 steps.
Found uncertainty sample 64 after 58 steps.
Found uncertainty sample 65 after 7 steps.
Found uncertainty sample 66 after 13 steps.
Found uncertainty sample 67 after 157 steps.
Found uncertainty sample 68 after 101 steps.
Found uncertainty sample 69 after 17 steps.
Found uncertainty sample 70 after 136 steps.
Found uncertainty sample 71 after 11 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 44 steps.
Found uncertainty sample 74 after 111 steps.
Found uncertainty sample 75 after 42 steps.
Found uncertainty sample 76 after 42 steps.
Found uncertainty sample 77 after 69 steps.
Found uncertainty sample 78 after 94 steps.
Found uncertainty sample 79 after 11 steps.
Found uncertainty sample 80 after 54 steps.
Found uncertainty sample 81 after 27 steps.
Found uncertainty sample 82 after 25 steps.
Found uncertainty sample 83 after 183 steps.
Found uncertainty sample 84 after 12 steps.
Found uncertainty sample 85 after 35 steps.
Found uncertainty sample 86 after 118 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 6 steps.
Found uncertainty sample 90 after 122 steps.
Found uncertainty sample 91 after 27 steps.
Found uncertainty sample 92 after 12 steps.
Found uncertainty sample 93 after 18 steps.
Found uncertainty sample 94 after 15 steps.
Found uncertainty sample 95 after 6 steps.
Found uncertainty sample 96 after 7 steps.
Found uncertainty sample 97 after 7 steps.
Found uncertainty sample 98 after 29 steps.
Found uncertainty sample 99 after 27 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_005143-84gtjtwp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_85
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/84gtjtwp
Training model 85. Added 100 samples to the dataset.
Epoch 0, Batch 100/265, Loss: 0.07306001335382462
Epoch 0, Batch 200/265, Loss: 0.13002744317054749

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.5428348167369452, Training Loss Force: 1.9581214481158946, time: 3.793098211288452
Validation Loss Energy: 0.6878273520638835, Validation Loss Force: 2.047993279468572, time: 0.23237037658691406
Test Loss Energy: 9.928020257233223, Test Loss Force: 11.333920280261221, time: 9.890402555465698

Epoch 1, Batch 100/265, Loss: 0.20299582183361053
Epoch 1, Batch 200/265, Loss: 0.1785726249217987

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.73474787290517, Training Loss Force: 1.9082850674776541, time: 3.8418784141540527
Validation Loss Energy: 3.313289057762242, Validation Loss Force: 2.1362671223344547, time: 0.22251534461975098
Test Loss Energy: 11.361600671171058, Test Loss Force: 11.260839408533732, time: 9.913874626159668

Epoch 2, Batch 100/265, Loss: 0.15151193737983704
Epoch 2, Batch 200/265, Loss: 0.0699319988489151

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.4941148822861352, Training Loss Force: 1.9299243096779777, time: 3.7337019443511963
Validation Loss Energy: 0.6930517937009579, Validation Loss Force: 1.9881598755332306, time: 0.22229242324829102
Test Loss Energy: 10.113658689475372, Test Loss Force: 11.34442749399147, time: 9.89203667640686

Epoch 3, Batch 100/265, Loss: 0.2603987157344818
Epoch 3, Batch 200/265, Loss: 0.2449663281440735

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.3935509560455315, Training Loss Force: 1.9430837343823746, time: 3.8101918697357178
Validation Loss Energy: 2.9329873911094753, Validation Loss Force: 2.0136901623279835, time: 0.21696972846984863
Test Loss Energy: 10.919198459699475, Test Loss Force: 11.334065255362326, time: 9.869158744812012

Epoch 4, Batch 100/265, Loss: 0.2777736186981201
Epoch 4, Batch 200/265, Loss: 0.08687484264373779

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.976062292764889, Training Loss Force: 1.8925900209811801, time: 4.014365911483765
Validation Loss Energy: 1.074926328399815, Validation Loss Force: 2.0139829722548788, time: 0.22184181213378906
Test Loss Energy: 10.192520565186244, Test Loss Force: 11.359522825292279, time: 9.939316511154175

Epoch 5, Batch 100/265, Loss: 0.045317452400922775
Epoch 5, Batch 200/265, Loss: 0.09318298101425171

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.1522218644947806, Training Loss Force: 1.9255162769020988, time: 3.8050363063812256
Validation Loss Energy: 4.057127497469385, Validation Loss Force: 2.206450922980701, time: 0.23904895782470703
Test Loss Energy: 9.697396875110707, Test Loss Force: 11.282327941740773, time: 9.86214303970337

Epoch 6, Batch 100/265, Loss: 0.657357931137085
Epoch 6, Batch 200/265, Loss: 0.1264171451330185

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.540415142426506, Training Loss Force: 4.974200359793587, time: 3.891779661178589
Validation Loss Energy: 4.675876066790604, Validation Loss Force: 3.8173529522852534, time: 0.2273564338684082
Test Loss Energy: 13.709443526209999, Test Loss Force: 11.451889097396327, time: 10.069853782653809

Epoch 7, Batch 100/265, Loss: 0.12671934068202972
Epoch 7, Batch 200/265, Loss: 0.40497058629989624

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.169197462807485, Training Loss Force: 4.687901273683909, time: 3.7535455226898193
Validation Loss Energy: 3.2492097317516064, Validation Loss Force: 2.8788076681238133, time: 0.23123693466186523
Test Loss Energy: 9.026569573916227, Test Loss Force: 11.416625024393182, time: 9.831166744232178

Epoch 8, Batch 100/265, Loss: 0.3498447835445404
Epoch 8, Batch 200/265, Loss: 0.1885681450366974

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 5.197393173557072, Training Loss Force: 2.6325264448069565, time: 3.829664945602417
Validation Loss Energy: 3.0683083743448316, Validation Loss Force: 2.775580389695492, time: 0.22551560401916504
Test Loss Energy: 9.812736704090844, Test Loss Force: 11.32325813077446, time: 10.051702499389648

Epoch 9, Batch 100/265, Loss: 0.34385305643081665
Epoch 9, Batch 200/265, Loss: 0.4218830466270447

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.599264560860227, Training Loss Force: 3.5707190496101187, time: 3.7369275093078613
Validation Loss Energy: 4.554278672890898, Validation Loss Force: 4.307790875626189, time: 0.2227485179901123
Test Loss Energy: 9.6022876781974, Test Loss Force: 12.093028716444659, time: 9.848527193069458

Epoch 10, Batch 100/265, Loss: 0.46613311767578125
Epoch 10, Batch 200/265, Loss: 0.43701276183128357

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.521793120593854, Training Loss Force: 4.142504360129787, time: 3.8923232555389404
Validation Loss Energy: 11.551438813246913, Validation Loss Force: 3.9677433817080776, time: 0.2190251350402832
Test Loss Energy: 15.792197685816195, Test Loss Force: 11.461560795869088, time: 10.128561973571777

Epoch 11, Batch 100/265, Loss: 0.8612074255943298
Epoch 11, Batch 200/265, Loss: 1.2609829902648926

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.260541327786415, Training Loss Force: 4.443565187194566, time: 3.8044304847717285
Validation Loss Energy: 19.005408001977145, Validation Loss Force: 6.342634161479021, time: 0.22980999946594238
Test Loss Energy: 14.776978965874989, Test Loss Force: 12.882082086420409, time: 9.872698307037354

Epoch 12, Batch 100/265, Loss: 0.2881513833999634
Epoch 12, Batch 200/265, Loss: 0.8462291955947876

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.105064286700909, Training Loss Force: 5.172706296148046, time: 3.9070253372192383
Validation Loss Energy: 2.323886229738554, Validation Loss Force: 4.663939486418681, time: 0.21869826316833496
Test Loss Energy: 9.594155917568822, Test Loss Force: 12.036167783443693, time: 9.927130937576294

Epoch 13, Batch 100/265, Loss: 0.40166354179382324
Epoch 13, Batch 200/265, Loss: 0.3361664414405823

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.5496261090615, Training Loss Force: 4.4796019340429085, time: 3.7718377113342285
Validation Loss Energy: 6.1358765964485995, Validation Loss Force: 3.249499780581091, time: 0.22062945365905762
Test Loss Energy: 9.522263520840294, Test Loss Force: 11.97316154574692, time: 9.775291681289673

Epoch 14, Batch 100/265, Loss: 0.45770421624183655
Epoch 14, Batch 200/265, Loss: 0.9887938499450684

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.076152524786938, Training Loss Force: 4.108458252758994, time: 3.7923717498779297
Validation Loss Energy: 3.5440935545379886, Validation Loss Force: 5.142783250000304, time: 0.22066164016723633
Test Loss Energy: 11.424275012928081, Test Loss Force: 12.17798268941773, time: 11.132921695709229

Epoch 15, Batch 100/265, Loss: 0.9736132025718689
Epoch 15, Batch 200/265, Loss: 0.7545840740203857

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.289039439747548, Training Loss Force: 4.1389720569561, time: 3.889876127243042
Validation Loss Energy: 15.44703178041191, Validation Loss Force: 4.300859719519408, time: 0.22767400741577148
Test Loss Energy: 20.442571861490865, Test Loss Force: 12.48165917374388, time: 9.841176986694336

Epoch 16, Batch 100/265, Loss: 2.422136068344116
Epoch 16, Batch 200/265, Loss: 0.11092294007539749

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.103921914577642, Training Loss Force: 4.546223186323365, time: 3.866424322128296
Validation Loss Energy: 23.016304931369003, Validation Loss Force: 5.988275810619788, time: 0.2215268611907959
Test Loss Energy: 26.71387771970119, Test Loss Force: 13.564235674060038, time: 9.922164916992188

Epoch 17, Batch 100/265, Loss: 1.8520028591156006
Epoch 17, Batch 200/265, Loss: 0.5104206800460815

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.302069702609286, Training Loss Force: 4.749263760508643, time: 3.9848973751068115
Validation Loss Energy: 9.011400384542098, Validation Loss Force: 4.1743662764390415, time: 0.22773456573486328
Test Loss Energy: 13.389862108884586, Test Loss Force: 11.830989884163673, time: 9.852596998214722

Epoch 18, Batch 100/265, Loss: 0.4336850643157959
Epoch 18, Batch 200/265, Loss: 0.16197426617145538

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.102486194087783, Training Loss Force: 3.828023009040879, time: 3.825169324874878
Validation Loss Energy: 8.407073828599648, Validation Loss Force: 4.15734393277413, time: 0.2195730209350586
Test Loss Energy: 10.266344568957606, Test Loss Force: 11.715244941898021, time: 9.782978534698486

Epoch 19, Batch 100/265, Loss: 0.5685490965843201
Epoch 19, Batch 200/265, Loss: 1.0579802989959717

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.212946502318127, Training Loss Force: 3.282404144763922, time: 3.8634657859802246
Validation Loss Energy: 13.098009276932322, Validation Loss Force: 5.426922084082283, time: 0.2175440788269043
Test Loss Energy: 11.603352110571725, Test Loss Force: 11.923115459081622, time: 10.069572687149048

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–‚â–â–â–ƒâ–â–â–â–„â–ƒâ–â–â–‚â–†â–ˆâ–ƒâ–â–‚
wandb:   test_error_force â–â–â–â–â–â–â–‚â–â–â–„â–‚â–†â–ƒâ–ƒâ–„â–…â–ˆâ–ƒâ–‚â–ƒ
wandb:          test_loss â–â–â–â–â–â–â–‚â–â–â–‚â–ƒâ–„â–‚â–‚â–ƒâ–…â–ˆâ–ƒâ–‚â–‚
wandb: train_error_energy â–â–â–â–‚â–â–â–‡â–‡â–„â–†â–‡â–ˆâ–‡â–‡â–†â–†â–‡â–‡â–†â–…
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–ƒâ–…â–†â–†â–ˆâ–‡â–†â–†â–‡â–‡â–…â–„
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–ƒâ–…â–†â–‡â–ˆâ–‡â–†â–†â–‡â–‡â–†â–…
wandb: valid_error_energy â–â–‚â–â–‚â–â–‚â–‚â–‚â–‚â–‚â–„â–‡â–‚â–ƒâ–‚â–†â–ˆâ–„â–ƒâ–…
wandb:  valid_error_force â–â–â–â–â–â–â–„â–‚â–‚â–…â–„â–ˆâ–…â–ƒâ–†â–…â–‡â–…â–„â–‡
wandb:         valid_loss â–â–‚â–â–â–â–‚â–ƒâ–‚â–‚â–„â–„â–ˆâ–ƒâ–ƒâ–„â–…â–ˆâ–„â–„â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 8461
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.60335
wandb:   test_error_force 11.92312
wandb:          test_loss 4.76602
wandb: train_error_energy 7.21295
wandb:  train_error_force 3.2824
wandb:         train_loss 1.581
wandb: valid_error_energy 13.09801
wandb:  valid_error_force 5.42692
wandb:         valid_loss 2.69239
wandb: 
wandb: ğŸš€ View run al_77_85 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/84gtjtwp
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_005143-84gtjtwp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5468716621398926, Uncertainty Bias: -0.43296656012535095
3.0517578e-05 0.058357716
-6.1260777 14.707653
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 2 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 7 steps.
Found uncertainty sample 9 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 11 steps.
Found uncertainty sample 12 after 6 steps.
Found uncertainty sample 13 after 10 steps.
Found uncertainty sample 14 after 7 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 11 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 3 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 9 steps.
Found uncertainty sample 22 after 4 steps.
Found uncertainty sample 23 after 7 steps.
Found uncertainty sample 24 after 13 steps.
Found uncertainty sample 25 after 5 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 3 steps.
Found uncertainty sample 31 after 11 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 7 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 18 steps.
Found uncertainty sample 36 after 14 steps.
Found uncertainty sample 37 after 6 steps.
Found uncertainty sample 38 after 10 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 11 steps.
Found uncertainty sample 41 after 9 steps.
Found uncertainty sample 42 after 6 steps.
Found uncertainty sample 43 after 6 steps.
Found uncertainty sample 44 after 11 steps.
Found uncertainty sample 45 after 4 steps.
Found uncertainty sample 46 after 8 steps.
Found uncertainty sample 47 after 5 steps.
Found uncertainty sample 48 after 12 steps.
Found uncertainty sample 49 after 2 steps.
Found uncertainty sample 50 after 14 steps.
Found uncertainty sample 51 after 8 steps.
Found uncertainty sample 52 after 6 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 3 steps.
Found uncertainty sample 55 after 5 steps.
Found uncertainty sample 56 after 4 steps.
Found uncertainty sample 57 after 6 steps.
Found uncertainty sample 58 after 23 steps.
Found uncertainty sample 59 after 9 steps.
Found uncertainty sample 60 after 16 steps.
Found uncertainty sample 61 after 24 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 3 steps.
Found uncertainty sample 67 after 21 steps.
Found uncertainty sample 68 after 4 steps.
Found uncertainty sample 69 after 7 steps.
Found uncertainty sample 70 after 27 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 3 steps.
Found uncertainty sample 77 after 7 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 6 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 9 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 5 steps.
Found uncertainty sample 85 after 15 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 3 steps.
Found uncertainty sample 88 after 12 steps.
Found uncertainty sample 89 after 6 steps.
Found uncertainty sample 90 after 29 steps.
Found uncertainty sample 91 after 10 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 10 steps.
Found uncertainty sample 94 after 11 steps.
Found uncertainty sample 95 after 5 steps.
Found uncertainty sample 96 after 4 steps.
Found uncertainty sample 97 after 3 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_010127-liep2zr9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_86
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/liep2zr9
Training model 86. Added 100 samples to the dataset.
Epoch 0, Batch 100/268, Loss: 0.037231333553791046
Epoch 0, Batch 200/268, Loss: 0.09875410050153732

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.6146187756827566, Training Loss Force: 1.9830876278419702, time: 3.8620948791503906
Validation Loss Energy: 1.4627721147842734, Validation Loss Force: 2.1107598817963056, time: 0.22895073890686035
Test Loss Energy: 9.68665781033823, Test Loss Force: 11.386354559967598, time: 10.021106719970703

Epoch 1, Batch 100/268, Loss: 0.1563798040151596
Epoch 1, Batch 200/268, Loss: 0.22443214058876038

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.342452696732241, Training Loss Force: 1.963571094706048, time: 3.897554397583008
Validation Loss Energy: 2.3985466599621907, Validation Loss Force: 2.1110712623299706, time: 0.23025107383728027
Test Loss Energy: 9.663478651731376, Test Loss Force: 11.243121608270961, time: 10.274535179138184

Epoch 2, Batch 100/268, Loss: 0.1675402820110321
Epoch 2, Batch 200/268, Loss: 0.07473352551460266

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.0270937111073013, Training Loss Force: 1.9134250828711734, time: 3.8248465061187744
Validation Loss Energy: 0.9362683362759265, Validation Loss Force: 2.126901034583123, time: 0.24935412406921387
Test Loss Energy: 10.596983325705715, Test Loss Force: 11.403990082232239, time: 10.127838850021362

Epoch 3, Batch 100/268, Loss: 0.16247668862342834
Epoch 3, Batch 200/268, Loss: 0.17945247888565063

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.9801506373857143, Training Loss Force: 1.871947598335223, time: 4.020791530609131
Validation Loss Energy: 0.900350478156356, Validation Loss Force: 2.0198654012181696, time: 0.24131059646606445
Test Loss Energy: 10.3381892142295, Test Loss Force: 11.258518188775877, time: 10.36641550064087

Epoch 4, Batch 100/268, Loss: 0.1723407804965973
Epoch 4, Batch 200/268, Loss: 0.05018388479948044

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.3921791029803865, Training Loss Force: 1.8825886660198539, time: 4.079507350921631
Validation Loss Energy: 1.3268000024323952, Validation Loss Force: 1.9687306565361555, time: 0.22291016578674316
Test Loss Energy: 10.011203770339913, Test Loss Force: 11.39072796385952, time: 10.27224588394165

Epoch 5, Batch 100/268, Loss: 0.07874491810798645
Epoch 5, Batch 200/268, Loss: 0.2398206740617752

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5208604763676814, Training Loss Force: 1.8588716107210976, time: 3.9562630653381348
Validation Loss Energy: 2.6033672706260154, Validation Loss Force: 1.9510554246684682, time: 0.23458242416381836
Test Loss Energy: 11.371235031175727, Test Loss Force: 11.388633549530995, time: 10.10507845878601

Epoch 6, Batch 100/268, Loss: 0.4542829394340515
Epoch 6, Batch 200/268, Loss: 0.8896260261535645

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.836657661007134, Training Loss Force: 5.22597215261818, time: 4.0558764934539795
Validation Loss Energy: 16.069716249556546, Validation Loss Force: 5.676962701171757, time: 0.23414278030395508
Test Loss Energy: 12.684963523171993, Test Loss Force: 12.732126410460944, time: 10.375792741775513

Epoch 7, Batch 100/268, Loss: 0.746552586555481
Epoch 7, Batch 200/268, Loss: 0.21318235993385315

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.952100068405493, Training Loss Force: 4.465166182579814, time: 3.8971242904663086
Validation Loss Energy: 10.564547247851, Validation Loss Force: 3.8259808472840056, time: 0.23250770568847656
Test Loss Energy: 16.595145405315797, Test Loss Force: 11.282008132888482, time: 10.112338304519653

Epoch 8, Batch 100/268, Loss: 0.10314133018255234
Epoch 8, Batch 200/268, Loss: 0.3191951513290405

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.035928953638171, Training Loss Force: 4.476019412417765, time: 4.089861631393433
Validation Loss Energy: 12.381107613680186, Validation Loss Force: 5.662283816666247, time: 0.2232043743133545
Test Loss Energy: 17.292654989385436, Test Loss Force: 12.880788796041493, time: 10.234268188476562

Epoch 9, Batch 100/268, Loss: 0.9528319835662842
Epoch 9, Batch 200/268, Loss: 0.6452856063842773

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.767960568741184, Training Loss Force: 4.845699203472226, time: 3.794649124145508
Validation Loss Energy: 18.091530362072216, Validation Loss Force: 4.073910516336254, time: 0.2290174961090088
Test Loss Energy: 22.73135413879305, Test Loss Force: 11.4751643464832, time: 10.19591236114502

Epoch 10, Batch 100/268, Loss: 0.1797965168952942
Epoch 10, Batch 200/268, Loss: 0.6704543232917786

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.745546500324868, Training Loss Force: 4.256077578010862, time: 3.90781569480896
Validation Loss Energy: 4.401938418824843, Validation Loss Force: 2.9069571898152002, time: 0.23840951919555664
Test Loss Energy: 11.992189321419064, Test Loss Force: 11.57626908851909, time: 10.238511800765991

Epoch 11, Batch 100/268, Loss: 0.1684953272342682
Epoch 11, Batch 200/268, Loss: 0.4934884011745453

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.92470969795079, Training Loss Force: 4.167057544249615, time: 3.9501307010650635
Validation Loss Energy: 10.913698347131223, Validation Loss Force: 3.1295349968593285, time: 0.24890518188476562
Test Loss Energy: 15.721554819733278, Test Loss Force: 11.999081684596884, time: 10.000755310058594

Epoch 12, Batch 100/268, Loss: 0.4125317335128784
Epoch 12, Batch 200/268, Loss: 0.2896048128604889

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.94397870774365, Training Loss Force: 3.997919350489861, time: 3.846404790878296
Validation Loss Energy: 12.311681867874336, Validation Loss Force: 3.04040681701435, time: 0.2338552474975586
Test Loss Energy: 18.594103499487122, Test Loss Force: 11.190388634809525, time: 10.291354179382324

Epoch 13, Batch 100/268, Loss: 0.37717026472091675
Epoch 13, Batch 200/268, Loss: 0.5836698412895203

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.75417801888321, Training Loss Force: 4.3373099902018355, time: 3.9824111461639404
Validation Loss Energy: 17.425107259795958, Validation Loss Force: 6.059751003244758, time: 0.2226729393005371
Test Loss Energy: 14.254515496402846, Test Loss Force: 12.740569968515784, time: 10.045077323913574

Epoch 14, Batch 100/268, Loss: 0.7211294174194336
Epoch 14, Batch 200/268, Loss: 0.8165735602378845

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.272308876809033, Training Loss Force: 4.304777313446467, time: 3.887341260910034
Validation Loss Energy: 7.591728710850302, Validation Loss Force: 3.762627957095044, time: 0.22357439994812012
Test Loss Energy: 9.896169979699229, Test Loss Force: 11.78194232988682, time: 10.179120779037476

Epoch 15, Batch 100/268, Loss: 0.35092827677726746
Epoch 15, Batch 200/268, Loss: 0.42080774903297424

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.10319009827906, Training Loss Force: 4.273076889211364, time: 3.8562302589416504
Validation Loss Energy: 2.1866929107227486, Validation Loss Force: 7.081928057519265, time: 0.2259821891784668
Test Loss Energy: 10.301819629720072, Test Loss Force: 13.357652953972552, time: 10.068440198898315

Epoch 16, Batch 100/268, Loss: 0.502381443977356
Epoch 16, Batch 200/268, Loss: 0.4561639428138733

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.083801570597071, Training Loss Force: 3.953011886631711, time: 3.828009605407715
Validation Loss Energy: 20.777605726660163, Validation Loss Force: 4.363958187942233, time: 0.2262134552001953
Test Loss Energy: 23.19505596210813, Test Loss Force: 12.483249791551996, time: 10.176761150360107

Epoch 17, Batch 100/268, Loss: 0.8765000104904175
Epoch 17, Batch 200/268, Loss: 0.3644621968269348

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.440874759973884, Training Loss Force: 4.194142816431645, time: 3.8375158309936523
Validation Loss Energy: 5.725102790777102, Validation Loss Force: 3.0114439740005476, time: 0.2326350212097168
Test Loss Energy: 12.867188593625215, Test Loss Force: 11.327579175940299, time: 10.007594347000122

Epoch 18, Batch 100/268, Loss: 0.9061455130577087
Epoch 18, Batch 200/268, Loss: 0.7073911428451538

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.499775527942157, Training Loss Force: 4.339673333217457, time: 3.78532338142395
Validation Loss Energy: 10.057596403317204, Validation Loss Force: 4.094624447237675, time: 0.22572803497314453
Test Loss Energy: 10.650864763668267, Test Loss Force: 11.794554552800037, time: 10.236880540847778

Epoch 19, Batch 100/268, Loss: 0.45251405239105225
Epoch 19, Batch 200/268, Loss: 0.3258058726787567

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.147027361237535, Training Loss Force: 3.9356325805634236, time: 3.818352222442627
Validation Loss Energy: 3.13083957085149, Validation Loss Force: 3.7800099114306294, time: 0.22947430610656738
Test Loss Energy: 9.821884838618912, Test Loss Force: 12.070871401703963, time: 10.15160322189331

wandb: - 0.039 MB of 0.057 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–‚â–ƒâ–…â–…â–ˆâ–‚â–„â–†â–ƒâ–â–â–ˆâ–ƒâ–‚â–
wandb:   test_error_force â–‚â–â–‚â–â–‚â–‚â–†â–â–†â–‚â–‚â–„â–â–†â–ƒâ–ˆâ–…â–â–ƒâ–„
wandb:          test_loss â–â–â–‚â–â–â–‚â–…â–„â–‡â–†â–‚â–„â–„â–…â–‚â–…â–ˆâ–‚â–‚â–ƒ
wandb: train_error_energy â–â–‚â–â–â–â–â–ˆâ–‡â–‡â–ˆâ–†â–‡â–‡â–‡â–†â–‡â–†â–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–†â–‡â–†â–†â–…â–†â–†â–†â–…â–†â–†â–…
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–†â–‡â–‡â–†â–†â–†â–‡â–†â–†â–†â–†â–†â–†
wandb: valid_error_energy â–â–‚â–â–â–â–‚â–†â–„â–…â–‡â–‚â–…â–…â–‡â–ƒâ–â–ˆâ–ƒâ–„â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–†â–„â–†â–„â–‚â–ƒâ–‚â–‡â–ƒâ–ˆâ–„â–‚â–„â–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–‡â–…â–‡â–†â–ƒâ–„â–„â–ˆâ–„â–†â–‡â–ƒâ–…â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 8551
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.82188
wandb:   test_error_force 12.07087
wandb:          test_loss 4.69624
wandb: train_error_energy 8.14703
wandb:  train_error_force 3.93563
wandb:         train_loss 1.86208
wandb: valid_error_energy 3.13084
wandb:  valid_error_force 3.78001
wandb:         valid_loss 1.47432
wandb: 
wandb: ğŸš€ View run al_77_86 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/liep2zr9
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_010127-liep2zr9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5618239641189575, Uncertainty Bias: -0.49117937684059143
7.43866e-05 0.023887634
-6.727877 35.181572
(48745, 22, 3)
Found uncertainty sample 0 after 8 steps.
Found uncertainty sample 1 after 5 steps.
Found uncertainty sample 2 after 14 steps.
Found uncertainty sample 3 after 10 steps.
Found uncertainty sample 4 after 26 steps.
Found uncertainty sample 5 after 11 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 6 steps.
Found uncertainty sample 8 after 14 steps.
Found uncertainty sample 9 after 10 steps.
Found uncertainty sample 10 after 25 steps.
Found uncertainty sample 11 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 27 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 14 steps.
Found uncertainty sample 16 after 6 steps.
Found uncertainty sample 17 after 8 steps.
Found uncertainty sample 18 after 7 steps.
Found uncertainty sample 19 after 5 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 17 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 24 steps.
Found uncertainty sample 24 after 5 steps.
Found uncertainty sample 25 after 13 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 7 steps.
Found uncertainty sample 29 after 6 steps.
Found uncertainty sample 30 after 2 steps.
Found uncertainty sample 31 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 4 steps.
Found uncertainty sample 34 after 7 steps.
Found uncertainty sample 35 after 10 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 18 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 13 steps.
Found uncertainty sample 40 after 36 steps.
Found uncertainty sample 41 after 23 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 6 steps.
Found uncertainty sample 46 after 20 steps.
Found uncertainty sample 47 after 24 steps.
Found uncertainty sample 48 after 15 steps.
Found uncertainty sample 49 after 2 steps.
Found uncertainty sample 50 after 5 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 10 steps.
Found uncertainty sample 53 after 5 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 14 steps.
Found uncertainty sample 59 after 4 steps.
Found uncertainty sample 60 after 3 steps.
Found uncertainty sample 61 after 17 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 28 steps.
Found uncertainty sample 64 after 8 steps.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 22 steps.
Found uncertainty sample 67 after 10 steps.
Found uncertainty sample 68 after 3 steps.
Found uncertainty sample 69 after 4 steps.
Found uncertainty sample 70 after 7 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 15 steps.
Found uncertainty sample 75 after 7 steps.
Found uncertainty sample 76 after 15 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 46 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 12 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 5 steps.
Found uncertainty sample 87 after 3 steps.
Found uncertainty sample 88 after 7 steps.
Found uncertainty sample 89 after 30 steps.
Found uncertainty sample 90 after 13 steps.
Found uncertainty sample 91 after 5 steps.
Found uncertainty sample 92 after 5 steps.
Found uncertainty sample 93 after 4 steps.
Found uncertainty sample 94 after 6 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 26 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 7 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_011121-htz5oi37
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_87
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/htz5oi37
Training model 87. Added 100 samples to the dataset.
Epoch 0, Batch 100/271, Loss: 0.09782768040895462
Epoch 0, Batch 200/271, Loss: 0.04882534593343735

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4005102915784924, Training Loss Force: 1.9685437616945758, time: 3.993115186691284
Validation Loss Energy: 0.7516641579742322, Validation Loss Force: 2.1536675582091522, time: 0.23850464820861816
Test Loss Energy: 10.204176896269555, Test Loss Force: 11.645666415679726, time: 9.867469072341919

Epoch 1, Batch 100/271, Loss: 0.08144746720790863
Epoch 1, Batch 200/271, Loss: 0.05086352676153183

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.4450524096435489, Training Loss Force: 1.923651312757127, time: 3.983243703842163
Validation Loss Energy: 3.3164339922454973, Validation Loss Force: 2.2663320003801855, time: 0.22124886512756348
Test Loss Energy: 11.53780176946267, Test Loss Force: 11.614145259464713, time: 10.03843069076538

Epoch 2, Batch 100/271, Loss: 0.04378574341535568
Epoch 2, Batch 200/271, Loss: 0.037595052272081375

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.1687158612698447, Training Loss Force: 1.8866579424910375, time: 3.856445789337158
Validation Loss Energy: 1.577761979913889, Validation Loss Force: 2.0075734186224325, time: 0.22818326950073242
Test Loss Energy: 10.812812457855312, Test Loss Force: 11.522588495794555, time: 11.19031023979187

Epoch 3, Batch 100/271, Loss: 0.05411434546113014
Epoch 3, Batch 200/271, Loss: 0.16115880012512207

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.4201996411349114, Training Loss Force: 1.8681843003279663, time: 3.9191675186157227
Validation Loss Energy: 0.8384500699397389, Validation Loss Force: 2.204125856292752, time: 0.2504708766937256
Test Loss Energy: 10.100647322208722, Test Loss Force: 11.39296324498512, time: 10.111206293106079

Epoch 4, Batch 100/271, Loss: 0.035675786435604095
Epoch 4, Batch 200/271, Loss: 0.0741548240184784

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6976040367833218, Training Loss Force: 1.8661686790884229, time: 3.913364887237549
Validation Loss Energy: 1.2472527415307395, Validation Loss Force: 1.9565288549445192, time: 0.23976635932922363
Test Loss Energy: 9.853203219019445, Test Loss Force: 11.40235622697237, time: 9.914804697036743

Epoch 5, Batch 100/271, Loss: 0.1655084490776062
Epoch 5, Batch 200/271, Loss: 0.1883859783411026

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.0026882332722042, Training Loss Force: 1.844093039346139, time: 3.8745484352111816
Validation Loss Energy: 2.23993982812229, Validation Loss Force: 1.982985133649527, time: 0.2242116928100586
Test Loss Energy: 9.777725224168904, Test Loss Force: 11.4564258308555, time: 9.831638813018799

Epoch 6, Batch 100/271, Loss: 0.4334278702735901
Epoch 6, Batch 200/271, Loss: 0.6792502403259277

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 11.019563234851168, Training Loss Force: 5.281936465370988, time: 4.054157257080078
Validation Loss Energy: 2.541882975375851, Validation Loss Force: 3.801024668348198, time: 0.2290785312652588
Test Loss Energy: 8.981316008102608, Test Loss Force: 11.343755326954144, time: 9.899499654769897

Epoch 7, Batch 100/271, Loss: 0.5583862066268921
Epoch 7, Batch 200/271, Loss: 1.4758260250091553

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 10.320454176810488, Training Loss Force: 5.232032482488565, time: 3.877516269683838
Validation Loss Energy: 13.20670156874298, Validation Loss Force: 5.224327541404127, time: 0.22734355926513672
Test Loss Energy: 18.446012009617363, Test Loss Force: 12.314137956585169, time: 9.968281984329224

Epoch 8, Batch 100/271, Loss: 0.6340972185134888
Epoch 8, Batch 200/271, Loss: 0.12083554267883301

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.654752631129092, Training Loss Force: 3.5831030101978687, time: 3.844654083251953
Validation Loss Energy: 3.2156040143760634, Validation Loss Force: 3.0696944007105738, time: 0.26009178161621094
Test Loss Energy: 9.09470108177519, Test Loss Force: 11.932827643701499, time: 10.142383337020874

Epoch 9, Batch 100/271, Loss: 0.16567155718803406
Epoch 9, Batch 200/271, Loss: 0.38825732469558716

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.970849333690605, Training Loss Force: 3.7114777183379317, time: 3.9662740230560303
Validation Loss Energy: 4.83594579038542, Validation Loss Force: 3.8245917926178783, time: 0.23829293251037598
Test Loss Energy: 11.342597750505151, Test Loss Force: 11.710259246818698, time: 10.06570315361023

Epoch 10, Batch 100/271, Loss: 0.29417744278907776
Epoch 10, Batch 200/271, Loss: 0.07392781972885132

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.948226526426726, Training Loss Force: 4.283794802171119, time: 4.029759407043457
Validation Loss Energy: 16.33471960977876, Validation Loss Force: 5.85814074725926, time: 0.23167991638183594
Test Loss Energy: 12.595867680249718, Test Loss Force: 12.342218643665806, time: 10.148034811019897

Epoch 11, Batch 100/271, Loss: 0.379033625125885
Epoch 11, Batch 200/271, Loss: 0.7199180126190186

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 10.815226673879966, Training Loss Force: 4.6077340556098285, time: 3.951035499572754
Validation Loss Energy: 4.71953560784159, Validation Loss Force: 4.159337979509742, time: 0.22193121910095215
Test Loss Energy: 12.949679043769072, Test Loss Force: 11.95639568755978, time: 9.860303163528442

Epoch 12, Batch 100/271, Loss: 0.8289338946342468
Epoch 12, Batch 200/271, Loss: 0.9109756946563721

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.792106217597295, Training Loss Force: 4.240660616185821, time: 3.8511812686920166
Validation Loss Energy: 7.425187242130581, Validation Loss Force: 4.005400920464409, time: 0.23584651947021484
Test Loss Energy: 9.882414897485178, Test Loss Force: 11.396797724258697, time: 10.155434370040894

Epoch 13, Batch 100/271, Loss: 0.7821714878082275
Epoch 13, Batch 200/271, Loss: 0.3366033434867859

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.041460504461115, Training Loss Force: 4.011182174648056, time: 3.895185947418213
Validation Loss Energy: 1.5840949353601357, Validation Loss Force: 4.386234009300848, time: 0.22735333442687988
Test Loss Energy: 9.720454645224626, Test Loss Force: 11.72546659818701, time: 9.879976511001587

Epoch 14, Batch 100/271, Loss: 0.16968147456645966
Epoch 14, Batch 200/271, Loss: 0.3635799288749695

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.470215524498613, Training Loss Force: 4.677394938554794, time: 3.8445417881011963
Validation Loss Energy: 1.7140092890632193, Validation Loss Force: 4.092966096506463, time: 0.24346661567687988
Test Loss Energy: 8.988934207410953, Test Loss Force: 12.108069301446559, time: 10.16814923286438

Epoch 15, Batch 100/271, Loss: 0.5360479950904846
Epoch 15, Batch 200/271, Loss: 1.6469480991363525

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.012843064683194, Training Loss Force: 4.302502238720841, time: 3.9120993614196777
Validation Loss Energy: 1.1827270529501765, Validation Loss Force: 3.5005583519380883, time: 0.22270750999450684
Test Loss Energy: 9.443020370266398, Test Loss Force: 11.939987303118997, time: 9.784784317016602

Epoch 16, Batch 100/271, Loss: 0.342867374420166
Epoch 16, Batch 200/271, Loss: 0.3222898840904236

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.145160393714718, Training Loss Force: 4.590754807709382, time: 3.860468864440918
Validation Loss Energy: 3.806533324883135, Validation Loss Force: 4.850799425636654, time: 0.23232674598693848
Test Loss Energy: 9.579273886988512, Test Loss Force: 12.989644937835664, time: 10.046293258666992

Epoch 17, Batch 100/271, Loss: 1.7580195665359497
Epoch 17, Batch 200/271, Loss: 0.2678373456001282

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.2015295428558925, Training Loss Force: 3.4342202083225275, time: 3.8366219997406006
Validation Loss Energy: 7.871131283130739, Validation Loss Force: 3.082410723897705, time: 0.22057318687438965
Test Loss Energy: 12.594543761788065, Test Loss Force: 11.645382597622582, time: 9.958405494689941

Epoch 18, Batch 100/271, Loss: 0.14870232343673706
Epoch 18, Batch 200/271, Loss: 0.3610805571079254

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.081595766777568, Training Loss Force: 4.640081830184984, time: 3.8486740589141846
Validation Loss Energy: 12.355153119895526, Validation Loss Force: 4.792875699097534, time: 0.23561334609985352
Test Loss Energy: 11.83736048806611, Test Loss Force: 11.932662593403052, time: 9.899413824081421

Epoch 19, Batch 100/271, Loss: 0.7431666851043701
Epoch 19, Batch 200/271, Loss: 0.25434112548828125

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.663763877505845, Training Loss Force: 4.437422521679681, time: 4.094542741775513
Validation Loss Energy: 11.437588791475303, Validation Loss Force: 4.50957588235741, time: 0.22240734100341797
Test Loss Energy: 19.034788318629097, Test Loss Force: 12.270503929920883, time: 9.97224760055542

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.060 MB uploadedwandb: | 0.039 MB of 0.060 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ˆâ–â–ƒâ–„â–„â–‚â–‚â–â–â–â–„â–ƒâ–ˆ
wandb:   test_error_force â–‚â–‚â–‚â–â–â–â–â–…â–„â–ƒâ–…â–„â–â–ƒâ–„â–„â–ˆâ–‚â–„â–…
wandb:          test_loss â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ˆâ–‚â–ƒâ–…â–„â–‚â–‚â–ƒâ–ƒâ–…â–ƒâ–„â–ˆ
wandb: train_error_energy â–â–â–â–â–â–‚â–ˆâ–ˆâ–…â–‡â–‡â–ˆâ–‡â–‡â–†â–†â–‡â–…â–‡â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–…â–…â–†â–‡â–†â–…â–‡â–†â–‡â–„â–‡â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–ˆâ–…â–…â–†â–‡â–†â–†â–‡â–†â–‡â–„â–‡â–†
wandb: valid_error_energy â–â–‚â–â–â–â–‚â–‚â–‡â–‚â–ƒâ–ˆâ–ƒâ–„â–â–â–â–‚â–„â–†â–†
wandb:  valid_error_force â–â–‚â–â–â–â–â–„â–‡â–ƒâ–„â–ˆâ–…â–…â–…â–…â–„â–†â–ƒâ–†â–†
wandb:         valid_loss â–â–‚â–â–â–â–â–ƒâ–‡â–ƒâ–„â–ˆâ–„â–„â–„â–ƒâ–ƒâ–„â–ƒâ–†â–†
wandb: 
wandb: Run summary:
wandb:       dataset_size 8641
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 19.03479
wandb:   test_error_force 12.2705
wandb:          test_loss 5.37957
wandb: train_error_energy 8.66376
wandb:  train_error_force 4.43742
wandb:         train_loss 2.06456
wandb: valid_error_energy 11.43759
wandb:  valid_error_force 4.50958
wandb:         valid_loss 2.27433
wandb: 
wandb: ğŸš€ View run al_77_87 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/htz5oi37
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_011121-htz5oi37/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5817703604698181, Uncertainty Bias: -0.5855289697647095
5.340576e-05 0.011757374
-6.0474634 19.825342
(48745, 22, 3)
Found uncertainty sample 0 after 5 steps.
Found uncertainty sample 1 after 7 steps.
Found uncertainty sample 2 after 11 steps.
Found uncertainty sample 3 after 5 steps.
Found uncertainty sample 4 after 42 steps.
Found uncertainty sample 5 after 2 steps.
Found uncertainty sample 6 after 22 steps.
Found uncertainty sample 7 after 67 steps.
Found uncertainty sample 8 after 3 steps.
Found uncertainty sample 9 after 11 steps.
Found uncertainty sample 10 after 8 steps.
Found uncertainty sample 11 after 17 steps.
Found uncertainty sample 12 after 13 steps.
Found uncertainty sample 13 after 2 steps.
Found uncertainty sample 14 after 3 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 20 steps.
Found uncertainty sample 17 after 24 steps.
Found uncertainty sample 18 after 32 steps.
Found uncertainty sample 19 after 9 steps.
Found uncertainty sample 20 after 8 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 8 steps.
Found uncertainty sample 23 after 59 steps.
Found uncertainty sample 24 after 12 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 7 steps.
Found uncertainty sample 27 after 11 steps.
Found uncertainty sample 28 after 14 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 2 steps.
Found uncertainty sample 31 after 8 steps.
Found uncertainty sample 32 after 5 steps.
Found uncertainty sample 33 after 5 steps.
Found uncertainty sample 34 after 6 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 2 steps.
Found uncertainty sample 38 after 10 steps.
Found uncertainty sample 39 after 4 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 6 steps.
Found uncertainty sample 42 after 17 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 7 steps.
Found uncertainty sample 45 after 29 steps.
Found uncertainty sample 46 after 12 steps.
Found uncertainty sample 47 after 22 steps.
Found uncertainty sample 48 after 6 steps.
Found uncertainty sample 49 after 8 steps.
Found uncertainty sample 50 after 27 steps.
Found uncertainty sample 51 after 8 steps.
Found uncertainty sample 52 after 7 steps.
Found uncertainty sample 53 after 28 steps.
Found uncertainty sample 54 after 13 steps.
Found uncertainty sample 55 after 45 steps.
Found uncertainty sample 56 after 33 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 6 steps.
Found uncertainty sample 59 after 26 steps.
Found uncertainty sample 60 after 68 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 6 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 35 steps.
Found uncertainty sample 65 after 14 steps.
Found uncertainty sample 66 after 13 steps.
Found uncertainty sample 67 after 11 steps.
Found uncertainty sample 68 after 17 steps.
Found uncertainty sample 69 after 31 steps.
Found uncertainty sample 70 after 14 steps.
Found uncertainty sample 71 after 10 steps.
Found uncertainty sample 72 after 7 steps.
Found uncertainty sample 73 after 13 steps.
Found uncertainty sample 74 after 25 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 29 steps.
Found uncertainty sample 77 after 35 steps.
Found uncertainty sample 78 after 12 steps.
Found uncertainty sample 79 after 33 steps.
Found uncertainty sample 80 after 7 steps.
Found uncertainty sample 81 after 14 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 38 steps.
Found uncertainty sample 84 after 10 steps.
Found uncertainty sample 85 after 3 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 51 steps.
Found uncertainty sample 88 after 70 steps.
Found uncertainty sample 89 after 28 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 29 steps.
Found uncertainty sample 92 after 4 steps.
Found uncertainty sample 93 after 11 steps.
Found uncertainty sample 94 after 4 steps.
Found uncertainty sample 95 after 25 steps.
Found uncertainty sample 96 after 63 steps.
Found uncertainty sample 97 after 10 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 10 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_012125-6v8pkls0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_88
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/6v8pkls0
Training model 88. Added 100 samples to the dataset.
Epoch 0, Batch 100/273, Loss: 0.09397521615028381
Epoch 0, Batch 200/273, Loss: 0.11766405403614044

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4252977571254066, Training Loss Force: 1.9476402140496312, time: 3.9772586822509766
Validation Loss Energy: 1.4851429490674568, Validation Loss Force: 1.9797122980088513, time: 0.26581788063049316
Test Loss Energy: 9.612994389492714, Test Loss Force: 11.399065312229178, time: 10.32874584197998

Epoch 1, Batch 100/273, Loss: 0.08047260344028473
Epoch 1, Batch 200/273, Loss: 0.06934765726327896

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.2024741323365817, Training Loss Force: 1.8830602222554054, time: 3.9552721977233887
Validation Loss Energy: 1.7448094865346448, Validation Loss Force: 2.021677393660071, time: 0.23322010040283203
Test Loss Energy: 9.691248837763181, Test Loss Force: 11.495858060425139, time: 10.468467235565186

Epoch 2, Batch 100/273, Loss: 0.10365922749042511
Epoch 2, Batch 200/273, Loss: 0.052217401564121246

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.333950534995696, Training Loss Force: 1.8505919825617099, time: 3.8967247009277344
Validation Loss Energy: 3.3862057659041183, Validation Loss Force: 1.9822801631335198, time: 0.23399758338928223
Test Loss Energy: 9.740847086646317, Test Loss Force: 11.427958991175155, time: 10.407876968383789

Epoch 3, Batch 100/273, Loss: 0.07209401577711105
Epoch 3, Batch 200/273, Loss: 0.04067978262901306

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.612732746447615, Training Loss Force: 1.892965206940789, time: 4.041872024536133
Validation Loss Energy: 0.8587733055577141, Validation Loss Force: 2.130220051804664, time: 0.233870267868042
Test Loss Energy: 10.247854678003918, Test Loss Force: 11.462404989394233, time: 10.49105191230774

Epoch 4, Batch 100/273, Loss: 0.13041070103645325
Epoch 4, Batch 200/273, Loss: 0.14912419021129608

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5838379481069476, Training Loss Force: 1.8795674405574854, time: 3.9361026287078857
Validation Loss Energy: 1.4488588558701216, Validation Loss Force: 1.9272329656891525, time: 0.22987627983093262
Test Loss Energy: 10.618898744810672, Test Loss Force: 11.411355175504285, time: 10.26192569732666

Epoch 5, Batch 100/273, Loss: 0.06532197445631027
Epoch 5, Batch 200/273, Loss: 0.1241731196641922

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2654266342217193, Training Loss Force: 1.8223988375377222, time: 3.9194984436035156
Validation Loss Energy: 1.4465187367479082, Validation Loss Force: 1.933894358125643, time: 0.23272109031677246
Test Loss Energy: 9.924671826233569, Test Loss Force: 11.407489042237769, time: 10.366873741149902

Epoch 6, Batch 100/273, Loss: 0.1647683084011078
Epoch 6, Batch 200/273, Loss: 0.6081153750419617

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.843847999123323, Training Loss Force: 4.742563789419933, time: 4.12829327583313
Validation Loss Energy: 7.962046026601153, Validation Loss Force: 4.253019698855114, time: 0.2320413589477539
Test Loss Energy: 14.811039501652129, Test Loss Force: 12.364476800716048, time: 10.386666774749756

Epoch 7, Batch 100/273, Loss: 0.3230626583099365
Epoch 7, Batch 200/273, Loss: 0.21421974897384644

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.665800891195749, Training Loss Force: 4.442471101070269, time: 3.9153928756713867
Validation Loss Energy: 5.339800821249552, Validation Loss Force: 2.8194560979533447, time: 0.23919987678527832
Test Loss Energy: 9.617927371440615, Test Loss Force: 11.572127489037118, time: 10.226476669311523

Epoch 8, Batch 100/273, Loss: 0.4307428002357483
Epoch 8, Batch 200/273, Loss: 0.259141206741333

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 5.682145129341529, Training Loss Force: 2.677513707216568, time: 4.107302665710449
Validation Loss Energy: 7.946085992897634, Validation Loss Force: 5.1848861555859385, time: 0.24425911903381348
Test Loss Energy: 10.270630178252453, Test Loss Force: 12.912498272063608, time: 11.42029094696045

Epoch 9, Batch 100/273, Loss: 0.1775938868522644
Epoch 9, Batch 200/273, Loss: 0.9933651685714722

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.69768588019086, Training Loss Force: 4.835712666556053, time: 3.8616249561309814
Validation Loss Energy: 3.6199561754804903, Validation Loss Force: 7.762663085548544, time: 0.23446178436279297
Test Loss Energy: 11.743030894304326, Test Loss Force: 13.780989356482896, time: 10.423787355422974

Epoch 10, Batch 100/273, Loss: 1.3305153846740723
Epoch 10, Batch 200/273, Loss: 0.6732686758041382

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.07934291558824, Training Loss Force: 4.374155938754756, time: 4.118915557861328
Validation Loss Energy: 17.261740785894194, Validation Loss Force: 4.660308465189866, time: 0.23621749877929688
Test Loss Energy: 19.660518080961864, Test Loss Force: 12.303402473171941, time: 10.290244579315186

Epoch 11, Batch 100/273, Loss: 1.9179898500442505
Epoch 11, Batch 200/273, Loss: 0.8695873618125916

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.3474351898768, Training Loss Force: 4.113584816691126, time: 3.9854965209960938
Validation Loss Energy: 3.3745412426946912, Validation Loss Force: 4.076079759155883, time: 0.23219943046569824
Test Loss Energy: 9.451539775727703, Test Loss Force: 11.960658698850828, time: 10.241495370864868

Epoch 12, Batch 100/273, Loss: 0.79972243309021
Epoch 12, Batch 200/273, Loss: 0.6059544086456299

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.292462212934806, Training Loss Force: 4.35283982059945, time: 4.160639047622681
Validation Loss Energy: 8.618339464090866, Validation Loss Force: 3.5403875828746543, time: 0.23259282112121582
Test Loss Energy: 10.25668746638907, Test Loss Force: 11.497956909483674, time: 10.365411520004272

Epoch 13, Batch 100/273, Loss: 0.44934073090553284
Epoch 13, Batch 200/273, Loss: 0.12047138065099716

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.506083216242768, Training Loss Force: 4.489311805445499, time: 3.901681661605835
Validation Loss Energy: 3.8586530465321602, Validation Loss Force: 6.42399933782186, time: 0.23336267471313477
Test Loss Energy: 9.633823425653409, Test Loss Force: 14.15801071653711, time: 10.321519136428833

Epoch 14, Batch 100/273, Loss: 0.7731333374977112
Epoch 14, Batch 200/273, Loss: 0.5417652130126953

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.928044748539764, Training Loss Force: 3.877800787585901, time: 3.834271192550659
Validation Loss Energy: 6.537208606859148, Validation Loss Force: 3.0418567494249737, time: 0.24552369117736816
Test Loss Energy: 13.160248865925473, Test Loss Force: 12.028054363291579, time: 10.361965656280518

Epoch 15, Batch 100/273, Loss: 0.3699169158935547
Epoch 15, Batch 200/273, Loss: 0.5319063067436218

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.127142337560349, Training Loss Force: 2.5333280667555624, time: 3.9414446353912354
Validation Loss Energy: 1.2524513312799717, Validation Loss Force: 2.639678952530531, time: 0.23546934127807617
Test Loss Energy: 10.350138106922802, Test Loss Force: 11.710495784405248, time: 10.195270776748657

Epoch 16, Batch 100/273, Loss: 1.96548330783844
Epoch 16, Batch 200/273, Loss: 0.15792569518089294

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.789578577315694, Training Loss Force: 4.45422806133822, time: 3.917778968811035
Validation Loss Energy: 10.712979034932411, Validation Loss Force: 4.044949098404986, time: 0.2293705940246582
Test Loss Energy: 10.76033878319721, Test Loss Force: 11.453286713516244, time: 10.518610954284668

Epoch 17, Batch 100/273, Loss: 0.6433150768280029
Epoch 17, Batch 200/273, Loss: 0.39667925238609314

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.349123307703096, Training Loss Force: 4.384198322945564, time: 3.8532228469848633
Validation Loss Energy: 3.779514249911794, Validation Loss Force: 2.7507558527587577, time: 0.2302260398864746
Test Loss Energy: 10.8732623325129, Test Loss Force: 11.433095005415089, time: 10.26207447052002

Epoch 18, Batch 100/273, Loss: 0.4636104106903076
Epoch 18, Batch 200/273, Loss: 0.2775591015815735

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.36895496039954, Training Loss Force: 3.9045272839452543, time: 3.9493746757507324
Validation Loss Energy: 11.580069077624358, Validation Loss Force: 3.507966688024593, time: 0.23198866844177246
Test Loss Energy: 18.14096482066116, Test Loss Force: 11.996669741734651, time: 10.460306167602539

Epoch 19, Batch 100/273, Loss: 0.6816402077674866
Epoch 19, Batch 200/273, Loss: 0.9221735000610352

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.33672464470437, Training Loss Force: 4.0993067498616185, time: 3.9296929836273193
Validation Loss Energy: 5.180433930546942, Validation Loss Force: 3.292400197979781, time: 0.22871685028076172
Test Loss Energy: 10.030037429812316, Test Loss Force: 11.770906030949707, time: 10.326792240142822

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.060 MB uploadedwandb: | 0.039 MB of 0.060 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–‚â–â–…â–â–‚â–ƒâ–ˆâ–â–‚â–â–„â–‚â–‚â–‚â–‡â–
wandb:   test_error_force â–â–â–â–â–â–â–ƒâ–â–…â–‡â–ƒâ–‚â–â–ˆâ–ƒâ–‚â–â–â–ƒâ–‚
wandb:          test_loss â–â–â–â–â–‚â–â–†â–â–…â–ˆâ–ˆâ–‚â–‚â–ˆâ–„â–‚â–‚â–‚â–‡â–‚
wandb: train_error_energy â–â–â–â–â–â–â–‡â–†â–„â–ˆâ–‡â–‡â–†â–‡â–†â–„â–‡â–†â–†â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–ƒâ–ˆâ–‡â–†â–‡â–‡â–†â–ƒâ–‡â–‡â–†â–†
wandb:         train_loss â–â–â–â–â–â–â–‡â–‡â–ƒâ–ˆâ–‡â–‡â–‡â–‡â–†â–ƒâ–‡â–‡â–†â–‡
wandb: valid_error_energy â–â–â–‚â–â–â–â–„â–ƒâ–„â–‚â–ˆâ–‚â–„â–‚â–ƒâ–â–…â–‚â–†â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–„â–‚â–…â–ˆâ–„â–„â–ƒâ–†â–‚â–‚â–„â–‚â–ƒâ–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–…â–ƒâ–†â–ˆâ–ˆâ–„â–„â–‡â–ƒâ–‚â–…â–‚â–…â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 8731
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.03004
wandb:   test_error_force 11.77091
wandb:          test_loss 4.6098
wandb: train_error_energy 9.33672
wandb:  train_error_force 4.09931
wandb:         train_loss 1.99646
wandb: valid_error_energy 5.18043
wandb:  valid_error_force 3.2924
wandb:         valid_loss 1.44833
wandb: 
wandb: ğŸš€ View run al_77_88 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/6v8pkls0
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_012125-6v8pkls0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4158276617527008, Uncertainty Bias: -0.38696423172950745
2.2888184e-05 0.0027513504
-5.4231796 14.021829
(48745, 22, 3)
Found uncertainty sample 0 after 6 steps.
Found uncertainty sample 1 after 11 steps.
Found uncertainty sample 2 after 37 steps.
Found uncertainty sample 3 after 16 steps.
Found uncertainty sample 4 after 45 steps.
Found uncertainty sample 5 after 36 steps.
Found uncertainty sample 6 after 17 steps.
Found uncertainty sample 7 after 51 steps.
Found uncertainty sample 8 after 5 steps.
Found uncertainty sample 9 after 6 steps.
Found uncertainty sample 10 after 14 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 25 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 2 steps.
Found uncertainty sample 15 after 135 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 7 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 8 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 15 steps.
Found uncertainty sample 22 after 10 steps.
Found uncertainty sample 23 after 21 steps.
Found uncertainty sample 24 after 54 steps.
Found uncertainty sample 25 after 54 steps.
Found uncertainty sample 26 after 18 steps.
Found uncertainty sample 27 after 137 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 18 steps.
Found uncertainty sample 30 after 13 steps.
Found uncertainty sample 31 after 18 steps.
Found uncertainty sample 32 after 13 steps.
Found uncertainty sample 33 after 52 steps.
Found uncertainty sample 34 after 10 steps.
Found uncertainty sample 35 after 46 steps.
Found uncertainty sample 36 after 1 steps.
Found uncertainty sample 37 after 9 steps.
Found uncertainty sample 38 after 5 steps.
Found uncertainty sample 39 after 31 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 6 steps.
Found uncertainty sample 42 after 28 steps.
Found uncertainty sample 43 after 67 steps.
Found uncertainty sample 44 after 19 steps.
Found uncertainty sample 45 after 25 steps.
Found uncertainty sample 46 after 4 steps.
Found uncertainty sample 47 after 23 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 90 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 37 steps.
Found uncertainty sample 55 after 4 steps.
Found uncertainty sample 56 after 14 steps.
Found uncertainty sample 57 after 113 steps.
Found uncertainty sample 58 after 32 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 14 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 18 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 3 steps.
Found uncertainty sample 65 after 15 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 11 steps.
Found uncertainty sample 68 after 97 steps.
Found uncertainty sample 69 after 66 steps.
Found uncertainty sample 70 after 15 steps.
Found uncertainty sample 71 after 15 steps.
Found uncertainty sample 72 after 8 steps.
Found uncertainty sample 73 after 11 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 6 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 33 steps.
Found uncertainty sample 78 after 4 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 21 steps.
Found uncertainty sample 82 after 14 steps.
Found uncertainty sample 83 after 34 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 12 steps.
Found uncertainty sample 86 after 13 steps.
Found uncertainty sample 87 after 13 steps.
Found uncertainty sample 88 after 2 steps.
Found uncertainty sample 89 after 135 steps.
Found uncertainty sample 90 after 31 steps.
Found uncertainty sample 91 after 44 steps.
Found uncertainty sample 92 after 7 steps.
Found uncertainty sample 93 after 6 steps.
Found uncertainty sample 94 after 83 steps.
Found uncertainty sample 95 after 71 steps.
Found uncertainty sample 96 after 9 steps.
Found uncertainty sample 97 after 11 steps.
Found uncertainty sample 98 after 26 steps.
Found uncertainty sample 99 after 11 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_013156-12vwf570
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_89
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/12vwf570
Training model 89. Added 100 samples to the dataset.
Epoch 0, Batch 100/276, Loss: 0.05730068311095238
Epoch 0, Batch 200/276, Loss: 0.11305904388427734

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.0073508947016594, Training Loss Force: 1.92534459095331, time: 3.9235141277313232
Validation Loss Energy: 0.9416039120837334, Validation Loss Force: 1.986364605544188, time: 0.24455714225769043
Test Loss Energy: 9.847108715531506, Test Loss Force: 11.696680145509962, time: 10.166335105895996

Epoch 1, Batch 100/276, Loss: 0.09303057193756104
Epoch 1, Batch 200/276, Loss: 0.03852453827857971

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.3309534971672254, Training Loss Force: 1.878181160976958, time: 3.9946866035461426
Validation Loss Energy: 2.5831144429919832, Validation Loss Force: 2.0083041964924524, time: 0.2277510166168213
Test Loss Energy: 11.700422665923698, Test Loss Force: 11.447342697065126, time: 10.536996603012085

Epoch 2, Batch 100/276, Loss: 0.09284240007400513
Epoch 2, Batch 200/276, Loss: 0.10846292972564697

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.9809282474219716, Training Loss Force: 1.8605682949452156, time: 3.942617177963257
Validation Loss Energy: 2.766455007290909, Validation Loss Force: 2.0090222599542407, time: 0.23255133628845215
Test Loss Energy: 9.734932639725132, Test Loss Force: 11.508789010321706, time: 10.18235158920288

Epoch 3, Batch 100/276, Loss: 0.10708266496658325
Epoch 3, Batch 200/276, Loss: 0.09099636971950531

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.9949135279049401, Training Loss Force: 1.8455494639556116, time: 4.021369695663452
Validation Loss Energy: 2.583096273189675, Validation Loss Force: 1.9848189031339007, time: 0.22990012168884277
Test Loss Energy: 11.46036295710793, Test Loss Force: 11.512787108282685, time: 10.305843353271484

Epoch 4, Batch 100/276, Loss: 0.10876607149839401
Epoch 4, Batch 200/276, Loss: 0.12763068079948425

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.0009921582968757, Training Loss Force: 1.864577279461608, time: 3.9516851902008057
Validation Loss Energy: 3.0094949955000705, Validation Loss Force: 2.143512954478261, time: 0.2284696102142334
Test Loss Energy: 11.589560631861673, Test Loss Force: 11.495631273648723, time: 10.265613317489624

Epoch 5, Batch 100/276, Loss: 0.04359428584575653
Epoch 5, Batch 200/276, Loss: 0.07257674634456635

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.5606785845038709, Training Loss Force: 1.871424393617184, time: 4.03280234336853
Validation Loss Energy: 1.740097362068521, Validation Loss Force: 1.9494657005680314, time: 0.24093031883239746
Test Loss Energy: 10.098790975005597, Test Loss Force: 11.473618020016401, time: 10.067348957061768

Epoch 6, Batch 100/276, Loss: 0.7503593564033508
Epoch 6, Batch 200/276, Loss: 0.7378281354904175

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.302768293018739, Training Loss Force: 5.110236584920412, time: 4.150048017501831
Validation Loss Energy: 7.224262094573275, Validation Loss Force: 4.218510969025177, time: 0.23221898078918457
Test Loss Energy: 13.826436073208024, Test Loss Force: 12.367520256592273, time: 10.133104085922241

Epoch 7, Batch 100/276, Loss: 0.8213985562324524
Epoch 7, Batch 200/276, Loss: 0.05649619549512863

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.19802037928025, Training Loss Force: 4.454865590107718, time: 3.9276106357574463
Validation Loss Energy: 10.902937548917459, Validation Loss Force: 5.010757725590849, time: 0.2547605037689209
Test Loss Energy: 15.84313029879212, Test Loss Force: 12.523473683381258, time: 10.190768241882324

Epoch 8, Batch 100/276, Loss: 1.0347671508789062
Epoch 8, Batch 200/276, Loss: 1.092310905456543

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.111501396868588, Training Loss Force: 4.727016294619658, time: 4.224148511886597
Validation Loss Energy: 26.217592165804668, Validation Loss Force: 7.998642324207995, time: 0.22903728485107422
Test Loss Energy: 31.430615741305356, Test Loss Force: 14.730205938452155, time: 10.087653398513794

Epoch 9, Batch 100/276, Loss: 0.44578975439071655
Epoch 9, Batch 200/276, Loss: 0.47492191195487976

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.70934355629911, Training Loss Force: 4.486340376503735, time: 3.9428951740264893
Validation Loss Energy: 12.049243834348527, Validation Loss Force: 5.334755672808778, time: 0.24592804908752441
Test Loss Energy: 11.435633326424417, Test Loss Force: 13.148437757635412, time: 10.197233438491821

Epoch 10, Batch 100/276, Loss: 0.10581308603286743
Epoch 10, Batch 200/276, Loss: 0.25957491993904114

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.256165273683123, Training Loss Force: 5.149036320147423, time: 3.9368443489074707
Validation Loss Energy: 1.636326815457965, Validation Loss Force: 4.13946734119778, time: 0.23696279525756836
Test Loss Energy: 9.309895749087353, Test Loss Force: 12.652520824465897, time: 10.36369013786316

Epoch 11, Batch 100/276, Loss: 0.16835300624370575
Epoch 11, Batch 200/276, Loss: 0.48004910349845886

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 11.513747200625202, Training Loss Force: 5.445139868202494, time: 3.893217086791992
Validation Loss Energy: 2.0926918392882867, Validation Loss Force: 4.174885006481395, time: 0.22763657569885254
Test Loss Energy: 9.002963925785846, Test Loss Force: 11.428463104795082, time: 10.257273197174072

Epoch 12, Batch 100/276, Loss: 0.4220300316810608
Epoch 12, Batch 200/276, Loss: 0.11326932907104492

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 5.496752490722242, Training Loss Force: 2.898492002227113, time: 3.882327079772949
Validation Loss Energy: 4.53637411406982, Validation Loss Force: 2.539367615635792, time: 0.23284506797790527
Test Loss Energy: 9.655154365063868, Test Loss Force: 10.968920596565855, time: 10.380867958068848

Epoch 13, Batch 100/276, Loss: 0.2206561267375946
Epoch 13, Batch 200/276, Loss: 0.9295291900634766

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.401084530150657, Training Loss Force: 3.618243796339982, time: 3.984590530395508
Validation Loss Energy: 3.2663407379597316, Validation Loss Force: 3.0373928119889007, time: 0.22905993461608887
Test Loss Energy: 9.433229852739434, Test Loss Force: 11.097155864959914, time: 11.389658212661743

Epoch 14, Batch 100/276, Loss: 0.6707184314727783
Epoch 14, Batch 200/276, Loss: 0.3410176634788513

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.264489686817907, Training Loss Force: 4.049855995606914, time: 4.071796655654907
Validation Loss Energy: 9.228947605707598, Validation Loss Force: 5.439616719294602, time: 0.24997711181640625
Test Loss Energy: 10.456821520134278, Test Loss Force: 11.800088172528104, time: 10.135605335235596

Epoch 15, Batch 100/276, Loss: 0.6925844550132751
Epoch 15, Batch 200/276, Loss: 0.3013113737106323

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.746880706716588, Training Loss Force: 4.04555478930061, time: 4.0099780559539795
Validation Loss Energy: 11.224317886723519, Validation Loss Force: 4.84758007512483, time: 0.2432546615600586
Test Loss Energy: 11.185722897805787, Test Loss Force: 12.144485345698412, time: 10.003607273101807

Epoch 16, Batch 100/276, Loss: 0.4787255525588989
Epoch 16, Batch 200/276, Loss: 1.1098297834396362

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.123580273526784, Training Loss Force: 4.08888939346216, time: 3.923619270324707
Validation Loss Energy: 12.589449514217714, Validation Loss Force: 4.481270893068337, time: 0.23908352851867676
Test Loss Energy: 17.28641136518653, Test Loss Force: 12.104046152623757, time: 10.38771390914917

Epoch 17, Batch 100/276, Loss: 0.37079501152038574
Epoch 17, Batch 200/276, Loss: 0.10705447196960449

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.114642101128279, Training Loss Force: 4.662983239580256, time: 3.9182841777801514
Validation Loss Energy: 8.079725957682722, Validation Loss Force: 3.1788360833915412, time: 0.2350931167602539
Test Loss Energy: 10.240909214646736, Test Loss Force: 11.259409396967603, time: 10.19681429862976

Epoch 18, Batch 100/276, Loss: 0.999099850654602
Epoch 18, Batch 200/276, Loss: 0.8552112579345703

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.464176770406443, Training Loss Force: 4.04973023917289, time: 3.977991819381714
Validation Loss Energy: 6.197323394214591, Validation Loss Force: 3.0787923057425495, time: 0.23008322715759277
Test Loss Energy: 12.079803395605563, Test Loss Force: 11.642559782388023, time: 10.244512557983398

Epoch 19, Batch 100/276, Loss: 0.8794353008270264
Epoch 19, Batch 200/276, Loss: 0.7811233401298523

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.469962082504106, Training Loss Force: 4.015458232945849, time: 3.9581854343414307
Validation Loss Energy: 1.9804762313147002, Validation Loss Force: 3.1431562029216815, time: 0.23489975929260254
Test Loss Energy: 9.724395427447956, Test Loss Force: 11.975009433854115, time: 10.170223712921143

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–‚â–‚â–â–ƒâ–ƒâ–ˆâ–‚â–â–â–â–â–â–‚â–„â–â–‚â–
wandb:   test_error_force â–‚â–‚â–‚â–‚â–‚â–‚â–„â–„â–ˆâ–…â–„â–‚â–â–â–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒ
wandb:          test_loss â–‚â–‚â–â–‚â–‚â–‚â–ƒâ–ƒâ–ˆâ–ƒâ–‚â–â–â–â–‚â–‚â–ƒâ–â–‚â–‚
wandb: train_error_energy â–â–â–‚â–‚â–‚â–â–‡â–†â–‡â–†â–†â–ˆâ–„â–…â–†â–‡â–†â–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–‡â–†â–‡â–†â–‡â–ˆâ–ƒâ–„â–…â–…â–…â–†â–…â–…
wandb:         train_loss â–â–â–â–â–â–â–‡â–†â–‡â–†â–‡â–ˆâ–ƒâ–…â–…â–†â–…â–†â–†â–…
wandb: valid_error_energy â–â–â–‚â–â–‚â–â–ƒâ–„â–ˆâ–„â–â–â–‚â–‚â–ƒâ–„â–„â–ƒâ–‚â–
wandb:  valid_error_force â–â–â–â–â–â–â–„â–…â–ˆâ–…â–„â–„â–‚â–‚â–…â–„â–„â–‚â–‚â–‚
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–„â–ˆâ–…â–‚â–ƒâ–‚â–‚â–„â–„â–„â–ƒâ–‚â–‚
wandb: 
wandb: Run summary:
wandb:       dataset_size 8821
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.7244
wandb:   test_error_force 11.97501
wandb:          test_loss 4.65764
wandb: train_error_energy 8.46996
wandb:  train_error_force 4.01546
wandb:         train_loss 1.9104
wandb: valid_error_energy 1.98048
wandb:  valid_error_force 3.14316
wandb:         valid_loss 1.18424
wandb: 
wandb: ğŸš€ View run al_77_89 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/12vwf570
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_013156-12vwf570/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6012317538261414, Uncertainty Bias: -0.562292218208313
7.6293945e-06 0.0025219917
-6.363832 37.973446
(48745, 22, 3)
Found uncertainty sample 0 after 21 steps.
Found uncertainty sample 1 after 4 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 12 steps.
Found uncertainty sample 4 after 70 steps.
Found uncertainty sample 5 after 23 steps.
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 4 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 3 steps.
Found uncertainty sample 11 after 8 steps.
Found uncertainty sample 12 after 2 steps.
Found uncertainty sample 13 after 7 steps.
Found uncertainty sample 14 after 27 steps.
Found uncertainty sample 15 after 6 steps.
Found uncertainty sample 16 after 2 steps.
Found uncertainty sample 17 after 4 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 11 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 38 steps.
Found uncertainty sample 23 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 5 steps.
Found uncertainty sample 27 after 11 steps.
Found uncertainty sample 28 after 21 steps.
Found uncertainty sample 29 after 11 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 17 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 12 steps.
Found uncertainty sample 34 after 5 steps.
Found uncertainty sample 35 after 15 steps.
Found uncertainty sample 36 after 5 steps.
Found uncertainty sample 37 after 2 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 8 steps.
Found uncertainty sample 40 after 5 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 30 steps.
Found uncertainty sample 44 after 4 steps.
Found uncertainty sample 45 after 10 steps.
Found uncertainty sample 46 after 11 steps.
Found uncertainty sample 47 after 6 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 9 steps.
Found uncertainty sample 50 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 6 steps.
Found uncertainty sample 53 after 5 steps.
Found uncertainty sample 54 after 11 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 3 steps.
Found uncertainty sample 57 after 3 steps.
Found uncertainty sample 58 after 12 steps.
Found uncertainty sample 59 after 4 steps.
Found uncertainty sample 60 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 6 steps.
Found uncertainty sample 63 after 4 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 18 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 26 steps.
Found uncertainty sample 69 after 3 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 12 steps.
Found uncertainty sample 72 after 7 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 2 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 2 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 13 steps.
Found uncertainty sample 80 after 7 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 10 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 8 steps.
Found uncertainty sample 85 after 10 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 8 steps.
Found uncertainty sample 89 after 8 steps.
Found uncertainty sample 90 after 14 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 11 steps.
Found uncertainty sample 94 after 5 steps.
Found uncertainty sample 95 after 7 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 28 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 10 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_014153-qsnx7snf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_90
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/qsnx7snf
Training model 90. Added 100 samples to the dataset.
Epoch 0, Batch 100/279, Loss: 0.12031599879264832
Epoch 0, Batch 200/279, Loss: 0.06819374859333038

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.3249725079082113, Training Loss Force: 1.9197336017375048, time: 4.375497102737427
Validation Loss Energy: 0.9653688110888268, Validation Loss Force: 2.0318173012273313, time: 0.260758638381958
Test Loss Energy: 9.833662255085326, Test Loss Force: 11.430290055007713, time: 11.483327388763428

Epoch 1, Batch 100/279, Loss: 0.0507240891456604
Epoch 1, Batch 200/279, Loss: 0.03576347976922989

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.1192706517365316, Training Loss Force: 1.8480550628598906, time: 4.385956048965454
Validation Loss Energy: 0.9543538360449857, Validation Loss Force: 1.9772779453040812, time: 0.2595937252044678
Test Loss Energy: 9.864088723146676, Test Loss Force: 11.559394800950148, time: 11.604551076889038

Epoch 2, Batch 100/279, Loss: 0.11468477547168732
Epoch 2, Batch 200/279, Loss: 0.16487643122673035

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3113207609368707, Training Loss Force: 1.8624301758913668, time: 4.291618824005127
Validation Loss Energy: 3.9995539849629793, Validation Loss Force: 2.04960424620553, time: 0.2669515609741211
Test Loss Energy: 9.912407135064216, Test Loss Force: 11.460656684524972, time: 11.50635814666748

Epoch 3, Batch 100/279, Loss: 0.04437284544110298
Epoch 3, Batch 200/279, Loss: 0.0550469234585762

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.3502111996397748, Training Loss Force: 1.848914503910631, time: 4.269887685775757
Validation Loss Energy: 0.8743198760576181, Validation Loss Force: 1.9897797045021968, time: 0.2647135257720947
Test Loss Energy: 10.59782748192503, Test Loss Force: 11.470490514115337, time: 11.823007583618164

Epoch 4, Batch 100/279, Loss: 0.05142427980899811
Epoch 4, Batch 200/279, Loss: 0.09017464518547058

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.1368935167693, Training Loss Force: 1.8305003134361975, time: 4.283979415893555
Validation Loss Energy: 2.6390850493845126, Validation Loss Force: 1.9946643752895, time: 0.27213072776794434
Test Loss Energy: 10.108992019826001, Test Loss Force: 11.390984338818782, time: 10.55883264541626

Epoch 5, Batch 100/279, Loss: 0.05199936404824257
Epoch 5, Batch 200/279, Loss: 0.26721373200416565

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.277532517422878, Training Loss Force: 1.9015880276684978, time: 4.133734226226807
Validation Loss Energy: 3.6027458990564147, Validation Loss Force: 2.1548699270633334, time: 0.2713651657104492
Test Loss Energy: 9.964447912923264, Test Loss Force: 11.431161164685953, time: 12.308873653411865

Epoch 6, Batch 100/279, Loss: 0.9551975131034851
Epoch 6, Batch 200/279, Loss: 0.34115535020828247

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.206024137200915, Training Loss Force: 3.8169522282707904, time: 4.2945029735565186
Validation Loss Energy: 5.589372318982385, Validation Loss Force: 2.7185312630277463, time: 0.21753621101379395
Test Loss Energy: 13.728337686404902, Test Loss Force: 11.633783792122186, time: 9.130463600158691

Epoch 7, Batch 100/279, Loss: 0.1251887083053589
Epoch 7, Batch 200/279, Loss: 0.4342632293701172

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 5.329626055106831, Training Loss Force: 2.5870794408043816, time: 4.035986423492432
Validation Loss Energy: 4.4423854675737005, Validation Loss Force: 2.630425668968527, time: 0.21635961532592773
Test Loss Energy: 10.595208972043801, Test Loss Force: 11.40941451713554, time: 9.307941675186157

Epoch 8, Batch 100/279, Loss: 0.3096325397491455
Epoch 8, Batch 200/279, Loss: 2.3994970321655273

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.984972403084716, Training Loss Force: 3.463695008262063, time: 4.15339469909668
Validation Loss Energy: 13.37031386938185, Validation Loss Force: 5.893463822689687, time: 0.21902704238891602
Test Loss Energy: 11.707503964454737, Test Loss Force: 12.454701336051928, time: 8.974327802658081

Epoch 9, Batch 100/279, Loss: 0.34448790550231934
Epoch 9, Batch 200/279, Loss: 0.3995364308357239

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.1477469123981265, Training Loss Force: 2.879191219319112, time: 4.012730836868286
Validation Loss Energy: 15.810251116331852, Validation Loss Force: 4.06620794524032, time: 0.20671367645263672
Test Loss Energy: 20.594986275807422, Test Loss Force: 11.924940963693517, time: 9.349187850952148

Epoch 10, Batch 100/279, Loss: 0.10132363438606262
Epoch 10, Batch 200/279, Loss: 0.3193170726299286

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.096804407976344, Training Loss Force: 3.365407176831895, time: 4.004713296890259
Validation Loss Energy: 4.299893157290764, Validation Loss Force: 2.5776483162639185, time: 0.21365904808044434
Test Loss Energy: 9.711652498911889, Test Loss Force: 10.75662150483765, time: 9.183181762695312

Epoch 11, Batch 100/279, Loss: 0.22957929968833923
Epoch 11, Batch 200/279, Loss: 0.3997725546360016

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 6.477647915563612, Training Loss Force: 2.78118852570652, time: 4.069080352783203
Validation Loss Energy: 12.831841980550516, Validation Loss Force: 3.712282240531625, time: 0.20684814453125
Test Loss Energy: 11.702358222720807, Test Loss Force: 11.19668297161496, time: 9.281283855438232

Epoch 12, Batch 100/279, Loss: 0.34538185596466064
Epoch 12, Batch 200/279, Loss: 0.44192641973495483

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 6.07647311040809, Training Loss Force: 2.9783877890729253, time: 4.184568881988525
Validation Loss Energy: 21.652831209267728, Validation Loss Force: 3.6101726276143737, time: 0.2124919891357422
Test Loss Energy: 15.762282536062315, Test Loss Force: 10.976570808935707, time: 9.210283041000366

Epoch 13, Batch 100/279, Loss: 1.2470438480377197
Epoch 13, Batch 200/279, Loss: 0.14355997741222382

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.71195882125144, Training Loss Force: 4.653212433674077, time: 4.040364503860474
Validation Loss Energy: 10.976876531108287, Validation Loss Force: 3.226516361210546, time: 0.21162056922912598
Test Loss Energy: 15.113829002671292, Test Loss Force: 11.17685982175356, time: 9.927698850631714

Epoch 14, Batch 100/279, Loss: 0.43827471137046814
Epoch 14, Batch 200/279, Loss: 0.368723601102829

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.241522674591137, Training Loss Force: 3.328064815636929, time: 4.265408277511597
Validation Loss Energy: 4.572418370330075, Validation Loss Force: 2.3313396785940235, time: 0.2882373332977295
Test Loss Energy: 9.433898788159626, Test Loss Force: 10.786875568689702, time: 11.753437280654907

Epoch 15, Batch 100/279, Loss: 0.29357343912124634
Epoch 15, Batch 200/279, Loss: 1.5642890930175781

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.354634723565783, Training Loss Force: 3.4515660762258626, time: 4.089805364608765
Validation Loss Energy: 12.611542925011983, Validation Loss Force: 5.392136977308419, time: 0.21414589881896973
Test Loss Energy: 17.893726841484206, Test Loss Force: 13.247430136577483, time: 9.033469438552856

Epoch 16, Batch 100/279, Loss: 0.40658658742904663
Epoch 16, Batch 200/279, Loss: 0.5591453909873962

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 11.235713417224641, Training Loss Force: 5.0339709013709415, time: 4.145423889160156
Validation Loss Energy: 15.027690200411064, Validation Loss Force: 4.140961896505247, time: 0.2115921974182129
Test Loss Energy: 12.520845063621318, Test Loss Force: 11.978595806072008, time: 9.17098093032837

Epoch 17, Batch 100/279, Loss: 0.28113067150115967
Epoch 17, Batch 200/279, Loss: 0.6546792387962341

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.732249665551173, Training Loss Force: 4.658005159582444, time: 4.230592489242554
Validation Loss Energy: 15.888281154206984, Validation Loss Force: 3.49868606069147, time: 0.2187514305114746
Test Loss Energy: 12.687225706461001, Test Loss Force: 11.473335717508421, time: 9.206976652145386

Epoch 18, Batch 100/279, Loss: 0.41928631067276
Epoch 18, Batch 200/279, Loss: 0.543173611164093

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.537411483082726, Training Loss Force: 4.173194401605632, time: 4.080679893493652
Validation Loss Energy: 14.562530257455625, Validation Loss Force: 4.810867455156157, time: 0.20628809928894043
Test Loss Energy: 19.70213558319803, Test Loss Force: 12.708635799506709, time: 9.382041931152344

Epoch 19, Batch 100/279, Loss: 0.23106348514556885
Epoch 19, Batch 200/279, Loss: 0.18193569779396057

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.327203131784938, Training Loss Force: 4.21347210799491, time: 4.124830961227417
Validation Loss Energy: 4.019084384819965, Validation Loss Force: 5.018308258193945, time: 0.2241833209991455
Test Loss Energy: 9.527092918500477, Test Loss Force: 11.370293884473346, time: 9.018349170684814

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–‚â–â–â–„â–‚â–‚â–ˆâ–â–‚â–…â–…â–â–†â–ƒâ–ƒâ–‡â–
wandb:   test_error_force â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–†â–„â–â–‚â–‚â–‚â–â–ˆâ–„â–ƒâ–†â–ƒ
wandb:          test_loss â–‚â–‚â–‚â–ƒâ–‚â–‚â–„â–‚â–…â–‡â–â–‚â–ƒâ–„â–â–ˆâ–„â–ƒâ–ˆâ–‚
wandb: train_error_energy â–â–â–â–â–â–‚â–†â–„â–†â–„â–…â–…â–„â–ˆâ–…â–…â–ˆâ–†â–…â–†
wandb:  train_error_force â–â–â–â–â–â–â–…â–ƒâ–…â–ƒâ–„â–ƒâ–„â–‡â–„â–…â–ˆâ–‡â–†â–†
wandb:         train_loss â–â–â–â–â–â–â–†â–ƒâ–…â–„â–…â–„â–„â–‡â–…â–…â–ˆâ–‡â–†â–†
wandb: valid_error_energy â–â–â–‚â–â–‚â–‚â–ƒâ–‚â–…â–†â–‚â–…â–ˆâ–„â–‚â–…â–†â–†â–†â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–‚â–ˆâ–…â–‚â–„â–„â–ƒâ–‚â–‡â–…â–„â–†â–†
wandb:         valid_loss â–â–â–‚â–â–â–‚â–ƒâ–‚â–ˆâ–‡â–‚â–…â–‡â–…â–‚â–‡â–†â–†â–‡â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 8911
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.52709
wandb:   test_error_force 11.37029
wandb:          test_loss 4.4421
wandb: train_error_energy 8.3272
wandb:  train_error_force 4.21347
wandb:         train_loss 1.9671
wandb: valid_error_energy 4.01908
wandb:  valid_error_force 5.01831
wandb:         valid_loss 1.9481
wandb: 
wandb: ğŸš€ View run al_77_90 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/qsnx7snf
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_014153-qsnx7snf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.39270180463790894, Uncertainty Bias: -0.358196496963501
1.5258789e-05 0.16928267
-2.6602554 22.30334
(48745, 22, 3)
Found uncertainty sample 0 after 8 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 6 steps.
Found uncertainty sample 3 after 3 steps.
Found uncertainty sample 4 after 4 steps.
Found uncertainty sample 5 after 23 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 18 steps.
Found uncertainty sample 8 after 70 steps.
Found uncertainty sample 9 after 3 steps.
Found uncertainty sample 10 after 50 steps.
Found uncertainty sample 11 after 36 steps.
Found uncertainty sample 12 after 57 steps.
Found uncertainty sample 13 after 30 steps.
Found uncertainty sample 14 after 6 steps.
Found uncertainty sample 15 after 6 steps.
Found uncertainty sample 16 after 12 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 97 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 66 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 15 steps.
Found uncertainty sample 23 after 21 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 29 steps.
Found uncertainty sample 26 after 27 steps.
Found uncertainty sample 27 after 6 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 14 steps.
Found uncertainty sample 30 after 7 steps.
Found uncertainty sample 31 after 2 steps.
Found uncertainty sample 32 after 13 steps.
Found uncertainty sample 33 after 21 steps.
Found uncertainty sample 34 after 17 steps.
Found uncertainty sample 35 after 53 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 13 steps.
Found uncertainty sample 38 after 6 steps.
Found uncertainty sample 39 after 13 steps.
Found uncertainty sample 40 after 24 steps.
Found uncertainty sample 41 after 40 steps.
Found uncertainty sample 42 after 21 steps.
Found uncertainty sample 43 after 4 steps.
Found uncertainty sample 44 after 26 steps.
Found uncertainty sample 45 after 46 steps.
Found uncertainty sample 46 after 22 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 66 steps.
Found uncertainty sample 49 after 18 steps.
Found uncertainty sample 50 after 23 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 20 steps.
Found uncertainty sample 53 after 21 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 15 steps.
Found uncertainty sample 56 after 20 steps.
Found uncertainty sample 57 after 49 steps.
Found uncertainty sample 58 after 9 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 39 steps.
Found uncertainty sample 61 after 6 steps.
Found uncertainty sample 62 after 19 steps.
Found uncertainty sample 63 after 22 steps.
Found uncertainty sample 64 after 9 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 7 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 13 steps.
Found uncertainty sample 69 after 31 steps.
Found uncertainty sample 70 after 29 steps.
Found uncertainty sample 71 after 63 steps.
Found uncertainty sample 72 after 9 steps.
Found uncertainty sample 73 after 4 steps.
Found uncertainty sample 74 after 10 steps.
Found uncertainty sample 75 after 18 steps.
Found uncertainty sample 76 after 6 steps.
Found uncertainty sample 77 after 27 steps.
Found uncertainty sample 78 after 15 steps.
Found uncertainty sample 79 after 4 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 54 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 41 steps.
Found uncertainty sample 85 after 19 steps.
Found uncertainty sample 86 after 9 steps.
Found uncertainty sample 87 after 5 steps.
Found uncertainty sample 88 after 6 steps.
Found uncertainty sample 89 after 29 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 5 steps.
Found uncertainty sample 92 after 3 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 14 steps.
Found uncertainty sample 95 after 3 steps.
Found uncertainty sample 96 after 13 steps.
Found uncertainty sample 97 after 43 steps.
Found uncertainty sample 98 after 39 steps.
Found uncertainty sample 99 after 106 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_015211-8w1zpfyx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_91
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/8w1zpfyx
Training model 91. Added 100 samples to the dataset.
Epoch 0, Batch 100/282, Loss: 0.12397082149982452
Epoch 0, Batch 200/282, Loss: 0.1008690595626831

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.8985607277434167, Training Loss Force: 1.9242679550612305, time: 4.0508434772491455
Validation Loss Energy: 1.4413325274031894, Validation Loss Force: 2.3932821824126784, time: 0.23512721061706543
Test Loss Energy: 10.217763976361715, Test Loss Force: 11.29699734776867, time: 10.128028392791748

Epoch 1, Batch 100/282, Loss: 0.027894798666238785
Epoch 1, Batch 200/282, Loss: 0.058491840958595276

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5215815611695283, Training Loss Force: 1.8459113095571207, time: 4.068037986755371
Validation Loss Energy: 0.8071915636987075, Validation Loss Force: 1.9254070786925446, time: 0.2615623474121094
Test Loss Energy: 9.974425225381713, Test Loss Force: 11.261987775107055, time: 10.303842782974243

Epoch 2, Batch 100/282, Loss: 0.057334523648023605
Epoch 2, Batch 200/282, Loss: 0.12256208062171936

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.4678106773310322, Training Loss Force: 1.830063655076494, time: 4.11271333694458
Validation Loss Energy: 0.6757250390466477, Validation Loss Force: 2.0882704316590863, time: 0.23503541946411133
Test Loss Energy: 10.23970025874958, Test Loss Force: 11.481750396191101, time: 10.128519773483276

Epoch 3, Batch 100/282, Loss: 0.058074407279491425
Epoch 3, Batch 200/282, Loss: 0.04625071585178375

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.4553274059888839, Training Loss Force: 1.8349165605309163, time: 4.047709941864014
Validation Loss Energy: 1.9047577218434437, Validation Loss Force: 1.9378799577024335, time: 0.26024675369262695
Test Loss Energy: 10.116336669505673, Test Loss Force: 11.315872738470121, time: 11.678833961486816

Epoch 4, Batch 100/282, Loss: 0.07832666486501694
Epoch 4, Batch 200/282, Loss: 0.15889598429203033

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.586692023263725, Training Loss Force: 1.829957450579562, time: 4.152508497238159
Validation Loss Energy: 3.4629931318096583, Validation Loss Force: 1.9167253200273933, time: 0.23094511032104492
Test Loss Energy: 12.132187727474832, Test Loss Force: 11.443266559980438, time: 10.163238286972046

Epoch 5, Batch 100/282, Loss: 0.18606874346733093
Epoch 5, Batch 200/282, Loss: 0.12574931979179382

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.2211670567179755, Training Loss Force: 1.8247243647999054, time: 3.9785125255584717
Validation Loss Energy: 2.9489049840737818, Validation Loss Force: 1.9332137476142002, time: 0.2323596477508545
Test Loss Energy: 11.748927145309123, Test Loss Force: 11.553620917716202, time: 10.31984281539917

Epoch 6, Batch 100/282, Loss: 0.6173679232597351
Epoch 6, Batch 200/282, Loss: 0.587864875793457

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.7957942860515566, Training Loss Force: 4.512696233851112, time: 4.026270389556885
Validation Loss Energy: 6.870461040052471, Validation Loss Force: 4.309175254240252, time: 0.23420047760009766
Test Loss Energy: 10.037637461183648, Test Loss Force: 11.787504120176365, time: 10.134454250335693

Epoch 7, Batch 100/282, Loss: 0.07623860985040665
Epoch 7, Batch 200/282, Loss: 0.850129246711731

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.989294730076761, Training Loss Force: 4.550773741935037, time: 4.008672475814819
Validation Loss Energy: 1.5722534569804396, Validation Loss Force: 6.314622739191938, time: 0.23608732223510742
Test Loss Energy: 10.285827111969, Test Loss Force: 13.373766277313035, time: 10.164489984512329

Epoch 8, Batch 100/282, Loss: 0.2984825372695923
Epoch 8, Batch 200/282, Loss: 0.44996827840805054

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.763234208956757, Training Loss Force: 4.141687046688967, time: 4.345832824707031
Validation Loss Energy: 3.533992525003608, Validation Loss Force: 6.592086397617296, time: 0.23572373390197754
Test Loss Energy: 12.410374582177239, Test Loss Force: 13.412747139036778, time: 10.279404401779175

Epoch 9, Batch 100/282, Loss: 0.15972451865673065
Epoch 9, Batch 200/282, Loss: 0.9602692723274231

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.943054803776095, Training Loss Force: 4.987470507176255, time: 4.027475357055664
Validation Loss Energy: 5.755310176181324, Validation Loss Force: 4.802157617911704, time: 0.23557758331298828
Test Loss Energy: 10.40839103219049, Test Loss Force: 11.848883711964048, time: 10.083766460418701

Epoch 10, Batch 100/282, Loss: 3.14013671875
Epoch 10, Batch 200/282, Loss: 0.6407006978988647

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 11.150037784442125, Training Loss Force: 4.950329530517569, time: 4.324548244476318
Validation Loss Energy: 11.37210126125199, Validation Loss Force: 3.774361819476513, time: 0.23465347290039062
Test Loss Energy: 17.724684511933503, Test Loss Force: 11.982193258120073, time: 10.265421867370605

Epoch 11, Batch 100/282, Loss: 0.5331926345825195
Epoch 11, Batch 200/282, Loss: 0.4348827302455902

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.370456128066085, Training Loss Force: 3.9782724392980775, time: 4.08122706413269
Validation Loss Energy: 1.677590147028879, Validation Loss Force: 3.430965426137476, time: 0.23783516883850098
Test Loss Energy: 9.171042869698946, Test Loss Force: 11.510597368243559, time: 10.157270669937134

Epoch 12, Batch 100/282, Loss: 0.3417116403579712
Epoch 12, Batch 200/282, Loss: 1.4357848167419434

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.680567295337479, Training Loss Force: 5.444783381130892, time: 4.214750051498413
Validation Loss Energy: 11.020110506887578, Validation Loss Force: 2.9143682869991148, time: 0.23592352867126465
Test Loss Energy: 16.503129719370072, Test Loss Force: 11.354814205242139, time: 10.316060781478882

Epoch 13, Batch 100/282, Loss: 1.6290333271026611
Epoch 13, Batch 200/282, Loss: 0.12720544636249542

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 10.062481503690142, Training Loss Force: 4.73746261992449, time: 4.105916261672974
Validation Loss Energy: 12.131695368920521, Validation Loss Force: 5.231983262911759, time: 0.24970006942749023
Test Loss Energy: 18.626877777033457, Test Loss Force: 12.676095420159779, time: 10.211173057556152

Epoch 14, Batch 100/282, Loss: 0.1314905434846878
Epoch 14, Batch 200/282, Loss: 0.2896032929420471

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.55683724808041, Training Loss Force: 4.541580835190913, time: 4.061140537261963
Validation Loss Energy: 9.97652399236743, Validation Loss Force: 5.291381504535212, time: 0.22993779182434082
Test Loss Energy: 16.104309446303926, Test Loss Force: 12.659314653632126, time: 10.36394476890564

Epoch 15, Batch 100/282, Loss: 1.0235925912857056
Epoch 15, Batch 200/282, Loss: 0.4993184804916382

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.564061587045426, Training Loss Force: 4.1279979374336815, time: 4.122176647186279
Validation Loss Energy: 7.430736691653053, Validation Loss Force: 3.18023653871794, time: 0.2297346591949463
Test Loss Energy: 10.264244050463384, Test Loss Force: 11.608939254832919, time: 10.09069538116455

Epoch 16, Batch 100/282, Loss: 1.0027329921722412
Epoch 16, Batch 200/282, Loss: 0.6158809661865234

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.709759889428565, Training Loss Force: 4.00659736410841, time: 4.076706647872925
Validation Loss Energy: 7.887159542660571, Validation Loss Force: 3.920481588281929, time: 0.2381739616394043
Test Loss Energy: 10.529866121272924, Test Loss Force: 11.547203686195129, time: 10.304040431976318

Epoch 17, Batch 100/282, Loss: 0.7833374738693237
Epoch 17, Batch 200/282, Loss: 1.5885839462280273

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.1827279515107, Training Loss Force: 3.8191343863084226, time: 4.012115001678467
Validation Loss Energy: 15.68137734008152, Validation Loss Force: 3.8744318275508953, time: 0.2537808418273926
Test Loss Energy: 19.457586635902175, Test Loss Force: 12.700794124313957, time: 10.137397289276123

Epoch 18, Batch 100/282, Loss: 0.08250950276851654
Epoch 18, Batch 200/282, Loss: 0.444230854511261

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.668251809210329, Training Loss Force: 4.008050806953392, time: 4.098141431808472
Validation Loss Energy: 2.6926758570130245, Validation Loss Force: 5.794054696640417, time: 0.2510538101196289
Test Loss Energy: 11.197498980588941, Test Loss Force: 12.900536947203959, time: 10.234265327453613

Epoch 19, Batch 100/282, Loss: 0.5280918478965759
Epoch 19, Batch 200/282, Loss: 1.1663758754730225

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.597762889573657, Training Loss Force: 4.417572540858909, time: 4.052043437957764
Validation Loss Energy: 3.07057550252101, Validation Loss Force: 4.189157742464734, time: 0.234605073928833
Test Loss Energy: 9.004380227461109, Test Loss Force: 12.083077788531295, time: 10.301289081573486

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.050 MB uploadedwandb: / 0.039 MB of 0.050 MB uploadedwandb: - 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‡â–â–†â–‡â–†â–‚â–‚â–ˆâ–‚â–
wandb:   test_error_force â–â–â–‚â–â–‚â–‚â–ƒâ–ˆâ–ˆâ–ƒâ–ƒâ–‚â–â–†â–†â–‚â–‚â–†â–†â–„
wandb:          test_loss â–â–â–‚â–â–‚â–‚â–‚â–†â–‡â–‚â–†â–â–„â–ˆâ–‡â–‚â–‚â–ˆâ–…â–‚
wandb: train_error_energy â–â–â–â–â–â–‚â–†â–†â–†â–ˆâ–ˆâ–†â–ˆâ–‡â–†â–†â–†â–†â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–†â–†â–…â–‡â–‡â–…â–ˆâ–‡â–†â–…â–…â–…â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–†â–†â–†â–‡â–‡â–†â–ˆâ–‡â–†â–†â–†â–…â–†â–†
wandb: valid_error_energy â–â–â–â–‚â–‚â–‚â–„â–â–‚â–ƒâ–†â–â–†â–†â–…â–„â–„â–ˆâ–‚â–‚
wandb:  valid_error_force â–‚â–â–â–â–â–â–…â–ˆâ–ˆâ–…â–„â–ƒâ–‚â–†â–†â–ƒâ–„â–„â–‡â–„
wandb:         valid_loss â–‚â–â–â–â–‚â–‚â–†â–‡â–ˆâ–†â–†â–ƒâ–…â–ˆâ–ˆâ–„â–…â–‡â–†â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 9001
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.00438
wandb:   test_error_force 12.08308
wandb:          test_loss 4.64562
wandb: train_error_energy 8.59776
wandb:  train_error_force 4.41757
wandb:         train_loss 2.0535
wandb: valid_error_energy 3.07058
wandb:  valid_error_force 4.18916
wandb:         valid_loss 1.60719
wandb: 
wandb: ğŸš€ View run al_77_91 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/8w1zpfyx
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_015211-8w1zpfyx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.22818872332572937, Uncertainty Bias: -0.09522686898708344
1.5258789e-05 0.0033783913
-1.230723 28.720684
(48745, 22, 3)
Found uncertainty sample 0 after 29 steps.
Found uncertainty sample 1 after 164 steps.
Found uncertainty sample 2 after 9 steps.
Found uncertainty sample 3 after 42 steps.
Found uncertainty sample 4 after 89 steps.
Found uncertainty sample 5 after 72 steps.
Found uncertainty sample 6 after 16 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 45 steps.
Found uncertainty sample 9 after 109 steps.
Found uncertainty sample 10 after 85 steps.
Found uncertainty sample 11 after 43 steps.
Found uncertainty sample 12 after 11 steps.
Found uncertainty sample 13 after 26 steps.
Found uncertainty sample 14 after 12 steps.
Found uncertainty sample 15 after 18 steps.
Found uncertainty sample 16 after 27 steps.
Found uncertainty sample 17 after 31 steps.
Found uncertainty sample 18 after 9 steps.
Found uncertainty sample 19 after 11 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 29 steps.
Found uncertainty sample 22 after 69 steps.
Found uncertainty sample 23 after 13 steps.
Found uncertainty sample 24 after 17 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 8 steps.
Found uncertainty sample 27 after 72 steps.
Found uncertainty sample 28 after 56 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 36 steps.
Found uncertainty sample 31 after 35 steps.
Found uncertainty sample 32 after 11 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 4 steps.
Found uncertainty sample 35 after 122 steps.
Found uncertainty sample 36 after 18 steps.
Found uncertainty sample 37 after 46 steps.
Found uncertainty sample 38 after 6 steps.
Found uncertainty sample 39 after 35 steps.
Found uncertainty sample 40 after 87 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 154 steps.
Found uncertainty sample 43 after 31 steps.
Found uncertainty sample 44 after 90 steps.
Found uncertainty sample 45 after 22 steps.
Found uncertainty sample 46 after 83 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 20 steps.
Found uncertainty sample 49 after 9 steps.
Found uncertainty sample 50 after 18 steps.
Found uncertainty sample 51 after 81 steps.
Found uncertainty sample 52 after 10 steps.
Found uncertainty sample 53 after 15 steps.
Found uncertainty sample 54 after 30 steps.
Found uncertainty sample 55 after 19 steps.
Found uncertainty sample 56 after 43 steps.
Found uncertainty sample 57 after 11 steps.
Found uncertainty sample 58 after 31 steps.
Found uncertainty sample 59 after 78 steps.
Found uncertainty sample 60 after 12 steps.
Found uncertainty sample 61 after 31 steps.
Found uncertainty sample 62 after 49 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 48 steps.
Found uncertainty sample 65 after 46 steps.
Found uncertainty sample 66 after 18 steps.
Found uncertainty sample 67 after 26 steps.
Found uncertainty sample 68 after 24 steps.
Found uncertainty sample 69 after 77 steps.
Found uncertainty sample 70 after 13 steps.
Found uncertainty sample 71 after 59 steps.
Found uncertainty sample 72 after 7 steps.
Found uncertainty sample 73 after 7 steps.
Found uncertainty sample 74 after 272 steps.
Found uncertainty sample 75 after 38 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 33 steps.
Found uncertainty sample 78 after 15 steps.
Found uncertainty sample 79 after 6 steps.
Found uncertainty sample 80 after 85 steps.
Found uncertainty sample 81 after 32 steps.
Found uncertainty sample 82 after 59 steps.
Found uncertainty sample 83 after 154 steps.
Found uncertainty sample 84 after 38 steps.
Found uncertainty sample 85 after 39 steps.
Found uncertainty sample 86 after 12 steps.
Found uncertainty sample 87 after 54 steps.
Found uncertainty sample 88 after 26 steps.
Found uncertainty sample 89 after 34 steps.
Found uncertainty sample 90 after 22 steps.
Found uncertainty sample 91 after 3 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 88 steps.
Found uncertainty sample 94 after 7 steps.
Found uncertainty sample 95 after 96 steps.
Found uncertainty sample 96 after 3 steps.
Found uncertainty sample 97 after 52 steps.
Found uncertainty sample 98 after 12 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_020315-q2ig1x27
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_92
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/q2ig1x27
Training model 92. Added 100 samples to the dataset.
Epoch 0, Batch 100/285, Loss: 0.04679851233959198
Epoch 0, Batch 200/285, Loss: 0.10531599819660187

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7055933351476715, Training Loss Force: 1.909839493499008, time: 4.3847222328186035
Validation Loss Energy: 1.9865587285056079, Validation Loss Force: 2.0448754883542053, time: 0.27652812004089355
Test Loss Energy: 10.969855340655934, Test Loss Force: 11.594107256027828, time: 11.718670845031738

Epoch 1, Batch 100/285, Loss: 0.2067990005016327
Epoch 1, Batch 200/285, Loss: 0.24202677607536316

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.5846064472728, Training Loss Force: 1.9294638894247933, time: 4.530332326889038
Validation Loss Energy: 4.405101923914868, Validation Loss Force: 2.054010289663662, time: 0.28682899475097656
Test Loss Energy: 12.243191099319972, Test Loss Force: 11.57288729678499, time: 11.86494755744934

Epoch 2, Batch 100/285, Loss: 0.19899743795394897
Epoch 2, Batch 200/285, Loss: 0.25991958379745483

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.31112095020048, Training Loss Force: 1.9071129422308755, time: 4.411626815795898
Validation Loss Energy: 1.1089449857064806, Validation Loss Force: 1.9439696916405833, time: 0.27129101753234863
Test Loss Energy: 10.126908333712695, Test Loss Force: 11.53508744423188, time: 11.739520072937012

Epoch 3, Batch 100/285, Loss: 0.17504453659057617
Epoch 3, Batch 200/285, Loss: 0.15694162249565125

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.1487667683242453, Training Loss Force: 1.8770425854658186, time: 4.236171722412109
Validation Loss Energy: 1.5390844383338906, Validation Loss Force: 2.355547199327392, time: 0.28699517250061035
Test Loss Energy: 10.269445256633372, Test Loss Force: 11.357714784469945, time: 11.939201354980469

Epoch 4, Batch 100/285, Loss: 0.10945026576519012
Epoch 4, Batch 200/285, Loss: 0.11138471961021423

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.302784050508616, Training Loss Force: 1.816379269403452, time: 4.241326808929443
Validation Loss Energy: 1.1253689728424559, Validation Loss Force: 1.9047801668933535, time: 0.27443718910217285
Test Loss Energy: 10.229312689464923, Test Loss Force: 11.56897673694354, time: 11.602131605148315

Epoch 5, Batch 100/285, Loss: 0.10203107446432114
Epoch 5, Batch 200/285, Loss: 0.0943412333726883

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.3232074371417009, Training Loss Force: 1.7865518130341456, time: 4.440141677856445
Validation Loss Energy: 2.294888249533524, Validation Loss Force: 1.9598983512614425, time: 0.2720375061035156
Test Loss Energy: 10.117403280362423, Test Loss Force: 11.549366295572664, time: 12.203248262405396

Epoch 6, Batch 100/285, Loss: 0.9745540618896484
Epoch 6, Batch 200/285, Loss: 0.7481192350387573

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.309566390776162, Training Loss Force: 4.550051333923097, time: 4.308285236358643
Validation Loss Energy: 7.317214438864246, Validation Loss Force: 4.889862897380375, time: 0.29169726371765137
Test Loss Energy: 12.625056555529614, Test Loss Force: 11.55930711827055, time: 11.745449781417847

Epoch 7, Batch 100/285, Loss: 0.5140008926391602
Epoch 7, Batch 200/285, Loss: 0.5072979927062988

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.184614637764334, Training Loss Force: 4.218980416525821, time: 4.636721134185791
Validation Loss Energy: 36.27219251122958, Validation Loss Force: 4.446697500118421, time: 0.2845749855041504
Test Loss Energy: 36.36951249053289, Test Loss Force: 12.32101709018462, time: 12.761697769165039

Epoch 8, Batch 100/285, Loss: 0.7047648429870605
Epoch 8, Batch 200/285, Loss: 0.8201263546943665

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.582167904399924, Training Loss Force: 3.87955722438927, time: 4.431124925613403
Validation Loss Energy: 6.272896055395264, Validation Loss Force: 2.85003574163573, time: 0.27286791801452637
Test Loss Energy: 13.148127960734039, Test Loss Force: 12.42526888637414, time: 11.626667022705078

Epoch 9, Batch 100/285, Loss: 0.16737011075019836
Epoch 9, Batch 200/285, Loss: 0.24086952209472656

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 6.2478587026176315, Training Loss Force: 2.9396830890484007, time: 4.581106424331665
Validation Loss Energy: 12.85142973997949, Validation Loss Force: 3.1026628389904714, time: 0.25263237953186035
Test Loss Energy: 18.87157549996138, Test Loss Force: 12.23011587380653, time: 11.686455249786377

Epoch 10, Batch 100/285, Loss: 0.6533421277999878
Epoch 10, Batch 200/285, Loss: 0.3643844723701477

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.87547707166027, Training Loss Force: 4.439196738366565, time: 4.3726582527160645
Validation Loss Energy: 2.114881354085941, Validation Loss Force: 3.186888779452532, time: 0.27635717391967773
Test Loss Energy: 9.93194901299106, Test Loss Force: 11.705270772455366, time: 11.683016538619995

Epoch 11, Batch 100/285, Loss: 0.8360147476196289
Epoch 11, Batch 200/285, Loss: 0.6287592053413391

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.062729240697298, Training Loss Force: 3.8839721457053153, time: 4.35079026222229
Validation Loss Energy: 4.214466050387911, Validation Loss Force: 3.818587029225815, time: 0.2496776580810547
Test Loss Energy: 11.135057347908134, Test Loss Force: 12.53941560976973, time: 11.509876251220703

Epoch 12, Batch 100/285, Loss: 0.7618865966796875
Epoch 12, Batch 200/285, Loss: 0.6618514060974121

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.345383505792356, Training Loss Force: 4.21918692223564, time: 4.395656585693359
Validation Loss Energy: 2.641331157593315, Validation Loss Force: 5.478872131842111, time: 0.2743401527404785
Test Loss Energy: 9.80736105121761, Test Loss Force: 13.472166267725601, time: 10.681609869003296

Epoch 13, Batch 100/285, Loss: 0.1036633774638176
Epoch 13, Batch 200/285, Loss: 0.6736886501312256

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.45910513887831, Training Loss Force: 4.0657267197137985, time: 4.202428579330444
Validation Loss Energy: 2.514074627349855, Validation Loss Force: 4.024373578644675, time: 0.29082274436950684
Test Loss Energy: 9.625275271390334, Test Loss Force: 12.228170715422586, time: 12.379300832748413

Epoch 14, Batch 100/285, Loss: 0.8053522109985352
Epoch 14, Batch 200/285, Loss: 0.7768198251724243

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.257606816061525, Training Loss Force: 4.274522560603871, time: 4.27439546585083
Validation Loss Energy: 15.046614940191125, Validation Loss Force: 3.663066881620595, time: 0.2593832015991211
Test Loss Energy: 12.957345540131756, Test Loss Force: 11.784380993046565, time: 9.420440912246704

Epoch 15, Batch 100/285, Loss: 0.7687587738037109
Epoch 15, Batch 200/285, Loss: 0.8882461786270142

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.794930821492509, Training Loss Force: 4.324382472469314, time: 4.177652597427368
Validation Loss Energy: 2.8236769252158944, Validation Loss Force: 2.7935818541288784, time: 0.21797633171081543
Test Loss Energy: 9.405217150862537, Test Loss Force: 10.797470959401739, time: 9.264580726623535

Epoch 16, Batch 100/285, Loss: 3.1097729206085205
Epoch 16, Batch 200/285, Loss: 0.5501081943511963

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.09251229191994, Training Loss Force: 4.1708755352095706, time: 4.149533033370972
Validation Loss Energy: 9.183456477329866, Validation Loss Force: 3.5928875788910957, time: 0.22408223152160645
Test Loss Energy: 15.014013284160526, Test Loss Force: 11.685315499349748, time: 9.344389915466309

Epoch 17, Batch 100/285, Loss: 0.0694517120718956
Epoch 17, Batch 200/285, Loss: 0.3143867254257202

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.725768397968272, Training Loss Force: 3.854694668063136, time: 4.3619420528411865
Validation Loss Energy: 4.7442526846485, Validation Loss Force: 3.233711736525886, time: 0.22647476196289062
Test Loss Energy: 9.506195020449146, Test Loss Force: 11.075420120037, time: 9.251920938491821

Epoch 18, Batch 100/285, Loss: 0.09220292419195175
Epoch 18, Batch 200/285, Loss: 0.1952020823955536

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.05642171012517, Training Loss Force: 4.444450185589753, time: 4.177020311355591
Validation Loss Energy: 13.83310675024841, Validation Loss Force: 4.235513450602272, time: 0.21732378005981445
Test Loss Energy: 11.845709377925527, Test Loss Force: 11.119373624464375, time: 9.252821683883667

Epoch 19, Batch 100/285, Loss: 1.3683818578720093
Epoch 19, Batch 200/285, Loss: 0.4184662103652954

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.964807419463847, Training Loss Force: 4.197246196846675, time: 4.327820301055908
Validation Loss Energy: 5.891614332763609, Validation Loss Force: 4.54887992584639, time: 0.2228856086730957
Test Loss Energy: 9.93342935944224, Test Loss Force: 11.73760753194351, time: 9.317757368087769

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–â–â–â–â–‚â–ˆâ–‚â–ƒâ–â–â–â–â–‚â–â–‚â–â–‚â–
wandb:   test_error_force â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–…â–…â–…â–ƒâ–†â–ˆâ–…â–„â–â–ƒâ–‚â–‚â–ƒ
wandb:          test_loss â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–ƒâ–„â–‚â–ƒâ–„â–‚â–ƒâ–â–ƒâ–â–‚â–‚
wandb: train_error_energy â–â–‚â–‚â–‚â–â–â–‡â–‡â–†â–…â–ˆâ–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–†â–„â–ˆâ–†â–‡â–‡â–‡â–‡â–‡â–†â–ˆâ–‡
wandb:         train_loss â–â–‚â–‚â–â–â–â–ˆâ–‡â–†â–„â–ˆâ–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–ˆ
wandb: valid_error_energy â–â–‚â–â–â–â–â–‚â–ˆâ–‚â–ƒâ–â–‚â–â–â–„â–â–ƒâ–‚â–„â–‚
wandb:  valid_error_force â–â–â–â–‚â–â–â–‡â–†â–ƒâ–ƒâ–„â–…â–ˆâ–…â–„â–ƒâ–„â–„â–†â–†
wandb:         valid_loss â–â–‚â–â–â–â–â–„â–ˆâ–‚â–„â–‚â–ƒâ–„â–ƒâ–„â–‚â–ƒâ–ƒâ–…â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 9091
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.93343
wandb:   test_error_force 11.73761
wandb:          test_loss 4.59219
wandb: train_error_energy 9.96481
wandb:  train_error_force 4.19725
wandb:         train_loss 2.07126
wandb: valid_error_energy 5.89161
wandb:  valid_error_force 4.54888
wandb:         valid_loss 1.91634
wandb: 
wandb: ğŸš€ View run al_77_92 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/q2ig1x27
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_020315-q2ig1x27/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.34442201256752014, Uncertainty Bias: -0.26508599519729614
1.5258789e-05 0.00018882751
-1.9329474 21.228424
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 7 steps.
Found uncertainty sample 2 after 4 steps.
Found uncertainty sample 3 after 132 steps.
Found uncertainty sample 4 after 43 steps.
Found uncertainty sample 5 after 47 steps.
Found uncertainty sample 6 after 185 steps.
Found uncertainty sample 7 after 3 steps.
Found uncertainty sample 8 after 42 steps.
Found uncertainty sample 9 after 34 steps.
Found uncertainty sample 10 after 6 steps.
Found uncertainty sample 11 after 30 steps.
Found uncertainty sample 12 after 58 steps.
Found uncertainty sample 13 after 19 steps.
Found uncertainty sample 14 after 11 steps.
Found uncertainty sample 15 after 66 steps.
Found uncertainty sample 16 after 7 steps.
Found uncertainty sample 17 after 5 steps.
Found uncertainty sample 18 after 9 steps.
Found uncertainty sample 19 after 12 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 10 steps.
Found uncertainty sample 22 after 19 steps.
Found uncertainty sample 23 after 6 steps.
Found uncertainty sample 24 after 30 steps.
Found uncertainty sample 25 after 30 steps.
Found uncertainty sample 26 after 21 steps.
Found uncertainty sample 27 after 40 steps.
Found uncertainty sample 28 after 9 steps.
Found uncertainty sample 29 after 7 steps.
Found uncertainty sample 30 after 39 steps.
Found uncertainty sample 31 after 78 steps.
Found uncertainty sample 32 after 9 steps.
Found uncertainty sample 33 after 15 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 3 steps.
Found uncertainty sample 36 after 5 steps.
Found uncertainty sample 37 after 10 steps.
Found uncertainty sample 38 after 16 steps.
Found uncertainty sample 39 after 28 steps.
Found uncertainty sample 40 after 3 steps.
Found uncertainty sample 41 after 13 steps.
Found uncertainty sample 42 after 2 steps.
Found uncertainty sample 43 after 18 steps.
Found uncertainty sample 44 after 5 steps.
Found uncertainty sample 45 after 43 steps.
Found uncertainty sample 46 after 47 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 81 steps.
Found uncertainty sample 49 after 2 steps.
Found uncertainty sample 50 after 95 steps.
Found uncertainty sample 51 after 29 steps.
Found uncertainty sample 52 after 7 steps.
Found uncertainty sample 53 after 19 steps.
Found uncertainty sample 54 after 50 steps.
Found uncertainty sample 55 after 16 steps.
Found uncertainty sample 56 after 6 steps.
Found uncertainty sample 57 after 128 steps.
Found uncertainty sample 58 after 32 steps.
Found uncertainty sample 59 after 17 steps.
Found uncertainty sample 60 after 62 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 23 steps.
Found uncertainty sample 63 after 44 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 4 steps.
Found uncertainty sample 66 after 13 steps.
Found uncertainty sample 67 after 9 steps.
Found uncertainty sample 68 after 56 steps.
Found uncertainty sample 69 after 30 steps.
Found uncertainty sample 70 after 40 steps.
Found uncertainty sample 71 after 6 steps.
Found uncertainty sample 72 after 2 steps.
Found uncertainty sample 73 after 4 steps.
Found uncertainty sample 74 after 45 steps.
Found uncertainty sample 75 after 22 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 7 steps.
Found uncertainty sample 78 after 24 steps.
Found uncertainty sample 79 after 30 steps.
Found uncertainty sample 80 after 37 steps.
Found uncertainty sample 81 after 9 steps.
Found uncertainty sample 82 after 9 steps.
Found uncertainty sample 83 after 34 steps.
Found uncertainty sample 84 after 15 steps.
Found uncertainty sample 85 after 69 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 38 steps.
Found uncertainty sample 88 after 20 steps.
Found uncertainty sample 89 after 5 steps.
Found uncertainty sample 90 after 39 steps.
Found uncertainty sample 91 after 30 steps.
Found uncertainty sample 92 after 9 steps.
Found uncertainty sample 93 after 17 steps.
Found uncertainty sample 94 after 28 steps.
Found uncertainty sample 95 after 5 steps.
Found uncertainty sample 96 after 79 steps.
Found uncertainty sample 97 after 64 steps.
Found uncertainty sample 98 after 12 steps.
Found uncertainty sample 99 after 120 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_021418-6xrxh3jv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_93
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/6xrxh3jv
Training model 93. Added 100 samples to the dataset.
Epoch 0, Batch 100/287, Loss: 0.07419238984584808
Epoch 0, Batch 200/287, Loss: 0.13139501214027405

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.696854453183264, Training Loss Force: 1.8851460729633425, time: 4.157733201980591
Validation Loss Energy: 2.515065193312429, Validation Loss Force: 2.0020264852613106, time: 0.23281335830688477
Test Loss Energy: 9.786417863894126, Test Loss Force: 11.494247595917857, time: 9.94890308380127

Epoch 1, Batch 100/287, Loss: 0.14657911658287048
Epoch 1, Batch 200/287, Loss: 0.22180715203285217

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.845188558879272, Training Loss Force: 1.8452547662912406, time: 4.110319137573242
Validation Loss Energy: 0.944133990254338, Validation Loss Force: 1.9517159058891191, time: 0.23207354545593262
Test Loss Energy: 10.404001125134018, Test Loss Force: 11.646663427855785, time: 10.201685428619385

Epoch 2, Batch 100/287, Loss: 0.045417722314596176
Epoch 2, Batch 200/287, Loss: 0.10761407762765884

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.4567529843832305, Training Loss Force: 1.8353509597848527, time: 4.117143154144287
Validation Loss Energy: 0.8618490233369572, Validation Loss Force: 1.933022652953165, time: 0.2380986213684082
Test Loss Energy: 10.808742909912054, Test Loss Force: 11.576847613271196, time: 10.11450481414795

Epoch 3, Batch 100/287, Loss: 0.24108046293258667
Epoch 3, Batch 200/287, Loss: 0.04463278874754906

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.9278222327200207, Training Loss Force: 1.909146658492288, time: 4.168826580047607
Validation Loss Energy: 1.4484094427328957, Validation Loss Force: 1.919580116613504, time: 0.258195161819458
Test Loss Energy: 10.720734307968025, Test Loss Force: 11.617427453971196, time: 10.376115322113037

Epoch 4, Batch 100/287, Loss: 0.07982461154460907
Epoch 4, Batch 200/287, Loss: 0.05572181195020676

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6460748978898265, Training Loss Force: 1.8278467267695124, time: 4.125524997711182
Validation Loss Energy: 2.8495999335833195, Validation Loss Force: 2.217496782235082, time: 0.2512943744659424
Test Loss Energy: 11.594602747338836, Test Loss Force: 11.56132080745593, time: 10.112680673599243

Epoch 5, Batch 100/287, Loss: 0.22086986899375916
Epoch 5, Batch 200/287, Loss: 0.2728074789047241

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7929605624184124, Training Loss Force: 1.8600953638372626, time: 4.100766658782959
Validation Loss Energy: 0.6619392470850822, Validation Loss Force: 1.888948479685546, time: 0.23621726036071777
Test Loss Energy: 10.514702431710122, Test Loss Force: 11.55971866214773, time: 10.14799451828003

Epoch 6, Batch 100/287, Loss: 0.3429058790206909
Epoch 6, Batch 200/287, Loss: 0.17968185245990753

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.880513833497833, Training Loss Force: 5.102387623238162, time: 4.269512891769409
Validation Loss Energy: 14.220124430079167, Validation Loss Force: 5.374674372208178, time: 0.23449325561523438
Test Loss Energy: 20.296170801165335, Test Loss Force: 12.507851999525302, time: 10.163158416748047

Epoch 7, Batch 100/287, Loss: 1.2779921293258667
Epoch 7, Batch 200/287, Loss: 0.1932535171508789

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.77520850914552, Training Loss Force: 4.573772063894468, time: 4.102110147476196
Validation Loss Energy: 5.976557000672584, Validation Loss Force: 4.025480867773544, time: 0.2346057891845703
Test Loss Energy: 9.921109861352006, Test Loss Force: 11.894815917375716, time: 9.986368656158447

Epoch 8, Batch 100/287, Loss: 0.8887091875076294
Epoch 8, Batch 200/287, Loss: 0.08514870703220367

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.011814208683502, Training Loss Force: 4.074643886711331, time: 4.4246721267700195
Validation Loss Energy: 7.5567564729262005, Validation Loss Force: 3.1584590509079105, time: 0.23694944381713867
Test Loss Energy: 12.319643676218996, Test Loss Force: 11.950576421789123, time: 10.016845226287842

Epoch 9, Batch 100/287, Loss: 0.7671142816543579
Epoch 9, Batch 200/287, Loss: 0.6310071349143982

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 10.240085735203541, Training Loss Force: 4.422291361920627, time: 4.167771577835083
Validation Loss Energy: 5.263051398166478, Validation Loss Force: 5.837616174199537, time: 0.2520623207092285
Test Loss Energy: 12.806959298987325, Test Loss Force: 12.320599022451203, time: 10.196420907974243

Epoch 10, Batch 100/287, Loss: 0.28434860706329346
Epoch 10, Batch 200/287, Loss: 0.42553770542144775

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 5.254307929191463, Training Loss Force: 3.032623574360963, time: 4.3322343826293945
Validation Loss Energy: 3.569373538945804, Validation Loss Force: 2.7129236523243594, time: 0.2357163429260254
Test Loss Energy: 11.532070881192606, Test Loss Force: 11.753371430269487, time: 10.113082885742188

Epoch 11, Batch 100/287, Loss: 0.338649719953537
Epoch 11, Batch 200/287, Loss: 0.754890501499176

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.177152469117395, Training Loss Force: 3.8979747392276414, time: 4.169958591461182
Validation Loss Energy: 1.9347174100897355, Validation Loss Force: 3.2304690502875686, time: 0.23628520965576172
Test Loss Energy: 9.89263567708727, Test Loss Force: 11.76255616181237, time: 10.019646644592285

Epoch 12, Batch 100/287, Loss: 0.33798056840896606
Epoch 12, Batch 200/287, Loss: 0.5071145296096802

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.259004522758849, Training Loss Force: 4.56487368810664, time: 4.112913608551025
Validation Loss Energy: 1.9561787522463767, Validation Loss Force: 5.273586498933931, time: 0.24965739250183105
Test Loss Energy: 11.083910263741432, Test Loss Force: 12.889529555618376, time: 10.223603010177612

Epoch 13, Batch 100/287, Loss: 0.8506380915641785
Epoch 13, Batch 200/287, Loss: 0.5631507635116577

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.880263455523641, Training Loss Force: 4.396545852517215, time: 4.136146306991577
Validation Loss Energy: 7.206916435308476, Validation Loss Force: 3.679726675382107, time: 0.23913240432739258
Test Loss Energy: 14.389091579997684, Test Loss Force: 11.956012393518952, time: 10.107738494873047

Epoch 14, Batch 100/287, Loss: 0.6261987090110779
Epoch 14, Batch 200/287, Loss: 0.1402871012687683

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.189560413200072, Training Loss Force: 3.3807211888261395, time: 4.07983136177063
Validation Loss Energy: 8.809520520754713, Validation Loss Force: 2.314964055196544, time: 0.23667526245117188
Test Loss Energy: 15.272690148090437, Test Loss Force: 12.072341102005192, time: 10.231645107269287

Epoch 15, Batch 100/287, Loss: 0.21764039993286133
Epoch 15, Batch 200/287, Loss: 1.048715591430664

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.063631384255416, Training Loss Force: 3.821309822887116, time: 4.101344585418701
Validation Loss Energy: 2.3757279712258024, Validation Loss Force: 5.35593266646607, time: 0.24489092826843262
Test Loss Energy: 9.803935098712577, Test Loss Force: 12.565799856404995, time: 10.069782972335815

Epoch 16, Batch 100/287, Loss: 0.5068499445915222
Epoch 16, Batch 200/287, Loss: 0.4286484718322754

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.712223555137789, Training Loss Force: 3.81552238721937, time: 4.1782753467559814
Validation Loss Energy: 15.973433042045475, Validation Loss Force: 4.852592446667323, time: 0.23801183700561523
Test Loss Energy: 13.227123988193393, Test Loss Force: 12.59219890412653, time: 10.22141146659851

Epoch 17, Batch 100/287, Loss: 0.0868477076292038
Epoch 17, Batch 200/287, Loss: 1.1955251693725586

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.701675595137523, Training Loss Force: 4.630117948176017, time: 4.129279375076294
Validation Loss Energy: 9.046541851017105, Validation Loss Force: 2.9213661993158455, time: 0.24566984176635742
Test Loss Energy: 10.440010251431051, Test Loss Force: 11.251097262672753, time: 10.084694623947144

Epoch 18, Batch 100/287, Loss: 1.696197509765625
Epoch 18, Batch 200/287, Loss: 0.1841181218624115

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.59809142138749, Training Loss Force: 4.539758569020659, time: 4.148863077163696
Validation Loss Energy: 1.7991498653090936, Validation Loss Force: 5.785268172142107, time: 0.2417151927947998
Test Loss Energy: 9.609798128955259, Test Loss Force: 12.370195228840393, time: 10.1981942653656

Epoch 19, Batch 100/287, Loss: 0.6206220388412476
Epoch 19, Batch 200/287, Loss: 0.4768800139427185

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.626253286072695, Training Loss Force: 4.098265267460612, time: 4.080970287322998
Validation Loss Energy: 8.732250942546752, Validation Loss Force: 3.3068918622673946, time: 0.27346324920654297
Test Loss Energy: 14.076650777465847, Test Loss Force: 11.414975332810391, time: 10.07108211517334

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–‚â–‚â–‚â–‚â–ˆâ–â–ƒâ–ƒâ–‚â–â–‚â–„â–…â–â–ƒâ–‚â–â–„
wandb:   test_error_force â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–†â–„â–„â–†â–ƒâ–ƒâ–ˆâ–„â–…â–‡â–‡â–â–†â–‚
wandb:          test_loss â–â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–ƒâ–„â–ƒâ–‚â–…â–„â–…â–„â–…â–â–ƒâ–ƒ
wandb: train_error_energy â–â–â–â–â–â–â–ˆâ–‡â–†â–ˆâ–„â–†â–†â–ˆâ–†â–†â–†â–‡â–‡â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–‡â–†â–‡â–„â–…â–‡â–†â–„â–…â–…â–‡â–‡â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–†â–‡â–„â–†â–‡â–‡â–…â–†â–†â–‡â–‡â–†
wandb: valid_error_energy â–‚â–â–â–â–‚â–â–‡â–ƒâ–„â–ƒâ–‚â–‚â–‚â–„â–…â–‚â–ˆâ–…â–‚â–…
wandb:  valid_error_force â–â–â–â–â–‚â–â–‡â–…â–ƒâ–ˆâ–‚â–ƒâ–‡â–„â–‚â–‡â–†â–ƒâ–ˆâ–„
wandb:         valid_loss â–‚â–â–â–â–‚â–â–ˆâ–…â–„â–†â–ƒâ–ƒâ–…â–…â–ƒâ–…â–ˆâ–„â–†â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 9181
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 14.07665
wandb:   test_error_force 11.41498
wandb:          test_loss 4.76151
wandb: train_error_energy 7.62625
wandb:  train_error_force 4.09827
wandb:         train_loss 1.88165
wandb: valid_error_energy 8.73225
wandb:  valid_error_force 3.30689
wandb:         valid_loss 1.69086
wandb: 
wandb: ğŸš€ View run al_77_93 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/6xrxh3jv
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_021418-6xrxh3jv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.13944868743419647, Uncertainty Bias: -0.047610193490982056
0.0 0.0004081726
-0.49249426 11.965009
(48745, 22, 3)
Found uncertainty sample 0 after 139 steps.
Found uncertainty sample 1 after 49 steps.
Found uncertainty sample 2 after 530 steps.
Found uncertainty sample 3 after 1382 steps.
Found uncertainty sample 4 after 59 steps.
Found uncertainty sample 5 after 142 steps.
Found uncertainty sample 6 after 487 steps.
Found uncertainty sample 7 after 809 steps.
Found uncertainty sample 8 after 35 steps.
Found uncertainty sample 9 after 169 steps.
Found uncertainty sample 10 after 5 steps.
Found uncertainty sample 11 after 1901 steps.
Found uncertainty sample 12 after 2898 steps.
Found uncertainty sample 13 after 84 steps.
Found uncertainty sample 14 after 660 steps.
Found uncertainty sample 15 after 120 steps.
Found uncertainty sample 16 after 651 steps.
Found uncertainty sample 17 after 1346 steps.
Found uncertainty sample 18 after 16 steps.
Found uncertainty sample 19 after 84 steps.
Found uncertainty sample 20 after 151 steps.
Found uncertainty sample 21 after 667 steps.
Found uncertainty sample 22 after 366 steps.
Found uncertainty sample 23 after 1137 steps.
Found uncertainty sample 24 after 1044 steps.
Found uncertainty sample 25 after 528 steps.
Found uncertainty sample 26 after 809 steps.
Found uncertainty sample 27 after 1016 steps.
Found uncertainty sample 28 after 522 steps.
Found uncertainty sample 29 after 500 steps.
Found uncertainty sample 30 after 1444 steps.
Found uncertainty sample 31 after 378 steps.
Found uncertainty sample 32 after 21 steps.
Found uncertainty sample 33 after 690 steps.
Found uncertainty sample 34 after 271 steps.
Found uncertainty sample 35 after 1093 steps.
Found uncertainty sample 36 after 72 steps.
Found uncertainty sample 37 after 397 steps.
Found uncertainty sample 38 after 136 steps.
Found uncertainty sample 39 after 738 steps.
Found uncertainty sample 40 after 652 steps.
Found uncertainty sample 41 after 52 steps.
Found uncertainty sample 42 after 1657 steps.
Found uncertainty sample 43 after 361 steps.
Found uncertainty sample 44 after 766 steps.
Found uncertainty sample 45 after 615 steps.
Found uncertainty sample 46 after 104 steps.
Found uncertainty sample 47 after 1238 steps.
Found uncertainty sample 48 after 326 steps.
Found uncertainty sample 49 after 986 steps.
Found uncertainty sample 50 after 615 steps.
Found uncertainty sample 51 after 516 steps.
Found uncertainty sample 52 after 1562 steps.
Found uncertainty sample 53 after 127 steps.
Found uncertainty sample 54 after 419 steps.
Found uncertainty sample 55 after 2316 steps.
Found uncertainty sample 56 after 20 steps.
Found uncertainty sample 57 after 96 steps.
Found uncertainty sample 58 after 383 steps.
Found uncertainty sample 59 after 30 steps.
Found uncertainty sample 60 after 149 steps.
Found uncertainty sample 61 after 247 steps.
Found uncertainty sample 62 after 54 steps.
Found uncertainty sample 63 after 2913 steps.
Found uncertainty sample 64 after 3 steps.
Found uncertainty sample 65 after 52 steps.
Found uncertainty sample 66 after 340 steps.
Found uncertainty sample 67 after 484 steps.
Found uncertainty sample 68 after 2035 steps.
Found uncertainty sample 69 after 1520 steps.
Found uncertainty sample 70 after 14 steps.
Found uncertainty sample 71 after 527 steps.
Found uncertainty sample 72 after 121 steps.
Found uncertainty sample 73 after 824 steps.
Found uncertainty sample 74 after 363 steps.
Found uncertainty sample 75 after 11 steps.
Did not find any uncertainty samples for sample 76.
Found uncertainty sample 77 after 687 steps.
Found uncertainty sample 78 after 436 steps.
Found uncertainty sample 79 after 1399 steps.
Found uncertainty sample 80 after 1093 steps.
Found uncertainty sample 81 after 940 steps.
Found uncertainty sample 82 after 823 steps.
Found uncertainty sample 83 after 1414 steps.
Found uncertainty sample 84 after 20 steps.
Found uncertainty sample 85 after 978 steps.
Found uncertainty sample 86 after 801 steps.
Found uncertainty sample 87 after 165 steps.
Found uncertainty sample 88 after 301 steps.
Found uncertainty sample 89 after 301 steps.
Found uncertainty sample 90 after 1133 steps.
Found uncertainty sample 91 after 2584 steps.
Found uncertainty sample 92 after 135 steps.
Found uncertainty sample 93 after 301 steps.
Found uncertainty sample 94 after 1144 steps.
Found uncertainty sample 95 after 979 steps.
Found uncertainty sample 96 after 254 steps.
Found uncertainty sample 97 after 75 steps.
Found uncertainty sample 98 after 268 steps.
Found uncertainty sample 99 after 94 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_024618-nlm343ek
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_94
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/nlm343ek
Training model 94. Added 99 samples to the dataset.
Epoch 0, Batch 100/290, Loss: 0.06325506418943405
Epoch 0, Batch 200/290, Loss: 0.14919254183769226

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.969683592276871, Training Loss Force: 1.9160655087337417, time: 4.110103607177734
Validation Loss Energy: 3.75183211075771, Validation Loss Force: 2.1255079273385404, time: 0.2523534297943115
Test Loss Energy: 9.837340609479208, Test Loss Force: 11.686834098261745, time: 9.999070644378662

Epoch 1, Batch 100/290, Loss: 0.1878838837146759
Epoch 1, Batch 200/290, Loss: 0.12982770800590515

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.4784481500435662, Training Loss Force: 1.9033098898171266, time: 4.237261533737183
Validation Loss Energy: 0.6940081465492606, Validation Loss Force: 1.9286743253376577, time: 0.24178767204284668
Test Loss Energy: 10.507086895105068, Test Loss Force: 11.829661260504782, time: 10.363660097122192

Epoch 2, Batch 100/290, Loss: 0.06003136932849884
Epoch 2, Batch 200/290, Loss: 0.14933279156684875

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.1890630845169556, Training Loss Force: 1.883060267083146, time: 4.163594484329224
Validation Loss Energy: 3.622321322614706, Validation Loss Force: 2.1164934787024583, time: 0.2361457347869873
Test Loss Energy: 10.138333023599044, Test Loss Force: 11.585319355877203, time: 10.06415319442749

Epoch 3, Batch 100/290, Loss: 0.1089181900024414
Epoch 3, Batch 200/290, Loss: 0.17893680930137634

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.4256736671795323, Training Loss Force: 1.9056587663632838, time: 4.130284547805786
Validation Loss Energy: 1.1309305132995504, Validation Loss Force: 1.986015950195494, time: 0.23777198791503906
Test Loss Energy: 10.555358856606635, Test Loss Force: 11.638937274814909, time: 10.265388011932373

Epoch 4, Batch 100/290, Loss: 0.08274666219949722
Epoch 4, Batch 200/290, Loss: 0.10814248025417328

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4022771357661017, Training Loss Force: 1.8076147204235475, time: 4.1372363567352295
Validation Loss Energy: 2.176815173807905, Validation Loss Force: 1.8970438168879817, time: 0.24290204048156738
Test Loss Energy: 10.241660852418825, Test Loss Force: 11.548468871297764, time: 10.07470417022705

Epoch 5, Batch 100/290, Loss: 0.07867012172937393
Epoch 5, Batch 200/290, Loss: 0.031026095151901245

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8172356219154953, Training Loss Force: 1.8085761597261856, time: 4.137312173843384
Validation Loss Energy: 2.5235750736638396, Validation Loss Force: 1.9333599967092454, time: 0.24338197708129883
Test Loss Energy: 11.704417070607953, Test Loss Force: 11.645170047681821, time: 10.063995599746704

Epoch 6, Batch 100/290, Loss: 0.37791505455970764
Epoch 6, Batch 200/290, Loss: 0.30416804552078247

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 5.445899361879501, Training Loss Force: 2.7538994804044727, time: 4.386526346206665
Validation Loss Energy: 3.964071834694704, Validation Loss Force: 2.303743133459634, time: 0.2412428855895996
Test Loss Energy: 10.445037314007624, Test Loss Force: 11.717470253041752, time: 10.05626130104065

Epoch 7, Batch 100/290, Loss: 0.692782461643219
Epoch 7, Batch 200/290, Loss: 1.4186885356903076

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.230817742968013, Training Loss Force: 3.2919389634143803, time: 4.176316261291504
Validation Loss Energy: 11.823026232911236, Validation Loss Force: 4.93211552569973, time: 0.24370956420898438
Test Loss Energy: 20.97856652692857, Test Loss Force: 14.007587537962213, time: 10.070969343185425

Epoch 8, Batch 100/290, Loss: 1.5134332180023193
Epoch 8, Batch 200/290, Loss: 0.796859622001648

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 10.092268776541609, Training Loss Force: 5.003018918546626, time: 4.3137147426605225
Validation Loss Energy: 10.861215229436043, Validation Loss Force: 4.09451311266471, time: 0.2464613914489746
Test Loss Energy: 10.81458844831032, Test Loss Force: 11.285287328939242, time: 10.148340225219727

Epoch 9, Batch 100/290, Loss: 1.0648741722106934
Epoch 9, Batch 200/290, Loss: 0.8836005926132202

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.819555390486935, Training Loss Force: 4.0699296542375, time: 4.111645460128784
Validation Loss Energy: 1.430400184879402, Validation Loss Force: 3.6287501816621037, time: 0.2483537197113037
Test Loss Energy: 9.025630129292136, Test Loss Force: 12.66131571013235, time: 11.465976238250732

Epoch 10, Batch 100/290, Loss: 0.44822704792022705
Epoch 10, Batch 200/290, Loss: 0.21790215373039246

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.124269664045422, Training Loss Force: 3.8826028622812263, time: 4.387395620346069
Validation Loss Energy: 4.634890069640943, Validation Loss Force: 4.919055713088092, time: 0.2508883476257324
Test Loss Energy: 11.792397245512952, Test Loss Force: 12.906998843124606, time: 10.193278551101685

Epoch 11, Batch 100/290, Loss: 0.5720094442367554
Epoch 11, Batch 200/290, Loss: 0.41970527172088623

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.859967564567677, Training Loss Force: 4.529382398097942, time: 4.206526041030884
Validation Loss Energy: 4.147200106229901, Validation Loss Force: 3.2235163398074613, time: 0.24489951133728027
Test Loss Energy: 11.760039557456704, Test Loss Force: 11.217134976450284, time: 10.134119033813477

Epoch 12, Batch 100/290, Loss: 0.29149171710014343
Epoch 12, Batch 200/290, Loss: 0.27170324325561523

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.175627295259993, Training Loss Force: 4.731264384152636, time: 4.392937660217285
Validation Loss Energy: 5.2456103474395634, Validation Loss Force: 5.280416919519095, time: 0.2823147773742676
Test Loss Energy: 12.191952042303742, Test Loss Force: 11.852868230604544, time: 10.245558261871338

Epoch 13, Batch 100/290, Loss: 0.1487102508544922
Epoch 13, Batch 200/290, Loss: 0.7079801559448242

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.158996754227488, Training Loss Force: 4.136000683582943, time: 4.1493823528289795
Validation Loss Energy: 11.333840823516143, Validation Loss Force: 3.238157058821359, time: 0.24044036865234375
Test Loss Energy: 18.04775711099983, Test Loss Force: 11.124414452890555, time: 10.200226306915283

Epoch 14, Batch 100/290, Loss: 0.1364617943763733
Epoch 14, Batch 200/290, Loss: 0.10939054936170578

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.33730725860766, Training Loss Force: 4.615794122625876, time: 4.140968561172485
Validation Loss Energy: 3.2477373097572437, Validation Loss Force: 3.148533039322229, time: 0.24190521240234375
Test Loss Energy: 9.321150865343911, Test Loss Force: 11.678388632946579, time: 10.364378690719604

Epoch 15, Batch 100/290, Loss: 0.2032271921634674
Epoch 15, Batch 200/290, Loss: 0.5284431576728821

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.773370196457493, Training Loss Force: 3.2430540547298023, time: 4.1940765380859375
Validation Loss Energy: 4.155835126641323, Validation Loss Force: 2.4621221747632505, time: 0.25095343589782715
Test Loss Energy: 9.400189719890937, Test Loss Force: 11.13869794880661, time: 10.240696668624878

Epoch 16, Batch 100/290, Loss: 0.09050404280424118
Epoch 16, Batch 200/290, Loss: 0.4967764914035797

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.958071152025737, Training Loss Force: 2.48949754263499, time: 4.040138006210327
Validation Loss Energy: 5.9536668796741825, Validation Loss Force: 2.4150814027313503, time: 0.24631810188293457
Test Loss Energy: 12.683013067360825, Test Loss Force: 11.537216193909543, time: 10.363832712173462

Epoch 17, Batch 100/290, Loss: 1.6785271167755127
Epoch 17, Batch 200/290, Loss: 0.08828212320804596

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.471758776415017, Training Loss Force: 4.035226127296106, time: 4.043040990829468
Validation Loss Energy: 17.015930080968424, Validation Loss Force: 5.189503155792922, time: 0.23469090461730957
Test Loss Energy: 13.461815962297388, Test Loss Force: 11.496427324006065, time: 10.117050647735596

Epoch 18, Batch 100/290, Loss: 0.12444884330034256
Epoch 18, Batch 200/290, Loss: 0.0685262680053711

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.592915007023226, Training Loss Force: 4.538771780113236, time: 4.14281964302063
Validation Loss Energy: 3.6222000569978343, Validation Loss Force: 6.170881700338639, time: 0.25077390670776367
Test Loss Energy: 9.595001211615816, Test Loss Force: 13.57839243276043, time: 10.254074573516846

Epoch 19, Batch 100/290, Loss: 0.0903446301817894
Epoch 19, Batch 200/290, Loss: 0.8861556053161621

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.385604454613349, Training Loss Force: 4.8360274293403895, time: 4.084201097488403
Validation Loss Energy: 3.018729574853817, Validation Loss Force: 4.8815944794508095, time: 0.24500107765197754
Test Loss Energy: 9.623100537619512, Test Loss Force: 13.752305956485552, time: 10.175711154937744

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–‚â–‚â–‚â–ƒâ–‚â–ˆâ–‚â–â–ƒâ–ƒâ–ƒâ–†â–â–â–ƒâ–„â–â–
wandb:   test_error_force â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–ˆâ–â–…â–…â–â–ƒâ–â–‚â–â–‚â–‚â–‡â–‡
wandb:          test_loss â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ˆâ–‚â–ƒâ–„â–‚â–ƒâ–ƒâ–‚â–â–‚â–ƒâ–„â–…
wandb: train_error_energy â–â–‚â–‚â–‚â–â–â–„â–…â–‡â–†â–…â–‡â–ˆâ–‡â–‡â–„â–„â–‡â–…â–†
wandb:  train_error_force â–â–â–â–â–â–â–ƒâ–„â–ˆâ–†â–†â–‡â–‡â–†â–‡â–„â–‚â–†â–‡â–ˆ
wandb:         train_loss â–â–â–â–â–â–â–ƒâ–…â–ˆâ–†â–†â–‡â–ˆâ–‡â–‡â–„â–ƒâ–†â–‡â–‡
wandb: valid_error_energy â–‚â–â–‚â–â–‚â–‚â–‚â–†â–…â–â–ƒâ–‚â–ƒâ–†â–‚â–‚â–ƒâ–ˆâ–‚â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–†â–…â–„â–†â–ƒâ–‡â–ƒâ–ƒâ–‚â–‚â–†â–ˆâ–†
wandb:         valid_loss â–‚â–â–‚â–â–â–â–‚â–‡â–†â–ƒâ–…â–ƒâ–†â–…â–ƒâ–‚â–ƒâ–ˆâ–†â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 9270
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.6231
wandb:   test_error_force 13.75231
wandb:          test_loss 5.24555
wandb: train_error_energy 8.3856
wandb:  train_error_force 4.83603
wandb:         train_loss 2.17932
wandb: valid_error_energy 3.01873
wandb:  valid_error_force 4.88159
wandb:         valid_loss 1.83541
wandb: 
wandb: ğŸš€ View run al_77_94 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/nlm343ek
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_024618-nlm343ek/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4269338548183441, Uncertainty Bias: -0.32065272331237793
1.5258789e-05 0.0012331009
-4.1085687 16.786203
(48745, 22, 3)
Found uncertainty sample 0 after 22 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 7 steps.
Found uncertainty sample 3 after 5 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 29 steps.
Found uncertainty sample 6 after 12 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 12 steps.
Found uncertainty sample 9 after 23 steps.
Found uncertainty sample 10 after 4 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 9 steps.
Found uncertainty sample 14 after 30 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 9 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 16 steps.
Found uncertainty sample 19 after 7 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 5 steps.
Found uncertainty sample 24 after 57 steps.
Found uncertainty sample 25 after 6 steps.
Found uncertainty sample 26 after 36 steps.
Found uncertainty sample 27 after 21 steps.
Found uncertainty sample 28 after 18 steps.
Found uncertainty sample 29 after 13 steps.
Found uncertainty sample 30 after 2 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 12 steps.
Found uncertainty sample 33 after 9 steps.
Found uncertainty sample 34 after 11 steps.
Found uncertainty sample 35 after 28 steps.
Found uncertainty sample 36 after 6 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 3 steps.
Found uncertainty sample 39 after 22 steps.
Found uncertainty sample 40 after 20 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 11 steps.
Found uncertainty sample 43 after 11 steps.
Found uncertainty sample 44 after 5 steps.
Found uncertainty sample 45 after 5 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 4 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 16 steps.
Found uncertainty sample 52 after 8 steps.
Found uncertainty sample 53 after 9 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 22 steps.
Found uncertainty sample 56 after 13 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 11 steps.
Found uncertainty sample 59 after 9 steps.
Found uncertainty sample 60 after 9 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 5 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 18 steps.
Found uncertainty sample 68 after 4 steps.
Found uncertainty sample 69 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 3 steps.
Found uncertainty sample 72 after 14 steps.
Found uncertainty sample 73 after 41 steps.
Found uncertainty sample 74 after 5 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 9 steps.
Found uncertainty sample 78 after 24 steps.
Found uncertainty sample 79 after 26 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 7 steps.
Found uncertainty sample 82 after 9 steps.
Found uncertainty sample 83 after 4 steps.
Found uncertainty sample 84 after 31 steps.
Found uncertainty sample 85 after 10 steps.
Found uncertainty sample 86 after 13 steps.
Found uncertainty sample 87 after 11 steps.
Found uncertainty sample 88 after 14 steps.
Found uncertainty sample 89 after 18 steps.
Found uncertainty sample 90 after 5 steps.
Found uncertainty sample 91 after 14 steps.
Found uncertainty sample 92 after 12 steps.
Found uncertainty sample 93 after 27 steps.
Found uncertainty sample 94 after 6 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 31 steps.
Found uncertainty sample 98 after 51 steps.
Found uncertainty sample 99 after 11 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_025626-exjdlp49
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_95
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/exjdlp49
Training model 95. Added 100 samples to the dataset.
Epoch 0, Batch 100/293, Loss: 0.05331847816705704
Epoch 0, Batch 200/293, Loss: 0.05943780019879341

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4933735068395269, Training Loss Force: 1.8832046292832025, time: 4.2969746589660645
Validation Loss Energy: 0.7075629304053832, Validation Loss Force: 1.9420221382226406, time: 0.26157474517822266
Test Loss Energy: 10.551827333278677, Test Loss Force: 11.551503578957911, time: 10.180969715118408

Epoch 1, Batch 100/293, Loss: 0.04867926985025406
Epoch 1, Batch 200/293, Loss: 0.07310076057910919

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.04887229033547, Training Loss Force: 1.8159506375185708, time: 4.25140643119812
Validation Loss Energy: 1.532742553854892, Validation Loss Force: 1.9251671749547221, time: 0.24347734451293945
Test Loss Energy: 10.798795679938634, Test Loss Force: 11.468356499653048, time: 10.262476682662964

Epoch 2, Batch 100/293, Loss: 0.05546491965651512
Epoch 2, Batch 200/293, Loss: 0.2514849901199341

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.8049102718844015, Training Loss Force: 1.854317725643713, time: 4.259805202484131
Validation Loss Energy: 2.1534000434972347, Validation Loss Force: 1.9759918528510316, time: 0.24080252647399902
Test Loss Energy: 10.000160355316584, Test Loss Force: 11.565297137467732, time: 10.117433786392212

Epoch 3, Batch 100/293, Loss: 0.11631084233522415
Epoch 3, Batch 200/293, Loss: 0.05672682821750641

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.439001883290114, Training Loss Force: 1.7950329125191535, time: 4.212170600891113
Validation Loss Energy: 1.1822241769510131, Validation Loss Force: 1.9562895078795612, time: 0.25226688385009766
Test Loss Energy: 10.168923343509201, Test Loss Force: 11.53615966028891, time: 10.369398593902588

Epoch 4, Batch 100/293, Loss: 0.044977787882089615
Epoch 4, Batch 200/293, Loss: 0.05160905048251152

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.6887025032508465, Training Loss Force: 1.8252382587907736, time: 4.206434488296509
Validation Loss Energy: 1.5566758130012215, Validation Loss Force: 1.8668225814628834, time: 0.25479841232299805
Test Loss Energy: 10.134364252680134, Test Loss Force: 11.583826264823387, time: 10.17526912689209

Epoch 5, Batch 100/293, Loss: 0.05159192532300949
Epoch 5, Batch 200/293, Loss: 0.13411152362823486

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7268687607155375, Training Loss Force: 1.8159757915324313, time: 4.257563591003418
Validation Loss Energy: 1.3020359647026092, Validation Loss Force: 1.9148326322870068, time: 0.24765706062316895
Test Loss Energy: 10.912205007061022, Test Loss Force: 11.683937161015157, time: 10.208637237548828

Epoch 6, Batch 100/293, Loss: 0.09804783761501312
Epoch 6, Batch 200/293, Loss: 0.17156338691711426

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.830322532882589, Training Loss Force: 4.622747085610228, time: 4.207305192947388
Validation Loss Energy: 10.22998008695246, Validation Loss Force: 4.80928703862602, time: 0.2372133731842041
Test Loss Energy: 11.281473269941893, Test Loss Force: 12.156359650056865, time: 10.121227025985718

Epoch 7, Batch 100/293, Loss: 1.4485249519348145
Epoch 7, Batch 200/293, Loss: 0.9352248311042786

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.248118886817931, Training Loss Force: 4.6089946971743965, time: 4.261696815490723
Validation Loss Energy: 1.6077036542262753, Validation Loss Force: 4.787573234191726, time: 0.2521200180053711
Test Loss Energy: 9.840500436214839, Test Loss Force: 11.67866905151037, time: 10.22963833808899

Epoch 8, Batch 100/293, Loss: 1.264437198638916
Epoch 8, Batch 200/293, Loss: 0.12243197113275528

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.594726195934483, Training Loss Force: 4.6940587696214715, time: 4.375311851501465
Validation Loss Energy: 7.285410337548406, Validation Loss Force: 4.532826192966264, time: 0.2621493339538574
Test Loss Energy: 13.581782142655994, Test Loss Force: 12.517998190792541, time: 10.261579751968384

Epoch 9, Batch 100/293, Loss: 0.15433326363563538
Epoch 9, Batch 200/293, Loss: 0.4905513525009155

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.359936365322453, Training Loss Force: 4.146790443335038, time: 4.190751314163208
Validation Loss Energy: 12.228045733446008, Validation Loss Force: 3.5252229233603884, time: 0.24559593200683594
Test Loss Energy: 16.290531197096758, Test Loss Force: 11.952354818469871, time: 10.181106805801392

Epoch 10, Batch 100/293, Loss: 0.6376531720161438
Epoch 10, Batch 200/293, Loss: 0.847520112991333

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.47845589688399, Training Loss Force: 3.7618331879882945, time: 4.422709226608276
Validation Loss Energy: 20.892327364536598, Validation Loss Force: 3.3369243633244055, time: 0.24648594856262207
Test Loss Energy: 24.938144050017243, Test Loss Force: 11.793414659312328, time: 10.329251050949097

Epoch 11, Batch 100/293, Loss: 1.29041588306427
Epoch 11, Batch 200/293, Loss: 0.20926529169082642

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.707503247193815, Training Loss Force: 4.512412839725099, time: 4.224772214889526
Validation Loss Energy: 16.450407097342566, Validation Loss Force: 4.353104937499078, time: 0.25113534927368164
Test Loss Energy: 17.541630767178546, Test Loss Force: 12.369842021947827, time: 10.16526746749878

Epoch 12, Batch 100/293, Loss: 0.8603755235671997
Epoch 12, Batch 200/293, Loss: 0.3345450162887573

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.12241260802755, Training Loss Force: 4.154167578534439, time: 4.501418352127075
Validation Loss Energy: 13.248412843834757, Validation Loss Force: 3.521295484964514, time: 0.2516336441040039
Test Loss Energy: 18.659745720181142, Test Loss Force: 11.391587242927848, time: 10.154346704483032

Epoch 13, Batch 100/293, Loss: 0.4038451611995697
Epoch 13, Batch 200/293, Loss: 0.5086061954498291

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.95535726485879, Training Loss Force: 3.7929308379243136, time: 4.2541663646698
Validation Loss Energy: 1.942556989511159, Validation Loss Force: 4.614757750548685, time: 0.2425391674041748
Test Loss Energy: 10.427142977915373, Test Loss Force: 12.15616518877824, time: 10.267679691314697

Epoch 14, Batch 100/293, Loss: 0.4428287446498871
Epoch 14, Batch 200/293, Loss: 0.33197909593582153

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.075321928165344, Training Loss Force: 3.9298447296568826, time: 4.404639959335327
Validation Loss Energy: 12.283625911758069, Validation Loss Force: 3.415111739218051, time: 0.24915504455566406
Test Loss Energy: 17.903929136049765, Test Loss Force: 11.467671839827407, time: 10.130611419677734

Epoch 15, Batch 100/293, Loss: 0.28219515085220337
Epoch 15, Batch 200/293, Loss: 0.3494018614292145

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.714473799912735, Training Loss Force: 4.48330122453924, time: 4.203046083450317
Validation Loss Energy: 1.5380498952149955, Validation Loss Force: 3.679408703841722, time: 0.24083399772644043
Test Loss Energy: 8.932986964024803, Test Loss Force: 11.401187416806678, time: 10.097307920455933

Epoch 16, Batch 100/293, Loss: 0.18134573101997375
Epoch 16, Batch 200/293, Loss: 0.498007208108902

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.832117306026389, Training Loss Force: 3.9551205488597314, time: 4.183470726013184
Validation Loss Energy: 12.348430471176991, Validation Loss Force: 3.9042910029447158, time: 0.25228142738342285
Test Loss Energy: 18.507046242725515, Test Loss Force: 12.261790352301103, time: 10.263468742370605

Epoch 17, Batch 100/293, Loss: 0.3699547350406647
Epoch 17, Batch 200/293, Loss: 0.1990201473236084

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 5.654568995271456, Training Loss Force: 2.785295390726944, time: 4.275281667709351
Validation Loss Energy: 3.7849059160774607, Validation Loss Force: 2.7028422176748204, time: 0.2440643310546875
Test Loss Energy: 10.796559494146768, Test Loss Force: 11.641492723861372, time: 10.225737810134888

Epoch 18, Batch 100/293, Loss: 0.13316459953784943
Epoch 18, Batch 200/293, Loss: 0.8280278444290161

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.310464505272645, Training Loss Force: 4.3256442533741035, time: 4.216266393661499
Validation Loss Energy: 16.935823697841347, Validation Loss Force: 3.712626843301863, time: 0.24882769584655762
Test Loss Energy: 19.65884537168474, Test Loss Force: 11.718483367344742, time: 10.3279128074646

Epoch 19, Batch 100/293, Loss: 0.2342330515384674
Epoch 19, Batch 200/293, Loss: 0.08742234110832214

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.9790438348805885, Training Loss Force: 4.22006943660922, time: 4.1712470054626465
Validation Loss Energy: 2.588526015779295, Validation Loss Force: 3.138013658996479, time: 0.24482226371765137
Test Loss Energy: 10.349005735222162, Test Loss Force: 12.003197878811694, time: 10.195093631744385

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.039 MB of 0.057 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–ƒâ–„â–ˆâ–…â–…â–‚â–…â–â–…â–‚â–†â–‚
wandb:   test_error_force â–‚â–â–‚â–‚â–‚â–ƒâ–†â–ƒâ–ˆâ–„â–ƒâ–‡â–â–†â–â–â–†â–ƒâ–ƒâ–…
wandb:          test_loss â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–…â–…â–ˆâ–†â–…â–ƒâ–…â–â–†â–‚â–†â–ƒ
wandb: train_error_energy â–â–â–‚â–â–â–‚â–‡â–‡â–‡â–‡â–‡â–ˆâ–†â–†â–ˆâ–‡â–†â–…â–‡â–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–‡â–†â–†â–‡â–†â–ƒâ–‡â–‡
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–†â–ˆâ–‡â–†â–‡â–ˆâ–‡â–„â–ˆâ–‡
wandb: valid_error_energy â–â–â–‚â–â–â–â–„â–â–ƒâ–…â–ˆâ–†â–…â–â–…â–â–…â–‚â–‡â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–‡â–…â–„â–‡â–…â–ˆâ–…â–…â–†â–ƒâ–…â–„
wandb:         valid_loss â–â–â–â–â–â–â–‡â–…â–†â–†â–ˆâ–ˆâ–†â–…â–†â–ƒâ–†â–ƒâ–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 9360
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.34901
wandb:   test_error_force 12.0032
wandb:          test_loss 4.70887
wandb: train_error_energy 6.97904
wandb:  train_error_force 4.22007
wandb:         train_loss 1.87909
wandb: valid_error_energy 2.58853
wandb:  valid_error_force 3.13801
wandb:         valid_loss 1.22322
wandb: 
wandb: ğŸš€ View run al_77_95 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/exjdlp49
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_025626-exjdlp49/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.23149561882019043, Uncertainty Bias: -0.12083098292350769
0.00011062622 0.82209015
-1.6428525 15.115779
(48745, 22, 3)
Found uncertainty sample 0 after 14 steps.
Found uncertainty sample 1 after 19 steps.
Found uncertainty sample 2 after 102 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 4 steps.
Found uncertainty sample 5 after 87 steps.
Found uncertainty sample 6 after 15 steps.
Found uncertainty sample 7 after 49 steps.
Found uncertainty sample 8 after 86 steps.
Found uncertainty sample 9 after 33 steps.
Found uncertainty sample 10 after 6 steps.
Found uncertainty sample 11 after 4 steps.
Found uncertainty sample 12 after 44 steps.
Found uncertainty sample 13 after 21 steps.
Found uncertainty sample 14 after 45 steps.
Found uncertainty sample 15 after 137 steps.
Found uncertainty sample 16 after 69 steps.
Found uncertainty sample 17 after 11 steps.
Found uncertainty sample 18 after 37 steps.
Found uncertainty sample 19 after 9 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 22 steps.
Found uncertainty sample 22 after 52 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 79 steps.
Found uncertainty sample 25 after 12 steps.
Found uncertainty sample 26 after 56 steps.
Found uncertainty sample 27 after 27 steps.
Found uncertainty sample 28 after 119 steps.
Found uncertainty sample 29 after 76 steps.
Found uncertainty sample 30 after 131 steps.
Found uncertainty sample 31 after 48 steps.
Found uncertainty sample 32 after 23 steps.
Found uncertainty sample 33 after 29 steps.
Found uncertainty sample 34 after 18 steps.
Found uncertainty sample 35 after 73 steps.
Found uncertainty sample 36 after 57 steps.
Found uncertainty sample 37 after 8 steps.
Found uncertainty sample 38 after 42 steps.
Found uncertainty sample 39 after 25 steps.
Found uncertainty sample 40 after 6 steps.
Found uncertainty sample 41 after 71 steps.
Found uncertainty sample 42 after 44 steps.
Found uncertainty sample 43 after 61 steps.
Found uncertainty sample 44 after 10 steps.
Found uncertainty sample 45 after 47 steps.
Found uncertainty sample 46 after 95 steps.
Found uncertainty sample 47 after 5 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 53 steps.
Found uncertainty sample 50 after 26 steps.
Found uncertainty sample 51 after 86 steps.
Found uncertainty sample 52 after 19 steps.
Found uncertainty sample 53 after 95 steps.
Found uncertainty sample 54 after 8 steps.
Found uncertainty sample 55 after 42 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 132 steps.
Found uncertainty sample 58 after 70 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 26 steps.
Found uncertainty sample 61 after 76 steps.
Found uncertainty sample 62 after 35 steps.
Found uncertainty sample 63 after 24 steps.
Found uncertainty sample 64 after 4 steps.
Found uncertainty sample 65 after 145 steps.
Found uncertainty sample 66 after 38 steps.
Found uncertainty sample 67 after 137 steps.
Found uncertainty sample 68 after 3 steps.
Found uncertainty sample 69 after 245 steps.
Found uncertainty sample 70 after 128 steps.
Found uncertainty sample 71 after 67 steps.
Found uncertainty sample 72 after 31 steps.
Found uncertainty sample 73 after 21 steps.
Found uncertainty sample 74 after 97 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 31 steps.
Found uncertainty sample 77 after 13 steps.
Found uncertainty sample 78 after 47 steps.
Found uncertainty sample 79 after 9 steps.
Found uncertainty sample 80 after 60 steps.
Found uncertainty sample 81 after 41 steps.
Found uncertainty sample 82 after 166 steps.
Found uncertainty sample 83 after 22 steps.
Found uncertainty sample 84 after 5 steps.
Found uncertainty sample 85 after 66 steps.
Found uncertainty sample 86 after 42 steps.
Found uncertainty sample 87 after 20 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 6 steps.
Found uncertainty sample 90 after 11 steps.
Found uncertainty sample 91 after 16 steps.
Found uncertainty sample 92 after 20 steps.
Found uncertainty sample 93 after 98 steps.
Found uncertainty sample 94 after 17 steps.
Found uncertainty sample 95 after 12 steps.
Found uncertainty sample 96 after 102 steps.
Found uncertainty sample 97 after 18 steps.
Found uncertainty sample 98 after 163 steps.
Found uncertainty sample 99 after 37 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_030751-5hpy4kzx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_96
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/5hpy4kzx
Training model 96. Added 100 samples to the dataset.
Epoch 0, Batch 100/296, Loss: 0.04781283065676689
Epoch 0, Batch 200/296, Loss: 0.2581172585487366

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 2.1742387077113445, Training Loss Force: 1.9228221445485458, time: 4.224707365036011
Validation Loss Energy: 2.6822388197374387, Validation Loss Force: 2.013057960088865, time: 0.2418656349182129
Test Loss Energy: 11.111175650658769, Test Loss Force: 11.251476798151664, time: 10.002885341644287

Epoch 1, Batch 100/296, Loss: 0.12065283209085464
Epoch 1, Batch 200/296, Loss: 0.18004290759563446

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.7024185810141845, Training Loss Force: 1.8600546604209043, time: 4.2660651206970215
Validation Loss Energy: 1.1596590415710932, Validation Loss Force: 1.957099956769995, time: 0.24199819564819336
Test Loss Energy: 9.9709960832286, Test Loss Force: 11.380408351799232, time: 10.168019771575928

Epoch 2, Batch 100/296, Loss: 0.04245582967996597
Epoch 2, Batch 200/296, Loss: 0.1832410991191864

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.4762798730042996, Training Loss Force: 1.8144970932872098, time: 4.204209327697754
Validation Loss Energy: 1.7860023210539584, Validation Loss Force: 2.004842381547628, time: 0.24694299697875977
Test Loss Energy: 10.933369394613553, Test Loss Force: 11.460479766310785, time: 9.936491966247559

Epoch 3, Batch 100/296, Loss: 0.05736773461103439
Epoch 3, Batch 200/296, Loss: 0.10215410590171814

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.0843407074281366, Training Loss Force: 1.7814972813760073, time: 4.221665382385254
Validation Loss Energy: 0.7719717733516206, Validation Loss Force: 1.9974848808271382, time: 0.2493572235107422
Test Loss Energy: 10.060203633782097, Test Loss Force: 11.56852320069653, time: 10.077839851379395

Epoch 4, Batch 100/296, Loss: 0.1322006732225418
Epoch 4, Batch 200/296, Loss: 0.07057841122150421

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.8688331823706525, Training Loss Force: 1.8208691383772502, time: 4.225103378295898
Validation Loss Energy: 4.615893363192918, Validation Loss Force: 1.9622190759010454, time: 0.25464510917663574
Test Loss Energy: 12.912826292860734, Test Loss Force: 11.520534530307323, time: 10.033606767654419

Epoch 5, Batch 100/296, Loss: 0.1681722104549408
Epoch 5, Batch 200/296, Loss: 0.1433141529560089

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.9433615245798905, Training Loss Force: 1.7963519775404055, time: 4.196271657943726
Validation Loss Energy: 2.688921343475878, Validation Loss Force: 1.950722712430623, time: 0.2774481773376465
Test Loss Energy: 10.062913860035309, Test Loss Force: 11.519076418128135, time: 10.034996509552002

Epoch 6, Batch 100/296, Loss: 0.37039804458618164
Epoch 6, Batch 200/296, Loss: 0.07949551194906235

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 9.661078645404045, Training Loss Force: 4.675707581774103, time: 4.443377494812012
Validation Loss Energy: 21.661953875108733, Validation Loss Force: 6.204687241005738, time: 0.2371981143951416
Test Loss Energy: 26.688599199080706, Test Loss Force: 13.022657814115409, time: 10.15067195892334

Epoch 7, Batch 100/296, Loss: 1.102189064025879
Epoch 7, Batch 200/296, Loss: 0.17448163032531738

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.006909044586227, Training Loss Force: 4.699142937664926, time: 4.184137344360352
Validation Loss Energy: 14.582023961437363, Validation Loss Force: 5.27535814372659, time: 0.23926234245300293
Test Loss Energy: 12.689936259140191, Test Loss Force: 13.322334647625592, time: 9.998077630996704

Epoch 8, Batch 100/296, Loss: 0.36477333307266235
Epoch 8, Batch 200/296, Loss: 0.7354439496994019

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.902610577900866, Training Loss Force: 4.55032245932238, time: 4.59652304649353
Validation Loss Energy: 5.950711703837087, Validation Loss Force: 2.9996556467459845, time: 0.24158334732055664
Test Loss Energy: 15.104530175924364, Test Loss Force: 12.082750429912567, time: 9.935657978057861

Epoch 9, Batch 100/296, Loss: 0.3428126871585846
Epoch 9, Batch 200/296, Loss: 1.4951670169830322

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.173563424990443, Training Loss Force: 4.344918117580751, time: 4.229154586791992
Validation Loss Energy: 4.497745559701925, Validation Loss Force: 5.132326172501584, time: 0.2518155574798584
Test Loss Energy: 11.088874167555442, Test Loss Force: 12.237839858446298, time: 10.000341892242432

Epoch 10, Batch 100/296, Loss: 0.44672366976737976
Epoch 10, Batch 200/296, Loss: 0.7897285223007202

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.194111387829462, Training Loss Force: 4.4008711796004585, time: 4.517415761947632
Validation Loss Energy: 6.6820332428441915, Validation Loss Force: 2.8969558715243657, time: 0.2398676872253418
Test Loss Energy: 10.353864366659849, Test Loss Force: 11.067559787268273, time: 11.362257719039917

Epoch 11, Batch 100/296, Loss: 0.2597713768482208
Epoch 11, Batch 200/296, Loss: 0.2705531120300293

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 5.6860839215815755, Training Loss Force: 2.795573280891175, time: 4.290503263473511
Validation Loss Energy: 19.639675185686997, Validation Loss Force: 4.392771664492946, time: 0.24856209754943848
Test Loss Energy: 15.176171056777399, Test Loss Force: 11.955373382034036, time: 9.93331527709961

Epoch 12, Batch 100/296, Loss: 0.6618494391441345
Epoch 12, Batch 200/296, Loss: 0.7144405245780945

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.717059822114441, Training Loss Force: 4.014840481511291, time: 4.427281618118286
Validation Loss Energy: 14.385880767390343, Validation Loss Force: 3.534375382333117, time: 0.24904918670654297
Test Loss Energy: 12.466685692595592, Test Loss Force: 10.92455400006954, time: 9.99018907546997

Epoch 13, Batch 100/296, Loss: 1.0698001384735107
Epoch 13, Batch 200/296, Loss: 0.09260809421539307

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.526826458578324, Training Loss Force: 4.062792467895216, time: 4.141024589538574
Validation Loss Energy: 3.9310340572427642, Validation Loss Force: 3.31309502386848, time: 0.2456493377685547
Test Loss Energy: 9.452862654289467, Test Loss Force: 11.340395513014487, time: 9.888771772384644

Epoch 14, Batch 100/296, Loss: 0.28003549575805664
Epoch 14, Batch 200/296, Loss: 0.8338468074798584

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.606766642279496, Training Loss Force: 4.065143354575156, time: 4.282118797302246
Validation Loss Energy: 1.3673675151891997, Validation Loss Force: 3.5053242732698684, time: 0.24274158477783203
Test Loss Energy: 9.71730108120445, Test Loss Force: 11.648400914035651, time: 10.07967758178711

Epoch 15, Batch 100/296, Loss: 0.2928839921951294
Epoch 15, Batch 200/296, Loss: 0.13096481561660767

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.842593597362469, Training Loss Force: 3.3444578206644064, time: 4.211369752883911
Validation Loss Energy: 8.050582129187111, Validation Loss Force: 3.6964007645027883, time: 0.24504828453063965
Test Loss Energy: 15.375339015549422, Test Loss Force: 11.505260914466465, time: 10.03266167640686

Epoch 16, Batch 100/296, Loss: 0.4025146961212158
Epoch 16, Batch 200/296, Loss: 1.0426156520843506

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.597368658448977, Training Loss Force: 4.225959336510555, time: 4.27262806892395
Validation Loss Energy: 4.487597725113071, Validation Loss Force: 4.441582166362851, time: 0.24756646156311035
Test Loss Energy: 9.76877491777021, Test Loss Force: 12.058540973006856, time: 10.155861616134644

Epoch 17, Batch 100/296, Loss: 0.5376575589179993
Epoch 17, Batch 200/296, Loss: 0.7329638004302979

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.222358573264292, Training Loss Force: 3.9026511764810192, time: 4.243703365325928
Validation Loss Energy: 11.593242184297445, Validation Loss Force: 3.653000543939149, time: 0.25002145767211914
Test Loss Energy: 18.306549357184768, Test Loss Force: 11.890852457873715, time: 9.948453664779663

Epoch 18, Batch 100/296, Loss: 0.9965664744377136
Epoch 18, Batch 200/296, Loss: 0.3931156396865845

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 6.6724203580147705, Training Loss Force: 3.1834932597323435, time: 4.356108903884888
Validation Loss Energy: 9.152272108780972, Validation Loss Force: 3.745116498384187, time: 0.2655673027038574
Test Loss Energy: 10.932283048975115, Test Loss Force: 12.743705621076495, time: 10.115689516067505

Epoch 19, Batch 100/296, Loss: 1.075099229812622
Epoch 19, Batch 200/296, Loss: 0.4787537753582001

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.468289017504858, Training Loss Force: 4.118931581270256, time: 4.152699947357178
Validation Loss Energy: 11.245293320269914, Validation Loss Force: 3.7267930988996185, time: 0.23861122131347656
Test Loss Energy: 14.618571324060255, Test Loss Force: 12.729855180509249, time: 9.934210777282715

wandb: - 0.039 MB of 0.057 MB uploadedwandb: \ 0.039 MB of 0.057 MB uploadedwandb: | 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–â–‚â–â–‚â–â–ˆâ–‚â–ƒâ–‚â–â–ƒâ–‚â–â–â–ƒâ–â–…â–‚â–ƒ
wandb:   test_error_force â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‡â–ˆâ–„â–…â–â–„â–â–‚â–ƒâ–ƒâ–„â–„â–†â–†
wandb:          test_loss â–â–â–‚â–‚â–‚â–‚â–ˆâ–…â–„â–ƒâ–â–ƒâ–â–â–‚â–ƒâ–‚â–„â–ƒâ–„
wandb: train_error_energy â–‚â–‚â–â–â–‚â–‚â–ˆâ–‡â–‡â–ˆâ–‡â–…â–‡â–†â–‡â–‡â–ˆâ–†â–†â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–‡â–ƒâ–†â–†â–†â–…â–‡â–†â–„â–‡
wandb:         train_loss â–‚â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–‡â–‡â–„â–‡â–†â–‡â–…â–‡â–†â–…â–‡
wandb: valid_error_energy â–‚â–â–â–â–‚â–‚â–ˆâ–†â–ƒâ–‚â–ƒâ–‡â–†â–‚â–â–ƒâ–‚â–…â–„â–…
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–†â–ƒâ–†â–ƒâ–…â–„â–ƒâ–„â–„â–…â–„â–„â–„
wandb:         valid_loss â–â–â–â–â–‚â–â–ˆâ–†â–ƒâ–„â–ƒâ–†â–…â–ƒâ–‚â–„â–„â–„â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 9450
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 14.61857
wandb:   test_error_force 12.72986
wandb:          test_loss 5.23773
wandb: train_error_energy 8.46829
wandb:  train_error_force 4.11893
wandb:         train_loss 1.94491
wandb: valid_error_energy 11.24529
wandb:  valid_error_force 3.72679
wandb:         valid_loss 1.99954
wandb: 
wandb: ğŸš€ View run al_77_96 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/5hpy4kzx
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_030751-5hpy4kzx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4902465045452118, Uncertainty Bias: -0.4280710816383362
2.7656555e-05 0.055545807
-4.250958 48.761803
(48745, 22, 3)
Found uncertainty sample 0 after 15 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 47 steps.
Found uncertainty sample 3 after 3 steps.
Found uncertainty sample 4 after 5 steps.
Found uncertainty sample 5 after 5 steps.
Found uncertainty sample 6 after 6 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 3 steps.
Found uncertainty sample 9 after 28 steps.
Found uncertainty sample 10 after 29 steps.
Found uncertainty sample 11 after 51 steps.
Found uncertainty sample 12 after 15 steps.
Found uncertainty sample 13 after 7 steps.
Found uncertainty sample 14 after 21 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 17 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 6 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 1 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 7 steps.
Found uncertainty sample 25 after 12 steps.
Found uncertainty sample 26 after 12 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 12 steps.
Found uncertainty sample 30 after 8 steps.
Found uncertainty sample 31 after 16 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 15 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 26 steps.
Found uncertainty sample 38 after 8 steps.
Found uncertainty sample 39 after 18 steps.
Found uncertainty sample 40 after 35 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 6 steps.
Found uncertainty sample 43 after 21 steps.
Found uncertainty sample 44 after 20 steps.
Found uncertainty sample 45 after 12 steps.
Found uncertainty sample 46 after 2 steps.
Found uncertainty sample 47 after 8 steps.
Found uncertainty sample 48 after 52 steps.
Found uncertainty sample 49 after 14 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 9 steps.
Found uncertainty sample 52 after 43 steps.
Found uncertainty sample 53 after 13 steps.
Found uncertainty sample 54 after 19 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 9 steps.
Found uncertainty sample 57 after 27 steps.
Found uncertainty sample 58 after 22 steps.
Found uncertainty sample 59 after 36 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 7 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 36 steps.
Found uncertainty sample 65 after 7 steps.
Found uncertainty sample 66 after 18 steps.
Found uncertainty sample 67 after 32 steps.
Found uncertainty sample 68 after 18 steps.
Found uncertainty sample 69 after 36 steps.
Found uncertainty sample 70 after 6 steps.
Found uncertainty sample 71 after 8 steps.
Found uncertainty sample 72 after 12 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 15 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 11 steps.
Found uncertainty sample 77 after 8 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 18 steps.
Found uncertainty sample 80 after 11 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 13 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 9 steps.
Found uncertainty sample 86 after 9 steps.
Found uncertainty sample 87 after 34 steps.
Found uncertainty sample 88 after 27 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 5 steps.
Found uncertainty sample 91 after 16 steps.
Found uncertainty sample 92 after 3 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 6 steps.
Found uncertainty sample 95 after 11 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 22 steps.
Found uncertainty sample 98 after 15 steps.
Found uncertainty sample 99 after 10 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_031802-xn2uu9bx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_97
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/xn2uu9bx
Training model 97. Added 100 samples to the dataset.
Epoch 0, Batch 100/299, Loss: 0.02441970445215702
Epoch 0, Batch 200/299, Loss: 0.06137658655643463

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7618415711741253, Training Loss Force: 1.8829916902492654, time: 4.351994514465332
Validation Loss Energy: 1.3795532178015852, Validation Loss Force: 1.9340808212684428, time: 0.2500600814819336
Test Loss Energy: 10.484503888252865, Test Loss Force: 11.413157578483741, time: 9.922508955001831

Epoch 1, Batch 100/299, Loss: 0.0746140331029892
Epoch 1, Batch 200/299, Loss: 0.08853673934936523

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.8144324032068657, Training Loss Force: 1.86301159039166, time: 4.433649301528931
Validation Loss Energy: 1.64613561400523, Validation Loss Force: 2.020868481186745, time: 0.24741673469543457
Test Loss Energy: 9.901464179965721, Test Loss Force: 11.492888078372228, time: 10.060380458831787

Epoch 2, Batch 100/299, Loss: 0.054613303393125534
Epoch 2, Batch 200/299, Loss: 0.03396354615688324

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.6259103223154712, Training Loss Force: 1.836076861003371, time: 4.363256454467773
Validation Loss Energy: 5.563831455403201, Validation Loss Force: 2.373875720202317, time: 0.24489665031433105
Test Loss Energy: 13.347945661079041, Test Loss Force: 11.865514802856586, time: 10.028968572616577

Epoch 3, Batch 100/299, Loss: 0.033848606050014496
Epoch 3, Batch 200/299, Loss: 0.032727211713790894

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.241949246240215, Training Loss Force: 1.80829992317861, time: 4.308304786682129
Validation Loss Energy: 1.7936730504620244, Validation Loss Force: 1.9196923418630518, time: 0.24894356727600098
Test Loss Energy: 11.180068770934914, Test Loss Force: 11.741429765579872, time: 10.23307204246521

Epoch 4, Batch 100/299, Loss: 0.14426270127296448
Epoch 4, Batch 200/299, Loss: 0.04664667323231697

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.679469060448451, Training Loss Force: 1.781582629466607, time: 4.321732997894287
Validation Loss Energy: 0.8479939590040935, Validation Loss Force: 1.9118064027254462, time: 0.24989581108093262
Test Loss Energy: 10.82862303043221, Test Loss Force: 11.768216066422967, time: 9.999441146850586

Epoch 5, Batch 100/299, Loss: 0.06490850448608398
Epoch 5, Batch 200/299, Loss: 0.11801809817552567

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2987986813561105, Training Loss Force: 1.7701910096369, time: 4.332945823669434
Validation Loss Energy: 0.6413731462296435, Validation Loss Force: 1.8796038795067964, time: 0.24455618858337402
Test Loss Energy: 10.543171057406857, Test Loss Force: 11.477357835757791, time: 10.050835609436035

Epoch 6, Batch 100/299, Loss: 0.20552197098731995
Epoch 6, Batch 200/299, Loss: 0.4950331747531891

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.818727162652246, Training Loss Force: 4.026299479312723, time: 4.467784404754639
Validation Loss Energy: 8.28168776371581, Validation Loss Force: 3.747222814903176, time: 0.24946212768554688
Test Loss Energy: 15.647988725123254, Test Loss Force: 11.460301099386031, time: 10.110669612884521

Epoch 7, Batch 100/299, Loss: 0.5289602875709534
Epoch 7, Batch 200/299, Loss: 0.7013747096061707

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.927099121313851, Training Loss Force: 4.289813628361441, time: 4.273880481719971
Validation Loss Energy: 6.007678665589986, Validation Loss Force: 4.785695153547313, time: 0.2559506893157959
Test Loss Energy: 9.851637586787952, Test Loss Force: 12.131031111650843, time: 9.969919681549072

Epoch 8, Batch 100/299, Loss: 0.8094963431358337
Epoch 8, Batch 200/299, Loss: 0.2972384989261627

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.569064167374679, Training Loss Force: 4.036587966404989, time: 4.567921161651611
Validation Loss Energy: 15.42132115387587, Validation Loss Force: 6.234546282798071, time: 0.24771475791931152
Test Loss Energy: 13.248783927678899, Test Loss Force: 12.135178101824602, time: 10.01379132270813

Epoch 9, Batch 100/299, Loss: 0.5897220969200134
Epoch 9, Batch 200/299, Loss: 0.19449198246002197

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.779882536289023, Training Loss Force: 4.451930029932954, time: 4.257884979248047
Validation Loss Energy: 10.263387227983666, Validation Loss Force: 5.385175270994816, time: 0.25193071365356445
Test Loss Energy: 10.965958929048801, Test Loss Force: 12.01958619738159, time: 10.075122594833374

Epoch 10, Batch 100/299, Loss: 0.23320989310741425
Epoch 10, Batch 200/299, Loss: 0.6695556640625

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.803749981436118, Training Loss Force: 4.101415247907005, time: 4.440544843673706
Validation Loss Energy: 1.4537955867037429, Validation Loss Force: 2.8143291408683586, time: 0.24339032173156738
Test Loss Energy: 9.594342894366203, Test Loss Force: 10.666570110501127, time: 9.95889401435852

Epoch 11, Batch 100/299, Loss: 0.278372585773468
Epoch 11, Batch 200/299, Loss: 1.304445505142212

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.24526371716397, Training Loss Force: 4.391035296301761, time: 4.3224098682403564
Validation Loss Energy: 11.014503875928431, Validation Loss Force: 3.6858195712221287, time: 0.24169325828552246
Test Loss Energy: 11.311178089451623, Test Loss Force: 11.557800872250018, time: 10.02137804031372

Epoch 12, Batch 100/299, Loss: 0.8576050996780396
Epoch 12, Batch 200/299, Loss: 0.6959924697875977

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.153679433673934, Training Loss Force: 4.025194905928249, time: 4.520808458328247
Validation Loss Energy: 12.88905797547622, Validation Loss Force: 4.245984422655697, time: 0.24778199195861816
Test Loss Energy: 16.936240318108737, Test Loss Force: 12.387008633302127, time: 10.102963924407959

Epoch 13, Batch 100/299, Loss: 0.18255645036697388
Epoch 13, Batch 200/299, Loss: 0.1149275004863739

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.647474776732892, Training Loss Force: 4.811447577241711, time: 4.287684440612793
Validation Loss Energy: 2.0631762869204624, Validation Loss Force: 4.250852148320995, time: 0.24529480934143066
Test Loss Energy: 10.396394430009607, Test Loss Force: 11.846862771458834, time: 9.993975400924683

Epoch 14, Batch 100/299, Loss: 0.5783299803733826
Epoch 14, Batch 200/299, Loss: 0.31923115253448486

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 5.112994538183241, Training Loss Force: 3.0213485652612184, time: 4.29165506362915
Validation Loss Energy: 4.777283603890744, Validation Loss Force: 2.4083461509391557, time: 0.25348520278930664
Test Loss Energy: 9.444795222893728, Test Loss Force: 11.342073706247957, time: 10.206815481185913

Epoch 15, Batch 100/299, Loss: 0.5561095476150513
Epoch 15, Batch 200/299, Loss: 0.14432722330093384

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 5.037065769349493, Training Loss Force: 2.4645588840374026, time: 4.2246153354644775
Validation Loss Energy: 6.272789797678827, Validation Loss Force: 2.733773634073941, time: 0.24995994567871094
Test Loss Energy: 13.086448952390926, Test Loss Force: 11.453797164463273, time: 9.991635799407959

Epoch 16, Batch 100/299, Loss: 0.6806504130363464
Epoch 16, Batch 200/299, Loss: 1.1972754001617432

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.098709440661745, Training Loss Force: 4.720386837878622, time: 4.232174634933472
Validation Loss Energy: 7.468516698439645, Validation Loss Force: 6.744239965871404, time: 0.24702763557434082
Test Loss Energy: 13.587056822649293, Test Loss Force: 12.702158067263134, time: 10.174643993377686

Epoch 17, Batch 100/299, Loss: 0.14168435335159302
Epoch 17, Batch 200/299, Loss: 0.3082433342933655

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 5.0608583567043475, Training Loss Force: 2.8778514918096954, time: 4.29705023765564
Validation Loss Energy: 3.591639963666462, Validation Loss Force: 2.3269317647150665, time: 0.246596097946167
Test Loss Energy: 9.604135006717211, Test Loss Force: 11.93166009260742, time: 10.03067684173584

Epoch 18, Batch 100/299, Loss: 0.4763321280479431
Epoch 18, Batch 200/299, Loss: 0.49720507860183716

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 5.686862223507092, Training Loss Force: 2.8086742292806735, time: 4.344191789627075
Validation Loss Energy: 6.250823041094757, Validation Loss Force: 2.6700419391339274, time: 0.24430155754089355
Test Loss Energy: 13.338396055246667, Test Loss Force: 12.751334037517799, time: 10.117714643478394

Epoch 19, Batch 100/299, Loss: 0.07140371948480606
Epoch 19, Batch 200/299, Loss: 0.4780637323856354

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 6.141698774790094, Training Loss Force: 2.70251178917344, time: 4.27960991859436
Validation Loss Energy: 38.443354204490035, Validation Loss Force: 4.454578919072364, time: 0.24471211433410645
Test Loss Energy: 40.10635553586687, Test Loss Force: 12.053614132530477, time: 10.089853048324585

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.060 MB uploadedwandb: | 0.039 MB of 0.060 MB uploadedwandb: / 0.060 MB of 0.060 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–â–â–â–‚â–â–‚â–â–â–â–ƒâ–â–â–‚â–‚â–â–‚â–ˆ
wandb:   test_error_force â–„â–„â–…â–…â–…â–„â–„â–†â–†â–†â–â–„â–‡â–…â–ƒâ–„â–ˆâ–…â–ˆâ–†
wandb:          test_loss â–‚â–‚â–ƒâ–‚â–‚â–‚â–ƒâ–‚â–ƒâ–ƒâ–â–‚â–„â–‚â–‚â–‚â–„â–‚â–„â–ˆ
wandb: train_error_energy â–â–â–â–â–â–â–‡â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–„â–„â–ˆâ–„â–…â–…
wandb:  train_error_force â–â–â–â–â–â–â–†â–‡â–†â–‡â–†â–‡â–†â–ˆâ–„â–ƒâ–ˆâ–„â–ƒâ–ƒ
wandb:         train_loss â–â–â–â–â–â–â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–„â–ƒâ–ˆâ–„â–„â–„
wandb: valid_error_energy â–â–â–‚â–â–â–â–‚â–‚â–„â–ƒâ–â–ƒâ–ƒâ–â–‚â–‚â–‚â–‚â–‚â–ˆ
wandb:  valid_error_force â–â–â–‚â–â–â–â–„â–…â–‡â–†â–‚â–„â–„â–„â–‚â–‚â–ˆâ–‚â–‚â–…
wandb:         valid_loss â–â–â–‚â–â–â–â–ƒâ–„â–†â–…â–‚â–„â–„â–ƒâ–‚â–‚â–…â–‚â–‚â–ˆ
wandb: 
wandb: Run summary:
wandb:       dataset_size 9540
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 40.10636
wandb:   test_error_force 12.05361
wandb:          test_loss 6.71712
wandb: train_error_energy 6.1417
wandb:  train_error_force 2.70251
wandb:         train_loss 1.31528
wandb: valid_error_energy 38.44335
wandb:  valid_error_force 4.45458
wandb:         valid_loss 4.06317
wandb: 
wandb: ğŸš€ View run al_77_97 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/xn2uu9bx
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_031802-xn2uu9bx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6726632118225098, Uncertainty Bias: -0.5373075604438782
0.0 0.0006084442
-7.2512608 46.303524
(48745, 22, 3)
Found uncertainty sample 0 after 4 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 11 steps.
Found uncertainty sample 3 after 13 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 6 after 1 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 5 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 7 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 7 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 2 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 2 steps.
Found uncertainty sample 30 after 1 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 6 steps.
Found uncertainty sample 36 after 5 steps.
Found uncertainty sample 37 after 5 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 6 steps.
Found uncertainty sample 44 after 2 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 4 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 3 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 5 steps.
Found uncertainty sample 53 after 1 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 10 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 3 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 1 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 6 steps.
Found uncertainty sample 81 after 2 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 5 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 2 steps.
Found uncertainty sample 88 after 2 steps.
Found uncertainty sample 89 after 4 steps.
Found uncertainty sample 90 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 8 steps.
Found uncertainty sample 94 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_032754-51gs0lfa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_98
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/51gs0lfa
Training model 98. Added 100 samples to the dataset.
Epoch 0, Batch 100/301, Loss: 0.19691932201385498
Epoch 0, Batch 200/301, Loss: 0.16728058457374573
Epoch 0, Batch 300/301, Loss: 0.05962362512946129

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.6684354081386283, Training Loss Force: 1.8841489175583805, time: 4.383014678955078
Validation Loss Energy: 0.6640085225916463, Validation Loss Force: 1.9259686502785747, time: 0.25394344329833984
Test Loss Energy: 10.17695587064196, Test Loss Force: 11.486888140488693, time: 10.168823003768921

Epoch 1, Batch 100/301, Loss: 0.03829759731888771
Epoch 1, Batch 200/301, Loss: 0.1914406418800354
Epoch 1, Batch 300/301, Loss: 0.27548748254776

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6163052877110293, Training Loss Force: 1.8200893275056962, time: 4.282384157180786
Validation Loss Energy: 3.8912934040924476, Validation Loss Force: 1.9240575032659912, time: 0.2513902187347412
Test Loss Energy: 11.998695788474723, Test Loss Force: 11.578477434838685, time: 10.290918350219727

Epoch 2, Batch 100/301, Loss: 0.27593573927879333
Epoch 2, Batch 200/301, Loss: 0.1945376694202423
Epoch 2, Batch 300/301, Loss: 0.2361486852169037

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.6659103567403606, Training Loss Force: 1.8694728610989766, time: 4.294142723083496
Validation Loss Energy: 1.1392041862209603, Validation Loss Force: 1.9460518040234103, time: 0.24773883819580078
Test Loss Energy: 10.549356027521625, Test Loss Force: 11.508529277838148, time: 10.052478313446045

Epoch 3, Batch 100/301, Loss: 0.15319940447807312
Epoch 3, Batch 200/301, Loss: 0.21103934943675995
Epoch 3, Batch 300/301, Loss: 0.25906187295913696

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.683782868182023, Training Loss Force: 1.8593215552386921, time: 4.297970771789551
Validation Loss Energy: 1.3806095602298667, Validation Loss Force: 2.0194798495919954, time: 0.2524678707122803
Test Loss Energy: 10.054170885425036, Test Loss Force: 11.42583430316924, time: 10.287189483642578

Epoch 4, Batch 100/301, Loss: 0.0641435831785202
Epoch 4, Batch 200/301, Loss: 0.0456160232424736
Epoch 4, Batch 300/301, Loss: 0.1488647162914276

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.563017660196084, Training Loss Force: 1.8249124850907072, time: 4.325449705123901
Validation Loss Energy: 0.9709586527943689, Validation Loss Force: 2.040166726192185, time: 0.2590970993041992
Test Loss Energy: 10.453071431301195, Test Loss Force: 11.95605903512728, time: 10.139216899871826

Epoch 5, Batch 100/301, Loss: 0.06984543800354004
Epoch 5, Batch 200/301, Loss: 0.07371927797794342
Epoch 5, Batch 300/301, Loss: 0.05087394639849663

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.304772634400594, Training Loss Force: 1.763046344435037, time: 4.4301536083221436
Validation Loss Energy: 3.7570134436481326, Validation Loss Force: 2.0152959186069266, time: 0.2512636184692383
Test Loss Energy: 12.408204110596985, Test Loss Force: 11.98974946913613, time: 10.220016956329346

Epoch 6, Batch 100/301, Loss: 0.9108372926712036
Epoch 6, Batch 200/301, Loss: 0.36698687076568604
Epoch 6, Batch 300/301, Loss: 0.6641592979431152

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.993769125946969, Training Loss Force: 4.909850429966185, time: 4.407869100570679
Validation Loss Energy: 4.953740262475654, Validation Loss Force: 3.705624344000726, time: 0.28154683113098145
Test Loss Energy: 9.196644095036428, Test Loss Force: 11.368448220725998, time: 10.167250394821167

Epoch 7, Batch 100/301, Loss: 0.35088208317756653
Epoch 7, Batch 200/301, Loss: 0.35479816794395447
Epoch 7, Batch 300/301, Loss: 0.9583475589752197

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.153156644541715, Training Loss Force: 3.9156787441151195, time: 4.27529239654541
Validation Loss Energy: 4.060483121498517, Validation Loss Force: 4.592675562279368, time: 0.26574134826660156
Test Loss Energy: 9.968071078011553, Test Loss Force: 11.775031080634845, time: 10.062515020370483

Epoch 8, Batch 100/301, Loss: 0.10144905000925064
Epoch 8, Batch 200/301, Loss: 0.5500664710998535
Epoch 8, Batch 300/301, Loss: 0.9206342697143555

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.756531757237991, Training Loss Force: 3.7378410272755986, time: 4.53387188911438
Validation Loss Energy: 21.836622322316526, Validation Loss Force: 4.998830586390715, time: 0.2522716522216797
Test Loss Energy: 25.704321273290507, Test Loss Force: 13.497204622768006, time: 10.133848667144775

Epoch 9, Batch 100/301, Loss: 0.11844784766435623
Epoch 9, Batch 200/301, Loss: 0.5919181108474731
Epoch 9, Batch 300/301, Loss: 0.5487874746322632

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.560165160451096, Training Loss Force: 4.810698755832851, time: 4.358965873718262
Validation Loss Energy: 7.3410377217194025, Validation Loss Force: 2.9340000013174965, time: 0.24750709533691406
Test Loss Energy: 10.306485077603659, Test Loss Force: 10.814820947541264, time: 10.11409878730774

Epoch 10, Batch 100/301, Loss: 0.5079377293586731
Epoch 10, Batch 200/301, Loss: 1.3753658533096313
Epoch 10, Batch 300/301, Loss: 0.8722291588783264

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.94986141047679, Training Loss Force: 4.313883247121059, time: 4.52479362487793
Validation Loss Energy: 12.668644735199656, Validation Loss Force: 5.432470731995828, time: 0.2528092861175537
Test Loss Energy: 17.338880675063045, Test Loss Force: 13.154529612409743, time: 10.188296556472778

Epoch 11, Batch 100/301, Loss: 0.6562970876693726
Epoch 11, Batch 200/301, Loss: 0.6469376683235168
Epoch 11, Batch 300/301, Loss: 0.14009472727775574

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.751385838393571, Training Loss Force: 3.910409701182328, time: 4.217350721359253
Validation Loss Energy: 19.679516218088295, Validation Loss Force: 4.898463448608007, time: 0.25466203689575195
Test Loss Energy: 14.998606351289032, Test Loss Force: 11.982349704762521, time: 10.15846586227417

Epoch 12, Batch 100/301, Loss: 0.5185143351554871
Epoch 12, Batch 200/301, Loss: 1.005568504333496
Epoch 12, Batch 300/301, Loss: 0.28812113404273987

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 7.7481165539558905, Training Loss Force: 3.9069136764891597, time: 4.534042835235596
Validation Loss Energy: 13.463752255884552, Validation Loss Force: 3.9310353041899817, time: 0.2515103816986084
Test Loss Energy: 19.568936625498452, Test Loss Force: 11.657255540318404, time: 11.388165950775146

Epoch 13, Batch 100/301, Loss: 0.32726359367370605
Epoch 13, Batch 200/301, Loss: 0.2132624089717865
Epoch 13, Batch 300/301, Loss: 0.5251935124397278

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.498103325210482, Training Loss Force: 3.545100203574875, time: 4.2838475704193115
Validation Loss Energy: 3.031075153912979, Validation Loss Force: 3.9774314490444196, time: 0.2697479724884033
Test Loss Energy: 11.06851084333055, Test Loss Force: 12.067697202899057, time: 10.091778755187988

Epoch 14, Batch 100/301, Loss: 0.2014036774635315
Epoch 14, Batch 200/301, Loss: 0.5034496784210205
Epoch 14, Batch 300/301, Loss: 1.9859957695007324

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.582059378677114, Training Loss Force: 3.9335353826126056, time: 4.578917503356934
Validation Loss Energy: 3.5294958997369137, Validation Loss Force: 4.727008028108293, time: 0.25385189056396484
Test Loss Energy: 11.556637839824047, Test Loss Force: 13.177128949887056, time: 10.135442018508911

Epoch 15, Batch 100/301, Loss: 0.601195216178894
Epoch 15, Batch 200/301, Loss: 0.7524086833000183
Epoch 15, Batch 300/301, Loss: 0.25717002153396606

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.432013471575127, Training Loss Force: 4.221019453393001, time: 4.357413291931152
Validation Loss Energy: 6.796127220932081, Validation Loss Force: 2.898897012070906, time: 0.24477410316467285
Test Loss Energy: 13.58367864025381, Test Loss Force: 11.507325866519311, time: 10.183359146118164

Epoch 16, Batch 100/301, Loss: 0.7449436187744141
Epoch 16, Batch 200/301, Loss: 0.2666762173175812
Epoch 16, Batch 300/301, Loss: 0.7038906812667847

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.000952258266944, Training Loss Force: 4.527959447932926, time: 4.536444425582886
Validation Loss Energy: 6.3140776451227065, Validation Loss Force: 3.7017269659395518, time: 0.2482619285583496
Test Loss Energy: 12.524146890318468, Test Loss Force: 12.117261783274682, time: 10.152564764022827

Epoch 17, Batch 100/301, Loss: 0.35275352001190186
Epoch 17, Batch 200/301, Loss: 0.9579159617424011
Epoch 17, Batch 300/301, Loss: 1.030583143234253

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 6.668200747027949, Training Loss Force: 3.7664619901488128, time: 4.323010206222534
Validation Loss Energy: 18.857921935292318, Validation Loss Force: 4.152770397584431, time: 0.2616567611694336
Test Loss Energy: 14.369523200945052, Test Loss Force: 12.090169779844565, time: 10.099639892578125

Epoch 18, Batch 100/301, Loss: 0.16251946985721588
Epoch 18, Batch 200/301, Loss: 0.36654841899871826
Epoch 18, Batch 300/301, Loss: 0.9983605742454529

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.940593699731691, Training Loss Force: 4.127890965971368, time: 4.565204858779907
Validation Loss Energy: 2.4794174217334115, Validation Loss Force: 3.3847980960973922, time: 0.253695011138916
Test Loss Energy: 9.673902832020525, Test Loss Force: 12.02205452799906, time: 10.184522151947021

Epoch 19, Batch 100/301, Loss: 0.9697547554969788
Epoch 19, Batch 200/301, Loss: 0.23639270663261414
Epoch 19, Batch 300/301, Loss: 0.10599300265312195

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.16077122692595, Training Loss Force: 4.026491138697933, time: 4.3131468296051025
Validation Loss Energy: 6.585119792582482, Validation Loss Force: 2.956159589381938, time: 0.26993584632873535
Test Loss Energy: 10.24385619765823, Test Loss Force: 10.867264320542606, time: 10.132504224777222

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.059 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–‚â–â–‚â–‚â–â–â–ˆâ–â–„â–ƒâ–…â–‚â–‚â–ƒâ–‚â–ƒâ–â–
wandb:   test_error_force â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–‚â–„â–ˆâ–â–‡â–„â–ƒâ–„â–‡â–ƒâ–„â–„â–„â–
wandb:          test_loss â–‚â–‚â–‚â–‚â–‚â–ƒâ–â–‚â–ˆâ–â–†â–„â–„â–ƒâ–„â–ƒâ–ƒâ–„â–‚â–
wandb: train_error_energy â–â–â–‚â–‚â–â–â–‡â–‡â–…â–ˆâ–ˆâ–†â–†â–…â–ˆâ–ˆâ–ˆâ–…â–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–…â–ˆâ–‡â–†â–†â–…â–†â–†â–‡â–…â–†â–†
wandb:         train_loss â–â–â–‚â–‚â–â–â–ˆâ–‡â–†â–ˆâ–‡â–†â–†â–…â–‡â–‡â–ˆâ–†â–‡â–‡
wandb: valid_error_energy â–â–‚â–â–â–â–‚â–‚â–‚â–ˆâ–ƒâ–…â–‡â–…â–‚â–‚â–ƒâ–ƒâ–‡â–‚â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–…â–†â–‡â–ƒâ–ˆâ–‡â–…â–…â–‡â–ƒâ–…â–…â–„â–ƒ
wandb:         valid_loss â–â–‚â–â–â–â–‚â–„â–„â–ˆâ–ƒâ–‡â–‡â–…â–ƒâ–„â–ƒâ–„â–‡â–ƒâ–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 9630
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 10.24386
wandb:   test_error_force 10.86726
wandb:          test_loss 4.32175
wandb: train_error_energy 9.16077
wandb:  train_error_force 4.02649
wandb:         train_loss 1.96032
wandb: valid_error_energy 6.58512
wandb:  valid_error_force 2.95616
wandb:         valid_loss 1.42982
wandb: 
wandb: ğŸš€ View run al_77_98 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/51gs0lfa
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_032754-51gs0lfa/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5699585676193237, Uncertainty Bias: -0.5114961266517639
6.1035156e-05 0.0012187958
-7.4305944 22.89188
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 3 steps.
Found uncertainty sample 2 after 49 steps.
Found uncertainty sample 3 after 9 steps.
Found uncertainty sample 4 after 12 steps.
Found uncertainty sample 5 after 2 steps.
Found uncertainty sample 6 after 13 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 23 steps.
Found uncertainty sample 9 after 12 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 4 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 9 steps.
Found uncertainty sample 19 after 6 steps.
Found uncertainty sample 20 after 1 steps.
Found uncertainty sample 21 after 2 steps.
Found uncertainty sample 22 after 11 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 14 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 6 steps.
Found uncertainty sample 30 after 13 steps.
Found uncertainty sample 31 after 12 steps.
Found uncertainty sample 32 after 15 steps.
Found uncertainty sample 33 after 10 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 11 steps.
Found uncertainty sample 37 after 16 steps.
Found uncertainty sample 38 after 14 steps.
Found uncertainty sample 39 after 19 steps.
Found uncertainty sample 40 after 4 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 7 steps.
Found uncertainty sample 43 after 14 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 3 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 18 steps.
Found uncertainty sample 48 after 17 steps.
Found uncertainty sample 49 after 3 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 8 steps.
Found uncertainty sample 57 after 7 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 6 steps.
Found uncertainty sample 61 after 14 steps.
Found uncertainty sample 62 after 4 steps.
Found uncertainty sample 63 after 2 steps.
Found uncertainty sample 64 after 3 steps.
Found uncertainty sample 65 after 4 steps.
Found uncertainty sample 66 after 12 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 12 steps.
Found uncertainty sample 70 after 4 steps.
Found uncertainty sample 71 after 12 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 9 steps.
Found uncertainty sample 74 after 11 steps.
Found uncertainty sample 75 after 36 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 16 steps.
Found uncertainty sample 80 after 7 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 17 steps.
Found uncertainty sample 83 after 3 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 13 steps.
Found uncertainty sample 86 after 13 steps.
Found uncertainty sample 87 after 30 steps.
Found uncertainty sample 88 after 5 steps.
Found uncertainty sample 89 after 19 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 19 steps.
Found uncertainty sample 93 after 3 steps.
Found uncertainty sample 94 after 25 steps.
Found uncertainty sample 95 after 2 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 12 steps.
Found uncertainty sample 98 after 7 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_033803-a7qot013
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_99
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/a7qot013
Training model 99. Added 100 samples to the dataset.
Epoch 0, Batch 100/304, Loss: 0.1326959729194641
Epoch 0, Batch 200/304, Loss: 0.05128895863890648
Epoch 0, Batch 300/304, Loss: 0.03397871181368828

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.3272437748437675, Training Loss Force: 1.8265789505008665, time: 4.426408290863037
Validation Loss Energy: 0.8372570420562297, Validation Loss Force: 1.912407297682647, time: 0.2513704299926758
Test Loss Energy: 9.84015160343031, Test Loss Force: 11.458121506531441, time: 10.439926862716675

Epoch 1, Batch 100/304, Loss: 0.10357391834259033
Epoch 1, Batch 200/304, Loss: 0.04025860130786896
Epoch 1, Batch 300/304, Loss: 0.03152288496494293

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.2223183384471497, Training Loss Force: 1.8080019174549897, time: 4.406203746795654
Validation Loss Energy: 1.228108539040452, Validation Loss Force: 1.9107011176188897, time: 0.2521083354949951
Test Loss Energy: 10.640261827853024, Test Loss Force: 11.659641308480824, time: 10.6077721118927

Epoch 2, Batch 100/304, Loss: 0.07416419684886932
Epoch 2, Batch 200/304, Loss: 0.11672832071781158
Epoch 2, Batch 300/304, Loss: 0.13253623247146606

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3331653048020695, Training Loss Force: 1.7786368206514152, time: 4.28873348236084
Validation Loss Energy: 2.3383829263488654, Validation Loss Force: 1.9001667516596192, time: 0.26041626930236816
Test Loss Energy: 11.777449570669454, Test Loss Force: 11.66139807244602, time: 10.208625793457031

Epoch 3, Batch 100/304, Loss: 0.14230501651763916
Epoch 3, Batch 200/304, Loss: 0.12224752455949783
Epoch 3, Batch 300/304, Loss: 0.07097429037094116

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.814294459989426, Training Loss Force: 1.7736791353327266, time: 4.314743518829346
Validation Loss Energy: 0.8599591523612672, Validation Loss Force: 1.8819851256202629, time: 0.2607264518737793
Test Loss Energy: 10.843796629354909, Test Loss Force: 11.48759610246558, time: 10.478676795959473

Epoch 4, Batch 100/304, Loss: 0.04415297135710716
Epoch 4, Batch 200/304, Loss: 0.06778153777122498
Epoch 4, Batch 300/304, Loss: 0.17983989417552948

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4020040077957416, Training Loss Force: 1.787473825856529, time: 4.330924987792969
Validation Loss Energy: 2.7490727412185185, Validation Loss Force: 1.9151691076334185, time: 0.2620689868927002
Test Loss Energy: 11.815948940093207, Test Loss Force: 11.697517530721955, time: 10.320631742477417

Epoch 5, Batch 100/304, Loss: 0.133085235953331
Epoch 5, Batch 200/304, Loss: 0.19255778193473816
Epoch 5, Batch 300/304, Loss: 0.10241489857435226

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.6158085348891378, Training Loss Force: 1.7663598335042208, time: 4.398290395736694
Validation Loss Energy: 0.9000016558058028, Validation Loss Force: 1.8678723106175306, time: 0.2723112106323242
Test Loss Energy: 10.417786010039412, Test Loss Force: 11.601464962746421, time: 10.469309091567993

Epoch 6, Batch 100/304, Loss: 0.5096539258956909
Epoch 6, Batch 200/304, Loss: 1.1232655048370361
Epoch 6, Batch 300/304, Loss: 0.3644351661205292

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.722435042904632, Training Loss Force: 4.795486670038997, time: 4.2915239334106445
Validation Loss Energy: 6.921768108386519, Validation Loss Force: 4.505860068717825, time: 0.27637529373168945
Test Loss Energy: 10.269506736633788, Test Loss Force: 12.033552160100713, time: 10.310099124908447

Epoch 7, Batch 100/304, Loss: 0.15546604990959167
Epoch 7, Batch 200/304, Loss: 0.5534754991531372
Epoch 7, Batch 300/304, Loss: 0.3206287622451782

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 5.402480079920127, Training Loss Force: 2.883294061010789, time: 4.358895301818848
Validation Loss Energy: 13.872362073728263, Validation Loss Force: 5.739904855854141, time: 0.2657766342163086
Test Loss Energy: 12.493503451576034, Test Loss Force: 13.130299641963237, time: 10.599881410598755

Epoch 8, Batch 100/304, Loss: 0.28850793838500977
Epoch 8, Batch 200/304, Loss: 0.5281815528869629
Epoch 8, Batch 300/304, Loss: 0.09802800416946411

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.324645552309011, Training Loss Force: 4.559967765266437, time: 4.3816611766815186
Validation Loss Energy: 8.590784068214642, Validation Loss Force: 5.278212227771435, time: 0.27686119079589844
Test Loss Energy: 15.330733498556945, Test Loss Force: 12.999322563271448, time: 10.37498927116394

Epoch 9, Batch 100/304, Loss: 0.16009770333766937
Epoch 9, Batch 200/304, Loss: 0.32211098074913025
Epoch 9, Batch 300/304, Loss: 0.4624425172805786

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.760283718231918, Training Loss Force: 4.5331452648145145, time: 4.441671848297119
Validation Loss Energy: 3.5347477742127666, Validation Loss Force: 4.0836576355300975, time: 0.25880980491638184
Test Loss Energy: 9.521711529856574, Test Loss Force: 11.895237976832462, time: 10.497365951538086

Epoch 10, Batch 100/304, Loss: 0.409403920173645
Epoch 10, Batch 200/304, Loss: 0.13711553812026978
Epoch 10, Batch 300/304, Loss: 0.7939941883087158

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.9981009169245505, Training Loss Force: 3.9116580400573513, time: 4.459198713302612
Validation Loss Energy: 2.891396936296279, Validation Loss Force: 3.918988725259971, time: 0.2539029121398926
Test Loss Energy: 9.334234023008502, Test Loss Force: 11.882991971218452, time: 10.277368545532227

Epoch 11, Batch 100/304, Loss: 0.35439711809158325
Epoch 11, Batch 200/304, Loss: 0.7139400243759155
Epoch 11, Batch 300/304, Loss: 0.16928289830684662

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.17539807351493, Training Loss Force: 3.717485816751291, time: 4.419641017913818
Validation Loss Energy: 4.831615320835416, Validation Loss Force: 3.1243968172493863, time: 0.25632143020629883
Test Loss Energy: 12.333346488308296, Test Loss Force: 12.304876333072713, time: 10.35599970817566

Epoch 12, Batch 100/304, Loss: 0.12331569939851761
Epoch 12, Batch 200/304, Loss: 0.1883876621723175
Epoch 12, Batch 300/304, Loss: 0.783883810043335

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.485643966126343, Training Loss Force: 3.91339873706157, time: 4.325037717819214
Validation Loss Energy: 5.925799657926002, Validation Loss Force: 6.824299143139546, time: 0.26971006393432617
Test Loss Energy: 12.844932129326804, Test Loss Force: 13.451663471008866, time: 10.32804012298584

Epoch 13, Batch 100/304, Loss: 0.38156914710998535
Epoch 13, Batch 200/304, Loss: 0.8520644903182983
Epoch 13, Batch 300/304, Loss: 0.07664722949266434

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.263145773336642, Training Loss Force: 4.797069178926641, time: 4.306131362915039
Validation Loss Energy: 12.91246834066717, Validation Loss Force: 3.856004129838374, time: 0.2626497745513916
Test Loss Energy: 11.622692187298778, Test Loss Force: 11.892565182311907, time: 10.393999338150024

Epoch 14, Batch 100/304, Loss: 0.4015059471130371
Epoch 14, Batch 200/304, Loss: 0.17419537901878357
Epoch 14, Batch 300/304, Loss: 0.3702073395252228

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 4.916773632202662, Training Loss Force: 2.590577595094965, time: 4.314895153045654
Validation Loss Energy: 5.733350099671634, Validation Loss Force: 2.373809275729172, time: 0.25972914695739746
Test Loss Energy: 9.872906626507458, Test Loss Force: 11.575738305897168, time: 10.3420889377594

Epoch 15, Batch 100/304, Loss: 0.2612423896789551
Epoch 15, Batch 200/304, Loss: 0.2421121597290039
Epoch 15, Batch 300/304, Loss: 1.0343306064605713

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 6.779179327355328, Training Loss Force: 3.389325690686979, time: 4.303862571716309
Validation Loss Energy: 10.024402846530773, Validation Loss Force: 7.525489983996287, time: 0.25873875617980957
Test Loss Energy: 15.030502393744456, Test Loss Force: 13.883523531844626, time: 10.472389698028564

Epoch 16, Batch 100/304, Loss: 0.16099128127098083
Epoch 16, Batch 200/304, Loss: 0.32188650965690613
Epoch 16, Batch 300/304, Loss: 0.3035736680030823

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.859699295423852, Training Loss Force: 4.428063231213772, time: 4.451289176940918
Validation Loss Energy: 6.653517252272437, Validation Loss Force: 3.4067944231440395, time: 0.26142096519470215
Test Loss Energy: 12.494208876756788, Test Loss Force: 12.218149204887377, time: 10.3630530834198

Epoch 17, Batch 100/304, Loss: 0.27355921268463135
Epoch 17, Batch 200/304, Loss: 0.46002280712127686
Epoch 17, Batch 300/304, Loss: 0.10865196585655212

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.766526078107714, Training Loss Force: 2.4925832427306425, time: 4.398926734924316
Validation Loss Energy: 6.871102327192743, Validation Loss Force: 2.465424180062043, time: 0.2632014751434326
Test Loss Energy: 13.473667808548894, Test Loss Force: 11.746553646100065, time: 10.389744520187378

Epoch 18, Batch 100/304, Loss: 0.6733424663543701
Epoch 18, Batch 200/304, Loss: 0.2081669420003891
Epoch 18, Batch 300/304, Loss: 0.9668599367141724

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.363730557074723, Training Loss Force: 4.209482366981506, time: 4.537142992019653
Validation Loss Energy: 14.30488121378512, Validation Loss Force: 4.115118291955259, time: 0.25623488426208496
Test Loss Energy: 18.66774296999267, Test Loss Force: 12.666446417618609, time: 10.314998865127563

Epoch 19, Batch 100/304, Loss: 0.31160762906074524
Epoch 19, Batch 200/304, Loss: 0.09242711961269379
Epoch 19, Batch 300/304, Loss: 0.2848964333534241

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 10.650869978291547, Training Loss Force: 4.8349517269058335, time: 4.248055696487427
Validation Loss Energy: 5.919664232279109, Validation Loss Force: 4.522725653304005, time: 0.2659735679626465
Test Loss Energy: 12.918247570758552, Test Loss Force: 13.188585839364036, time: 10.283751010894775

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–ƒâ–‚â–ƒâ–‚â–‚â–ƒâ–…â–â–â–ƒâ–„â–ƒâ–â–…â–ƒâ–„â–ˆâ–„
wandb:   test_error_force â–â–‚â–‚â–â–‚â–â–ƒâ–†â–…â–‚â–‚â–ƒâ–‡â–‚â–â–ˆâ–ƒâ–‚â–„â–†
wandb:          test_loss â–â–‚â–‚â–â–‚â–‚â–‚â–…â–†â–‚â–‚â–„â–†â–ƒâ–â–ˆâ–„â–ƒâ–‡â–†
wandb: train_error_energy â–â–â–â–â–â–â–‡â–„â–‡â–‡â–†â–…â–†â–†â–„â–…â–‡â–„â–‡â–ˆ
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–„â–‡â–‡â–†â–…â–†â–ˆâ–ƒâ–…â–‡â–ƒâ–‡â–ˆ
wandb:         train_loss â–â–â–â–â–â–â–‡â–„â–‡â–‡â–†â–…â–†â–‡â–ƒâ–…â–‡â–ƒâ–‡â–ˆ
wandb: valid_error_energy â–â–â–‚â–â–‚â–â–„â–ˆâ–…â–‚â–‚â–ƒâ–„â–‡â–„â–†â–„â–„â–ˆâ–„
wandb:  valid_error_force â–â–â–â–â–â–â–„â–†â–…â–„â–„â–ƒâ–‡â–ƒâ–‚â–ˆâ–ƒâ–‚â–„â–„
wandb:         valid_loss â–â–â–â–â–â–â–…â–‡â–†â–„â–ƒâ–ƒâ–‡â–…â–‚â–ˆâ–„â–ƒâ–†â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 9720
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 12.91825
wandb:   test_error_force 13.18859
wandb:          test_loss 5.27744
wandb: train_error_energy 10.65087
wandb:  train_error_force 4.83495
wandb:         train_loss 2.33055
wandb: valid_error_energy 5.91966
wandb:  valid_error_force 4.52273
wandb:         valid_loss 1.90947
wandb: 
wandb: ğŸš€ View run al_77_99 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/a7qot013
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_033803-a7qot013/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7595255970954895, Uncertainty Bias: -0.7709146738052368
2.2888184e-05 0.001364708
-11.335796 26.443876
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 13 steps.
Found uncertainty sample 2 after 6 steps.
Found uncertainty sample 3 after 7 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 1 steps.
Found uncertainty sample 6 after 4 steps.
Found uncertainty sample 7 after 8 steps.
Found uncertainty sample 8 after 13 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 12 steps.
Found uncertainty sample 11 after 4 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 4 steps.
Found uncertainty sample 15 after 14 steps.
Found uncertainty sample 16 after 9 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 12 steps.
Found uncertainty sample 19 after 3 steps.
Found uncertainty sample 20 after 4 steps.
Found uncertainty sample 21 after 1 steps.
Found uncertainty sample 22 after 5 steps.
Found uncertainty sample 23 after 2 steps.
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 2 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 10 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 4 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 4 steps.
Found uncertainty sample 32 after 28 steps.
Found uncertainty sample 33 after 8 steps.
Found uncertainty sample 34 after 2 steps.
Found uncertainty sample 35 after 2 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 7 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 1 steps.
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 24 steps.
Found uncertainty sample 44 after 19 steps.
Found uncertainty sample 45 after 23 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 4 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 2 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 6 steps.
Found uncertainty sample 53 after 8 steps.
Found uncertainty sample 54 after 9 steps.
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 13 steps.
Found uncertainty sample 57 after 11 steps.
Found uncertainty sample 58 after 23 steps.
Found uncertainty sample 59 after 12 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 13 steps.
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 1 steps.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 7 steps.
Found uncertainty sample 67 after 13 steps.
Found uncertainty sample 68 after 14 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 5 steps.
Found uncertainty sample 73 after 10 steps.
Found uncertainty sample 74 after 16 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 7 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 3 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 5 steps.
Found uncertainty sample 82 after 9 steps.
Found uncertainty sample 83 after 5 steps.
Found uncertainty sample 84 after 6 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 4 steps.
Found uncertainty sample 87 after 3 steps.
Found uncertainty sample 88 after 2 steps.
Found uncertainty sample 89 after 12 steps.
Found uncertainty sample 90 after 3 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 3 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 5 steps.
Found uncertainty sample 96 after 4 steps.
Found uncertainty sample 97 after 7 steps.
Found uncertainty sample 98 after 5 steps.
Found uncertainty sample 99 after 19 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_034812-9q9vy956
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_100
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/9q9vy956
Training model 100. Added 100 samples to the dataset.
Epoch 0, Batch 100/307, Loss: 0.06993649899959564
Epoch 0, Batch 200/307, Loss: 0.030292518436908722
Epoch 0, Batch 300/307, Loss: 0.034855060279369354

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.1624578269484347, Training Loss Force: 1.8234398449241005, time: 4.3870251178741455
Validation Loss Energy: 0.6449846950416473, Validation Loss Force: 1.9258571153033792, time: 0.2619807720184326
Test Loss Energy: 10.240931243736693, Test Loss Force: 11.631076419764623, time: 10.133130550384521

Epoch 1, Batch 100/307, Loss: 0.19849203526973724
Epoch 1, Batch 200/307, Loss: 0.050972193479537964
Epoch 1, Batch 300/307, Loss: 0.03269398957490921

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.3861631873192641, Training Loss Force: 1.8127082365319778, time: 4.416577577590942
Validation Loss Energy: 1.2632932033297688, Validation Loss Force: 1.8993707094629275, time: 0.2575802803039551
Test Loss Energy: 10.134255524851728, Test Loss Force: 11.685097078404699, time: 10.285885334014893

Epoch 2, Batch 100/307, Loss: 0.03484346717596054
Epoch 2, Batch 200/307, Loss: 0.11643879860639572
Epoch 2, Batch 300/307, Loss: 0.05628485977649689

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.1502000228335763, Training Loss Force: 1.7517645496919791, time: 4.407997131347656
Validation Loss Energy: 0.7098009334572744, Validation Loss Force: 1.8781542142991483, time: 0.2612006664276123
Test Loss Energy: 10.180026811585119, Test Loss Force: 11.874851910043706, time: 10.76056456565857

Epoch 3, Batch 100/307, Loss: 0.06810425966978073
Epoch 3, Batch 200/307, Loss: 0.032975368201732635
Epoch 3, Batch 300/307, Loss: 0.1739753782749176

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.2922462342136904, Training Loss Force: 1.7703030757859435, time: 4.856881856918335
Validation Loss Energy: 1.485399441656651, Validation Loss Force: 2.107960841564335, time: 0.29207372665405273
Test Loss Energy: 10.068345858694725, Test Loss Force: 11.842464526550362, time: 12.107285976409912

Epoch 4, Batch 100/307, Loss: 0.08994283527135849
Epoch 4, Batch 200/307, Loss: 0.05285292863845825
Epoch 4, Batch 300/307, Loss: 0.04009178653359413

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.303928376391279, Training Loss Force: 1.752281441299531, time: 4.784610986709595
Validation Loss Energy: 0.7598239801613291, Validation Loss Force: 1.9245043779749533, time: 0.34798336029052734
Test Loss Energy: 10.758235233604664, Test Loss Force: 12.163032156015351, time: 12.122202157974243

Epoch 5, Batch 100/307, Loss: 0.20618471503257751
Epoch 5, Batch 200/307, Loss: 0.2126908004283905
Epoch 5, Batch 300/307, Loss: 0.2731231153011322

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.2502268565306744, Training Loss Force: 1.8104966637274633, time: 4.72197699546814
Validation Loss Energy: 1.3948400113423927, Validation Loss Force: 1.887516383073511, time: 0.3215138912200928
Test Loss Energy: 10.359795857680057, Test Loss Force: 11.74874184261066, time: 11.909257888793945

Epoch 6, Batch 100/307, Loss: 0.8174867033958435
Epoch 6, Batch 200/307, Loss: 0.27795282006263733
Epoch 6, Batch 300/307, Loss: 0.29003047943115234

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.5534908323276335, Training Loss Force: 3.7657024579185006, time: 4.641932249069214
Validation Loss Energy: 1.5400695711693042, Validation Loss Force: 2.724273944835053, time: 0.31136655807495117
Test Loss Energy: 11.743564022530029, Test Loss Force: 11.69312229359566, time: 12.114796161651611

Epoch 7, Batch 100/307, Loss: 1.4092683792114258
Epoch 7, Batch 200/307, Loss: 0.6110097169876099
Epoch 7, Batch 300/307, Loss: 0.6474729776382446

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.723958802193444, Training Loss Force: 4.033328101138604, time: 4.946688890457153
Validation Loss Energy: 2.108682378664799, Validation Loss Force: 3.361395791529527, time: 0.3089303970336914
Test Loss Energy: 10.986033594785688, Test Loss Force: 11.200718907009447, time: 12.03936243057251

Epoch 8, Batch 100/307, Loss: 0.24524620175361633
Epoch 8, Batch 200/307, Loss: 0.148997962474823
Epoch 8, Batch 300/307, Loss: 1.012953281402588

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.304166875141998, Training Loss Force: 4.821461572821328, time: 4.789553880691528
Validation Loss Energy: 8.366077588669274, Validation Loss Force: 5.420293490810987, time: 0.3444240093231201
Test Loss Energy: 10.482278684363378, Test Loss Force: 11.146322284923253, time: 11.897388219833374

Epoch 9, Batch 100/307, Loss: 0.5608521699905396
Epoch 9, Batch 200/307, Loss: 0.7096949815750122
Epoch 9, Batch 300/307, Loss: 0.23486502468585968

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 7.875592540486157, Training Loss Force: 4.134164953899785, time: 4.962209224700928
Validation Loss Energy: 1.0440595751206714, Validation Loss Force: 3.6601435029375375, time: 0.2895808219909668
Test Loss Energy: 9.774129842594153, Test Loss Force: 12.302217644817421, time: 11.943341970443726

Epoch 10, Batch 100/307, Loss: 0.07214989513158798
Epoch 10, Batch 200/307, Loss: 0.5100047588348389
Epoch 10, Batch 300/307, Loss: 0.40022075176239014

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.264381566545592, Training Loss Force: 3.9607877525939528, time: 4.889654636383057
Validation Loss Energy: 5.896687893787404, Validation Loss Force: 2.815186372546351, time: 0.326815128326416
Test Loss Energy: 13.209987277458227, Test Loss Force: 11.350050657027182, time: 12.118028163909912

Epoch 11, Batch 100/307, Loss: 0.36886438727378845
Epoch 11, Batch 200/307, Loss: 0.4382622241973877
Epoch 11, Batch 300/307, Loss: 0.7834562063217163

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.192097460141746, Training Loss Force: 4.01590253890032, time: 4.814833164215088
Validation Loss Energy: 9.598860376154127, Validation Loss Force: 3.7851741433306456, time: 0.31116557121276855
Test Loss Energy: 10.744253401920023, Test Loss Force: 11.993288430313047, time: 11.831164836883545

Epoch 12, Batch 100/307, Loss: 0.1147991195321083
Epoch 12, Batch 200/307, Loss: 0.18043383955955505
Epoch 12, Batch 300/307, Loss: 0.2419125735759735

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.263816723229237, Training Loss Force: 4.056678025509789, time: 4.80709433555603
Validation Loss Energy: 1.7921505724436577, Validation Loss Force: 5.099282249381217, time: 0.30933237075805664
Test Loss Energy: 10.554056188475835, Test Loss Force: 12.341863263708904, time: 12.178003311157227

Epoch 13, Batch 100/307, Loss: 0.2168937772512436
Epoch 13, Batch 200/307, Loss: 0.12528766691684723
Epoch 13, Batch 300/307, Loss: 0.7540313601493835

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.27485374362351, Training Loss Force: 3.990796291694696, time: 4.742078065872192
Validation Loss Energy: 4.332096905189717, Validation Loss Force: 3.575590149763304, time: 0.2995584011077881
Test Loss Energy: 11.882032432744106, Test Loss Force: 12.09264610323515, time: 12.031117677688599

Epoch 14, Batch 100/307, Loss: 0.6886343359947205
Epoch 14, Batch 200/307, Loss: 0.09510917961597443
Epoch 14, Batch 300/307, Loss: 0.5189501047134399

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.399816408674963, Training Loss Force: 3.7437290538988544, time: 4.918375015258789
Validation Loss Energy: 4.900370743444366, Validation Loss Force: 3.715431272127165, time: 0.26608824729919434
Test Loss Energy: 13.005137674105795, Test Loss Force: 11.359437110494975, time: 11.978631019592285

Epoch 15, Batch 100/307, Loss: 0.4693566560745239
Epoch 15, Batch 200/307, Loss: 0.13525551557540894
Epoch 15, Batch 300/307, Loss: 0.6504046320915222

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.940941355954991, Training Loss Force: 4.175412673897527, time: 4.852535009384155
Validation Loss Energy: 8.199196683103503, Validation Loss Force: 4.278905877745804, time: 0.3093259334564209
Test Loss Energy: 10.389986524944907, Test Loss Force: 12.469307364145536, time: 10.513432264328003

Epoch 16, Batch 100/307, Loss: 0.10442490875720978
Epoch 16, Batch 200/307, Loss: 0.5368330478668213
Epoch 16, Batch 300/307, Loss: 0.41202807426452637

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 6.006266749330331, Training Loss Force: 3.018596531417208, time: 4.837370872497559
Validation Loss Energy: 8.02036325405496, Validation Loss Force: 2.538489230388936, time: 0.307450532913208
Test Loss Energy: 14.044571624029212, Test Loss Force: 11.316260383302849, time: 12.39162278175354

Epoch 17, Batch 100/307, Loss: 0.1907181292772293
Epoch 17, Batch 200/307, Loss: 0.36304420232772827
Epoch 17, Batch 300/307, Loss: 0.4580659866333008

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.9557754939369305, Training Loss Force: 2.34054512165376, time: 4.479145050048828
Validation Loss Energy: 4.461722323409044, Validation Loss Force: 2.3676229033816893, time: 0.23431992530822754
Test Loss Energy: 9.710850568223803, Test Loss Force: 11.365082969462273, time: 9.21548080444336

Epoch 18, Batch 100/307, Loss: 1.1016868352890015
Epoch 18, Batch 200/307, Loss: 0.6402102708816528
Epoch 18, Batch 300/307, Loss: 0.1329507827758789

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.153458851003355, Training Loss Force: 4.815005143852193, time: 4.735387563705444
Validation Loss Energy: 3.0379105177531827, Validation Loss Force: 3.8204456931207034, time: 0.23356914520263672
Test Loss Energy: 9.552632855029898, Test Loss Force: 11.197942071204812, time: 9.402570724487305

Epoch 19, Batch 100/307, Loss: 1.1909987926483154
Epoch 19, Batch 200/307, Loss: 0.9389318227767944
Epoch 19, Batch 300/307, Loss: 0.6341020464897156

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.605242428205748, Training Loss Force: 3.865597687105259, time: 4.44986891746521
Validation Loss Energy: 10.903712971951236, Validation Loss Force: 3.0726282893773185, time: 0.23073220252990723
Test Loss Energy: 19.199399704715276, Test Loss Force: 11.911759724350471, time: 9.189525604248047

wandb: - 0.039 MB of 0.040 MB uploadedwandb: \ 0.039 MB of 0.040 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–‚â–‚â–ƒâ–‚â–‚â–â–„â–‚â–‚â–ƒâ–„â–‚â–„â–â–â–ˆ
wandb:   test_error_force â–„â–„â–…â–…â–†â–„â–„â–â–â–‡â–‚â–…â–‡â–†â–‚â–ˆâ–‚â–‚â–â–…
wandb:          test_loss â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–‚â–â–„â–ƒâ–„â–…â–…â–ƒâ–…â–„â–‚â–â–ˆ
wandb: train_error_energy â–â–â–â–â–â–‚â–†â–‡â–‡â–†â–†â–†â–‡â–†â–†â–†â–…â–„â–ˆâ–†
wandb:  train_error_force â–â–â–â–â–â–â–†â–†â–ˆâ–†â–†â–†â–†â–†â–†â–‡â–„â–‚â–ˆâ–†
wandb:         train_loss â–â–â–â–â–â–â–†â–†â–ˆâ–†â–†â–†â–‡â–†â–†â–†â–„â–ƒâ–ˆâ–†
wandb: valid_error_energy â–â–â–â–‚â–â–‚â–‚â–‚â–†â–â–…â–‡â–‚â–„â–„â–†â–†â–„â–ƒâ–ˆ
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–„â–ˆâ–…â–ƒâ–…â–‡â–„â–…â–†â–‚â–‚â–…â–ƒ
wandb:         valid_loss â–â–â–â–‚â–â–â–‚â–ƒâ–ˆâ–„â–„â–†â–†â–„â–…â–†â–„â–ƒâ–„â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 9810
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 19.1994
wandb:   test_error_force 11.91176
wandb:          test_loss 5.27055
wandb: train_error_energy 7.60524
wandb:  train_error_force 3.8656
wandb:         train_loss 1.80239
wandb: valid_error_energy 10.90371
wandb:  valid_error_force 3.07263
wandb:         valid_loss 1.75779
wandb: 
wandb: ğŸš€ View run al_77_100 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/9q9vy956
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_034812-9q9vy956/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.47277942299842834, Uncertainty Bias: -0.40443387627601624
3.0517578e-05 0.0020618439
-5.1835423 34.06274
(48745, 22, 3)
Found uncertainty sample 0 after 12 steps.
Found uncertainty sample 1 after 13 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 13 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 21 steps.
Found uncertainty sample 6 after 3 steps.
Found uncertainty sample 7 after 13 steps.
Found uncertainty sample 8 after 12 steps.
Found uncertainty sample 9 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 2 steps.
Found uncertainty sample 12 after 5 steps.
Found uncertainty sample 13 after 2 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 5 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 16 steps.
Found uncertainty sample 20 after 4 steps.
Found uncertainty sample 21 after 25 steps.
Found uncertainty sample 22 after 6 steps.
Found uncertainty sample 23 after 3 steps.
Found uncertainty sample 24 after 13 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 6 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 6 steps.
Found uncertainty sample 30 after 5 steps.
Found uncertainty sample 31 after 3 steps.
Found uncertainty sample 32 after 9 steps.
Found uncertainty sample 33 after 37 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 14 steps.
Found uncertainty sample 37 after 5 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 5 steps.
Found uncertainty sample 40 after 15 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 6 steps.
Found uncertainty sample 43 after 1 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 6 steps.
Found uncertainty sample 46 after 7 steps.
Found uncertainty sample 47 after 12 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 10 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 15 steps.
Found uncertainty sample 56 after 1 steps.
Found uncertainty sample 57 after 11 steps.
Found uncertainty sample 58 after 6 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 17 steps.
Found uncertainty sample 61 after 4 steps.
Found uncertainty sample 62 after 32 steps.
Found uncertainty sample 63 after 3 steps.
Found uncertainty sample 64 after 21 steps.
Found uncertainty sample 65 after 6 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 26 steps.
Found uncertainty sample 68 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 22 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 49 steps.
Found uncertainty sample 74 after 13 steps.
Found uncertainty sample 75 after 3 steps.
Found uncertainty sample 76 after 5 steps.
Found uncertainty sample 77 after 2 steps.
Found uncertainty sample 78 after 7 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 17 steps.
Found uncertainty sample 82 after 12 steps.
Found uncertainty sample 83 after 28 steps.
Found uncertainty sample 84 after 14 steps.
Found uncertainty sample 85 after 5 steps.
Found uncertainty sample 86 after 3 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 5 steps.
Found uncertainty sample 89 after 10 steps.
Found uncertainty sample 90 after 8 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 9 steps.
Found uncertainty sample 93 after 3 steps.
Found uncertainty sample 94 after 8 steps.
Found uncertainty sample 95 after 9 steps.
Found uncertainty sample 96 after 3 steps.
Found uncertainty sample 97 after 3 steps.
Found uncertainty sample 98 after 8 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_035851-8k6fgn3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_101
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/8k6fgn3i
Training model 101. Added 100 samples to the dataset.
Epoch 0, Batch 100/310, Loss: 0.06439201533794403
Epoch 0, Batch 200/310, Loss: 0.06684920191764832
Epoch 0, Batch 300/310, Loss: 0.040333013981580734

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4852860172260094, Training Loss Force: 1.8297419012477363, time: 4.522922992706299
Validation Loss Energy: 1.4829805199236936, Validation Loss Force: 1.903423132050411, time: 0.26547718048095703
Test Loss Energy: 11.209871000618245, Test Loss Force: 11.960125701195517, time: 10.268397569656372

Epoch 1, Batch 100/310, Loss: 0.09098366647958755
Epoch 1, Batch 200/310, Loss: 0.05405235290527344
Epoch 1, Batch 300/310, Loss: 0.08057187497615814

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.2577998368027095, Training Loss Force: 1.7591462808398282, time: 4.458521366119385
Validation Loss Energy: 1.2826137375167164, Validation Loss Force: 1.8652988564972994, time: 0.26965999603271484
Test Loss Energy: 10.313434532684168, Test Loss Force: 11.954589334826045, time: 10.443646907806396

Epoch 2, Batch 100/310, Loss: 0.04469958320260048
Epoch 2, Batch 200/310, Loss: 0.08418860286474228
Epoch 2, Batch 300/310, Loss: 0.08115297555923462

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3191757786426732, Training Loss Force: 1.7607155348220225, time: 4.437086582183838
Validation Loss Energy: 0.8235354456094353, Validation Loss Force: 1.8451536448614934, time: 0.2637920379638672
Test Loss Energy: 10.47109492016131, Test Loss Force: 12.030931439849319, time: 10.545774698257446

Epoch 3, Batch 100/310, Loss: 0.051604218780994415
Epoch 3, Batch 200/310, Loss: 0.08023674041032791
Epoch 3, Batch 300/310, Loss: 0.11442681401968002

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.2182890565941218, Training Loss Force: 1.7218482073009367, time: 4.475515842437744
Validation Loss Energy: 1.2357121114341063, Validation Loss Force: 1.8779661924723268, time: 0.276780366897583
Test Loss Energy: 10.450974419554568, Test Loss Force: 11.94263520532749, time: 10.708316326141357

Epoch 4, Batch 100/310, Loss: 0.11766241490840912
Epoch 4, Batch 200/310, Loss: 0.04702824726700783
Epoch 4, Batch 300/310, Loss: 0.3025764524936676

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5286120953388567, Training Loss Force: 1.769299313675809, time: 4.41878080368042
Validation Loss Energy: 3.038561557802047, Validation Loss Force: 1.959763903631131, time: 0.28797125816345215
Test Loss Energy: 10.268985149897848, Test Loss Force: 11.829512312333263, time: 10.530838012695312

Epoch 5, Batch 100/310, Loss: 0.0721168965101242
Epoch 5, Batch 200/310, Loss: 0.09323237836360931
Epoch 5, Batch 300/310, Loss: 0.08175761997699738

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2281886969700497, Training Loss Force: 1.7379899562664356, time: 4.486993074417114
Validation Loss Energy: 1.5632352007019619, Validation Loss Force: 1.847513826473755, time: 0.2712862491607666
Test Loss Energy: 11.587182977716301, Test Loss Force: 12.258228280165293, time: 10.633567333221436

Epoch 6, Batch 100/310, Loss: 0.43170005083084106
Epoch 6, Batch 200/310, Loss: 0.33766594529151917
Epoch 6, Batch 300/310, Loss: 0.10604865103960037

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 5.3412984285608935, Training Loss Force: 3.1444767297215734, time: 4.5380048751831055
Validation Loss Energy: 3.6155544963376194, Validation Loss Force: 2.6893495362434967, time: 0.2607712745666504
Test Loss Energy: 10.251011706802636, Test Loss Force: 11.805235280587894, time: 10.402170419692993

Epoch 7, Batch 100/310, Loss: 0.779973566532135
Epoch 7, Batch 200/310, Loss: 0.07011217623949051
Epoch 7, Batch 300/310, Loss: 1.0360195636749268

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.405766628188996, Training Loss Force: 4.537914133388871, time: 4.48608922958374
Validation Loss Energy: 5.863822996660363, Validation Loss Force: 3.450010763526972, time: 0.26456403732299805
Test Loss Energy: 12.5698264140476, Test Loss Force: 11.64178734195499, time: 10.43550181388855

Epoch 8, Batch 100/310, Loss: 0.18790696561336517
Epoch 8, Batch 200/310, Loss: 0.3817608952522278
Epoch 8, Batch 300/310, Loss: 0.9123982191085815

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.272366827580112, Training Loss Force: 3.080579162429314, time: 4.366058826446533
Validation Loss Energy: 5.256155156581778, Validation Loss Force: 3.556261844423163, time: 0.3060767650604248
Test Loss Energy: 10.067322974928686, Test Loss Force: 11.331861157243615, time: 11.588980197906494

Epoch 9, Batch 100/310, Loss: 0.0696176290512085
Epoch 9, Batch 200/310, Loss: 0.32428187131881714
Epoch 9, Batch 300/310, Loss: 0.4192781448364258

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 5.842218842594259, Training Loss Force: 3.0556084575640297, time: 4.394038438796997
Validation Loss Energy: 3.777958460589197, Validation Loss Force: 2.7117237328307837, time: 0.2682218551635742
Test Loss Energy: 12.172091759156292, Test Loss Force: 12.087109722875995, time: 10.603251934051514

Epoch 10, Batch 100/310, Loss: 0.15490949153900146
Epoch 10, Batch 200/310, Loss: 0.18136663734912872
Epoch 10, Batch 300/310, Loss: 1.0215147733688354

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.153216715143108, Training Loss Force: 3.2543884694690473, time: 4.4690492153167725
Validation Loss Energy: 2.892829099709057, Validation Loss Force: 4.214833187275715, time: 0.29912877082824707
Test Loss Energy: 9.596728336102652, Test Loss Force: 11.505322287212636, time: 11.769134998321533

Epoch 11, Batch 100/310, Loss: 0.8754445314407349
Epoch 11, Batch 200/310, Loss: 1.177901029586792
Epoch 11, Batch 300/310, Loss: 0.7812176942825317

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.070654874406378, Training Loss Force: 4.030147678350184, time: 4.8501410484313965
Validation Loss Energy: 4.531944334080732, Validation Loss Force: 3.6222796389505887, time: 0.3049774169921875
Test Loss Energy: 12.001261398695405, Test Loss Force: 11.546257279371353, time: 11.886025428771973

Epoch 12, Batch 100/310, Loss: 0.4283486604690552
Epoch 12, Batch 200/310, Loss: 0.2733820974826813
Epoch 12, Batch 300/310, Loss: 1.7644437551498413

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.993097196592249, Training Loss Force: 3.957016833600395, time: 4.595045804977417
Validation Loss Energy: 4.0449237141223495, Validation Loss Force: 4.89943713624932, time: 0.3012845516204834
Test Loss Energy: 9.64992880751629, Test Loss Force: 12.478185175998835, time: 11.738943338394165

Epoch 13, Batch 100/310, Loss: 0.13317066431045532
Epoch 13, Batch 200/310, Loss: 0.782465934753418
Epoch 13, Batch 300/310, Loss: 0.46732890605926514

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.525842936800359, Training Loss Force: 4.446637316958002, time: 4.726165056228638
Validation Loss Energy: 10.77519742582287, Validation Loss Force: 3.8227463998040876, time: 0.29754042625427246
Test Loss Energy: 15.060235654282902, Test Loss Force: 12.418644145861974, time: 11.886701107025146

Epoch 14, Batch 100/310, Loss: 0.32862526178359985
Epoch 14, Batch 200/310, Loss: 0.21497800946235657
Epoch 14, Batch 300/310, Loss: 1.5265135765075684

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.938869514802708, Training Loss Force: 4.477060180731313, time: 4.595655918121338
Validation Loss Energy: 11.213939545034798, Validation Loss Force: 6.395381988254879, time: 0.29391908645629883
Test Loss Energy: 16.188245851591088, Test Loss Force: 13.385515523217967, time: 11.787146806716919

Epoch 15, Batch 100/310, Loss: 0.12006816267967224
Epoch 15, Batch 200/310, Loss: 0.9222625494003296
Epoch 15, Batch 300/310, Loss: 1.1060900688171387

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.43681983706892, Training Loss Force: 4.20274290126551, time: 4.819749355316162
Validation Loss Energy: 1.7635039864178164, Validation Loss Force: 3.7457441136388985, time: 0.3164031505584717
Test Loss Energy: 9.166437600775112, Test Loss Force: 11.782893437244605, time: 11.912103176116943

Epoch 16, Batch 100/310, Loss: 0.7836021780967712
Epoch 16, Batch 200/310, Loss: 0.3297944664955139
Epoch 16, Batch 300/310, Loss: 0.14315621554851532

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.630890149117069, Training Loss Force: 3.660588940353078, time: 4.6456568241119385
Validation Loss Energy: 5.160903352554639, Validation Loss Force: 3.557624267859437, time: 0.2974271774291992
Test Loss Energy: 12.093803642378317, Test Loss Force: 11.73448247890047, time: 11.97000765800476

Epoch 17, Batch 100/310, Loss: 0.37319421768188477
Epoch 17, Batch 200/310, Loss: 0.3690049350261688
Epoch 17, Batch 300/310, Loss: 0.3725871443748474

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 4.764452991813884, Training Loss Force: 2.4116601042333463, time: 4.651264667510986
Validation Loss Energy: 7.910723994900501, Validation Loss Force: 2.3213844973502256, time: 0.2998661994934082
Test Loss Energy: 10.669140822293441, Test Loss Force: 12.27264536824245, time: 11.717995405197144

Epoch 18, Batch 100/310, Loss: 0.24111270904541016
Epoch 18, Batch 200/310, Loss: 0.09973111748695374
Epoch 18, Batch 300/310, Loss: 1.0380427837371826

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.752899632495416, Training Loss Force: 3.8717152230281853, time: 4.7296130657196045
Validation Loss Energy: 10.22570038596773, Validation Loss Force: 5.854498948843782, time: 0.2886936664581299
Test Loss Energy: 16.096267928467224, Test Loss Force: 12.547576911948871, time: 11.82382345199585

Epoch 19, Batch 100/310, Loss: 0.6380166411399841
Epoch 19, Batch 200/310, Loss: 0.05922010913491249
Epoch 19, Batch 300/310, Loss: 0.7101178169250488

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.490934807866946, Training Loss Force: 4.053095214228719, time: 4.808540344238281
Validation Loss Energy: 14.513047651035654, Validation Loss Force: 4.4365567018745775, time: 0.2974507808685303
Test Loss Energy: 18.273413632821665, Test Loss Force: 11.811487317250446, time: 11.722536325454712

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.039 MB of 0.051 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–„â–‚â–ƒâ–â–ƒâ–â–†â–†â–â–ƒâ–‚â–†â–ˆ
wandb:   test_error_force â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–‚â–â–„â–‚â–‚â–…â–…â–ˆâ–ƒâ–‚â–„â–…â–ƒ
wandb:          test_loss â–ƒâ–‚â–ƒâ–‚â–‚â–„â–‚â–ƒâ–â–„â–â–‚â–ƒâ–…â–ˆâ–‚â–ƒâ–ƒâ–†â–†
wandb: train_error_energy â–â–â–â–â–â–â–„â–‡â–…â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–†â–„â–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–…â–ˆâ–„â–„â–…â–‡â–‡â–ˆâ–ˆâ–‡â–†â–ƒâ–†â–‡
wandb:         train_loss â–â–â–â–â–â–â–…â–ˆâ–…â–…â–†â–‡â–‡â–ˆâ–ˆâ–‡â–†â–ƒâ–‡â–‡
wandb: valid_error_energy â–â–â–â–â–‚â–â–‚â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–†â–†â–â–ƒâ–…â–†â–ˆ
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–ƒâ–„â–‚â–…â–„â–†â–„â–ˆâ–„â–„â–‚â–‡â–…
wandb:         valid_loss â–â–â–â–â–‚â–â–‚â–„â–„â–ƒâ–„â–„â–…â–…â–ˆâ–ƒâ–„â–ƒâ–‡â–‡
wandb: 
wandb: Run summary:
wandb:       dataset_size 9900
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 18.27341
wandb:   test_error_force 11.81149
wandb:          test_loss 5.17503
wandb: train_error_energy 8.49093
wandb:  train_error_force 4.0531
wandb:         train_loss 1.9244
wandb: valid_error_energy 14.51305
wandb:  valid_error_force 4.43656
wandb:         valid_loss 2.45571
wandb: 
wandb: ğŸš€ View run al_77_101 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/8k6fgn3i
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_035851-8k6fgn3i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.37948861718177795, Uncertainty Bias: -0.2802862524986267
1.335144e-05 0.0025718212
-4.1195083 24.826576
(48745, 22, 3)
Found uncertainty sample 0 after 5 steps.
Found uncertainty sample 1 after 4 steps.
Found uncertainty sample 2 after 7 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 5 steps.
Found uncertainty sample 5 after 16 steps.
Found uncertainty sample 6 after 23 steps.
Found uncertainty sample 7 after 4 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 13 steps.
Found uncertainty sample 12 after 32 steps.
Found uncertainty sample 13 after 14 steps.
Found uncertainty sample 14 after 6 steps.
Found uncertainty sample 15 after 15 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 2 steps.
Found uncertainty sample 18 after 4 steps.
Found uncertainty sample 19 after 12 steps.
Found uncertainty sample 20 after 14 steps.
Found uncertainty sample 21 after 3 steps.
Found uncertainty sample 22 after 29 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 2 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 10 steps.
Found uncertainty sample 27 after 19 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 9 steps.
Found uncertainty sample 30 after 2 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 10 steps.
Found uncertainty sample 33 after 7 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 3 steps.
Found uncertainty sample 38 after 8 steps.
Found uncertainty sample 39 after 8 steps.
Found uncertainty sample 40 after 5 steps.
Found uncertainty sample 41 after 1 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 3 steps.
Found uncertainty sample 44 after 4 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 6 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 50 after 1 steps.
Found uncertainty sample 51 after 15 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 7 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 11 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 5 steps.
Found uncertainty sample 64 after 8 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 13 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 9 steps.
Found uncertainty sample 71 after 6 steps.
Found uncertainty sample 72 after 1 steps.
Found uncertainty sample 73 after 6 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 27 steps.
Found uncertainty sample 78 after 1 steps.
Found uncertainty sample 79 after 11 steps.
Found uncertainty sample 80 after 3 steps.
Found uncertainty sample 81 after 16 steps.
Found uncertainty sample 82 after 4 steps.
Found uncertainty sample 83 after 1 steps.
Found uncertainty sample 84 after 8 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 3 steps.
Found uncertainty sample 87 after 5 steps.
Found uncertainty sample 88 after 2 steps.
Found uncertainty sample 89 after 5 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 7 steps.
Found uncertainty sample 92 after 6 steps.
Found uncertainty sample 93 after 5 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 12 steps.
Found uncertainty sample 97 after 68 steps.
Found uncertainty sample 98 after 29 steps.
Found uncertainty sample 99 after 1 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_040929-qqxqqtuc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_102
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/qqxqqtuc
Training model 102. Added 100 samples to the dataset.
Epoch 0, Batch 100/313, Loss: 0.10438463091850281
Epoch 0, Batch 200/313, Loss: 0.09881570935249329
Epoch 0, Batch 300/313, Loss: 0.047489557415246964

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.7967961228190066, Training Loss Force: 1.8032891643625, time: 4.502235174179077
Validation Loss Energy: 2.1798782996469463, Validation Loss Force: 1.9027825797183204, time: 0.2740788459777832
Test Loss Energy: 11.300884647427926, Test Loss Force: 11.880507557126966, time: 10.196645736694336

Epoch 1, Batch 100/313, Loss: 0.22634460031986237
Epoch 1, Batch 200/313, Loss: 0.2142726480960846
Epoch 1, Batch 300/313, Loss: 0.08739558607339859

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 2.085507592076936, Training Loss Force: 1.8072370751588438, time: 4.487138271331787
Validation Loss Energy: 1.1625263076293764, Validation Loss Force: 1.833723614529643, time: 0.25516247749328613
Test Loss Energy: 10.125396995446982, Test Loss Force: 11.587659223884579, time: 10.18929147720337

Epoch 2, Batch 100/313, Loss: 0.07668459415435791
Epoch 2, Batch 200/313, Loss: 0.09961843490600586
Epoch 2, Batch 300/313, Loss: 0.06012130528688431

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.1790315608224804, Training Loss Force: 1.7376718885982914, time: 4.506971836090088
Validation Loss Energy: 1.9833489527670587, Validation Loss Force: 1.8368705307682966, time: 0.2527942657470703
Test Loss Energy: 10.325853408830294, Test Loss Force: 12.216881385060814, time: 10.169647455215454

Epoch 3, Batch 100/313, Loss: 0.10626813769340515
Epoch 3, Batch 200/313, Loss: 0.09243215620517731
Epoch 3, Batch 300/313, Loss: 0.09135417640209198

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.0776387599023902, Training Loss Force: 1.7090055346032527, time: 4.497582912445068
Validation Loss Energy: 1.1717147944699156, Validation Loss Force: 1.8580080451860148, time: 0.27245616912841797
Test Loss Energy: 11.26017816744005, Test Loss Force: 12.174913408583805, time: 10.39092469215393

Epoch 4, Batch 100/313, Loss: 0.07056108117103577
Epoch 4, Batch 200/313, Loss: 0.07112281024456024
Epoch 4, Batch 300/313, Loss: 0.19378849864006042

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.4058340572828094, Training Loss Force: 1.7106338884569143, time: 4.527677297592163
Validation Loss Energy: 3.133243729620072, Validation Loss Force: 2.154624679266008, time: 0.2585482597351074
Test Loss Energy: 12.13007179234822, Test Loss Force: 12.12076158266129, time: 10.246780395507812

Epoch 5, Batch 100/313, Loss: 0.15678253769874573
Epoch 5, Batch 200/313, Loss: 0.13862542808055878
Epoch 5, Batch 300/313, Loss: 0.1722741723060608

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.7995909390421778, Training Loss Force: 1.740059981752425, time: 4.489751577377319
Validation Loss Energy: 2.214172109217235, Validation Loss Force: 1.8244768106397848, time: 0.2548050880432129
Test Loss Energy: 10.514186268308562, Test Loss Force: 11.919767345181487, time: 10.221536636352539

Epoch 6, Batch 100/313, Loss: 0.33272358775138855
Epoch 6, Batch 200/313, Loss: 2.165292263031006
Epoch 6, Batch 300/313, Loss: 0.22075407207012177

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 10.216535965144454, Training Loss Force: 4.955288576090734, time: 4.494633913040161
Validation Loss Energy: 4.089521226683643, Validation Loss Force: 4.926371641479037, time: 0.2918410301208496
Test Loss Energy: 10.652080942905883, Test Loss Force: 12.587605004153017, time: 10.038689613342285

Epoch 7, Batch 100/313, Loss: 1.4401402473449707
Epoch 7, Batch 200/313, Loss: 0.6040419340133667
Epoch 7, Batch 300/313, Loss: 1.127363681793213

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.48782981365973, Training Loss Force: 4.168754917495412, time: 4.507349014282227
Validation Loss Energy: 3.0652437343555983, Validation Loss Force: 5.227676040020588, time: 0.25470399856567383
Test Loss Energy: 8.810507403932828, Test Loss Force: 12.425113973933767, time: 10.259428024291992

Epoch 8, Batch 100/313, Loss: 0.6839885711669922
Epoch 8, Batch 200/313, Loss: 0.7044858932495117
Epoch 8, Batch 300/313, Loss: 0.11588175594806671

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.422819158787101, Training Loss Force: 4.273228553245542, time: 4.440251350402832
Validation Loss Energy: 6.30169465764703, Validation Loss Force: 4.043176562935742, time: 0.259427547454834
Test Loss Energy: 10.654275220266033, Test Loss Force: 12.21024972431802, time: 10.093982219696045

Epoch 9, Batch 100/313, Loss: 0.903938889503479
Epoch 9, Batch 200/313, Loss: 0.1347486674785614
Epoch 9, Batch 300/313, Loss: 0.21713219583034515

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.378475181483335, Training Loss Force: 4.707182312336127, time: 4.505021333694458
Validation Loss Energy: 15.384927039853531, Validation Loss Force: 3.8339646049774165, time: 0.26018333435058594
Test Loss Energy: 13.206378218635205, Test Loss Force: 12.51136893161226, time: 10.245447397232056

Epoch 10, Batch 100/313, Loss: 2.0450439453125
Epoch 10, Batch 200/313, Loss: 0.20728611946105957
Epoch 10, Batch 300/313, Loss: 0.6766738891601562

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.697035954760105, Training Loss Force: 4.828364707860065, time: 4.597021579742432
Validation Loss Energy: 5.145819299271199, Validation Loss Force: 6.477598384208771, time: 0.2555694580078125
Test Loss Energy: 10.335758846952698, Test Loss Force: 14.627707035748434, time: 10.061050653457642

Epoch 11, Batch 100/313, Loss: 0.5972824096679688
Epoch 11, Batch 200/313, Loss: 0.17021940648555756
Epoch 11, Batch 300/313, Loss: 0.7434831857681274

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.28751691919265, Training Loss Force: 4.507317611433418, time: 4.500354766845703
Validation Loss Energy: 11.773186287045878, Validation Loss Force: 4.91666682942292, time: 0.2540853023529053
Test Loss Energy: 11.809559650635428, Test Loss Force: 12.987657912989055, time: 10.114675283432007

Epoch 12, Batch 100/313, Loss: 0.6079047918319702
Epoch 12, Batch 200/313, Loss: 0.5925326347351074
Epoch 12, Batch 300/313, Loss: 0.9842922687530518

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.966870277986276, Training Loss Force: 4.3928298541299835, time: 4.596363544464111
Validation Loss Energy: 11.306565278222523, Validation Loss Force: 6.988176330994413, time: 0.2647287845611572
Test Loss Energy: 15.622302717634108, Test Loss Force: 13.053835374351149, time: 10.14209794998169

Epoch 13, Batch 100/313, Loss: 0.12802676856517792
Epoch 13, Batch 200/313, Loss: 0.22090837359428406
Epoch 13, Batch 300/313, Loss: 0.09271751344203949

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.524222930309683, Training Loss Force: 4.691063060233628, time: 4.59615683555603
Validation Loss Energy: 4.212797635011302, Validation Loss Force: 2.8957447740396645, time: 0.25884294509887695
Test Loss Energy: 10.108982031191225, Test Loss Force: 10.823572768295406, time: 10.19876217842102

Epoch 14, Batch 100/313, Loss: 0.7937783002853394
Epoch 14, Batch 200/313, Loss: 0.6906167268753052
Epoch 14, Batch 300/313, Loss: 0.5186545848846436

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.90470687895535, Training Loss Force: 3.7414191244532167, time: 4.672612190246582
Validation Loss Energy: 12.393730638495175, Validation Loss Force: 3.4301374977190875, time: 0.2599513530731201
Test Loss Energy: 17.206009197226134, Test Loss Force: 11.575548722355228, time: 10.08621096611023

Epoch 15, Batch 100/313, Loss: 0.6184712052345276
Epoch 15, Batch 200/313, Loss: 0.6632217168807983
Epoch 15, Batch 300/313, Loss: 0.34619957208633423

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.483725893247737, Training Loss Force: 4.037990665256688, time: 4.431466341018677
Validation Loss Energy: 6.840709413856134, Validation Loss Force: 2.994954789338645, time: 0.2555582523345947
Test Loss Energy: 11.71480062018991, Test Loss Force: 10.971524440594436, time: 10.076137781143188

Epoch 16, Batch 100/313, Loss: 0.5261803865432739
Epoch 16, Batch 200/313, Loss: 0.6431865096092224
Epoch 16, Batch 300/313, Loss: 0.5598598718643188

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.529065262280909, Training Loss Force: 3.4457876444084468, time: 4.71049165725708
Validation Loss Energy: 11.626276844381342, Validation Loss Force: 4.9535940133806085, time: 0.2622518539428711
Test Loss Energy: 17.25785761692247, Test Loss Force: 11.897198859621856, time: 10.121438980102539

Epoch 17, Batch 100/313, Loss: 0.6664713621139526
Epoch 17, Batch 200/313, Loss: 0.5789228677749634
Epoch 17, Batch 300/313, Loss: 0.66269451379776

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.363203607820145, Training Loss Force: 4.289203875000009, time: 4.579050064086914
Validation Loss Energy: 7.4270190075926426, Validation Loss Force: 3.2292168035447006, time: 0.25558948516845703
Test Loss Energy: 10.205648960827478, Test Loss Force: 11.072977758937306, time: 10.311574220657349

Epoch 18, Batch 100/313, Loss: 1.0684548616409302
Epoch 18, Batch 200/313, Loss: 0.6449040770530701
Epoch 18, Batch 300/313, Loss: 0.5017428398132324

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.763151432762555, Training Loss Force: 3.759792580815473, time: 4.691558122634888
Validation Loss Energy: 1.4511774206190091, Validation Loss Force: 3.5655623338441047, time: 0.2649879455566406
Test Loss Energy: 9.534262944274232, Test Loss Force: 11.445444903089873, time: 10.096465349197388

Epoch 19, Batch 100/313, Loss: 0.9371540546417236
Epoch 19, Batch 200/313, Loss: 0.5247964262962341
Epoch 19, Batch 300/313, Loss: 0.2838208079338074

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.450564177274574, Training Loss Force: 3.9357682536990524, time: 4.494151830673218
Validation Loss Energy: 9.12476106859862, Validation Loss Force: 3.129391508861693, time: 0.2573986053466797
Test Loss Energy: 14.545940029499047, Test Loss Force: 11.547407539215323, time: 10.126673221588135

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.040 MB uploadedwandb: / 0.039 MB of 0.040 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–ƒâ–‚â–‚â–ƒâ–„â–‚â–ƒâ–â–ƒâ–…â–‚â–ƒâ–‡â–‚â–ˆâ–ƒâ–ˆâ–‚â–‚â–†
wandb:   test_error_force â–ƒâ–‚â–„â–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–ˆâ–…â–…â–â–‚â–â–ƒâ–â–‚â–‚
wandb:          test_loss â–ƒâ–‚â–„â–„â–„â–ƒâ–„â–ƒâ–„â–…â–ˆâ–†â–‡â–â–…â–‚â–†â–â–‚â–„
wandb: train_error_energy â–‚â–‚â–â–â–â–‚â–ˆâ–‡â–†â–‡â–ˆâ–‡â–‡â–‡â–†â–‡â–†â–‡â–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–‡â–‡â–ˆâ–‡â–‡â–‡â–…â–†â–…â–‡â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–†â–‡â–ˆâ–‡â–‡â–‡â–†â–‡â–…â–‡â–†â–†
wandb: valid_error_energy â–‚â–â–â–â–‚â–‚â–‚â–‚â–„â–ˆâ–ƒâ–†â–†â–ƒâ–‡â–„â–†â–„â–â–…
wandb:  valid_error_force â–â–â–â–â–â–â–…â–†â–„â–„â–‡â–…â–ˆâ–‚â–ƒâ–ƒâ–…â–ƒâ–ƒâ–ƒ
wandb:         valid_loss â–â–â–â–â–‚â–â–…â–…â–„â–†â–†â–†â–ˆâ–ƒâ–…â–ƒâ–†â–„â–ƒâ–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 9990
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 14.54594
wandb:   test_error_force 11.54741
wandb:          test_loss 4.83722
wandb: train_error_energy 8.45056
wandb:  train_error_force 3.93577
wandb:         train_loss 1.88244
wandb: valid_error_energy 9.12476
wandb:  valid_error_force 3.12939
wandb:         valid_loss 1.65774
wandb: 
wandb: ğŸš€ View run al_77_102 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/qqxqqtuc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_040929-qqxqqtuc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.4062424898147583, Uncertainty Bias: -0.3332085609436035
6.1035156e-05 0.0032920837
-2.1003563 26.45567
(48745, 22, 3)
Found uncertainty sample 0 after 1 steps.
Found uncertainty sample 1 after 37 steps.
Found uncertainty sample 2 after 15 steps.
Found uncertainty sample 3 after 26 steps.
Found uncertainty sample 4 after 41 steps.
Found uncertainty sample 5 after 18 steps.
Found uncertainty sample 6 after 8 steps.
Found uncertainty sample 7 after 27 steps.
Found uncertainty sample 8 after 23 steps.
Found uncertainty sample 9 after 4 steps.
Found uncertainty sample 10 after 5 steps.
Found uncertainty sample 11 after 1 steps.
Found uncertainty sample 12 after 9 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 8 steps.
Found uncertainty sample 15 after 31 steps.
Found uncertainty sample 16 after 50 steps.
Found uncertainty sample 17 after 51 steps.
Found uncertainty sample 18 after 45 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 58 steps.
Found uncertainty sample 22 after 14 steps.
Found uncertainty sample 23 after 27 steps.
Found uncertainty sample 24 after 18 steps.
Found uncertainty sample 25 after 23 steps.
Found uncertainty sample 26 after 24 steps.
Found uncertainty sample 27 after 11 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 63 steps.
Found uncertainty sample 30 after 14 steps.
Found uncertainty sample 31 after 13 steps.
Found uncertainty sample 32 after 2 steps.
Found uncertainty sample 33 after 9 steps.
Found uncertainty sample 34 after 5 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 15 steps.
Found uncertainty sample 37 after 50 steps.
Found uncertainty sample 38 after 26 steps.
Found uncertainty sample 39 after 6 steps.
Found uncertainty sample 40 after 3 steps.
Found uncertainty sample 41 after 6 steps.
Found uncertainty sample 42 after 16 steps.
Found uncertainty sample 43 after 5 steps.
Found uncertainty sample 44 after 5 steps.
Found uncertainty sample 45 after 11 steps.
Found uncertainty sample 46 after 6 steps.
Found uncertainty sample 47 after 8 steps.
Found uncertainty sample 48 after 5 steps.
Found uncertainty sample 49 after 18 steps.
Found uncertainty sample 50 after 13 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 10 steps.
Found uncertainty sample 53 after 31 steps.
Found uncertainty sample 54 after 11 steps.
Found uncertainty sample 55 after 15 steps.
Found uncertainty sample 56 after 5 steps.
Found uncertainty sample 57 after 38 steps.
Found uncertainty sample 58 after 8 steps.
Found uncertainty sample 59 after 15 steps.
Found uncertainty sample 60 after 68 steps.
Found uncertainty sample 61 after 13 steps.
Found uncertainty sample 62 after 3 steps.
Found uncertainty sample 63 after 14 steps.
Found uncertainty sample 64 after 8 steps.
Found uncertainty sample 65 after 3 steps.
Found uncertainty sample 66 after 10 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 35 steps.
Found uncertainty sample 69 after 17 steps.
Found uncertainty sample 70 after 45 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 11 steps.
Found uncertainty sample 73 after 41 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 19 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 27 steps.
Found uncertainty sample 79 after 22 steps.
Found uncertainty sample 80 after 16 steps.
Found uncertainty sample 81 after 6 steps.
Found uncertainty sample 82 after 29 steps.
Found uncertainty sample 83 after 18 steps.
Found uncertainty sample 84 after 4 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 21 steps.
Found uncertainty sample 90 after 19 steps.
Found uncertainty sample 91 after 5 steps.
Found uncertainty sample 92 after 24 steps.
Found uncertainty sample 93 after 6 steps.
Found uncertainty sample 94 after 6 steps.
Found uncertainty sample 95 after 9 steps.
Found uncertainty sample 96 after 30 steps.
Found uncertainty sample 97 after 6 steps.
Found uncertainty sample 98 after 11 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_042000-w0yfbvr1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_103
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/w0yfbvr1
Training model 103. Added 100 samples to the dataset.
Epoch 0, Batch 100/315, Loss: 0.05606342479586601
Epoch 0, Batch 200/315, Loss: 0.12239553034305573
Epoch 0, Batch 300/315, Loss: 0.045210257172584534

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.3664948105680272, Training Loss Force: 1.8377662121061449, time: 4.6604533195495605
Validation Loss Energy: 0.6440318826327651, Validation Loss Force: 1.8575791398771286, time: 0.264310359954834
Test Loss Energy: 10.314682659698095, Test Loss Force: 11.631231392826963, time: 10.33010721206665

Epoch 1, Batch 100/315, Loss: 0.07430122047662735
Epoch 1, Batch 200/315, Loss: 0.06790181249380112
Epoch 1, Batch 300/315, Loss: 0.03028351441025734

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.1773626124172778, Training Loss Force: 1.7608895845019785, time: 4.488687515258789
Validation Loss Energy: 0.9141248583241927, Validation Loss Force: 1.845272772854565, time: 0.26877427101135254
Test Loss Energy: 10.137058222919888, Test Loss Force: 11.6624169924681, time: 10.411536931991577

Epoch 2, Batch 100/315, Loss: 0.030526448041200638
Epoch 2, Batch 200/315, Loss: 0.1880982220172882
Epoch 2, Batch 300/315, Loss: 0.03887428715825081

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3558937573202094, Training Loss Force: 1.7528194636967112, time: 4.535445690155029
Validation Loss Energy: 0.7580482827902133, Validation Loss Force: 1.8567802697822837, time: 0.27730417251586914
Test Loss Energy: 10.601220451417099, Test Loss Force: 11.80703752968082, time: 10.365042924880981

Epoch 3, Batch 100/315, Loss: 0.08287495374679565
Epoch 3, Batch 200/315, Loss: 0.04586933180689812
Epoch 3, Batch 300/315, Loss: 0.028898421674966812

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.2683419100588382, Training Loss Force: 1.7409502573532583, time: 4.6224524974823
Validation Loss Energy: 0.6048820931946258, Validation Loss Force: 1.83160244597862, time: 0.2645454406738281
Test Loss Energy: 10.700471462320877, Test Loss Force: 11.796346594086465, time: 10.513439178466797

Epoch 4, Batch 100/315, Loss: 0.06548982113599777
Epoch 4, Batch 200/315, Loss: 0.09541218727827072
Epoch 4, Batch 300/315, Loss: 0.03895726054906845

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.3352681613896376, Training Loss Force: 1.7291865961849742, time: 4.620059013366699
Validation Loss Energy: 2.7510372620253807, Validation Loss Force: 1.8859092685127776, time: 0.26792359352111816
Test Loss Energy: 10.62613364541496, Test Loss Force: 12.01266534654278, time: 10.266148805618286

Epoch 5, Batch 100/315, Loss: 0.04782266914844513
Epoch 5, Batch 200/315, Loss: 0.022577013820409775
Epoch 5, Batch 300/315, Loss: 0.20713287591934204

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.1637600509196613, Training Loss Force: 1.7372997790824056, time: 4.518797397613525
Validation Loss Energy: 1.866716457347934, Validation Loss Force: 1.8548748453667503, time: 0.2911946773529053
Test Loss Energy: 11.358162864739972, Test Loss Force: 11.99655461595717, time: 10.478889226913452

Epoch 6, Batch 100/315, Loss: 0.2751297950744629
Epoch 6, Batch 200/315, Loss: 0.07312226295471191
Epoch 6, Batch 300/315, Loss: 0.2714482545852661

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 7.92272482981219, Training Loss Force: 4.149467363412808, time: 4.445465564727783
Validation Loss Energy: 24.51011135796473, Validation Loss Force: 4.443914492064957, time: 0.26389336585998535
Test Loss Energy: 26.54575776735942, Test Loss Force: 12.167233864508658, time: 10.419074058532715

Epoch 7, Batch 100/315, Loss: 0.3284558057785034
Epoch 7, Batch 200/315, Loss: 0.3684002757072449
Epoch 7, Batch 300/315, Loss: 0.1447327435016632

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.185042738458693, Training Loss Force: 4.099880208173731, time: 4.512132883071899
Validation Loss Energy: 9.74320752042264, Validation Loss Force: 3.648075948573981, time: 0.27440857887268066
Test Loss Energy: 10.807647746109112, Test Loss Force: 12.138807445690242, time: 10.480124473571777

Epoch 8, Batch 100/315, Loss: 0.4388199746608734
Epoch 8, Batch 200/315, Loss: 0.8946778774261475
Epoch 8, Batch 300/315, Loss: 1.099024772644043

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.276171446414873, Training Loss Force: 4.72123133517944, time: 4.470351219177246
Validation Loss Energy: 15.091035475315232, Validation Loss Force: 3.8928951077929357, time: 0.2694063186645508
Test Loss Energy: 19.853441958538625, Test Loss Force: 12.28663162066741, time: 11.770191669464111

Epoch 9, Batch 100/315, Loss: 1.0158791542053223
Epoch 9, Batch 200/315, Loss: 0.326381653547287
Epoch 9, Batch 300/315, Loss: 0.19123053550720215

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.63099841686222, Training Loss Force: 4.358410786269018, time: 4.517658948898315
Validation Loss Energy: 6.4196866818035305, Validation Loss Force: 5.122315323971304, time: 0.263397216796875
Test Loss Energy: 9.782238221796602, Test Loss Force: 11.878315564748627, time: 10.577467441558838

Epoch 10, Batch 100/315, Loss: 0.6767364740371704
Epoch 10, Batch 200/315, Loss: 0.3951922655105591
Epoch 10, Batch 300/315, Loss: 0.9617730379104614

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.624853122971338, Training Loss Force: 3.7624459430938453, time: 4.6225810050964355
Validation Loss Energy: 5.669755524391682, Validation Loss Force: 5.30380608660735, time: 0.2652323246002197
Test Loss Energy: 11.872147205052661, Test Loss Force: 11.993542650860888, time: 10.386127471923828

Epoch 11, Batch 100/315, Loss: 0.34108519554138184
Epoch 11, Batch 200/315, Loss: 0.9327751994132996
Epoch 11, Batch 300/315, Loss: 0.32202333211898804

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.8228858823295475, Training Loss Force: 4.3718679410148304, time: 4.56153416633606
Validation Loss Energy: 1.2252725134633426, Validation Loss Force: 3.423718213444965, time: 0.26452159881591797
Test Loss Energy: 9.902585625722718, Test Loss Force: 10.852250255319408, time: 10.454885482788086

Epoch 12, Batch 100/315, Loss: 0.39284372329711914
Epoch 12, Batch 200/315, Loss: 0.24346241354942322
Epoch 12, Batch 300/315, Loss: 0.16199055314064026

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 4.8999346807362105, Training Loss Force: 2.636000685857153, time: 4.451345682144165
Validation Loss Energy: 4.1178060866819255, Validation Loss Force: 2.8564979119873586, time: 0.26923489570617676
Test Loss Energy: 9.65300048991273, Test Loss Force: 11.586390000426334, time: 10.376803159713745

Epoch 13, Batch 100/315, Loss: 0.912912130355835
Epoch 13, Batch 200/315, Loss: 0.13851645588874817
Epoch 13, Batch 300/315, Loss: 0.3928469717502594

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.882058283588137, Training Loss Force: 4.821761191798244, time: 4.523933410644531
Validation Loss Energy: 9.45306980141405, Validation Loss Force: 3.3604029988754944, time: 0.260181188583374
Test Loss Energy: 10.457784043074664, Test Loss Force: 10.81613664473162, time: 10.46882963180542

Epoch 14, Batch 100/315, Loss: 0.5456333756446838
Epoch 14, Batch 200/315, Loss: 0.47264620661735535
Epoch 14, Batch 300/315, Loss: 0.9521520137786865

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.908048752351689, Training Loss Force: 3.518556170252571, time: 4.598567247390747
Validation Loss Energy: 5.149670139480875, Validation Loss Force: 3.37296327552516, time: 0.26901698112487793
Test Loss Energy: 9.771291620053663, Test Loss Force: 11.557955808006527, time: 10.39121699333191

Epoch 15, Batch 100/315, Loss: 0.24443009495735168
Epoch 15, Batch 200/315, Loss: 0.15618060529232025
Epoch 15, Batch 300/315, Loss: 0.08446775376796722

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.831811302171399, Training Loss Force: 5.015248376962458, time: 4.558435916900635
Validation Loss Energy: 6.701325119512058, Validation Loss Force: 2.688476806792947, time: 0.26574230194091797
Test Loss Energy: 9.885413754247022, Test Loss Force: 10.69461077775746, time: 10.575750827789307

Epoch 16, Batch 100/315, Loss: 0.39043113589286804
Epoch 16, Batch 200/315, Loss: 0.4293994903564453
Epoch 16, Batch 300/315, Loss: 0.2606775760650635

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 5.6672018166066565, Training Loss Force: 2.8662341027208966, time: 4.456943035125732
Validation Loss Energy: 2.079928310242219, Validation Loss Force: 4.809572322335783, time: 0.2625770568847656
Test Loss Energy: 9.80926569547507, Test Loss Force: 11.722447036692015, time: 10.399972438812256

Epoch 17, Batch 100/315, Loss: 0.4127195477485657
Epoch 17, Batch 200/315, Loss: 0.20495998859405518
Epoch 17, Batch 300/315, Loss: 1.0373635292053223

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.796148933053972, Training Loss Force: 3.8563075077136237, time: 4.479812383651733
Validation Loss Energy: 0.9549040069707442, Validation Loss Force: 3.6118629091011116, time: 0.2685065269470215
Test Loss Energy: 10.230671073846105, Test Loss Force: 11.66896146949671, time: 10.548995971679688

Epoch 18, Batch 100/315, Loss: 0.8327776789665222
Epoch 18, Batch 200/315, Loss: 0.10809694230556488
Epoch 18, Batch 300/315, Loss: 0.5434405207633972

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 9.478764146240763, Training Loss Force: 4.5711123118769414, time: 4.482693195343018
Validation Loss Energy: 5.398584496030246, Validation Loss Force: 3.5373227647501055, time: 0.27545595169067383
Test Loss Energy: 10.159957513223874, Test Loss Force: 11.145472187751226, time: 10.312675952911377

Epoch 19, Batch 100/315, Loss: 1.770338773727417
Epoch 19, Batch 200/315, Loss: 0.8124169111251831
Epoch 19, Batch 300/315, Loss: 0.3183545470237732

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.276715437275275, Training Loss Force: 4.35405468785376, time: 4.522768974304199
Validation Loss Energy: 7.930890159702938, Validation Loss Force: 3.1203615624530543, time: 0.26756978034973145
Test Loss Energy: 14.276953164422146, Test Loss Force: 13.183333416958876, time: 10.576666831970215

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–‚â–ˆâ–â–…â–â–‚â–â–â–â–â–â–â–â–â–ƒ
wandb:   test_error_force â–„â–„â–„â–„â–…â–…â–…â–…â–…â–„â–…â–â–„â–â–ƒâ–â–„â–„â–‚â–ˆ
wandb:          test_loss â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒâ–†â–ƒâ–ƒâ–â–‚â–â–‚â–â–‚â–ƒâ–‚â–†
wandb: train_error_energy â–â–â–â–â–â–â–†â–†â–‡â–‡â–†â–†â–„â–‡â–†â–ˆâ–„â–†â–‡â–†
wandb:  train_error_force â–â–â–â–â–â–â–†â–†â–‡â–‡â–…â–‡â–ƒâ–ˆâ–…â–ˆâ–ƒâ–†â–‡â–‡
wandb:         train_loss â–â–â–â–â–â–â–†â–†â–‡â–‡â–†â–†â–ƒâ–‡â–…â–ˆâ–„â–†â–‡â–†
wandb: valid_error_energy â–â–â–â–â–‚â–â–ˆâ–„â–…â–ƒâ–‚â–â–‚â–„â–‚â–ƒâ–â–â–‚â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–†â–…â–…â–ˆâ–ˆâ–„â–ƒâ–„â–„â–ƒâ–‡â–…â–„â–„
wandb:         valid_loss â–â–â–â–â–â–â–ˆâ–„â–†â–…â–…â–ƒâ–ƒâ–„â–ƒâ–ƒâ–„â–ƒâ–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 10080
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 14.27695
wandb:   test_error_force 13.18333
wandb:          test_loss 5.36661
wandb: train_error_energy 8.27672
wandb:  train_error_force 4.35405
wandb:         train_loss 2.01076
wandb: valid_error_energy 7.93089
wandb:  valid_error_force 3.12036
wandb:         valid_loss 1.57482
wandb: 
wandb: ğŸš€ View run al_77_103 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/w0yfbvr1
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_042000-w0yfbvr1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.5833188891410828, Uncertainty Bias: -0.613685131072998
7.6293945e-06 0.0027899742
-8.849926 18.737387
(48745, 22, 3)
Found uncertainty sample 0 after 13 steps.
Found uncertainty sample 1 after 25 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 18 steps.
Found uncertainty sample 4 after 6 steps.
Found uncertainty sample 5 after 17 steps.
Found uncertainty sample 6 after 20 steps.
Found uncertainty sample 7 after 1 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 6 steps.
Found uncertainty sample 10 after 5 steps.
Found uncertainty sample 11 after 12 steps.
Found uncertainty sample 12 after 15 steps.
Found uncertainty sample 13 after 7 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 4 steps.
Found uncertainty sample 16 after 3 steps.
Found uncertainty sample 17 after 3 steps.
Found uncertainty sample 18 after 12 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 24 steps.
Found uncertainty sample 21 after 36 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 5 steps.
Found uncertainty sample 24 after 19 steps.
Found uncertainty sample 25 after 10 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 30 steps.
Found uncertainty sample 29 after 9 steps.
Found uncertainty sample 30 after 17 steps.
Found uncertainty sample 31 after 9 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 3 steps.
Found uncertainty sample 34 after 7 steps.
Found uncertainty sample 35 after 23 steps.
Found uncertainty sample 36 after 24 steps.
Found uncertainty sample 37 after 15 steps.
Found uncertainty sample 38 after 9 steps.
Found uncertainty sample 39 after 24 steps.
Found uncertainty sample 40 after 2 steps.
Found uncertainty sample 41 after 4 steps.
Found uncertainty sample 42 after 5 steps.
Found uncertainty sample 43 after 6 steps.
Found uncertainty sample 44 after 38 steps.
Found uncertainty sample 45 after 12 steps.
Found uncertainty sample 46 after 12 steps.
Found uncertainty sample 47 after 20 steps.
Found uncertainty sample 48 after 8 steps.
Found uncertainty sample 49 after 45 steps.
Found uncertainty sample 50 after 53 steps.
Found uncertainty sample 51 after 6 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 20 steps.
Found uncertainty sample 54 after 49 steps.
Found uncertainty sample 55 after 3 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 20 steps.
Found uncertainty sample 58 after 20 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 8 steps.
Found uncertainty sample 61 after 1 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 13 steps.
Found uncertainty sample 64 after 12 steps.
Found uncertainty sample 65 after 13 steps.
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 26 steps.
Found uncertainty sample 68 after 1 steps.
Found uncertainty sample 69 after 22 steps.
Found uncertainty sample 70 after 40 steps.
Found uncertainty sample 71 after 19 steps.
Found uncertainty sample 72 after 17 steps.
Found uncertainty sample 73 after 47 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 27 steps.
Found uncertainty sample 76 after 3 steps.
Found uncertainty sample 77 after 18 steps.
Found uncertainty sample 78 after 30 steps.
Found uncertainty sample 79 after 4 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 7 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 8 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 7 steps.
Found uncertainty sample 87 after 3 steps.
Found uncertainty sample 88 after 9 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 3 steps.
Found uncertainty sample 92 after 15 steps.
Found uncertainty sample 93 after 20 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 11 steps.
Found uncertainty sample 96 after 77 steps.
Found uncertainty sample 97 after 11 steps.
Found uncertainty sample 98 after 2 steps.
Found uncertainty sample 99 after 22 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_043030-kv718ahe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_104
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/kv718ahe
Training model 104. Added 100 samples to the dataset.
Epoch 0, Batch 100/318, Loss: 0.03365509957075119
Epoch 0, Batch 200/318, Loss: 0.06578915566205978
Epoch 0, Batch 300/318, Loss: 0.09747318923473358

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.166212121065433, Training Loss Force: 1.7916149821456533, time: 4.611029863357544
Validation Loss Energy: 0.7284015337616083, Validation Loss Force: 1.8540157432679323, time: 0.27320051193237305
Test Loss Energy: 10.19685859382914, Test Loss Force: 11.82488085938316, time: 10.232993602752686

Epoch 1, Batch 100/318, Loss: 0.07141783088445663
Epoch 1, Batch 200/318, Loss: 0.10993905365467072
Epoch 1, Batch 300/318, Loss: 0.039592303335666656

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.1246528991535565, Training Loss Force: 1.7374929292083954, time: 4.505888223648071
Validation Loss Energy: 1.9401757656646073, Validation Loss Force: 1.8533657720309686, time: 0.2645833492279053
Test Loss Energy: 10.209514212743864, Test Loss Force: 11.973706485929624, time: 10.393328666687012

Epoch 2, Batch 100/318, Loss: 0.08628048002719879
Epoch 2, Batch 200/318, Loss: 0.06325557827949524
Epoch 2, Batch 300/318, Loss: 0.04761223867535591

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.1698210569810488, Training Loss Force: 1.7409227927412794, time: 4.665279865264893
Validation Loss Energy: 0.6172509634464518, Validation Loss Force: 1.8526183162419183, time: 0.2728409767150879
Test Loss Energy: 10.633301679990279, Test Loss Force: 11.842209541233096, time: 10.165645360946655

Epoch 3, Batch 100/318, Loss: 0.2659960389137268
Epoch 3, Batch 200/318, Loss: 0.0360703244805336
Epoch 3, Batch 300/318, Loss: 0.1162065863609314

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.247673050777772, Training Loss Force: 1.7614339247071105, time: 4.532025337219238
Validation Loss Energy: 0.93334808609437, Validation Loss Force: 1.7936357756469963, time: 0.2661771774291992
Test Loss Energy: 11.034257347834261, Test Loss Force: 12.010921877441147, time: 10.498126983642578

Epoch 4, Batch 100/318, Loss: 0.06172523275017738
Epoch 4, Batch 200/318, Loss: 0.05460964888334274
Epoch 4, Batch 300/318, Loss: 0.11705204099416733

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.2088028604216356, Training Loss Force: 1.6891842025958377, time: 4.548439025878906
Validation Loss Energy: 0.7896699846751676, Validation Loss Force: 1.8455457429605577, time: 0.27683234214782715
Test Loss Energy: 10.899083387090343, Test Loss Force: 11.975499166078311, time: 10.172955751419067

Epoch 5, Batch 100/318, Loss: 0.0713072270154953
Epoch 5, Batch 200/318, Loss: 0.18934014439582825
Epoch 5, Batch 300/318, Loss: 0.036483634263277054

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.3288957937379675, Training Loss Force: 1.7096322999373919, time: 4.587503433227539
Validation Loss Energy: 2.075031949324754, Validation Loss Force: 1.880230314615035, time: 0.27047038078308105
Test Loss Energy: 11.977406776148895, Test Loss Force: 11.961169858608546, time: 10.528889656066895

Epoch 6, Batch 100/318, Loss: 0.4602016806602478
Epoch 6, Batch 200/318, Loss: 0.7127063274383545
Epoch 6, Batch 300/318, Loss: 0.2947084307670593

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.2432449659013, Training Loss Force: 4.461907032017005, time: 4.601034879684448
Validation Loss Energy: 4.452839160501401, Validation Loss Force: 5.224393718674297, time: 0.2673063278198242
Test Loss Energy: 9.96691020932346, Test Loss Force: 12.550920889565603, time: 10.196978569030762

Epoch 7, Batch 100/318, Loss: 0.07156898081302643
Epoch 7, Batch 200/318, Loss: 0.6110146641731262
Epoch 7, Batch 300/318, Loss: 0.06269723176956177

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.858582314082447, Training Loss Force: 3.8374118408349664, time: 4.6009299755096436
Validation Loss Energy: 15.622905847443414, Validation Loss Force: 4.952839432178889, time: 0.2743721008300781
Test Loss Energy: 20.364801395684243, Test Loss Force: 12.98371024784092, time: 10.401231050491333

Epoch 8, Batch 100/318, Loss: 0.05558950453996658
Epoch 8, Batch 200/318, Loss: 1.0543301105499268
Epoch 8, Batch 300/318, Loss: 0.3560684621334076

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 7.806759222571616, Training Loss Force: 4.053919791713632, time: 4.590752363204956
Validation Loss Energy: 11.375731837142514, Validation Loss Force: 7.453598756339067, time: 0.27057504653930664
Test Loss Energy: 16.019440684499525, Test Loss Force: 13.892207255477555, time: 10.225570917129517

Epoch 9, Batch 100/318, Loss: 0.38535672426223755
Epoch 9, Batch 200/318, Loss: 0.5697636604309082
Epoch 9, Batch 300/318, Loss: 0.12768177688121796

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.479184234328706, Training Loss Force: 4.490083900490446, time: 4.589298486709595
Validation Loss Energy: 9.42781591383014, Validation Loss Force: 3.0222009868388318, time: 0.27290797233581543
Test Loss Energy: 10.815981764933847, Test Loss Force: 11.674104278060112, time: 10.47571086883545

Epoch 10, Batch 100/318, Loss: 0.2806340456008911
Epoch 10, Batch 200/318, Loss: 0.6809142231941223
Epoch 10, Batch 300/318, Loss: 0.1726505607366562

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 9.573996622419317, Training Loss Force: 3.839028834063344, time: 4.583822727203369
Validation Loss Energy: 21.080241597626706, Validation Loss Force: 3.5500632252726545, time: 0.27694249153137207
Test Loss Energy: 15.382897977870112, Test Loss Force: 11.241967911627823, time: 10.257904529571533

Epoch 11, Batch 100/318, Loss: 0.68284672498703
Epoch 11, Batch 200/318, Loss: 0.15451495349407196
Epoch 11, Batch 300/318, Loss: 0.4469161629676819

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.815349992202993, Training Loss Force: 4.260733505920178, time: 4.512058258056641
Validation Loss Energy: 8.274728373161325, Validation Loss Force: 3.7130151782923613, time: 0.2672717571258545
Test Loss Energy: 10.264223816716282, Test Loss Force: 10.765311631082882, time: 10.361900806427002

Epoch 12, Batch 100/318, Loss: 0.6677724719047546
Epoch 12, Batch 200/318, Loss: 0.17552915215492249
Epoch 12, Batch 300/318, Loss: 0.8606544733047485

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.43569526628112, Training Loss Force: 3.591015311591913, time: 4.563718795776367
Validation Loss Energy: 14.941820249110787, Validation Loss Force: 3.9014497001088477, time: 0.2712264060974121
Test Loss Energy: 12.324988934170904, Test Loss Force: 11.725302074833348, time: 10.304481744766235

Epoch 13, Batch 100/318, Loss: 0.5132409930229187
Epoch 13, Batch 200/318, Loss: 0.22913023829460144
Epoch 13, Batch 300/318, Loss: 0.7942871451377869

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.30759768038409, Training Loss Force: 4.24862809079964, time: 4.582460641860962
Validation Loss Energy: 3.8170287905426123, Validation Loss Force: 2.7264711103655808, time: 0.2609715461730957
Test Loss Energy: 9.593446948334643, Test Loss Force: 10.875374816850652, time: 10.40011978149414

Epoch 14, Batch 100/318, Loss: 0.8460273742675781
Epoch 14, Batch 200/318, Loss: 0.223139226436615
Epoch 14, Batch 300/318, Loss: 0.4279120862483978

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 6.029325429841148, Training Loss Force: 2.936507465265658, time: 4.531442642211914
Validation Loss Energy: 6.010785701784603, Validation Loss Force: 2.7146493382052994, time: 0.2706315517425537
Test Loss Energy: 12.083002482059301, Test Loss Force: 11.99769112394121, time: 10.221232414245605

Epoch 15, Batch 100/318, Loss: 0.3203902542591095
Epoch 15, Batch 200/318, Loss: 2.301820755004883
Epoch 15, Batch 300/318, Loss: 0.8956281542778015

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.453236629016892, Training Loss Force: 3.4279055589703584, time: 4.548033237457275
Validation Loss Energy: 5.5455920912543055, Validation Loss Force: 3.4380123702001404, time: 0.27941060066223145
Test Loss Energy: 13.847697166786697, Test Loss Force: 11.29623663535453, time: 10.407952070236206

Epoch 16, Batch 100/318, Loss: 0.58011794090271
Epoch 16, Batch 200/318, Loss: 0.6595789194107056
Epoch 16, Batch 300/318, Loss: 0.973607063293457

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 8.479222595741156, Training Loss Force: 4.4948270193813356, time: 4.541729688644409
Validation Loss Energy: 3.2350134603529797, Validation Loss Force: 5.3029087299492605, time: 0.268237829208374
Test Loss Energy: 11.238705451903025, Test Loss Force: 12.849318119271603, time: 10.205092906951904

Epoch 17, Batch 100/318, Loss: 0.4522889256477356
Epoch 17, Batch 200/318, Loss: 0.6047356128692627
Epoch 17, Batch 300/318, Loss: 0.39895522594451904

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.842545815058989, Training Loss Force: 3.792955497711322, time: 4.512110710144043
Validation Loss Energy: 2.486401350427779, Validation Loss Force: 2.5101191326415937, time: 0.2728402614593506
Test Loss Energy: 9.804498915642979, Test Loss Force: 11.865763334266038, time: 10.452247381210327

Epoch 18, Batch 100/318, Loss: 0.37355589866638184
Epoch 18, Batch 200/318, Loss: 0.39398181438446045
Epoch 18, Batch 300/318, Loss: 0.747036337852478

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 5.716700447509415, Training Loss Force: 2.713128430494564, time: 4.4814229011535645
Validation Loss Energy: 1.3507418566077576, Validation Loss Force: 2.962791968028714, time: 0.2645859718322754
Test Loss Energy: 10.878733721460812, Test Loss Force: 11.436404787579528, time: 10.123140811920166

Epoch 19, Batch 100/318, Loss: 0.13403844833374023
Epoch 19, Batch 200/318, Loss: 0.7634258270263672
Epoch 19, Batch 300/318, Loss: 0.4163651764392853

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.751256077582124, Training Loss Force: 4.2213949155232715, time: 4.598027944564819
Validation Loss Energy: 6.049119798473923, Validation Loss Force: 2.586154411239847, time: 0.26650381088256836
Test Loss Energy: 11.577954789549944, Test Loss Force: 10.88331411206227, time: 11.60215711593628

wandb: - 0.039 MB of 0.058 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–‚â–‚â–ƒâ–â–ˆâ–…â–‚â–…â–â–ƒâ–â–ƒâ–„â–‚â–â–‚â–‚
wandb:   test_error_force â–ƒâ–„â–ƒâ–„â–„â–„â–…â–†â–ˆâ–ƒâ–‚â–â–ƒâ–â–„â–‚â–†â–ƒâ–ƒâ–
wandb:          test_loss â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–ˆâ–ˆâ–ƒâ–ƒâ–â–ƒâ–â–„â–ƒâ–…â–ƒâ–‚â–‚
wandb: train_error_energy â–â–â–â–â–â–â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–…â–†â–‡â–‡â–…â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–‡â–ˆâ–†â–‡â–†â–‡â–„â–…â–ˆâ–†â–„â–‡
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–‡â–ˆâ–…â–†â–ˆâ–‡â–„â–ˆ
wandb: valid_error_energy â–â–â–â–â–â–â–‚â–†â–…â–„â–ˆâ–„â–†â–‚â–ƒâ–ƒâ–‚â–‚â–â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–…â–…â–ˆâ–ƒâ–ƒâ–ƒâ–„â–‚â–‚â–ƒâ–…â–‚â–‚â–‚
wandb:         valid_loss â–â–â–â–â–â–â–…â–‡â–ˆâ–„â–†â–„â–…â–‚â–ƒâ–ƒâ–…â–‚â–‚â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 10170
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.57795
wandb:   test_error_force 10.88331
wandb:          test_loss 4.4164
wandb: train_error_energy 8.75126
wandb:  train_error_force 4.22139
wandb:         train_loss 1.99813
wandb: valid_error_energy 6.04912
wandb:  valid_error_force 2.58615
wandb:         valid_loss 1.27015
wandb: 
wandb: ğŸš€ View run al_77_104 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/kv718ahe
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_043030-kv718ahe/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.47839275002479553, Uncertainty Bias: -0.5129777193069458
2.2888184e-05 0.00043678284
-6.811805 18.629438
(48745, 22, 3)
Found uncertainty sample 0 after 43 steps.
Found uncertainty sample 1 after 13 steps.
Found uncertainty sample 2 after 5 steps.
Found uncertainty sample 3 after 22 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 9 steps.
Found uncertainty sample 6 after 11 steps.
Found uncertainty sample 7 after 5 steps.
Found uncertainty sample 8 after 5 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 14 steps.
Found uncertainty sample 11 after 68 steps.
Found uncertainty sample 12 after 21 steps.
Found uncertainty sample 13 after 1 steps.
Found uncertainty sample 14 after 14 steps.
Found uncertainty sample 15 after 3 steps.
Found uncertainty sample 16 after 6 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 12 steps.
Found uncertainty sample 19 after 1 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 54 steps.
Found uncertainty sample 22 after 3 steps.
Found uncertainty sample 23 after 6 steps.
Found uncertainty sample 24 after 4 steps.
Found uncertainty sample 25 after 19 steps.
Found uncertainty sample 26 after 15 steps.
Found uncertainty sample 27 after 18 steps.
Found uncertainty sample 28 after 3 steps.
Found uncertainty sample 29 after 3 steps.
Found uncertainty sample 30 after 4 steps.
Found uncertainty sample 31 after 2 steps.
Found uncertainty sample 32 after 15 steps.
Found uncertainty sample 33 after 10 steps.
Found uncertainty sample 34 after 3 steps.
Found uncertainty sample 35 after 3 steps.
Found uncertainty sample 36 after 2 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 2 steps.
Found uncertainty sample 40 after 41 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 1 steps.
Found uncertainty sample 43 after 16 steps.
Found uncertainty sample 44 after 11 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 30 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 26 steps.
Found uncertainty sample 49 after 8 steps.
Found uncertainty sample 50 after 2 steps.
Found uncertainty sample 51 after 13 steps.
Found uncertainty sample 52 after 1 steps.
Found uncertainty sample 53 after 22 steps.
Found uncertainty sample 54 after 1 steps.
Found uncertainty sample 55 after 34 steps.
Found uncertainty sample 56 after 58 steps.
Found uncertainty sample 57 after 3 steps.
Found uncertainty sample 58 after 12 steps.
Found uncertainty sample 59 after 5 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 3 steps.
Found uncertainty sample 62 after 38 steps.
Found uncertainty sample 63 after 9 steps.
Found uncertainty sample 64 after 9 steps.
Found uncertainty sample 65 after 35 steps.
Found uncertainty sample 66 after 7 steps.
Found uncertainty sample 67 after 34 steps.
Found uncertainty sample 68 after 15 steps.
Found uncertainty sample 69 after 8 steps.
Found uncertainty sample 70 after 16 steps.
Found uncertainty sample 71 after 12 steps.
Found uncertainty sample 72 after 24 steps.
Found uncertainty sample 73 after 19 steps.
Found uncertainty sample 74 after 7 steps.
Found uncertainty sample 75 after 13 steps.
Found uncertainty sample 76 after 19 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 4 steps.
Found uncertainty sample 79 after 15 steps.
Found uncertainty sample 80 after 2 steps.
Found uncertainty sample 81 after 15 steps.
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 5 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 5 steps.
Found uncertainty sample 88 after 2 steps.
Found uncertainty sample 89 after 1 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 20 steps.
Found uncertainty sample 93 after 3 steps.
Found uncertainty sample 94 after 10 steps.
Found uncertainty sample 95 after 19 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 17 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 4 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_044050-k3ktc3i6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_105
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/k3ktc3i6
Training model 105. Added 100 samples to the dataset.
Epoch 0, Batch 100/321, Loss: 0.07115571945905685
Epoch 0, Batch 200/321, Loss: 0.06085694581270218
Epoch 0, Batch 300/321, Loss: 0.03915594145655632

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.4342865898779005, Training Loss Force: 1.8073803784294933, time: 4.68803596496582
Validation Loss Energy: 2.069527701638907, Validation Loss Force: 1.9146189812414893, time: 0.26520705223083496
Test Loss Energy: 11.863190850447014, Test Loss Force: 11.968257401202127, time: 10.222566604614258

Epoch 1, Batch 100/321, Loss: 0.07586120814085007
Epoch 1, Batch 200/321, Loss: 0.03012484312057495
Epoch 1, Batch 300/321, Loss: 0.06416525691747665

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.6379364877310816, Training Loss Force: 1.7548474463167374, time: 4.647948503494263
Validation Loss Energy: 0.9134665592611718, Validation Loss Force: 1.8383713876125938, time: 0.27329349517822266
Test Loss Energy: 10.463616897492262, Test Loss Force: 12.064071883399347, time: 10.489719867706299

Epoch 2, Batch 100/321, Loss: 0.13805878162384033
Epoch 2, Batch 200/321, Loss: 0.07438569515943527
Epoch 2, Batch 300/321, Loss: 0.12887156009674072

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.7889699975438427, Training Loss Force: 1.7646220960185666, time: 4.560999393463135
Validation Loss Energy: 3.028084974486144, Validation Loss Force: 1.987097855041728, time: 0.26686716079711914
Test Loss Energy: 10.212768031506833, Test Loss Force: 11.903836526602557, time: 10.305583715438843

Epoch 3, Batch 100/321, Loss: 0.10040130466222763
Epoch 3, Batch 200/321, Loss: 0.15690821409225464
Epoch 3, Batch 300/321, Loss: 0.20424911379814148

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 2.480071703057721, Training Loss Force: 1.8005446846782434, time: 4.698629379272461
Validation Loss Energy: 0.6038125019521617, Validation Loss Force: 1.8200428667376687, time: 0.27288818359375
Test Loss Energy: 11.043598983015949, Test Loss Force: 12.007437583288267, time: 10.505943775177002

Epoch 4, Batch 100/321, Loss: 0.043420374393463135
Epoch 4, Batch 200/321, Loss: 0.15685787796974182
Epoch 4, Batch 300/321, Loss: 0.13028259575366974

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.5387960855010663, Training Loss Force: 1.7294037505583877, time: 4.6657445430755615
Validation Loss Energy: 1.2640222221135362, Validation Loss Force: 1.8359237415598115, time: 0.27980804443359375
Test Loss Energy: 10.64183310725385, Test Loss Force: 12.035245121777848, time: 10.32781720161438

Epoch 5, Batch 100/321, Loss: 0.0758451446890831
Epoch 5, Batch 200/321, Loss: 0.09581761062145233
Epoch 5, Batch 300/321, Loss: 0.027348998934030533

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.2678487852819798, Training Loss Force: 1.689761609082291, time: 4.684139013290405
Validation Loss Energy: 1.1663752106174818, Validation Loss Force: 1.8360758691203096, time: 0.28018689155578613
Test Loss Energy: 11.441930689664948, Test Loss Force: 12.17288238151602, time: 10.47998046875

Epoch 6, Batch 100/321, Loss: 0.20983466506004333
Epoch 6, Batch 200/321, Loss: 0.27896279096603394
Epoch 6, Batch 300/321, Loss: 1.372032642364502

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.658290460921526, Training Loss Force: 4.400849783543546, time: 4.686400651931763
Validation Loss Energy: 28.958421695240936, Validation Loss Force: 3.8206652448985876, time: 0.2693929672241211
Test Loss Energy: 21.952214204376038, Test Loss Force: 11.747838402771398, time: 10.405241250991821

Epoch 7, Batch 100/321, Loss: 0.19588333368301392
Epoch 7, Batch 200/321, Loss: 0.26285678148269653
Epoch 7, Batch 300/321, Loss: 0.4336988925933838

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.02163462974228, Training Loss Force: 3.8844073933572782, time: 4.505295276641846
Validation Loss Energy: 5.236094180683876, Validation Loss Force: 2.438609960704956, time: 0.26802539825439453
Test Loss Energy: 9.880686234133043, Test Loss Force: 11.614827116690211, time: 10.400198221206665

Epoch 8, Batch 100/321, Loss: 0.4714958667755127
Epoch 8, Batch 200/321, Loss: 0.6837990283966064
Epoch 8, Batch 300/321, Loss: 0.6742490530014038

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 6.777934405247886, Training Loss Force: 3.2858420194904276, time: 4.536086320877075
Validation Loss Energy: 8.543563424046663, Validation Loss Force: 5.874692902755632, time: 0.2738516330718994
Test Loss Energy: 10.347978491337768, Test Loss Force: 12.19236739603835, time: 11.710176229476929

Epoch 9, Batch 100/321, Loss: 0.1298716962337494
Epoch 9, Batch 200/321, Loss: 0.6158114671707153
Epoch 9, Batch 300/321, Loss: 0.31196504831314087

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.399406060344385, Training Loss Force: 4.1216311970209, time: 4.530498027801514
Validation Loss Energy: 5.815396910803988, Validation Loss Force: 2.320895382304281, time: 0.27024340629577637
Test Loss Energy: 12.827331045002323, Test Loss Force: 11.462337745643776, time: 10.52867865562439

Epoch 10, Batch 100/321, Loss: 0.38180115818977356
Epoch 10, Batch 200/321, Loss: 0.31300973892211914
Epoch 10, Batch 300/321, Loss: 0.6494554877281189

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 6.122856543112464, Training Loss Force: 2.7981720907531336, time: 4.630788326263428
Validation Loss Energy: 9.064328306123183, Validation Loss Force: 3.8280547431753478, time: 0.2683138847351074
Test Loss Energy: 10.749875874797965, Test Loss Force: 11.511588630659265, time: 10.295563697814941

Epoch 11, Batch 100/321, Loss: 0.08243890106678009
Epoch 11, Batch 200/321, Loss: 0.40383005142211914
Epoch 11, Batch 300/321, Loss: 0.3196776211261749

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.985765577509238, Training Loss Force: 4.372543159821321, time: 4.651853322982788
Validation Loss Energy: 20.501828742481383, Validation Loss Force: 5.5994972631830375, time: 0.27042245864868164
Test Loss Energy: 23.157028968373258, Test Loss Force: 12.225407776037944, time: 10.316102504730225

Epoch 12, Batch 100/321, Loss: 0.3051328659057617
Epoch 12, Batch 200/321, Loss: 0.5026156306266785
Epoch 12, Batch 300/321, Loss: 0.2508004903793335

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 9.100991908566227, Training Loss Force: 4.526619481838638, time: 4.633584499359131
Validation Loss Energy: 13.3950255130616, Validation Loss Force: 3.0647593398955473, time: 0.2703711986541748
Test Loss Energy: 16.885444855337017, Test Loss Force: 11.481870651519587, time: 10.25615382194519

Epoch 13, Batch 100/321, Loss: 0.6858618259429932
Epoch 13, Batch 200/321, Loss: 0.6450316905975342
Epoch 13, Batch 300/321, Loss: 0.10629439353942871

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.898731208639422, Training Loss Force: 3.6442224070615397, time: 4.620565891265869
Validation Loss Energy: 7.568353883655045, Validation Loss Force: 3.6082172362424942, time: 0.27840685844421387
Test Loss Energy: 10.33642798755493, Test Loss Force: 11.407292291314103, time: 10.444894552230835

Epoch 14, Batch 100/321, Loss: 0.3529820442199707
Epoch 14, Batch 200/321, Loss: 0.9798741340637207
Epoch 14, Batch 300/321, Loss: 1.194754719734192

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 9.384992683338222, Training Loss Force: 4.252911600129602, time: 4.623824119567871
Validation Loss Energy: 1.3471698204356986, Validation Loss Force: 4.616703255410487, time: 0.2826709747314453
Test Loss Energy: 9.242102437189422, Test Loss Force: 11.949312155342437, time: 10.365532159805298

Epoch 15, Batch 100/321, Loss: 0.44914186000823975
Epoch 15, Batch 200/321, Loss: 0.6027582287788391
Epoch 15, Batch 300/321, Loss: 0.8316934108734131

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.062369193053758, Training Loss Force: 3.3818684306527484, time: 4.514899492263794
Validation Loss Energy: 1.0707262093678376, Validation Loss Force: 3.4713067515758467, time: 0.26816892623901367
Test Loss Energy: 10.191076013674628, Test Loss Force: 11.686783236431914, time: 10.420900821685791

Epoch 16, Batch 100/321, Loss: 0.32535991072654724
Epoch 16, Batch 200/321, Loss: 0.13435927033424377
Epoch 16, Batch 300/321, Loss: 0.2742418348789215

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.197472209069269, Training Loss Force: 3.9859175143410512, time: 4.58891749382019
Validation Loss Energy: 10.549564642630441, Validation Loss Force: 3.6541121973832786, time: 0.2732880115509033
Test Loss Energy: 16.623982911015037, Test Loss Force: 12.152897692767, time: 10.253991603851318

Epoch 17, Batch 100/321, Loss: 0.9085550308227539
Epoch 17, Batch 200/321, Loss: 1.1462451219558716
Epoch 17, Batch 300/321, Loss: 0.1303725242614746

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.59382943879121, Training Loss Force: 4.433469395553635, time: 4.6484925746917725
Validation Loss Energy: 15.694127909783294, Validation Loss Force: 4.026396483301599, time: 0.2675774097442627
Test Loss Energy: 19.450838291863683, Test Loss Force: 12.578556345608634, time: 10.461665868759155

Epoch 18, Batch 100/321, Loss: 0.6486014127731323
Epoch 18, Batch 200/321, Loss: 0.21053318679332733
Epoch 18, Batch 300/321, Loss: 0.3021230101585388

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.970729539774952, Training Loss Force: 3.8665147551740153, time: 4.606932163238525
Validation Loss Energy: 6.21154509442968, Validation Loss Force: 3.4703740350573846, time: 0.27663207054138184
Test Loss Energy: 10.266373303769695, Test Loss Force: 11.817406138279667, time: 10.281781673431396

Epoch 19, Batch 100/321, Loss: 0.4902065396308899
Epoch 19, Batch 200/321, Loss: 0.21662801504135132
Epoch 19, Batch 300/321, Loss: 0.4422227144241333

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.542626891769826, Training Loss Force: 3.9359802860600204, time: 4.630732297897339
Validation Loss Energy: 13.395488486749812, Validation Loss Force: 3.5900240982082936, time: 0.2795393466949463
Test Loss Energy: 19.098959729173576, Test Loss Force: 12.036188600094318, time: 10.412629127502441

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.039 MB of 0.051 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–â–‚â–‚â–‚â–‡â–â–‚â–ƒâ–‚â–ˆâ–…â–‚â–â–â–…â–†â–‚â–†
wandb:   test_error_force â–„â–…â–„â–…â–…â–†â–ƒâ–‚â–†â–â–‚â–†â–â–â–„â–ƒâ–…â–ˆâ–ƒâ–…
wandb:          test_loss â–ƒâ–‚â–‚â–ƒâ–‚â–ƒâ–‡â–â–ƒâ–‚â–â–ˆâ–„â–â–‚â–‚â–…â–‡â–‚â–†
wandb: train_error_energy â–â–â–â–‚â–â–â–‡â–‡â–†â–‡â–…â–‡â–ˆâ–‡â–ˆâ–†â–†â–ˆâ–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–…â–‡â–„â–ˆâ–ˆâ–†â–‡â–…â–‡â–ˆâ–†â–‡
wandb:         train_loss â–â–â–â–‚â–â–â–ˆâ–‡â–…â–‡â–„â–ˆâ–ˆâ–†â–ˆâ–†â–‡â–ˆâ–‡â–‡
wandb: valid_error_energy â–â–â–‚â–â–â–â–ˆâ–‚â–ƒâ–‚â–ƒâ–†â–„â–ƒâ–â–â–ƒâ–…â–‚â–„
wandb:  valid_error_force â–â–â–â–â–â–â–„â–‚â–ˆâ–‚â–„â–ˆâ–ƒâ–„â–†â–„â–„â–…â–„â–„
wandb:         valid_loss â–â–â–‚â–â–â–â–ˆâ–‚â–†â–‚â–„â–ˆâ–„â–„â–„â–ƒâ–„â–†â–„â–…
wandb: 
wandb: Run summary:
wandb:       dataset_size 10260
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 19.09896
wandb:   test_error_force 12.03619
wandb:          test_loss 5.30546
wandb: train_error_energy 8.54263
wandb:  train_error_force 3.93598
wandb:         train_loss 1.88867
wandb: valid_error_energy 13.39549
wandb:  valid_error_force 3.59002
wandb:         valid_loss 2.09767
wandb: 
wandb: ğŸš€ View run al_77_105 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/k3ktc3i6
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_044050-k3ktc3i6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.46514421701431274, Uncertainty Bias: -0.45877695083618164
1.5258789e-05 0.00013542175
-6.4026027 41.117584
(48745, 22, 3)
Found uncertainty sample 0 after 47 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 25 steps.
Found uncertainty sample 3 after 9 steps.
Found uncertainty sample 4 after 15 steps.
Found uncertainty sample 5 after 36 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 5 steps.
Found uncertainty sample 8 after 6 steps.
Found uncertainty sample 9 after 8 steps.
Found uncertainty sample 10 after 10 steps.
Found uncertainty sample 11 after 16 steps.
Found uncertainty sample 12 after 9 steps.
Found uncertainty sample 13 after 7 steps.
Found uncertainty sample 14 after 33 steps.
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 17 steps.
Found uncertainty sample 17 after 12 steps.
Found uncertainty sample 18 after 1 steps.
Found uncertainty sample 19 after 15 steps.
Found uncertainty sample 20 after 48 steps.
Found uncertainty sample 21 after 13 steps.
Found uncertainty sample 22 after 2 steps.
Found uncertainty sample 23 after 21 steps.
Found uncertainty sample 24 after 5 steps.
Found uncertainty sample 25 after 3 steps.
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 3 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 64 steps.
Found uncertainty sample 31 after 36 steps.
Found uncertainty sample 32 after 37 steps.
Found uncertainty sample 33 after 22 steps.
Found uncertainty sample 34 after 3 steps.
Found uncertainty sample 35 after 21 steps.
Found uncertainty sample 36 after 23 steps.
Found uncertainty sample 37 after 24 steps.
Found uncertainty sample 38 after 27 steps.
Found uncertainty sample 39 after 32 steps.
Found uncertainty sample 40 after 19 steps.
Found uncertainty sample 41 after 11 steps.
Found uncertainty sample 42 after 14 steps.
Found uncertainty sample 43 after 25 steps.
Found uncertainty sample 44 after 13 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 18 steps.
Found uncertainty sample 47 after 13 steps.
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 46 steps.
Found uncertainty sample 51 after 2 steps.
Found uncertainty sample 52 after 10 steps.
Found uncertainty sample 53 after 39 steps.
Found uncertainty sample 54 after 10 steps.
Found uncertainty sample 55 after 16 steps.
Found uncertainty sample 56 after 4 steps.
Found uncertainty sample 57 after 18 steps.
Found uncertainty sample 58 after 19 steps.
Found uncertainty sample 59 after 21 steps.
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 17 steps.
Found uncertainty sample 62 after 3 steps.
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 5 steps.
Found uncertainty sample 65 after 4 steps.
Found uncertainty sample 66 after 23 steps.
Found uncertainty sample 67 after 10 steps.
Found uncertainty sample 68 after 71 steps.
Found uncertainty sample 69 after 54 steps.
Found uncertainty sample 70 after 20 steps.
Found uncertainty sample 71 after 30 steps.
Found uncertainty sample 72 after 21 steps.
Found uncertainty sample 73 after 6 steps.
Found uncertainty sample 74 after 4 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 4 steps.
Found uncertainty sample 77 after 13 steps.
Found uncertainty sample 78 after 6 steps.
Found uncertainty sample 79 after 26 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 21 steps.
Found uncertainty sample 82 after 37 steps.
Found uncertainty sample 83 after 6 steps.
Found uncertainty sample 84 after 11 steps.
Found uncertainty sample 85 after 28 steps.
Found uncertainty sample 86 after 18 steps.
Found uncertainty sample 87 after 5 steps.
Found uncertainty sample 88 after 64 steps.
Found uncertainty sample 89 after 7 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 12 steps.
Found uncertainty sample 92 after 28 steps.
Found uncertainty sample 93 after 8 steps.
Found uncertainty sample 94 after 18 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 17 steps.
Found uncertainty sample 97 after 38 steps.
Found uncertainty sample 98 after 4 steps.
Found uncertainty sample 99 after 6 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_045131-f2hqizqc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_106
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/f2hqizqc
Training model 106. Added 100 samples to the dataset.
Epoch 0, Batch 100/324, Loss: 0.06345155835151672
Epoch 0, Batch 200/324, Loss: 0.07441799342632294
Epoch 0, Batch 300/324, Loss: 0.2172567993402481

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.2236803750177112, Training Loss Force: 1.7920497444181294, time: 4.679951906204224
Validation Loss Energy: 0.8853460290925851, Validation Loss Force: 1.8263056368691193, time: 0.2778627872467041
Test Loss Energy: 10.519108137783695, Test Loss Force: 12.057880436060628, time: 10.525414228439331

Epoch 1, Batch 100/324, Loss: 0.03263252228498459
Epoch 1, Batch 200/324, Loss: 0.07999496161937714
Epoch 1, Batch 300/324, Loss: 0.07092325389385223

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 0.9707785689164246, Training Loss Force: 1.7203256267248506, time: 4.7268102169036865
Validation Loss Energy: 1.3955625059201846, Validation Loss Force: 1.825188528030914, time: 0.2741057872772217
Test Loss Energy: 10.472628991664433, Test Loss Force: 11.957092849500189, time: 10.633557558059692

Epoch 2, Batch 100/324, Loss: 0.18801575899124146
Epoch 2, Batch 200/324, Loss: 0.09363032877445221
Epoch 2, Batch 300/324, Loss: 0.038768965750932693

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.3511060277177884, Training Loss Force: 1.7072346670980982, time: 4.6103222370147705
Validation Loss Energy: 1.164316389427319, Validation Loss Force: 1.7852076148721419, time: 0.2809443473815918
Test Loss Energy: 11.445303845100446, Test Loss Force: 12.21051913766329, time: 10.428451299667358

Epoch 3, Batch 100/324, Loss: 0.05495431274175644
Epoch 3, Batch 200/324, Loss: 0.08459082990884781
Epoch 3, Batch 300/324, Loss: 0.1727936714887619

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.4895607547655032, Training Loss Force: 1.737451913790308, time: 4.603702068328857
Validation Loss Energy: 1.7360163932965293, Validation Loss Force: 1.7923644684252362, time: 0.28711843490600586
Test Loss Energy: 11.682481968016816, Test Loss Force: 12.278213882907037, time: 10.680899620056152

Epoch 4, Batch 100/324, Loss: 0.06474944949150085
Epoch 4, Batch 200/324, Loss: 0.04216738045215607
Epoch 4, Batch 300/324, Loss: 0.07101401686668396

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.342546790602526, Training Loss Force: 1.731646154624713, time: 4.585707426071167
Validation Loss Energy: 0.5734146676274275, Validation Loss Force: 1.7680752503822266, time: 0.2944321632385254
Test Loss Energy: 11.379641577188448, Test Loss Force: 12.217718714314069, time: 10.492873907089233

Epoch 5, Batch 100/324, Loss: 0.08510256558656693
Epoch 5, Batch 200/324, Loss: 0.04048030078411102
Epoch 5, Batch 300/324, Loss: 0.02973761409521103

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.417500626238392, Training Loss Force: 1.7226962861574788, time: 4.579549312591553
Validation Loss Energy: 0.7124772050556376, Validation Loss Force: 1.7742108541644377, time: 0.2812376022338867
Test Loss Energy: 11.268478000138943, Test Loss Force: 12.191272829128623, time: 10.678727388381958

Epoch 6, Batch 100/324, Loss: 1.3801443576812744
Epoch 6, Batch 200/324, Loss: 0.095669224858284
Epoch 6, Batch 300/324, Loss: 0.9822885394096375

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 8.256485654268081, Training Loss Force: 4.798366016972727, time: 4.669955253601074
Validation Loss Energy: 0.8695828460475223, Validation Loss Force: 2.745967041038776, time: 0.2716679573059082
Test Loss Energy: 10.194475833448903, Test Loss Force: 12.3062797176059, time: 10.532667875289917

Epoch 7, Batch 100/324, Loss: 0.2261582612991333
Epoch 7, Batch 200/324, Loss: 0.22632190585136414
Epoch 7, Batch 300/324, Loss: 0.6530492305755615

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 8.587164568189829, Training Loss Force: 4.123102146688435, time: 4.600459814071655
Validation Loss Energy: 2.1653388950944445, Validation Loss Force: 2.875932118773302, time: 0.2749612331390381
Test Loss Energy: 10.299830333881337, Test Loss Force: 11.50065817301231, time: 10.650591611862183

Epoch 8, Batch 100/324, Loss: 0.3402870297431946
Epoch 8, Batch 200/324, Loss: 0.32499128580093384
Epoch 8, Batch 300/324, Loss: 0.36972954869270325

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.7183020611886, Training Loss Force: 4.359896173125311, time: 4.64634370803833
Validation Loss Energy: 29.19255954913122, Validation Loss Force: 4.9366174504921, time: 0.28035974502563477
Test Loss Energy: 35.7354434679069, Test Loss Force: 15.12481008898667, time: 10.487737894058228

Epoch 9, Batch 100/324, Loss: 0.3575194776058197
Epoch 9, Batch 200/324, Loss: 0.6412011384963989
Epoch 9, Batch 300/324, Loss: 0.1019958108663559

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.225363705584968, Training Loss Force: 4.109128684634828, time: 4.667691469192505
Validation Loss Energy: 3.537171527866191, Validation Loss Force: 3.9309507789821874, time: 0.2830064296722412
Test Loss Energy: 11.150422223124604, Test Loss Force: 12.749066552138347, time: 10.632172346115112

Epoch 10, Batch 100/324, Loss: 0.5504002571105957
Epoch 10, Batch 200/324, Loss: 0.5346710681915283
Epoch 10, Batch 300/324, Loss: 0.6064950227737427

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.285561141876886, Training Loss Force: 3.659333940600186, time: 4.684978246688843
Validation Loss Energy: 6.504296236670197, Validation Loss Force: 3.556823549610681, time: 0.2724165916442871
Test Loss Energy: 10.466631525291836, Test Loss Force: 12.327015447233801, time: 10.459525346755981

Epoch 11, Batch 100/324, Loss: 1.485581398010254
Epoch 11, Batch 200/324, Loss: 0.35009703040122986
Epoch 11, Batch 300/324, Loss: 0.45582640171051025

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 7.830017456263342, Training Loss Force: 3.859752303765016, time: 4.7508203983306885
Validation Loss Energy: 3.8949639268277507, Validation Loss Force: 3.339916858510358, time: 0.27462005615234375
Test Loss Energy: 10.234206150553941, Test Loss Force: 11.7525776882101, time: 10.603694677352905

Epoch 12, Batch 100/324, Loss: 0.48474377393722534
Epoch 12, Batch 200/324, Loss: 0.3359188437461853
Epoch 12, Batch 300/324, Loss: 0.1506921648979187

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.331914572824829, Training Loss Force: 3.733254589423101, time: 4.670849323272705
Validation Loss Energy: 9.839386875044058, Validation Loss Force: 3.3311446293712157, time: 0.2776153087615967
Test Loss Energy: 17.16102025562244, Test Loss Force: 12.030653073105926, time: 10.472847700119019

Epoch 13, Batch 100/324, Loss: 0.8230806589126587
Epoch 13, Batch 200/324, Loss: 0.89821857213974
Epoch 13, Batch 300/324, Loss: 0.46305733919143677

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 9.521158282338853, Training Loss Force: 4.255389989371701, time: 4.654796123504639
Validation Loss Energy: 0.9970626355101473, Validation Loss Force: 3.157296851567655, time: 0.29952216148376465
Test Loss Energy: 9.35955876088355, Test Loss Force: 11.367852157572683, time: 10.615881443023682

Epoch 14, Batch 100/324, Loss: 0.8047881126403809
Epoch 14, Batch 200/324, Loss: 0.17575788497924805
Epoch 14, Batch 300/324, Loss: 0.46757009625434875

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.407815256843854, Training Loss Force: 3.594987325570679, time: 4.661513328552246
Validation Loss Energy: 13.136668737777672, Validation Loss Force: 5.1096577409048685, time: 0.27939319610595703
Test Loss Energy: 16.888658006086963, Test Loss Force: 13.402837033022955, time: 10.503681182861328

Epoch 15, Batch 100/324, Loss: 0.7164623737335205
Epoch 15, Batch 200/324, Loss: 0.33568811416625977
Epoch 15, Batch 300/324, Loss: 0.4293009638786316

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 9.6080159367225, Training Loss Force: 4.623832543760365, time: 4.574636697769165
Validation Loss Energy: 1.425650028523154, Validation Loss Force: 5.411474901955663, time: 0.2826552391052246
Test Loss Energy: 9.423859589999905, Test Loss Force: 12.705564947164882, time: 10.664631605148315

Epoch 16, Batch 100/324, Loss: 0.5814628601074219
Epoch 16, Batch 200/324, Loss: 0.458137571811676
Epoch 16, Batch 300/324, Loss: 0.3230897784233093

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 7.07448936018545, Training Loss Force: 3.648564411281799, time: 4.621502876281738
Validation Loss Energy: 1.6062257991047524, Validation Loss Force: 3.807586906214007, time: 0.2751178741455078
Test Loss Energy: 10.082584858087422, Test Loss Force: 12.179806734439204, time: 10.563833713531494

Epoch 17, Batch 100/324, Loss: 0.5407974720001221
Epoch 17, Batch 200/324, Loss: 0.4525449872016907
Epoch 17, Batch 300/324, Loss: 0.108988918364048

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.793646206872083, Training Loss Force: 4.041132810261504, time: 4.635359287261963
Validation Loss Energy: 4.2481788716226445, Validation Loss Force: 5.459884154024607, time: 0.27707529067993164
Test Loss Energy: 12.26761392223138, Test Loss Force: 13.704599010364031, time: 10.57790756225586

Epoch 18, Batch 100/324, Loss: 0.6300411224365234
Epoch 18, Batch 200/324, Loss: 0.7390272617340088
Epoch 18, Batch 300/324, Loss: 0.09123013913631439

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 10.15314091813536, Training Loss Force: 4.448878907958272, time: 4.61769700050354
Validation Loss Energy: 14.86748455956957, Validation Loss Force: 5.505898733030584, time: 0.2753317356109619
Test Loss Energy: 12.151931150123918, Test Loss Force: 12.839514264912024, time: 10.487352132797241

Epoch 19, Batch 100/324, Loss: 0.5079816579818726
Epoch 19, Batch 200/324, Loss: 0.4175480306148529
Epoch 19, Batch 300/324, Loss: 0.21077284216880798

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 7.463421786262483, Training Loss Force: 3.9433967190114703, time: 4.671220779418945
Validation Loss Energy: 10.43514654733374, Validation Loss Force: 3.0097475468453823, time: 0.27776288986206055
Test Loss Energy: 15.311274066504447, Test Loss Force: 11.869058403945548, time: 10.693541765213013

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.056 MB uploadedwandb: | 0.039 MB of 0.059 MB uploadedwandb: / 0.039 MB of 0.059 MB uploadedwandb: - 0.059 MB of 0.059 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–‚â–‚â–‚â–‚â–â–â–ˆâ–â–â–â–ƒâ–â–ƒâ–â–â–‚â–‚â–ƒ
wandb:   test_error_force â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ˆâ–„â–ƒâ–‚â–‚â–â–…â–ƒâ–ƒâ–…â–„â–‚
wandb:          test_loss â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–ˆâ–‚â–‚â–â–ƒâ–â–„â–‚â–‚â–ƒâ–ƒâ–‚
wandb: train_error_energy â–â–â–â–â–â–â–‡â–‡â–ˆâ–‡â–†â–†â–‡â–ˆâ–†â–ˆâ–†â–‡â–ˆâ–†
wandb:  train_error_force â–â–â–â–â–â–â–ˆâ–†â–‡â–†â–…â–†â–†â–‡â–…â–ˆâ–…â–†â–‡â–†
wandb:         train_loss â–â–â–â–â–â–â–ˆâ–‡â–ˆâ–‡â–†â–†â–†â–‡â–†â–ˆâ–†â–‡â–ˆâ–†
wandb: valid_error_energy â–â–â–â–â–â–â–â–â–ˆâ–‚â–‚â–‚â–ƒâ–â–„â–â–â–‚â–„â–ƒ
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–ƒâ–‡â–…â–„â–„â–„â–„â–‡â–ˆâ–…â–ˆâ–ˆâ–ƒ
wandb:         valid_loss â–â–â–â–â–â–â–‚â–‚â–ˆâ–ƒâ–ƒâ–ƒâ–„â–‚â–†â–„â–ƒâ–„â–†â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 10350
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 15.31127
wandb:   test_error_force 11.86906
wandb:          test_loss 4.99607
wandb: train_error_energy 7.46342
wandb:  train_error_force 3.9434
wandb:         train_loss 1.81893
wandb: valid_error_energy 10.43515
wandb:  valid_error_force 3.00975
wandb:         valid_loss 1.7054
wandb: 
wandb: ğŸš€ View run al_77_106 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/f2hqizqc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_045131-f2hqizqc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.1820460706949234, Uncertainty Bias: -0.09091214835643768
3.0517578e-05 0.0028629303
-1.2092823 8.971239
(48745, 22, 3)
Found uncertainty sample 0 after 11 steps.
Found uncertainty sample 1 after 79 steps.
Found uncertainty sample 2 after 9 steps.
Found uncertainty sample 3 after 103 steps.
Found uncertainty sample 4 after 78 steps.
Found uncertainty sample 5 after 2 steps.
Found uncertainty sample 6 after 129 steps.
Found uncertainty sample 7 after 270 steps.
Found uncertainty sample 8 after 106 steps.
Found uncertainty sample 9 after 97 steps.
Found uncertainty sample 10 after 162 steps.
Found uncertainty sample 11 after 118 steps.
Found uncertainty sample 12 after 84 steps.
Found uncertainty sample 13 after 61 steps.
Found uncertainty sample 14 after 259 steps.
Found uncertainty sample 15 after 57 steps.
Found uncertainty sample 16 after 83 steps.
Found uncertainty sample 17 after 470 steps.
Found uncertainty sample 18 after 162 steps.
Found uncertainty sample 19 after 292 steps.
Found uncertainty sample 20 after 7 steps.
Found uncertainty sample 21 after 35 steps.
Found uncertainty sample 22 after 225 steps.
Found uncertainty sample 23 after 10 steps.
Found uncertainty sample 24 after 433 steps.
Found uncertainty sample 25 after 85 steps.
Found uncertainty sample 26 after 25 steps.
Found uncertainty sample 27 after 68 steps.
Found uncertainty sample 28 after 45 steps.
Found uncertainty sample 29 after 204 steps.
Found uncertainty sample 30 after 72 steps.
Found uncertainty sample 31 after 46 steps.
Found uncertainty sample 32 after 100 steps.
Found uncertainty sample 33 after 112 steps.
Found uncertainty sample 34 after 139 steps.
Found uncertainty sample 35 after 139 steps.
Found uncertainty sample 36 after 276 steps.
Found uncertainty sample 37 after 44 steps.
Found uncertainty sample 38 after 74 steps.
Found uncertainty sample 39 after 12 steps.
Found uncertainty sample 40 after 9 steps.
Found uncertainty sample 41 after 43 steps.
Found uncertainty sample 42 after 128 steps.
Found uncertainty sample 43 after 53 steps.
Found uncertainty sample 44 after 15 steps.
Found uncertainty sample 45 after 216 steps.
Found uncertainty sample 46 after 27 steps.
Found uncertainty sample 47 after 50 steps.
Found uncertainty sample 48 after 20 steps.
Found uncertainty sample 49 after 101 steps.
Found uncertainty sample 50 after 76 steps.
Found uncertainty sample 51 after 190 steps.
Found uncertainty sample 52 after 118 steps.
Found uncertainty sample 53 after 128 steps.
Found uncertainty sample 54 after 11 steps.
Found uncertainty sample 55 after 25 steps.
Found uncertainty sample 56 after 56 steps.
Found uncertainty sample 57 after 30 steps.
Found uncertainty sample 58 after 232 steps.
Found uncertainty sample 59 after 1 steps.
Found uncertainty sample 60 after 56 steps.
Found uncertainty sample 61 after 22 steps.
Found uncertainty sample 62 after 243 steps.
Found uncertainty sample 63 after 241 steps.
Found uncertainty sample 64 after 104 steps.
Found uncertainty sample 65 after 8 steps.
Found uncertainty sample 66 after 57 steps.
Found uncertainty sample 67 after 224 steps.
Found uncertainty sample 68 after 136 steps.
Found uncertainty sample 69 after 28 steps.
Found uncertainty sample 70 after 389 steps.
Found uncertainty sample 71 after 20 steps.
Found uncertainty sample 72 after 73 steps.
Found uncertainty sample 73 after 95 steps.
Found uncertainty sample 74 after 35 steps.
Found uncertainty sample 75 after 29 steps.
Found uncertainty sample 76 after 252 steps.
Found uncertainty sample 77 after 142 steps.
Found uncertainty sample 78 after 117 steps.
Found uncertainty sample 79 after 60 steps.
Found uncertainty sample 80 after 415 steps.
Found uncertainty sample 81 after 18 steps.
Found uncertainty sample 82 after 214 steps.
Found uncertainty sample 83 after 73 steps.
Found uncertainty sample 84 after 56 steps.
Found uncertainty sample 85 after 432 steps.
Found uncertainty sample 86 after 33 steps.
Found uncertainty sample 87 after 9 steps.
Found uncertainty sample 88 after 17 steps.
Found uncertainty sample 89 after 111 steps.
Found uncertainty sample 90 after 169 steps.
Found uncertainty sample 91 after 216 steps.
Found uncertainty sample 92 after 65 steps.
Found uncertainty sample 93 after 25 steps.
Found uncertainty sample 94 after 201 steps.
Found uncertainty sample 95 after 12 steps.
Found uncertainty sample 96 after 84 steps.
Found uncertainty sample 97 after 272 steps.
Found uncertainty sample 98 after 59 steps.
Found uncertainty sample 99 after 144 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_050527-nca7ywdy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_107
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/nca7ywdy
Training model 107. Added 100 samples to the dataset.
Epoch 0, Batch 100/327, Loss: 0.035498231649398804
Epoch 0, Batch 200/327, Loss: 0.06939960271120071
Epoch 0, Batch 300/327, Loss: 0.0725635290145874

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.460947486157742, Training Loss Force: 1.78180512450345, time: 4.7795820236206055
Validation Loss Energy: 1.4592084509721013, Validation Loss Force: 1.8546856879272848, time: 0.27427172660827637
Test Loss Energy: 10.40944095335356, Test Loss Force: 12.040006273093516, time: 10.33819317817688

Epoch 1, Batch 100/327, Loss: 0.0985558032989502
Epoch 1, Batch 200/327, Loss: 0.04988282918930054
Epoch 1, Batch 300/327, Loss: 0.12274198979139328

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.1468867826347555, Training Loss Force: 1.7252968799852557, time: 4.6603615283966064
Validation Loss Energy: 2.2052957154516273, Validation Loss Force: 1.8322205977946304, time: 0.2759084701538086
Test Loss Energy: 11.758488390559048, Test Loss Force: 12.161461403559308, time: 10.522153854370117

Epoch 2, Batch 100/327, Loss: 0.11664842069149017
Epoch 2, Batch 200/327, Loss: 0.11316396296024323
Epoch 2, Batch 300/327, Loss: 0.11588529497385025

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.0524870895026974, Training Loss Force: 1.768357903599908, time: 4.692080974578857
Validation Loss Energy: 4.598654852321138, Validation Loss Force: 2.0526128626376483, time: 0.27326154708862305
Test Loss Energy: 13.374112685218902, Test Loss Force: 12.226872136942902, time: 10.360944986343384

Epoch 3, Batch 100/327, Loss: 0.14258161187171936
Epoch 3, Batch 200/327, Loss: 0.140274316072464
Epoch 3, Batch 300/327, Loss: 0.21807041764259338

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5652273021127547, Training Loss Force: 1.7402420353878154, time: 4.659284830093384
Validation Loss Energy: 3.4104915771091955, Validation Loss Force: 1.9759356288916847, time: 0.27263402938842773
Test Loss Energy: 10.808531278627244, Test Loss Force: 12.319596628909817, time: 10.358569622039795

Epoch 4, Batch 100/327, Loss: 0.09760604798793793
Epoch 4, Batch 200/327, Loss: 0.03504680097103119
Epoch 4, Batch 300/327, Loss: 0.08571362495422363

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.280157176078731, Training Loss Force: 1.7731985074822862, time: 4.735594034194946
Validation Loss Energy: 3.570079444672225, Validation Loss Force: 1.8077825031606536, time: 0.28502440452575684
Test Loss Energy: 10.81145992116228, Test Loss Force: 12.25925703493562, time: 10.245180368423462

Epoch 5, Batch 100/327, Loss: 0.23676884174346924
Epoch 5, Batch 200/327, Loss: 0.23076680302619934
Epoch 5, Batch 300/327, Loss: 0.20029102265834808

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 2.4804637081132332, Training Loss Force: 1.7644112550533428, time: 4.637329816818237
Validation Loss Energy: 5.641035657949916, Validation Loss Force: 1.8569986636806095, time: 0.31086254119873047
Test Loss Energy: 14.336662513780414, Test Loss Force: 12.363760053217252, time: 10.436660766601562

Epoch 6, Batch 100/327, Loss: 0.36805880069732666
Epoch 6, Batch 200/327, Loss: 0.3702787756919861
Epoch 6, Batch 300/327, Loss: 1.8632686138153076

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 6.786284834446516, Training Loss Force: 3.087814708229476, time: 4.7890784740448
Validation Loss Energy: 5.8018722540833085, Validation Loss Force: 8.295026536233468, time: 0.2729949951171875
Test Loss Energy: 11.166092409216938, Test Loss Force: 13.845182031347914, time: 10.335624694824219

Epoch 7, Batch 100/327, Loss: 0.053374987095594406
Epoch 7, Batch 200/327, Loss: 0.41228413581848145
Epoch 7, Batch 300/327, Loss: 0.4688461422920227

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 9.311453464980072, Training Loss Force: 4.768948459576638, time: 4.7821338176727295
Validation Loss Energy: 10.502851862250495, Validation Loss Force: 5.157793822465588, time: 0.277083158493042
Test Loss Energy: 16.43342466111816, Test Loss Force: 11.106985236504133, time: 10.463245630264282

Epoch 8, Batch 100/327, Loss: 0.26927536725997925
Epoch 8, Batch 200/327, Loss: 0.3565269708633423
Epoch 8, Batch 300/327, Loss: 0.14597651362419128

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.156638798843646, Training Loss Force: 4.573471792319723, time: 4.688499689102173
Validation Loss Energy: 3.9058090721139926, Validation Loss Force: 5.680098239017053, time: 0.2865884304046631
Test Loss Energy: 10.21221511720393, Test Loss Force: 12.675092919364145, time: 10.279936790466309

Epoch 9, Batch 100/327, Loss: 0.4185687303543091
Epoch 9, Batch 200/327, Loss: 0.9008892178535461
Epoch 9, Batch 300/327, Loss: 1.3019083738327026

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 9.517895100999885, Training Loss Force: 4.607795022272225, time: 4.707488059997559
Validation Loss Energy: 10.731544297643287, Validation Loss Force: 8.75987278289612, time: 0.2767505645751953
Test Loss Energy: 16.59833563163598, Test Loss Force: 14.032269030141153, time: 10.448899745941162

Epoch 10, Batch 100/327, Loss: 2.312265157699585
Epoch 10, Batch 200/327, Loss: 0.6863147616386414
Epoch 10, Batch 300/327, Loss: 0.09117819368839264

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.746404749236888, Training Loss Force: 4.246791369512031, time: 4.791426181793213
Validation Loss Energy: 1.671218937023362, Validation Loss Force: 4.7622464001956075, time: 0.28139209747314453
Test Loss Energy: 9.441337429866634, Test Loss Force: 11.380362456307417, time: 10.350444316864014

Epoch 11, Batch 100/327, Loss: 0.1455221176147461
Epoch 11, Batch 200/327, Loss: 2.1588010787963867
Epoch 11, Batch 300/327, Loss: 0.4364112615585327

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.636143058184103, Training Loss Force: 3.5752861855421836, time: 4.684585809707642
Validation Loss Energy: 6.517552710671519, Validation Loss Force: 4.203999620785062, time: 0.27171778678894043
Test Loss Energy: 10.1974152900101, Test Loss Force: 11.397201261373086, time: 10.455018043518066

Epoch 12, Batch 100/327, Loss: 0.7498968243598938
Epoch 12, Batch 200/327, Loss: 0.08845055103302002
Epoch 12, Batch 300/327, Loss: 0.17570608854293823

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.38234486183915, Training Loss Force: 3.83641139746514, time: 4.654875040054321
Validation Loss Energy: 6.278121565844239, Validation Loss Force: 3.0194811722889816, time: 0.26905298233032227
Test Loss Energy: 11.97286211053858, Test Loss Force: 11.316057910598195, time: 10.293537855148315

Epoch 13, Batch 100/327, Loss: 0.23942920565605164
Epoch 13, Batch 200/327, Loss: 0.08743788301944733
Epoch 13, Batch 300/327, Loss: 0.1562095582485199

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 7.892047965184653, Training Loss Force: 3.7948330170395943, time: 4.640730619430542
Validation Loss Energy: 13.148713357219197, Validation Loss Force: 3.4591218505723864, time: 0.2737619876861572
Test Loss Energy: 17.654357556244115, Test Loss Force: 11.431643916057553, time: 11.821344137191772

Epoch 14, Batch 100/327, Loss: 0.10465697199106216
Epoch 14, Batch 200/327, Loss: 0.0701955109834671
Epoch 14, Batch 300/327, Loss: 0.6469926238059998

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 10.321680296516082, Training Loss Force: 4.239440777050722, time: 4.719166994094849
Validation Loss Energy: 16.82958308239501, Validation Loss Force: 3.3312132114681625, time: 0.27170515060424805
Test Loss Energy: 20.943695522092536, Test Loss Force: 12.32004122108516, time: 10.298619747161865

Epoch 15, Batch 100/327, Loss: 0.5258870124816895
Epoch 15, Batch 200/327, Loss: 0.22251661121845245
Epoch 15, Batch 300/327, Loss: 0.8478255271911621

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 8.364336408692884, Training Loss Force: 3.978152345239074, time: 4.802515983581543
Validation Loss Energy: 11.636723946302324, Validation Loss Force: 5.183691758287181, time: 0.2857234477996826
Test Loss Energy: 18.936408650564964, Test Loss Force: 12.422474722013558, time: 10.477623462677002

Epoch 16, Batch 100/327, Loss: 0.8869948983192444
Epoch 16, Batch 200/327, Loss: 0.7117143273353577
Epoch 16, Batch 300/327, Loss: 0.24953047931194305

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 9.565387737027697, Training Loss Force: 4.9541864100030235, time: 4.684705018997192
Validation Loss Energy: 2.5086073872760606, Validation Loss Force: 4.586045811128515, time: 0.2733786106109619
Test Loss Energy: 9.005488000633065, Test Loss Force: 11.580504773292969, time: 10.295063495635986

Epoch 17, Batch 100/327, Loss: 0.25125253200531006
Epoch 17, Batch 200/327, Loss: 0.18331290781497955
Epoch 17, Batch 300/327, Loss: 0.737602949142456

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 8.185006044178802, Training Loss Force: 3.9478479631120496, time: 4.716893672943115
Validation Loss Energy: 14.512618523057622, Validation Loss Force: 3.94107625758879, time: 0.2755401134490967
Test Loss Energy: 12.871522313445265, Test Loss Force: 12.095423783084083, time: 10.453867673873901

Epoch 18, Batch 100/327, Loss: 0.6140123605728149
Epoch 18, Batch 200/327, Loss: 0.8375424146652222
Epoch 18, Batch 300/327, Loss: 0.4018760621547699

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 7.71881336005624, Training Loss Force: 3.511850430154125, time: 4.7171149253845215
Validation Loss Energy: 8.838734000159173, Validation Loss Force: 3.69925827765248, time: 0.27453184127807617
Test Loss Energy: 10.986849487178963, Test Loss Force: 12.602139624233399, time: 10.22238278388977

Epoch 19, Batch 100/327, Loss: 0.35120871663093567
Epoch 19, Batch 200/327, Loss: 0.6442022919654846
Epoch 19, Batch 300/327, Loss: 0.8728325366973877

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.52101268234902, Training Loss Force: 3.940343636331477, time: 4.709431886672974
Validation Loss Energy: 7.826670311289729, Validation Loss Force: 4.114424900234839, time: 0.2721443176269531
Test Loss Energy: 14.802935930237942, Test Loss Force: 12.969293122457938, time: 10.44145917892456

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.058 MB uploadedwandb: | 0.039 MB of 0.061 MB uploadedwandb: / 0.039 MB of 0.061 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–ƒâ–„â–‚â–‚â–„â–‚â–…â–‚â–…â–â–‚â–ƒâ–†â–ˆâ–‡â–â–ƒâ–‚â–„
wandb:   test_error_force â–ƒâ–„â–„â–„â–„â–„â–ˆâ–â–…â–ˆâ–‚â–‚â–‚â–‚â–„â–„â–‚â–ƒâ–…â–…
wandb:          test_loss â–‚â–ƒâ–„â–ƒâ–ƒâ–„â–†â–ƒâ–ƒâ–ˆâ–â–â–‚â–„â–‡â–†â–â–ƒâ–„â–†
wandb: train_error_energy â–â–â–‚â–â–‚â–‚â–…â–‡â–†â–‡â–‡â–‡â–‡â–†â–ˆâ–‡â–‡â–†â–†â–‡
wandb:  train_error_force â–â–â–â–â–â–â–„â–ˆâ–‡â–‡â–†â–…â–†â–…â–†â–†â–ˆâ–†â–…â–†
wandb:         train_loss â–â–â–â–â–â–â–…â–ˆâ–‡â–‡â–‡â–†â–†â–†â–‡â–†â–ˆâ–†â–…â–‡
wandb: valid_error_energy â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–…â–‚â–…â–â–ƒâ–ƒâ–†â–ˆâ–†â–â–‡â–„â–„
wandb:  valid_error_force â–â–â–â–â–â–â–ˆâ–„â–…â–ˆâ–„â–ƒâ–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–ƒ
wandb:         valid_loss â–â–â–‚â–â–â–‚â–‡â–…â–„â–ˆâ–ƒâ–„â–ƒâ–„â–…â–…â–ƒâ–…â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 10440
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 14.80294
wandb:   test_error_force 12.96929
wandb:          test_loss 5.33019
wandb: train_error_energy 9.52101
wandb:  train_error_force 3.94034
wandb:         train_loss 1.9556
wandb: valid_error_energy 7.82667
wandb:  valid_error_force 4.11442
wandb:         valid_loss 1.90047
wandb: 
wandb: ğŸš€ View run al_77_107 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/nca7ywdy
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_050527-nca7ywdy/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.6467638611793518, Uncertainty Bias: -0.685276985168457
2.861023e-05 0.015102923
-9.931102 29.765556
(48745, 22, 3)
Found uncertainty sample 0 after 16 steps.
Found uncertainty sample 1 after 25 steps.
Found uncertainty sample 2 after 1 steps.
Found uncertainty sample 3 after 5 steps.
Found uncertainty sample 4 after 5 steps.
Found uncertainty sample 5 after 14 steps.
Found uncertainty sample 6 after 34 steps.
Found uncertainty sample 7 after 2 steps.
Found uncertainty sample 8 after 13 steps.
Found uncertainty sample 9 after 7 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 27 steps.
Found uncertainty sample 12 after 8 steps.
Found uncertainty sample 13 after 3 steps.
Found uncertainty sample 14 after 1 steps.
Found uncertainty sample 15 after 5 steps.
Found uncertainty sample 16 after 1 steps.
Found uncertainty sample 17 after 27 steps.
Found uncertainty sample 18 after 38 steps.
Found uncertainty sample 19 after 3 steps.
Found uncertainty sample 20 after 52 steps.
Found uncertainty sample 21 after 8 steps.
Found uncertainty sample 22 after 13 steps.
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 12 steps.
Found uncertainty sample 25 after 1 steps.
Found uncertainty sample 26 after 3 steps.
Found uncertainty sample 27 after 1 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 43 steps.
Found uncertainty sample 31 after 2 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 39 steps.
Found uncertainty sample 34 after 4 steps.
Found uncertainty sample 35 after 1 steps.
Found uncertainty sample 36 after 18 steps.
Found uncertainty sample 37 after 11 steps.
Found uncertainty sample 38 after 18 steps.
Found uncertainty sample 39 after 26 steps.
Found uncertainty sample 40 after 40 steps.
Found uncertainty sample 41 after 12 steps.
Found uncertainty sample 42 after 24 steps.
Found uncertainty sample 43 after 4 steps.
Found uncertainty sample 44 after 2 steps.
Found uncertainty sample 45 after 8 steps.
Found uncertainty sample 46 after 5 steps.
Found uncertainty sample 47 after 34 steps.
Found uncertainty sample 48 after 7 steps.
Found uncertainty sample 49 after 6 steps.
Found uncertainty sample 50 after 12 steps.
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 17 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 24 steps.
Found uncertainty sample 55 after 46 steps.
Found uncertainty sample 56 after 23 steps.
Found uncertainty sample 57 after 8 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 58 after 1 steps.
Found uncertainty sample 59 after 6 steps.
Found uncertainty sample 60 after 12 steps.
Found uncertainty sample 61 after 12 steps.
Found uncertainty sample 62 after 3 steps.
Found uncertainty sample 63 after 5 steps.
Found uncertainty sample 64 after 4 steps.
Found uncertainty sample 65 after 1 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 35 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 8 steps.
Found uncertainty sample 70 after 12 steps.
Found uncertainty sample 71 after 1 steps.
Found uncertainty sample 72 after 16 steps.
Found uncertainty sample 73 after 9 steps.
Found uncertainty sample 74 after 13 steps.
Found uncertainty sample 75 after 17 steps.
Found uncertainty sample 76 after 3 steps.
Found uncertainty sample 77 after 3 steps.
Found uncertainty sample 78 after 6 steps.
Found uncertainty sample 79 after 5 steps.
Found uncertainty sample 80 after 18 steps.
Found uncertainty sample 81 after 3 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 31 steps.
Found uncertainty sample 84 after 10 steps.
Found uncertainty sample 85 after 1 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 4 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 13 steps.
Found uncertainty sample 90 after 3 steps.
Found uncertainty sample 91 after 57 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 5 steps.
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 24 steps.
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 8 steps.
Found uncertainty sample 98 after 5 steps.
Found uncertainty sample 99 after 8 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_051558-rpacbnus
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_108
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/rpacbnus
Training model 108. Added 100 samples to the dataset.
Epoch 0, Batch 100/330, Loss: 0.03312278538942337
Epoch 0, Batch 200/330, Loss: 0.0355464369058609
Epoch 0, Batch 300/330, Loss: 0.17674550414085388

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.5888272090223232, Training Loss Force: 1.8267409671949977, time: 4.7908453941345215
Validation Loss Energy: 0.5637633787394752, Validation Loss Force: 1.8035227384867867, time: 0.2836954593658447
Test Loss Energy: 10.364129036565638, Test Loss Force: 11.84221009615725, time: 10.102536678314209

Epoch 1, Batch 100/330, Loss: 0.0737517774105072
Epoch 1, Batch 200/330, Loss: 0.15346625447273254
Epoch 1, Batch 300/330, Loss: 0.16328734159469604

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5685170876892558, Training Loss Force: 1.762443423734465, time: 4.704249382019043
Validation Loss Energy: 1.573960739256469, Validation Loss Force: 1.865581817025639, time: 0.2735254764556885
Test Loss Energy: 10.403604256435939, Test Loss Force: 11.876959367421469, time: 10.391696691513062

Epoch 2, Batch 100/330, Loss: 0.08239537477493286
Epoch 2, Batch 200/330, Loss: 0.13050386309623718
Epoch 2, Batch 300/330, Loss: 0.037082329392433167

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 1.5741124288166357, Training Loss Force: 1.7440409196862956, time: 4.769129753112793
Validation Loss Energy: 1.7931615720115237, Validation Loss Force: 1.9120565974889636, time: 0.2823038101196289
Test Loss Energy: 10.473197024153112, Test Loss Force: 12.255318990027233, time: 10.221450090408325

Epoch 3, Batch 100/330, Loss: 0.04261557385325432
Epoch 3, Batch 200/330, Loss: 0.19678546488285065
Epoch 3, Batch 300/330, Loss: 0.03664863482117653

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.5422386196594962, Training Loss Force: 1.7509415222124813, time: 4.757467031478882
Validation Loss Energy: 0.5974035328262238, Validation Loss Force: 1.8038492085662412, time: 0.29348015785217285
Test Loss Energy: 10.906758398507694, Test Loss Force: 12.163779592218845, time: 10.422302007675171

Epoch 4, Batch 100/330, Loss: 0.07003892958164215
Epoch 4, Batch 200/330, Loss: 0.046156421303749084
Epoch 4, Batch 300/330, Loss: 0.061561115086078644

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 1.0401286258902518, Training Loss Force: 1.702762856864491, time: 4.7595603466033936
Validation Loss Energy: 0.8404714641576239, Validation Loss Force: 1.738410494842021, time: 0.28332996368408203
Test Loss Energy: 10.76564952859731, Test Loss Force: 11.984035382477229, time: 10.192301988601685

Epoch 5, Batch 100/330, Loss: 0.03274131938815117
Epoch 5, Batch 200/330, Loss: 0.060229476541280746
Epoch 5, Batch 300/330, Loss: 0.04927721619606018

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 0.9459117408205368, Training Loss Force: 1.692534527042006, time: 4.726754903793335
Validation Loss Energy: 1.341758366405264, Validation Loss Force: 1.7230089153564876, time: 0.28000593185424805
Test Loss Energy: 11.662003200914928, Test Loss Force: 12.162501886348526, time: 10.37188720703125

Epoch 6, Batch 100/330, Loss: 0.43217357993125916
Epoch 6, Batch 200/330, Loss: 0.2193680703639984
Epoch 6, Batch 300/330, Loss: 0.29818159341812134

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 5.450992390015135, Training Loss Force: 2.61251282550571, time: 4.74429178237915
Validation Loss Energy: 6.101876160900375, Validation Loss Force: 2.6205695383831427, time: 0.2780325412750244
Test Loss Energy: 15.471077944123554, Test Loss Force: 11.56184630664032, time: 10.258723020553589

Epoch 7, Batch 100/330, Loss: 0.261228084564209
Epoch 7, Batch 200/330, Loss: 0.21113790571689606
Epoch 7, Batch 300/330, Loss: 0.4510802626609802

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 7.669393993891425, Training Loss Force: 3.5662164932852765, time: 4.7924644947052
Validation Loss Energy: 13.583323212110216, Validation Loss Force: 3.874206441841387, time: 0.2808067798614502
Test Loss Energy: 20.174064646738817, Test Loss Force: 12.264953499255205, time: 10.541987419128418

Epoch 8, Batch 100/330, Loss: 1.1274186372756958
Epoch 8, Batch 200/330, Loss: 0.1287803053855896
Epoch 8, Batch 300/330, Loss: 0.5899900197982788

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 8.917651002116074, Training Loss Force: 4.417420109945796, time: 4.7629475593566895
Validation Loss Energy: 1.1471165858754069, Validation Loss Force: 3.1780197040795213, time: 0.2850914001464844
Test Loss Energy: 9.760076546937546, Test Loss Force: 11.875480289368356, time: 10.217037677764893

Epoch 9, Batch 100/330, Loss: 0.795466959476471
Epoch 9, Batch 200/330, Loss: 0.6188385486602783
Epoch 9, Batch 300/330, Loss: 1.3794286251068115

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.826547440607952, Training Loss Force: 3.9875544270090395, time: 4.664149761199951
Validation Loss Energy: 2.3951819732988198, Validation Loss Force: 3.114685659122611, time: 0.2781987190246582
Test Loss Energy: 10.824589146858127, Test Loss Force: 10.705757697793626, time: 10.40556263923645

Epoch 10, Batch 100/330, Loss: 0.3089430332183838
Epoch 10, Batch 200/330, Loss: 0.39292025566101074
Epoch 10, Batch 300/330, Loss: 0.28796228766441345

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 7.37887128347199, Training Loss Force: 4.141739181188036, time: 4.670732498168945
Validation Loss Energy: 21.06778542622507, Validation Loss Force: 4.025192822966372, time: 0.28186798095703125
Test Loss Energy: 26.077838660368883, Test Loss Force: 12.371524393433752, time: 10.180359363555908

Epoch 11, Batch 100/330, Loss: 0.5685166120529175
Epoch 11, Batch 200/330, Loss: 0.798670768737793
Epoch 11, Batch 300/330, Loss: 0.5889796018600464

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 8.310815128189722, Training Loss Force: 3.9846161274689407, time: 4.72841215133667
Validation Loss Energy: 5.261643565069045, Validation Loss Force: 3.594499146834748, time: 0.2764279842376709
Test Loss Energy: 12.83202594117191, Test Loss Force: 13.086375483230839, time: 10.30219841003418

Epoch 12, Batch 100/330, Loss: 1.2146430015563965
Epoch 12, Batch 200/330, Loss: 0.3713763952255249
Epoch 12, Batch 300/330, Loss: 0.3250167965888977

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 10.226794180165534, Training Loss Force: 4.660634771303859, time: 4.739819049835205
Validation Loss Energy: 14.542450666417125, Validation Loss Force: 4.11151979526636, time: 0.27931857109069824
Test Loss Energy: 13.185238424948128, Test Loss Force: 11.032870993410416, time: 10.16170859336853

Epoch 13, Batch 100/330, Loss: 0.4277365505695343
Epoch 13, Batch 200/330, Loss: 0.09395141899585724
Epoch 13, Batch 300/330, Loss: 0.09741102159023285

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 6.332871261790282, Training Loss Force: 3.1566369243208485, time: 4.672142744064331
Validation Loss Energy: 9.297225239707824, Validation Loss Force: 4.9642433137307025, time: 0.27738165855407715
Test Loss Energy: 13.904396467635474, Test Loss Force: 11.573701398498642, time: 10.30316162109375

Epoch 14, Batch 100/330, Loss: 0.3214125633239746
Epoch 14, Batch 200/330, Loss: 0.41367465257644653
Epoch 14, Batch 300/330, Loss: 1.4666690826416016

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 7.636492557938357, Training Loss Force: 3.6406855582711275, time: 4.816458225250244
Validation Loss Energy: 11.597755420682427, Validation Loss Force: 3.5898222040936805, time: 0.27400684356689453
Test Loss Energy: 16.93688888921143, Test Loss Force: 12.869830578197142, time: 10.165857791900635

Epoch 15, Batch 100/330, Loss: 0.6483398675918579
Epoch 15, Batch 200/330, Loss: 0.42519262433052063
Epoch 15, Batch 300/330, Loss: 0.3917168378829956

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 7.378807790714536, Training Loss Force: 3.514745400785543, time: 4.790610313415527
Validation Loss Energy: 8.48299988055952, Validation Loss Force: 5.4281718814645465, time: 0.2808547019958496
Test Loss Energy: 10.839909404307308, Test Loss Force: 13.305009071485625, time: 10.35480809211731

Epoch 16, Batch 100/330, Loss: 0.2236628532409668
Epoch 16, Batch 200/330, Loss: 0.4016057848930359
Epoch 16, Batch 300/330, Loss: 0.8459793925285339

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 10.886586730188078, Training Loss Force: 4.665644415486691, time: 4.744749069213867
Validation Loss Energy: 9.847654164783284, Validation Loss Force: 5.2656622978147505, time: 0.27327752113342285
Test Loss Energy: 11.773060377086889, Test Loss Force: 12.344258600815563, time: 10.207574367523193

Epoch 17, Batch 100/330, Loss: 1.299302577972412
Epoch 17, Batch 200/330, Loss: 0.45763009786605835
Epoch 17, Batch 300/330, Loss: 0.09061971306800842

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 9.393566514271496, Training Loss Force: 4.641271079113145, time: 4.803511619567871
Validation Loss Energy: 7.803846664453691, Validation Loss Force: 5.053342145782248, time: 0.31217455863952637
Test Loss Energy: 10.633344334538505, Test Loss Force: 12.149367386541599, time: 10.389862537384033

Epoch 18, Batch 100/330, Loss: 1.0751209259033203
Epoch 18, Batch 200/330, Loss: 0.08394227921962738
Epoch 18, Batch 300/330, Loss: 0.2763391435146332

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.23544052362336, Training Loss Force: 4.36393398515558, time: 4.67312216758728
Validation Loss Energy: 4.174624602337537, Validation Loss Force: 3.7254599267668147, time: 0.2751750946044922
Test Loss Energy: 9.921547183554608, Test Loss Force: 12.582509326836899, time: 10.216543436050415

Epoch 19, Batch 100/330, Loss: 0.20428401231765747
Epoch 19, Batch 200/330, Loss: 2.129861354827881
Epoch 19, Batch 300/330, Loss: 0.6825761198997498

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.74871600438047, Training Loss Force: 4.010322109666401, time: 4.71625542640686
Validation Loss Energy: 3.3725371610029016, Validation Loss Force: 3.590722641008289, time: 0.27713918685913086
Test Loss Energy: 11.127806191290382, Test Loss Force: 11.861962523499965, time: 10.329925775527954

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.051 MB uploadedwandb: | 0.039 MB of 0.051 MB uploadedwandb: / 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–â–â–â–â–‚â–ƒâ–…â–â–â–ˆâ–‚â–‚â–ƒâ–„â–â–‚â–â–â–‚
wandb:   test_error_force â–„â–„â–…â–…â–„â–…â–ƒâ–…â–„â–â–…â–‡â–‚â–ƒâ–‡â–ˆâ–…â–…â–†â–„
wandb:          test_loss â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–†â–‚â–â–ˆâ–…â–‚â–ƒâ–†â–…â–„â–ƒâ–„â–ƒ
wandb: train_error_energy â–â–â–â–â–â–â–„â–†â–‡â–‡â–†â–†â–ˆâ–…â–†â–†â–ˆâ–‡â–†â–†
wandb:  train_error_force â–â–â–â–â–â–â–ƒâ–…â–‡â–†â–‡â–†â–ˆâ–„â–†â–…â–ˆâ–ˆâ–‡â–†
wandb:         train_loss â–â–â–â–â–â–â–„â–†â–‡â–†â–†â–†â–ˆâ–…â–†â–…â–ˆâ–ˆâ–‡â–†
wandb: valid_error_energy â–â–â–â–â–â–â–ƒâ–…â–â–‚â–ˆâ–ƒâ–†â–„â–…â–„â–„â–ƒâ–‚â–‚
wandb:  valid_error_force â–â–â–â–â–â–â–ƒâ–…â–„â–„â–…â–…â–†â–‡â–…â–ˆâ–ˆâ–‡â–…â–…
wandb:         valid_loss â–â–â–â–â–â–â–ƒâ–†â–ƒâ–ƒâ–ˆâ–„â–‡â–†â–…â–‡â–‡â–†â–„â–„
wandb: 
wandb: Run summary:
wandb:       dataset_size 10530
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 11.12781
wandb:   test_error_force 11.86196
wandb:          test_loss 4.71373
wandb: train_error_energy 8.74872
wandb:  train_error_force 4.01032
wandb:         train_loss 1.92734
wandb: valid_error_energy 3.37254
wandb:  valid_error_force 3.59072
wandb:         valid_loss 1.42716
wandb: 
wandb: ğŸš€ View run al_77_108 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/rpacbnus
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_051558-rpacbnus/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.45954954624176025, Uncertainty Bias: -0.3803219199180603
5.722046e-06 0.0048446655
-5.6192217 16.216042
(48745, 22, 3)
Found uncertainty sample 0 after 3 steps.
Found uncertainty sample 1 after 1 steps.
Found uncertainty sample 2 after 10 steps.
Found uncertainty sample 3 after 2 steps.
Found uncertainty sample 4 after 3 steps.
Found uncertainty sample 5 after 4 steps.
Found uncertainty sample 6 after 6 steps.
Found uncertainty sample 7 after 20 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 1 steps.
Found uncertainty sample 10 after 1 steps.
Found uncertainty sample 11 after 12 steps.
Found uncertainty sample 12 after 1 steps.
Found uncertainty sample 13 after 9 steps.
Found uncertainty sample 14 after 7 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 15 after 1 steps.
Found uncertainty sample 16 after 10 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 10 steps.
Found uncertainty sample 19 after 2 steps.
Found uncertainty sample 20 after 2 steps.
Found uncertainty sample 21 after 5 steps.
Found uncertainty sample 22 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 23 after 1 steps.
Found uncertainty sample 24 after 7 steps.
Found uncertainty sample 25 after 8 steps.
Found uncertainty sample 26 after 2 steps.
Found uncertainty sample 27 after 4 steps.
Found uncertainty sample 28 after 5 steps.
Found uncertainty sample 29 after 1 steps.
Found uncertainty sample 30 after 5 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 1 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 3 steps.
Found uncertainty sample 35 after 5 steps.
Found uncertainty sample 36 after 13 steps.
Found uncertainty sample 37 after 1 steps.
Found uncertainty sample 38 after 2 steps.
Found uncertainty sample 39 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 7 steps.
Found uncertainty sample 42 after 3 steps.
Found uncertainty sample 43 after 8 steps.
Found uncertainty sample 44 after 1 steps.
Found uncertainty sample 45 after 1 steps.
Found uncertainty sample 46 after 1 steps.
Found uncertainty sample 47 after 1 steps.
Found uncertainty sample 48 after 2 steps.
Found uncertainty sample 49 after 1 steps.
Found uncertainty sample 50 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 51 after 1 steps.
Found uncertainty sample 52 after 3 steps.
Found uncertainty sample 53 after 16 steps.
Found uncertainty sample 54 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 14 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 5 steps.
Found uncertainty sample 59 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 14 steps.
Found uncertainty sample 62 after 1 steps.
Found uncertainty sample 63 after 7 steps.
Found uncertainty sample 64 after 4 steps.
Found uncertainty sample 65 after 7 steps.
Found uncertainty sample 66 after 3 steps.
Found uncertainty sample 67 after 1 steps.
Found uncertainty sample 68 after 7 steps.
Found uncertainty sample 69 after 1 steps.
Found uncertainty sample 70 after 13 steps.
Found uncertainty sample 71 after 5 steps.
Found uncertainty sample 72 after 7 steps.
Found uncertainty sample 73 after 3 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Found uncertainty sample 77 after 5 steps.
Found uncertainty sample 78 after 6 steps.
Found uncertainty sample 79 after 2 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 7 steps.
Found uncertainty sample 82 after 7 steps.
Found uncertainty sample 83 after 7 steps.
Found uncertainty sample 84 after 10 steps.
Found uncertainty sample 85 after 15 steps.
Found uncertainty sample 86 after 2 steps.
Found uncertainty sample 87 after 4 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 4 steps.
Found uncertainty sample 90 after 4 steps.
Found uncertainty sample 91 after 1 steps.
Found uncertainty sample 92 after 2 steps.
Found uncertainty sample 93 after 1 steps.
Found uncertainty sample 94 after 16 steps.
Found uncertainty sample 95 after 11 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 96 after 1 steps.
Found uncertainty sample 97 after 9 steps.
Found uncertainty sample 98 after 3 steps.
Found uncertainty sample 99 after 3 steps.
wandb: wandb version 0.19.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241208_052611-wvnwgnpc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_77_109
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/wvnwgnpc
Training model 109. Added 100 samples to the dataset.
Epoch 0, Batch 100/332, Loss: 0.1833782196044922
Epoch 0, Batch 200/332, Loss: 0.10645647346973419
Epoch 0, Batch 300/332, Loss: 0.06392629444599152

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 1.3084688440682646, Training Loss Force: 1.7576765176994, time: 4.904736042022705
Validation Loss Energy: 0.5658610751791527, Validation Loss Force: 1.7882339409236654, time: 0.29660701751708984
Test Loss Energy: 11.018457505345419, Test Loss Force: 12.364532400385722, time: 10.221411228179932

Epoch 1, Batch 100/332, Loss: 0.031213687732815742
Epoch 1, Batch 200/332, Loss: 0.06716827303171158
Epoch 1, Batch 300/332, Loss: 0.21667234599590302

Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 1.5538588786876837, Training Loss Force: 1.7484008883496358, time: 4.839349269866943
Validation Loss Energy: 3.9050022527306383, Validation Loss Force: 1.9511398756532137, time: 0.2804131507873535
Test Loss Energy: 10.616814043986459, Test Loss Force: 12.06858916187052, time: 10.474790334701538

Epoch 2, Batch 100/332, Loss: 0.1109374612569809
Epoch 2, Batch 200/332, Loss: 0.10117712616920471
Epoch 2, Batch 300/332, Loss: 0.10160708427429199

Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 2.493319689275986, Training Loss Force: 1.7843770690852534, time: 4.712017059326172
Validation Loss Energy: 3.550851763519129, Validation Loss Force: 1.8765310898268224, time: 0.2779688835144043
Test Loss Energy: 10.70399864884662, Test Loss Force: 12.195807729939117, time: 10.375592231750488

Epoch 3, Batch 100/332, Loss: 0.08616556227207184
Epoch 3, Batch 200/332, Loss: 0.1263347715139389
Epoch 3, Batch 300/332, Loss: 0.10545556992292404

Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 1.598540783777482, Training Loss Force: 1.734676504514376, time: 4.656373739242554
Validation Loss Energy: 3.1337089151551356, Validation Loss Force: 1.8241409365000674, time: 0.28169846534729004
Test Loss Energy: 12.682596445228146, Test Loss Force: 12.403526664227273, time: 10.488580226898193

Epoch 4, Batch 100/332, Loss: 0.25404092669487
Epoch 4, Batch 200/332, Loss: 0.1802772730588913
Epoch 4, Batch 300/332, Loss: 0.20502561330795288

Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 2.4955082696030924, Training Loss Force: 1.7616478105152888, time: 4.829986572265625
Validation Loss Energy: 1.3090420712439104, Validation Loss Force: 1.863155706790421, time: 0.2862207889556885
Test Loss Energy: 11.169586174386266, Test Loss Force: 12.085223111321936, time: 10.197282552719116

Epoch 5, Batch 100/332, Loss: 0.22223423421382904
Epoch 5, Batch 200/332, Loss: 0.0717315599322319
Epoch 5, Batch 300/332, Loss: 0.06981514394283295

Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 1.8606333756505187, Training Loss Force: 1.7055089607486866, time: 4.73405385017395
Validation Loss Energy: 2.9971005405633773, Validation Loss Force: 1.8864912106145073, time: 0.30121636390686035
Test Loss Energy: 10.969631902077353, Test Loss Force: 12.182401401234715, time: 10.358007907867432

Epoch 6, Batch 100/332, Loss: 0.10488182306289673
Epoch 6, Batch 200/332, Loss: 0.17840111255645752
Epoch 6, Batch 300/332, Loss: 0.4504771828651428

Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 6.065657112736372, Training Loss Force: 3.054499333846499, time: 4.76579737663269
Validation Loss Energy: 2.768566289331852, Validation Loss Force: 2.3909022352025633, time: 0.29342079162597656
Test Loss Energy: 9.92876124885536, Test Loss Force: 11.230037135576024, time: 10.27601146697998

Epoch 7, Batch 100/332, Loss: 0.35107743740081787
Epoch 7, Batch 200/332, Loss: 0.39209336042404175
Epoch 7, Batch 300/332, Loss: 0.26986801624298096

Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 4.888844888786945, Training Loss Force: 2.508315078488882, time: 4.8162055015563965
Validation Loss Energy: 3.9142381977884795, Validation Loss Force: 2.7062113577211098, time: 0.2816731929779053
Test Loss Energy: 9.948947869374562, Test Loss Force: 11.66778248883034, time: 10.396925687789917

Epoch 8, Batch 100/332, Loss: 1.7802786827087402
Epoch 8, Batch 200/332, Loss: 0.5110285878181458
Epoch 8, Batch 300/332, Loss: 0.362933874130249

Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 9.010479699568645, Training Loss Force: 4.028636326190843, time: 4.7939653396606445
Validation Loss Energy: 6.426414065952568, Validation Loss Force: 8.052686411994566, time: 0.2813270092010498
Test Loss Energy: 12.471343945118084, Test Loss Force: 13.959523206695668, time: 10.339461088180542

Epoch 9, Batch 100/332, Loss: 0.24504444003105164
Epoch 9, Batch 200/332, Loss: 0.47495222091674805
Epoch 9, Batch 300/332, Loss: 1.5956792831420898

Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 8.034664661474197, Training Loss Force: 4.106131900570531, time: 4.727021932601929
Validation Loss Energy: 8.421749536358677, Validation Loss Force: 3.2903566593153335, time: 0.28043293952941895
Test Loss Energy: 10.6514018509747, Test Loss Force: 11.304041111976032, time: 10.436256408691406

Epoch 10, Batch 100/332, Loss: 0.6719329953193665
Epoch 10, Batch 200/332, Loss: 0.5377945899963379
Epoch 10, Batch 300/332, Loss: 0.1519867181777954

Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 8.298520347393184, Training Loss Force: 3.6707800684009917, time: 4.700984239578247
Validation Loss Energy: 3.4019047348786486, Validation Loss Force: 3.3943925830140067, time: 0.275665283203125
Test Loss Energy: 10.448624063947001, Test Loss Force: 10.96229294153607, time: 10.252142906188965

Epoch 11, Batch 100/332, Loss: 0.21652470529079437
Epoch 11, Batch 200/332, Loss: 0.23144352436065674
Epoch 11, Batch 300/332, Loss: 0.7540793418884277

Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 9.127780598065698, Training Loss Force: 3.8920896796893287, time: 4.774657964706421
Validation Loss Energy: 1.3944578813987611, Validation Loss Force: 3.2084466264170186, time: 0.2799689769744873
Test Loss Energy: 10.913036381371876, Test Loss Force: 10.912229419060578, time: 10.541007041931152

Epoch 12, Batch 100/332, Loss: 0.5489381551742554
Epoch 12, Batch 200/332, Loss: 0.6912376880645752
Epoch 12, Batch 300/332, Loss: 0.16103090345859528

Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 8.820642386311192, Training Loss Force: 3.860774433325773, time: 4.802366495132446
Validation Loss Energy: 5.7920184615239565, Validation Loss Force: 2.777188521111992, time: 0.290264368057251
Test Loss Energy: 12.327958277361555, Test Loss Force: 10.945088786531638, time: 10.31784725189209

Epoch 13, Batch 100/332, Loss: 0.5147007703781128
Epoch 13, Batch 200/332, Loss: 0.24921612441539764
Epoch 13, Batch 300/332, Loss: 0.3619635999202728

Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 8.594936371428481, Training Loss Force: 4.243962589493433, time: 4.627809286117554
Validation Loss Energy: 6.282125869629287, Validation Loss Force: 4.085555236836304, time: 0.3069908618927002
Test Loss Energy: 12.579278451156615, Test Loss Force: 11.261841946635196, time: 11.791362285614014

Epoch 14, Batch 100/332, Loss: 0.24680928885936737
Epoch 14, Batch 200/332, Loss: 2.271827220916748
Epoch 14, Batch 300/332, Loss: 0.32988062500953674

Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 8.146675822686277, Training Loss Force: 3.646001197686773, time: 4.7109153270721436
Validation Loss Energy: 7.675748890176916, Validation Loss Force: 2.7791630768750313, time: 0.28674817085266113
Test Loss Energy: 10.729927008967874, Test Loss Force: 12.072099136421029, time: 10.202818870544434

Epoch 15, Batch 100/332, Loss: 0.26558831334114075
Epoch 15, Batch 200/332, Loss: 0.29664939641952515
Epoch 15, Batch 300/332, Loss: 0.30043092370033264

Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 4.934565927484401, Training Loss Force: 2.5697592416496344, time: 4.742522716522217
Validation Loss Energy: 4.655091549769924, Validation Loss Force: 2.1209945350851576, time: 0.28287792205810547
Test Loss Energy: 10.415373059919888, Test Loss Force: 12.180186820179038, time: 10.451576948165894

Epoch 16, Batch 100/332, Loss: 0.233629047870636
Epoch 16, Batch 200/332, Loss: 0.4677656292915344
Epoch 16, Batch 300/332, Loss: 0.16154098510742188

Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 4.728671239398444, Training Loss Force: 2.283694844471667, time: 4.854686737060547
Validation Loss Energy: 3.8974026661146954, Validation Loss Force: 2.345424325949796, time: 0.2959110736846924
Test Loss Energy: 10.503440898851576, Test Loss Force: 12.310854759484622, time: 10.31466031074524

Epoch 17, Batch 100/332, Loss: 0.46774211525917053
Epoch 17, Batch 200/332, Loss: 0.6381341218948364
Epoch 17, Batch 300/332, Loss: 0.12993299961090088

Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 7.002572065658095, Training Loss Force: 3.6476963524084938, time: 4.761066436767578
Validation Loss Energy: 3.6010275835242047, Validation Loss Force: 5.835845316171194, time: 0.28494811058044434
Test Loss Energy: 9.600195674757845, Test Loss Force: 12.692717151953346, time: 10.322367191314697

Epoch 18, Batch 100/332, Loss: 0.2016419619321823
Epoch 18, Batch 200/332, Loss: 0.7374401092529297
Epoch 18, Batch 300/332, Loss: 0.17988426983356476

Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 8.421419790237056, Training Loss Force: 4.135237328698824, time: 4.818937540054321
Validation Loss Energy: 11.104382198436824, Validation Loss Force: 5.984610349896403, time: 0.2818458080291748
Test Loss Energy: 17.02486750659822, Test Loss Force: 14.369188143336586, time: 10.23787260055542

Epoch 19, Batch 100/332, Loss: 0.2137220948934555
Epoch 19, Batch 200/332, Loss: 0.618280291557312
Epoch 19, Batch 300/332, Loss: 0.6843881011009216

Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 8.295373137325154, Training Loss Force: 4.136824327248431, time: 4.7896809577941895
Validation Loss Energy: 1.2458018627762457, Validation Loss Force: 4.124346725640578, time: 0.28775525093078613
Test Loss Energy: 9.880674370880534, Test Loss Force: 12.530750021431217, time: 10.354190349578857

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.058 MB uploadedwandb: / 0.039 MB of 0.058 MB uploadedwandb: - 0.061 MB of 0.061 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–‚â–‚â–‚â–„â–‚â–‚â–â–â–„â–‚â–‚â–‚â–„â–„â–‚â–‚â–‚â–â–ˆâ–
wandb:   test_error_force â–„â–ƒâ–„â–„â–ƒâ–„â–‚â–ƒâ–‡â–‚â–â–â–â–‚â–ƒâ–„â–„â–…â–ˆâ–„
wandb:          test_loss â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–â–‚â–†â–‚â–â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ƒ
wandb: train_error_energy â–â–â–‚â–â–‚â–â–…â–„â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–„â–„â–†â–‡â–‡
wandb:  train_error_force â–â–â–â–â–â–â–…â–ƒâ–‡â–ˆâ–†â–‡â–‡â–ˆâ–†â–ƒâ–ƒâ–†â–ˆâ–ˆ
wandb:         train_loss â–â–â–â–â–â–â–…â–„â–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–‡â–„â–ƒâ–†â–ˆâ–ˆ
wandb: valid_error_energy â–â–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–…â–†â–ƒâ–‚â–„â–…â–†â–„â–ƒâ–ƒâ–ˆâ–
wandb:  valid_error_force â–â–â–â–â–â–â–‚â–‚â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–„â–‚â–â–‚â–†â–†â–„
wandb:         valid_loss â–â–‚â–‚â–‚â–â–‚â–‚â–‚â–ˆâ–„â–ƒâ–‚â–ƒâ–„â–ƒâ–‚â–‚â–…â–‡â–ƒ
wandb: 
wandb: Run summary:
wandb:       dataset_size 10620
wandb:                 lr 0.001
wandb:    max_uncertainty 4
wandb:  test_error_energy 9.88067
wandb:   test_error_force 12.53075
wandb:          test_loss 4.85405
wandb: train_error_energy 8.29537
wandb:  train_error_force 4.13682
wandb:         train_loss 1.93933
wandb: valid_error_energy 1.2458
wandb:  valid_error_force 4.12435
wandb:         valid_loss 1.46339
wandb: 
wandb: ğŸš€ View run al_77_109 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG/runs/wvnwgnpc
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-SWAG
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241208_052611-wvnwgnpc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 0.7783614993095398, Uncertainty Bias: -0.7436385750770569
/var/lib/slurmd/job5124880/slurm_script: line 15: 284252 Killed                  python3 active_learning.py
slurmstepd: error: Detected 1 oom-kill event(s) in step 5124880.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
