wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_232240-uqmh5se5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_65
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/uqmh5se5
['H1', 'CH3', 'H2', 'H3', 'C', 'O', 'N', 'H', 'CA', 'HA', 'CB', 'HB1', 'HB2', 'HB3', 'C', 'O', 'N', 'H', 'C', 'H1', 'H2', 'H3']
64
Uncertainty Slope: 5.450337886810303, Uncertainty Bias: -0.39419710636138916
0.00031280518 0.00207901
1.6673831 2.277966
(48745, 22, 3)

Training and Validation Results of Epoch Initital validation:
================================
Training Loss Energy: 0.0, Training Loss Force: 0.0, time: 0
Validation Loss Energy: 0.0, Validation Loss Force: 0.0, time: 0
Test Loss Energy: 10.996634949766577, Test Loss Force: 13.3864807834733, time: 6.635629177093506

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.050 MB of 0.050 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–
wandb:    max_uncertainty â–
wandb:  test_error_energy â–
wandb:   test_error_force â–
wandb:          test_loss â–
wandb: train_error_energy â–
wandb:  train_error_force â–
wandb:         train_loss â–
wandb: valid_error_energy â–
wandb:  valid_error_force â–
wandb:         valid_loss â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 800
wandb:    max_uncertainty 8
wandb:  test_error_energy 10.99663
wandb:   test_error_force 13.38648
wandb:          test_loss 16.63181
wandb: train_error_energy 0.0
wandb:  train_error_force 0.0
wandb:         train_loss 0.0
wandb: valid_error_energy 0.0
wandb:  valid_error_force 0.0
wandb:         valid_loss 0.0
wandb: 
wandb: ğŸš€ View run al_65 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/uqmh5se5
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_232240-uqmh5se5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Did not find any uncertainty samples for sample 0.
Did not find any uncertainty samples for sample 1.
Found uncertainty sample 2 after 355 steps.
Found uncertainty sample 3 after 1192 steps.
Found uncertainty sample 4 after 2130 steps.
Found uncertainty sample 5 after 1899 steps.
Found uncertainty sample 6 after 1876 steps.
Did not find any uncertainty samples for sample 7.
Did not find any uncertainty samples for sample 8.
Did not find any uncertainty samples for sample 9.
Did not find any uncertainty samples for sample 10.
Did not find any uncertainty samples for sample 11.
Found uncertainty sample 12 after 2053 steps.
Found uncertainty sample 13 after 836 steps.
Found uncertainty sample 14 after 3236 steps.
Found uncertainty sample 15 after 2876 steps.
Did not find any uncertainty samples for sample 16.
Did not find any uncertainty samples for sample 17.
Did not find any uncertainty samples for sample 18.
Did not find any uncertainty samples for sample 19.
Did not find any uncertainty samples for sample 20.
Found uncertainty sample 21 after 2511 steps.
Found uncertainty sample 22 after 1224 steps.
Found uncertainty sample 23 after 1027 steps.
Did not find any uncertainty samples for sample 24.
Did not find any uncertainty samples for sample 25.
Did not find any uncertainty samples for sample 26.
Did not find any uncertainty samples for sample 27.
Did not find any uncertainty samples for sample 28.
Did not find any uncertainty samples for sample 29.
Found uncertainty sample 30 after 3459 steps.
Found uncertainty sample 31 after 9 steps.
Found uncertainty sample 32 after 7 steps.
Found uncertainty sample 33 after 4 steps.
Found uncertainty sample 34 after 4 steps.
Found uncertainty sample 35 after 4 steps.
Found uncertainty sample 36 after 3 steps.
Found uncertainty sample 37 after 3 steps.
Found uncertainty sample 38 after 4 steps.
Found uncertainty sample 39 after 3 steps.
Found uncertainty sample 40 after 3 steps.
Found uncertainty sample 41 after 3 steps.
Found uncertainty sample 42 after 3 steps.
Found uncertainty sample 43 after 2 steps.
Found uncertainty sample 44 after 3 steps.
Found uncertainty sample 45 after 3 steps.
Found uncertainty sample 46 after 3 steps.
Found uncertainty sample 47 after 3 steps.
Found uncertainty sample 48 after 3 steps.
Found uncertainty sample 49 after 4 steps.
Found uncertainty sample 50 after 2 steps.
Found uncertainty sample 51 after 2 steps.
Found uncertainty sample 52 after 2 steps.
Found uncertainty sample 53 after 2 steps.
Found uncertainty sample 54 after 2 steps.
Found uncertainty sample 55 after 2 steps.
Found uncertainty sample 56 after 2 steps.
Found uncertainty sample 57 after 2 steps.
Found uncertainty sample 58 after 2 steps.
Found uncertainty sample 59 after 2 steps.
Found uncertainty sample 60 after 2 steps.
Found uncertainty sample 61 after 2 steps.
Found uncertainty sample 62 after 2 steps.
Found uncertainty sample 63 after 2 steps.
Found uncertainty sample 64 after 2 steps.
Found uncertainty sample 65 after 2 steps.
Found uncertainty sample 66 after 2 steps.
Found uncertainty sample 67 after 2 steps.
Found uncertainty sample 68 after 2 steps.
Found uncertainty sample 69 after 2 steps.
Found uncertainty sample 70 after 2 steps.
Found uncertainty sample 71 after 2 steps.
Found uncertainty sample 72 after 2 steps.
Found uncertainty sample 73 after 2 steps.
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 2 steps.
Found uncertainty sample 76 after 2 steps.
Found uncertainty sample 77 after 1 steps.
Found uncertainty sample 78 after 2 steps.
Found uncertainty sample 79 after 1 steps.
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 1 steps.
Found uncertainty sample 82 after 2 steps.
Found uncertainty sample 83 after 2 steps.
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 2 steps.
Found uncertainty sample 86 after 1 steps.
Found uncertainty sample 87 after 1 steps.
Found uncertainty sample 88 after 1 steps.
Found uncertainty sample 89 after 2 steps.
Found uncertainty sample 90 after 1 steps.
Found uncertainty sample 91 after 2 steps.
Found uncertainty sample 92 after 1 steps.
Found uncertainty sample 93 after 2 steps.
Found uncertainty sample 94 after 2 steps.
Found uncertainty sample 95 after 1 steps.
Found uncertainty sample 96 after 2 steps.
Found uncertainty sample 97 after 1 steps.
Found uncertainty sample 98 after 1 steps.
Found uncertainty sample 99 after 2 steps.
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241130_233420-bth4ylya
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_65_0
wandb: â­ï¸ View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-MVE
wandb: ğŸš€ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-MVE/runs/bth4ylya
Training model 0. Added 82 samples to the dataset.

Training and Validation Results of Epoch 0:
================================
Training Loss Energy: 84.52580691733526, Training Loss Force: 26.24294745214615, time: 1.7600853443145752
Validation Loss Energy: 27.34756896488529, Validation Loss Force: 16.847744334207515, time: 0.04178333282470703
Test Loss Energy: 12.39603049693735, Test Loss Force: 19.12864209057636, time: 7.565440893173218


Training and Validation Results of Epoch 1:
================================
Training Loss Energy: 69.87604395750134, Training Loss Force: 33.51087981883275, time: 0.413088321685791
Validation Loss Energy: 19.673944857921867, Validation Loss Force: 17.593196100944564, time: 0.03399968147277832
Test Loss Energy: 13.28123646579647, Test Loss Force: 19.775720326057574, time: 7.207796335220337


Training and Validation Results of Epoch 2:
================================
Training Loss Energy: 52.189034896016786, Training Loss Force: 28.944746692312926, time: 0.45395660400390625
Validation Loss Energy: 10.241373443675977, Validation Loss Force: 14.101806240157709, time: 0.03279280662536621
Test Loss Energy: 14.730312003577266, Test Loss Force: 18.975319932260927, time: 7.156194686889648


Training and Validation Results of Epoch 3:
================================
Training Loss Energy: 29.07196322163482, Training Loss Force: 27.64375776883028, time: 0.40471768379211426
Validation Loss Energy: 7.181500800856961, Validation Loss Force: 14.178733086016258, time: 0.03541254997253418
Test Loss Energy: 15.706700936398306, Test Loss Force: 19.53756294853808, time: 7.382718086242676


Training and Validation Results of Epoch 4:
================================
Training Loss Energy: 18.24008686534702, Training Loss Force: 25.84819377699017, time: 0.4114811420440674
Validation Loss Energy: 5.67456642489094, Validation Loss Force: 12.77698449721848, time: 0.033797502517700195
Test Loss Energy: 14.822809652809028, Test Loss Force: 17.51876477698704, time: 7.141493320465088


Training and Validation Results of Epoch 5:
================================
Training Loss Energy: 17.843520667319417, Training Loss Force: 22.691959120868344, time: 0.4144096374511719
Validation Loss Energy: 7.923938963373023, Validation Loss Force: 12.063490206304989, time: 0.03670096397399902
Test Loss Energy: 13.943439686046531, Test Loss Force: 17.715982601710646, time: 7.123826265335083


Training and Validation Results of Epoch 6:
================================
Training Loss Energy: 14.159015053208211, Training Loss Force: 22.60156869643584, time: 0.41220855712890625
Validation Loss Energy: 6.882946009996983, Validation Loss Force: 11.358806488157567, time: 0.035727739334106445
Test Loss Energy: 13.020405871275496, Test Loss Force: 16.138113168778357, time: 7.1581926345825195


Training and Validation Results of Epoch 7:
================================
Training Loss Energy: 17.783020657171416, Training Loss Force: 20.53044185497261, time: 0.414595365524292
Validation Loss Energy: 7.333429295138367, Validation Loss Force: 12.30921451560214, time: 0.03452181816101074
Test Loss Energy: 15.570615473093598, Test Loss Force: 18.600948633489864, time: 7.399799108505249


Training and Validation Results of Epoch 8:
================================
Training Loss Energy: 13.19288153026677, Training Loss Force: 20.435580552188664, time: 0.4084584712982178
Validation Loss Energy: 5.519418580097711, Validation Loss Force: 10.325209267794373, time: 0.03285646438598633
Test Loss Energy: 16.094571003046127, Test Loss Force: 15.930505415176631, time: 7.1598498821258545


Training and Validation Results of Epoch 9:
================================
Training Loss Energy: 15.052665677604598, Training Loss Force: 18.835294570248085, time: 0.4095761775970459
Validation Loss Energy: 4.784008301165422, Validation Loss Force: 9.829603861583891, time: 0.03829789161682129
Test Loss Energy: 14.994058244216316, Test Loss Force: 15.591701038100087, time: 7.172203779220581


Training and Validation Results of Epoch 10:
================================
Training Loss Energy: 13.531105258907134, Training Loss Force: 18.59086937636803, time: 0.4487459659576416
Validation Loss Energy: 9.616463173748645, Validation Loss Force: 9.539015282088625, time: 0.03365659713745117
Test Loss Energy: 18.408804037301877, Test Loss Force: 15.624964161857253, time: 7.4758875370025635


Training and Validation Results of Epoch 11:
================================
Training Loss Energy: 13.111263136956921, Training Loss Force: 17.375733277747347, time: 0.4487760066986084
Validation Loss Energy: 4.803331529649036, Validation Loss Force: 9.338870456286308, time: 0.03389859199523926
Test Loss Energy: 16.534865174236423, Test Loss Force: 15.465746526632904, time: 7.305987119674683


Training and Validation Results of Epoch 12:
================================
Training Loss Energy: 11.225560925301954, Training Loss Force: 16.773266005992408, time: 0.3960535526275635
Validation Loss Energy: 4.970097359800193, Validation Loss Force: 9.39469984590649, time: 0.034378767013549805
Test Loss Energy: 15.157627613311208, Test Loss Force: 14.941469938082927, time: 7.132524013519287


Training and Validation Results of Epoch 13:
================================
Training Loss Energy: 11.439491710862942, Training Loss Force: 16.467120885190976, time: 0.4059786796569824
Validation Loss Energy: 4.351783440651825, Validation Loss Force: 8.605538125823818, time: 0.03751826286315918
Test Loss Energy: 15.256359476347386, Test Loss Force: 14.92686773849966, time: 7.213379859924316


Training and Validation Results of Epoch 14:
================================
Training Loss Energy: 13.840651972987624, Training Loss Force: 15.506424175379788, time: 0.4208967685699463
Validation Loss Energy: 9.194663682647388, Validation Loss Force: 8.674637705864713, time: 0.040169715881347656
Test Loss Energy: 12.444775459000379, Test Loss Force: 14.189405844347142, time: 7.16253662109375


Training and Validation Results of Epoch 15:
================================
Training Loss Energy: 10.885679397036514, Training Loss Force: 15.19158479463116, time: 0.40169548988342285
Validation Loss Energy: 4.014595998311669, Validation Loss Force: 8.805498581573307, time: 0.03473019599914551
Test Loss Energy: 14.236054284302504, Test Loss Force: 14.562890588828216, time: 7.3118908405303955


Training and Validation Results of Epoch 16:
================================
Training Loss Energy: 13.271915736803553, Training Loss Force: 16.548080881622433, time: 0.4068796634674072
Validation Loss Energy: 7.378867161056391, Validation Loss Force: 8.258948256127715, time: 0.033835411071777344
Test Loss Energy: 17.26013794483235, Test Loss Force: 14.549297064154707, time: 7.135086536407471


Training and Validation Results of Epoch 17:
================================
Training Loss Energy: 10.721377937776738, Training Loss Force: 14.592437503937067, time: 0.38236021995544434
Validation Loss Energy: 4.388114138501404, Validation Loss Force: 8.155114070675252, time: 0.03474617004394531
Test Loss Energy: 13.603848864862707, Test Loss Force: 14.105049240231283, time: 7.139423370361328


Training and Validation Results of Epoch 18:
================================
Training Loss Energy: 11.29112691697507, Training Loss Force: 14.854626017598935, time: 0.4105827808380127
Validation Loss Energy: 3.8462501079010076, Validation Loss Force: 8.041055808718369, time: 0.03835940361022949
Test Loss Energy: 13.170874730833711, Test Loss Force: 14.23267514128201, time: 7.193213224411011


Training and Validation Results of Epoch 19:
================================
Training Loss Energy: 9.245788884366004, Training Loss Force: 14.477699608031992, time: 0.4064197540283203
Validation Loss Energy: 4.47634678757529, Validation Loss Force: 7.920368240948925, time: 0.034529685974121094
Test Loss Energy: 15.069390643756407, Test Loss Force: 13.907854745638298, time: 7.369261264801025

wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.055 MB uploadedwandb: / 0.039 MB of 0.055 MB uploadedwandb: - 0.058 MB of 0.058 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:       dataset_size â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                 lr â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:    max_uncertainty â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:  test_error_energy â–â–‚â–„â–…â–„â–ƒâ–‚â–…â–…â–„â–ˆâ–†â–„â–„â–â–ƒâ–‡â–‚â–‚â–„
wandb:   test_error_force â–‡â–ˆâ–‡â–ˆâ–…â–†â–„â–‡â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–â–â–
wandb:          test_loss â–ˆâ–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–â–â–â–â–‚â–â–â–
wandb: train_error_energy â–ˆâ–‡â–…â–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–
wandb:  train_error_force â–…â–ˆâ–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–‚â–â–â–
wandb:         train_loss â–ˆâ–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: valid_error_energy â–ˆâ–†â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–ƒâ–â–â–â–ƒâ–â–‚â–â–â–
wandb:  valid_error_force â–‡â–ˆâ–…â–†â–…â–„â–ƒâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–
wandb:         valid_loss â–ˆâ–…â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–
wandb: 
wandb: Run summary:
wandb:       dataset_size 873
wandb:                 lr 0.0001
wandb:    max_uncertainty 8
wandb:  test_error_energy 15.06939
wandb:   test_error_force 13.90785
wandb:          test_loss 3.6585
wandb: train_error_energy 9.24579
wandb:  train_error_force 14.4777
wandb:         train_loss 3.18839
wandb: valid_error_energy 4.47635
wandb:  valid_error_force 7.92037
wandb:         valid_loss 1.60723
wandb: 
wandb: ğŸš€ View run al_65_0 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-MVE/runs/bth4ylya
wandb: â­ï¸ View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/GNN-Uncertainty-MVE
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241130_233420-bth4ylya/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
Uncertainty Slope: 2.3807408809661865, Uncertainty Bias: -1.860106110572815
4.5776367e-05 0.02908361
-8.85079 296.32138
(48745, 22, 3)
Found uncertainty sample 0 after 2 steps.
Found uncertainty sample 1 after 2 steps.
Found uncertainty sample 2 after 2 steps.
Found uncertainty sample 3 after 1 steps.
Found uncertainty sample 4 after 1 steps.
Found uncertainty sample 5 after 2 steps.
Found uncertainty sample 6 after 2 steps.
Found uncertainty sample 7 after 2 steps.
Found uncertainty sample 8 after 1 steps.
Found uncertainty sample 9 after 2 steps.
Found uncertainty sample 10 after 2 steps.
Found uncertainty sample 11 after 3 steps.
Found uncertainty sample 12 after 3 steps.
Found uncertainty sample 13 after 5 steps.
Distance between 16 and 17 is 1.557405722905161 which is too large.
Atomic distances for sample 14 are too large or too small. Resetting this trajectory.
Found uncertainty sample 15 after 4 steps.
Found uncertainty sample 16 after 5 steps.
Found uncertainty sample 17 after 1 steps.
Found uncertainty sample 18 after 6 steps.
Found uncertainty sample 19 after 6 steps.
Found uncertainty sample 20 after 2 steps.
Distance between 16 and 17 is 1.5926890200607782 which is too large.
Atomic distances for sample 21 are too large or too small. Resetting this trajectory.
Distance between 16 and 17 is 1.5854458287845838 which is too large.
Atomic distances for sample 22 are too large or too small. Resetting this trajectory.
Found uncertainty sample 23 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 24 after 1 steps.
Found uncertainty sample 25 after 26 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 26 after 1 steps.
Found uncertainty sample 27 after 20 steps.
Found uncertainty sample 28 after 1 steps.
Found uncertainty sample 29 after 34 steps.
Found uncertainty sample 30 after 37 steps.
Found uncertainty sample 31 after 1 steps.
Found uncertainty sample 32 after 17 steps.
Found uncertainty sample 33 after 1 steps.
Found uncertainty sample 34 after 1 steps.
Found uncertainty sample 35 after 21 steps.
Found uncertainty sample 36 after 38 steps.
Found uncertainty sample 37 after 23 steps.
Found uncertainty sample 38 after 1 steps.
Found uncertainty sample 39 after 29 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 40 after 1 steps.
Found uncertainty sample 41 after 11 steps.
Found uncertainty sample 42 after 8 steps.
Found uncertainty sample 43 after 17 steps.
Found uncertainty sample 44 after 20 steps.
Found uncertainty sample 45 after 27 steps.
Found uncertainty sample 46 after 27 steps.
Found uncertainty sample 47 after 34 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 48 after 1 steps.
Found uncertainty sample 49 after 8 steps.
Found uncertainty sample 50 after 24 steps.
Found uncertainty sample 51 after 97 steps.
Found uncertainty sample 52 after 125 steps.
Found uncertainty sample 53 after 39 steps.
Found uncertainty sample 54 after 16 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 55 after 1 steps.
Found uncertainty sample 56 after 30 steps.
Found uncertainty sample 57 after 1 steps.
Found uncertainty sample 58 after 9 steps.
Found uncertainty sample 59 after 22 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 60 after 1 steps.
Found uncertainty sample 61 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 62 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 63 after 1 steps.
Found uncertainty sample 64 after 32 steps.
Found uncertainty sample 65 after 17 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 66 after 1 steps.
Found uncertainty sample 67 after 55 steps.
Found uncertainty sample 68 after 42 steps.
Found uncertainty sample 69 after 14 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 70 after 1 steps.
Found uncertainty sample 71 after 73 steps.
Found uncertainty sample 72 after 34 steps.
Found uncertainty sample 73 after 43 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 74 after 1 steps.
Found uncertainty sample 75 after 1 steps.
Found uncertainty sample 76 after 1 steps.
Distance between 16 and 17 is 1.5721098240013605 which is too large.
Atomic distances for sample 77 are too large or too small. Resetting this trajectory.
Found uncertainty sample 78 after 17 steps.
Found uncertainty sample 79 after 25 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 80 after 1 steps.
Found uncertainty sample 81 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 82 after 1 steps.
Found uncertainty sample 83 after 38 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 84 after 1 steps.
Found uncertainty sample 85 after 24 steps.
Found uncertainty sample 86 after 24 steps.
Found uncertainty sample 87 after 19 steps.
Found uncertainty sample 88 after 15 steps.
Found uncertainty sample 89 after 37 steps.
Found uncertainty sample 90 after 26 steps.
Found uncertainty sample 91 after 43 steps.
Found uncertainty sample 92 after 29 steps.
Found uncertainty sample 93 after 32 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 94 after 1 steps.
Found uncertainty sample 95 after 16 steps.
Found uncertainty sample 96 after 15 steps.
Found uncertainty sample 97 after 23 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 98 after 1 steps.
Found 2 uncertainty samples at step 1. Dropping index -2
Found uncertainty sample 99 after 1 steps.
Traceback (most recent call last):
  File "/home/ws/fq0795/git/gnn_uncertainty/active_learning.py", line 941, in <module>
    al.improve_model(
  File "/home/ws/fq0795/git/gnn_uncertainty/active_learning.py", line 531, in improve_model
    self._log_md_steps(i, steps, f"al/run{run_idx}/eval/md_steps.csv")
  File "/home/ws/fq0795/git/gnn_uncertainty/active_learning.py", line 677, in _log_md_steps
    writer.writerow([iteration, sample, steps[sample]])
                                        ~~~~~^^^^^^^^
KeyError: 14
