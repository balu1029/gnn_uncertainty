wandb: Currently logged in as: l-baer-99 (l-baer-99-Karlsruhe Institute of Technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.18.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /home/ws/fq0795/git/gnn_uncertainty/wandb/run-20241129_144359-dq27avfi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run al_61
wandb: ‚≠êÔ∏è View project at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: üöÄ View run at https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/dq27avfi
['H1', 'CH3', 'H2', 'H3', 'C', 'O', 'N', 'H', 'CA', 'HA', 'CB', 'HB1', 'HB2', 'HB3', 'C', 'O', 'N', 'H', 'C', 'H1', 'H2', 'H3']
60
0.0 1.0
Uncertainty Slope: -1.1877511739730835, Uncertainty Bias: 5.417869567871094
5.4178696 -1.1877512
0.0030793697 0.11009693
61.96855 66.46502
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
5.4178696 -1.1877512
Traceback (most recent call last):
  File "/home/ws/fq0795/git/gnn_uncertainty/active_learning.py", line 941, in <module>
    al.improve_model(
  File "/home/ws/fq0795/git/gnn_uncertainty/active_learning.py", line 458, in improve_model
    self.model.valid_epoch(
  File "/home/ws/fq0795/git/gnn_uncertainty/uncertainty/evidential.py", line 329, in valid_epoch
    evidential_loss_energy = criterion((energy, v, alpha, beta), label_energy)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/loss.py", line 101, in forward
    return F.l1_loss(input, target, reduction=self.reduction)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ws/fq0795/miniconda3/envs/torch/lib/python3.12/site-packages/torch/nn/functional.py", line 3325, in l1_loss
    if not (target.size() == input.size()):
                             ^^^^^^^^^^
AttributeError: 'tuple' object has no attribute 'size'
wandb: - 0.039 MB of 0.039 MB uploadedwandb: \ 0.039 MB of 0.039 MB uploadedwandb: | 0.039 MB of 0.045 MB uploadedwandb: / 0.055 MB of 0.055 MB uploadedwandb: üöÄ View run al_61 at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning/runs/dq27avfi
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/l-baer-99-Karlsruhe%20Institute%20of%20Technology/ActiveLearning
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20241129_144359-dq27avfi/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
